{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Let there be light! A R documentation website. ugo_r_doc is a corpus; a catalog of books, manuals, articles, presentations, videos, podcasts, summaries, notes, code snippets, excerpts, websites, etc. The \u2018docs\u2019 is a searchable knowledge-based system. You type a keyword, it leads to several sources, you identify the document, and you go retrieve the document; whether it is a digital or a material document. Fast and easy! The corpus is unstructured. Knowledge is rather cumulated, layer after layer, resulting in a hotchpotch of information. Information may be repeted among many documents, with different explanations, some more comprehensive. Newer entries might also supplement or contradict older entries.","title":"Home"},{"location":"a_hands-on_introduction_to_statistics_with_r_course_five_correlation_and_regression/","text":"Foreword Output options: the \u2018tango\u2019 syntax and the \u2018readable\u2019 theme. Snippets and results. An introduction to Correlation \u00b6 Manual computation of correlation coefficients - Part 1 1 2 # Print the data set in the console str ( PE ) 1 2 3 4 5 ## 'data.frame': 200 obs. of 4 variables: ## $ pid : num 1 2 3 4 5 6 7 8 9 10 ... ## $ age : num 60 40 29 47 48 42 55 43 39 51 ... ## $ activeyears: num 10 9 2 10 9 6 8 19 9 14 ... ## $ endurance : num 18 36 51 18 23 30 8 40 28 15 ... 1 2 3 4 5 6 7 8 9 10 11 # Take a quick peek at both vectors A <- PE $ activeyears B <- PE $ endurance # Save the differences of each vector element with the mean in a new variable diff_A <- A - mean ( A ) diff_B <- B - mean ( B ) # Do the summation of the elements of the vectors and divide by N-1 in order to acquire the covariance between the two vectors cov <- sum ( diff_A * diff_B ) / ( length ( A ) - 1 ) cov 1 ## [1] 16.59045 Manual computation of correlation coefficients - Part 2 1 2 3 4 5 6 7 8 # Square the differences that were found in the previous step sq_diff_A <- diff_A^2 sq_diff_B <- diff_B^2 # Take the sum of the elements, divide them by N-1 and consequently take the square root to acquire the sample standard deviations sd_A <- sqrt ( sum ( sq_diff_A ) / ( length ( A ) - 1 )) sd_B <- sqrt ( sum ( sq_diff_B ) / ( length ( B ) - 1 )) sd_A 1 ## [1] 4.687134 1 sd_B 1 ## [1] 10.83963 Manual computation of correlation coefficients - part 3 1 2 3 # Combine all the pieces of the puzzle correlation <- cov / ( sd_A * sd_B ) correlation 1 ## [1] 0.3265402 1 2 # Check the validity of your result with the cor() command cor ( A , B ) 1 ## [1] 0.3265402 Creating scatterplots 1 2 3 4 library ( psych ) # Summary statistics describe ( PE ) 1 2 3 4 5 6 7 8 9 10 ## vars n mean sd median trimmed mad min max range skew ## pid 1 200 101.81 58.85 101.5 101.71 74.87 1 204 203 0.01 ## age 2 200 49.41 10.48 48.0 49.46 10.38 20 82 62 0.06 ## activeyears 3 200 10.68 4.69 11.0 10.57 4.45 0 26 26 0.30 ## endurance 4 200 26.50 10.84 27.0 26.22 10.38 3 55 52 0.22 ## kurtosis se ## pid -1.21 4.16 ## age -0.14 0.74 ## activeyears 0.46 0.33 ## endurance -0.44 0.77 1 2 3 4 5 6 # Scatter plots par ( mfrow = c ( 1 , 3 )) plot ( PE $ age ~ PE $ activeyears ) plot ( PE $ endurance ~ PE $ activeyears ) plot ( PE $ endurance ~ PE $ age ) 1 par ( mfrow = c ( 1 , 1 )) Correlation matrix 1 2 # Correlation Analysis round ( cor ( PE[2 : 4 ] , use = 'pairwise.complete.obs' , method = 'pearson' ), 2 ) 1 2 3 4 ## age activeyears endurance ## age 1.00 0.33 -0.08 ## activeyears 0.33 1.00 0.33 ## endurance -0.08 0.33 1.00 1 2 3 # Do some correlation tests. If the null hypothesis of no correlation can be rejected on a significance level of 5%, then the relationship between variables is significantly different from zero at the 95% confidence level cor.test ( PE $ age , PE $ activeyears ) 1 2 3 4 5 6 7 8 9 10 11 ## ## Pearson's product-moment correlation ## ## data: PE$age and PE$activeyears ## t = 4.9022, df = 198, p-value = 1.969e-06 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## 0.1993491 0.4473145 ## sample estimates: ## cor ## 0.3289909 1 cor.test ( PE $ age , PE $ endurance ) 1 2 3 4 5 6 7 8 9 10 11 ## ## Pearson's product-moment correlation ## ## data: PE$age and PE$endurance ## t = -1.1981, df = 198, p-value = 0.2323 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## -0.22097811 0.05454491 ## sample estimates: ## cor ## -0.08483813 1 cor.test ( PE $ endurance , PE $ activeyears ) 1 2 3 4 5 6 7 8 9 10 11 ## ## Pearson's product-moment correlation ## ## data: PE$endurance and PE$activeyears ## t = 4.8613, df = 198, p-value = 2.37e-06 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## 0.1967110 0.4451154 ## sample estimates: ## cor ## 0.3265402 CAUTION: The magnitude of a correlation depends upon many factors, including sampling. The magnitude of a correlation is also influenced by measurement of X & Y. The correlation coefficient is a sample statistic, just like the mean. An introduction to Linear Regression Models \u00b6 Impact experiment 1 2 3 4 5 6 7 # Create a correlation matrix for the dataset (9-14 are the '2' variables only) correlations <- cor ( PE[9 : 14 , ] ) # Create the scatterplot matrix for the dataset library ( corrplot ) corrplot ( correlations ) Manual computation of a simple linear regression 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 # Calculate the required means, standard deviations and correlation coefficient mean_ay <- mean ( PE $ activeyears ) mean_e <- mean ( PE $ endurance ) sd_ay <- sd ( PE $ activeyears ) sd_e <- sd ( PE $ endurance ) r <- cor ( PE $ activeyears , PE $ endurance ) # Calculate the slope B_1 <- r * ( sd_e ) / ( sd_ay ) # Calculate the intercept B_0 <- mean_e - B_1 * mean_ay # Plot of ic2 against sym2 plot ( PE $ activeyear , PE $ endurance , main = 'Scatterplot' , ylab = 'Endurance' , xlab = 'Active Years' ) # Add the regression line abline ( B_0 , B_1 , col = 'red' ) Executing a simple linear regression using R 1 2 3 4 5 # Construct the regression model model_1 <- lm ( PE $ endurance ~ PE $ activeyear ) # Look at the results of the regression by using the summary function summary ( model_1 ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 ## ## Call: ## lm(formula = PE$endurance ~ PE$activeyear) ## ## Residuals: ## Min 1Q Median 3Q Max ## -20.5006 -7.8066 0.5304 5.7649 31.0511 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 18.4386 1.8104 10.185 &lt; 2e-16 *** ## PE$activeyear 0.7552 0.1553 4.861 2.37e-06 *** ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## Residual standard error: 10.27 on 198 degrees of freedom ## Multiple R-squared: 0.1066, Adjusted R-squared: 0.1021 ## F-statistic: 23.63 on 1 and 198 DF, p-value: 2.37e-06 1 2 3 4 5 6 7 8 9 # Extract the predicted values predicted <- fitted ( model_1 ) # Create a scatter plot of Impulse Control against Symptom Score plot ( PE $ endurance ~ PE $ activeyear , main = 'Scatterplot' , ylab = 'Endurance' , xlab = 'Active Years' ) # Add a regression line abline ( model_1 , col = 'red' ) abline ( lm ( predicted ~ PE $ activeyears ), col = 'green' , lty = 2 ) Executing a multiple regression in R 1 2 3 4 5 # Multiple Regression model_2 <- lm ( PE $ endurance ~ PE $ activeyear + PE $ age ) # Examine the results of the regression summary ( model_2 ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 ## ## Call: ## lm(formula = PE$endurance ~ PE$activeyear + PE$age) ## ## Residuals: ## Min 1Q Median 3Q Max ## -21.4598 -7.7398 0.6984 5.5364 27.9230 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 27.7035 3.4779 7.966 1.29e-13 *** ## PE$activeyear 0.9192 0.1610 5.708 4.16e-08 *** ## PE$age -0.2229 0.0720 -3.096 0.00225 ** ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## Residual standard error: 10.06 on 197 degrees of freedom ## Multiple R-squared: 0.1481, Adjusted R-squared: 0.1394 ## F-statistic: 17.12 on 2 and 197 DF, p-value: 1.394e-07 1 2 3 4 5 6 # Extract the predicted values predicted <- fitted ( model_2 ) # Plotting predicted scores against observed scores plot ( predicted ~ PE $ activeyears , main = 'Scatterplot' , ylab = 'Endurance' , xlab = 'Active Years' ) abline ( lm ( predicted ~ PE $ activeyears ), col = 'green' ) Linear Regression Models continued \u00b6 Calculating the sum of squared residuals 1 2 3 4 5 6 7 8 9 10 11 12 # Create a linear regression with `ic2` and `vismem2` as regressors model_1 <- lm ( PE $ endurance ~ PE $ activeyear + PE $ age ) # Extract the predicted values predicted_1 <- fitted ( model_1 ) # Calculate the squared deviation of the predicted values from the observed values deviation_1 <- ( predicted_1 - PE $ endurance ) ^2 # Sum the squared deviations SSR_1 <- sum ( deviation_1 ) SSR_1 1 ## [1] 19919.55 Standardized linear regression 1 2 3 4 5 # Create a standardized simple linear regression model_1_z <- lm ( scale ( PE $ endurance ) ~ scale ( PE $ activeyear )) # Look at the output of this regression model summary ( model_1_z ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 ## ## Call: ## lm(formula = scale(PE$endurance) ~ scale(PE$activeyear)) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.89126 -0.72019 0.04893 0.53184 2.86459 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 4.871e-17 6.700e-02 0.000 1 ## scale(PE$activeyear) 3.265e-01 6.717e-02 4.861 2.37e-06 *** ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## Residual standard error: 0.9476 on 198 degrees of freedom ## Multiple R-squared: 0.1066, Adjusted R-squared: 0.1021 ## F-statistic: 23.63 on 1 and 198 DF, p-value: 2.37e-06 1 2 3 # Extract the R-Squared value for this regression r_square_1 <- summary ( model_1_z ) $ r.squared r_square_1 1 ## [1] 0.1066285 1 2 3 # Calculate the correlation coefficient corr_coef_1 <- sqrt ( r_square_1 ) corr_coef_1 1 ## [1] 0.3265402 1 2 3 4 5 # Create a standardized multiple linear regression model_2_z <- lm ( scale ( PE $ endurance ) ~ scale ( PE $ activeyear ) + scale ( PE $ age )) # Look at the output of this regression model summary ( model_2_z ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 ## ## Call: ## lm(formula = scale(PE$endurance) ~ scale(PE$activeyear) + scale(PE$age)) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.97975 -0.71403 0.06443 0.51076 2.57601 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 5.590e-17 6.560e-02 0.000 1.00000 ## scale(PE$activeyear) 3.975e-01 6.964e-02 5.708 4.16e-08 *** ## scale(PE$age) -2.156e-01 6.964e-02 -3.096 0.00225 ** ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## Residual standard error: 0.9277 on 197 degrees of freedom ## Multiple R-squared: 0.1481, Adjusted R-squared: 0.1394 ## F-statistic: 17.12 on 2 and 197 DF, p-value: 1.394e-07 1 2 3 # Extract the R-Squared value for this regression r_square_2 <- summary ( model_2_z ) $ r.squared r_square_2 1 ## [1] 0.1480817 1 2 3 # Calculate the correlation coefficient corr_coef_2 <- sqrt ( r_square_2 ) corr_coef_2 1 ## [1] 0.3848139 Assumptions of linear regression: Normal distribution for Y. Linear relationship between X and Y. Homoscedasticity. Reliability of X and Y. Validity of X and Y. Random and representative sampling. Check it out with Anscombe\u2019s quartet plots. Plotting residuals 1 2 3 4 5 # Extract the residuals from the model residual <- resid ( model_2 ) # Draw a histogram of the residuals hist ( residual ) 1 2 3 4 5 6 # Extract the predicted symptom scores from the model predicted <- fitted ( model_2 ) # Plot the residuals against the predicted symptom scores plot ( residual ~ predicted , main = 'Scatterplot' , ylab = 'Model 2 Residuals' , xlab = 'Model 2 Predicted Scores' ) abline ( lm ( residual ~ predicted ), col = 'red' )","title":"Statistics with R, Course Five, Correlation and Regression"},{"location":"a_hands-on_introduction_to_statistics_with_r_course_five_correlation_and_regression/#an-introduction-to-correlation","text":"Manual computation of correlation coefficients - Part 1 1 2 # Print the data set in the console str ( PE ) 1 2 3 4 5 ## 'data.frame': 200 obs. of 4 variables: ## $ pid : num 1 2 3 4 5 6 7 8 9 10 ... ## $ age : num 60 40 29 47 48 42 55 43 39 51 ... ## $ activeyears: num 10 9 2 10 9 6 8 19 9 14 ... ## $ endurance : num 18 36 51 18 23 30 8 40 28 15 ... 1 2 3 4 5 6 7 8 9 10 11 # Take a quick peek at both vectors A <- PE $ activeyears B <- PE $ endurance # Save the differences of each vector element with the mean in a new variable diff_A <- A - mean ( A ) diff_B <- B - mean ( B ) # Do the summation of the elements of the vectors and divide by N-1 in order to acquire the covariance between the two vectors cov <- sum ( diff_A * diff_B ) / ( length ( A ) - 1 ) cov 1 ## [1] 16.59045 Manual computation of correlation coefficients - Part 2 1 2 3 4 5 6 7 8 # Square the differences that were found in the previous step sq_diff_A <- diff_A^2 sq_diff_B <- diff_B^2 # Take the sum of the elements, divide them by N-1 and consequently take the square root to acquire the sample standard deviations sd_A <- sqrt ( sum ( sq_diff_A ) / ( length ( A ) - 1 )) sd_B <- sqrt ( sum ( sq_diff_B ) / ( length ( B ) - 1 )) sd_A 1 ## [1] 4.687134 1 sd_B 1 ## [1] 10.83963 Manual computation of correlation coefficients - part 3 1 2 3 # Combine all the pieces of the puzzle correlation <- cov / ( sd_A * sd_B ) correlation 1 ## [1] 0.3265402 1 2 # Check the validity of your result with the cor() command cor ( A , B ) 1 ## [1] 0.3265402 Creating scatterplots 1 2 3 4 library ( psych ) # Summary statistics describe ( PE ) 1 2 3 4 5 6 7 8 9 10 ## vars n mean sd median trimmed mad min max range skew ## pid 1 200 101.81 58.85 101.5 101.71 74.87 1 204 203 0.01 ## age 2 200 49.41 10.48 48.0 49.46 10.38 20 82 62 0.06 ## activeyears 3 200 10.68 4.69 11.0 10.57 4.45 0 26 26 0.30 ## endurance 4 200 26.50 10.84 27.0 26.22 10.38 3 55 52 0.22 ## kurtosis se ## pid -1.21 4.16 ## age -0.14 0.74 ## activeyears 0.46 0.33 ## endurance -0.44 0.77 1 2 3 4 5 6 # Scatter plots par ( mfrow = c ( 1 , 3 )) plot ( PE $ age ~ PE $ activeyears ) plot ( PE $ endurance ~ PE $ activeyears ) plot ( PE $ endurance ~ PE $ age ) 1 par ( mfrow = c ( 1 , 1 )) Correlation matrix 1 2 # Correlation Analysis round ( cor ( PE[2 : 4 ] , use = 'pairwise.complete.obs' , method = 'pearson' ), 2 ) 1 2 3 4 ## age activeyears endurance ## age 1.00 0.33 -0.08 ## activeyears 0.33 1.00 0.33 ## endurance -0.08 0.33 1.00 1 2 3 # Do some correlation tests. If the null hypothesis of no correlation can be rejected on a significance level of 5%, then the relationship between variables is significantly different from zero at the 95% confidence level cor.test ( PE $ age , PE $ activeyears ) 1 2 3 4 5 6 7 8 9 10 11 ## ## Pearson's product-moment correlation ## ## data: PE$age and PE$activeyears ## t = 4.9022, df = 198, p-value = 1.969e-06 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## 0.1993491 0.4473145 ## sample estimates: ## cor ## 0.3289909 1 cor.test ( PE $ age , PE $ endurance ) 1 2 3 4 5 6 7 8 9 10 11 ## ## Pearson's product-moment correlation ## ## data: PE$age and PE$endurance ## t = -1.1981, df = 198, p-value = 0.2323 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## -0.22097811 0.05454491 ## sample estimates: ## cor ## -0.08483813 1 cor.test ( PE $ endurance , PE $ activeyears ) 1 2 3 4 5 6 7 8 9 10 11 ## ## Pearson's product-moment correlation ## ## data: PE$endurance and PE$activeyears ## t = 4.8613, df = 198, p-value = 2.37e-06 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## 0.1967110 0.4451154 ## sample estimates: ## cor ## 0.3265402 CAUTION: The magnitude of a correlation depends upon many factors, including sampling. The magnitude of a correlation is also influenced by measurement of X & Y. The correlation coefficient is a sample statistic, just like the mean.","title":"An introduction to Correlation"},{"location":"a_hands-on_introduction_to_statistics_with_r_course_five_correlation_and_regression/#an-introduction-to-linear-regression-models","text":"Impact experiment 1 2 3 4 5 6 7 # Create a correlation matrix for the dataset (9-14 are the '2' variables only) correlations <- cor ( PE[9 : 14 , ] ) # Create the scatterplot matrix for the dataset library ( corrplot ) corrplot ( correlations ) Manual computation of a simple linear regression 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 # Calculate the required means, standard deviations and correlation coefficient mean_ay <- mean ( PE $ activeyears ) mean_e <- mean ( PE $ endurance ) sd_ay <- sd ( PE $ activeyears ) sd_e <- sd ( PE $ endurance ) r <- cor ( PE $ activeyears , PE $ endurance ) # Calculate the slope B_1 <- r * ( sd_e ) / ( sd_ay ) # Calculate the intercept B_0 <- mean_e - B_1 * mean_ay # Plot of ic2 against sym2 plot ( PE $ activeyear , PE $ endurance , main = 'Scatterplot' , ylab = 'Endurance' , xlab = 'Active Years' ) # Add the regression line abline ( B_0 , B_1 , col = 'red' ) Executing a simple linear regression using R 1 2 3 4 5 # Construct the regression model model_1 <- lm ( PE $ endurance ~ PE $ activeyear ) # Look at the results of the regression by using the summary function summary ( model_1 ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 ## ## Call: ## lm(formula = PE$endurance ~ PE$activeyear) ## ## Residuals: ## Min 1Q Median 3Q Max ## -20.5006 -7.8066 0.5304 5.7649 31.0511 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 18.4386 1.8104 10.185 &lt; 2e-16 *** ## PE$activeyear 0.7552 0.1553 4.861 2.37e-06 *** ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## Residual standard error: 10.27 on 198 degrees of freedom ## Multiple R-squared: 0.1066, Adjusted R-squared: 0.1021 ## F-statistic: 23.63 on 1 and 198 DF, p-value: 2.37e-06 1 2 3 4 5 6 7 8 9 # Extract the predicted values predicted <- fitted ( model_1 ) # Create a scatter plot of Impulse Control against Symptom Score plot ( PE $ endurance ~ PE $ activeyear , main = 'Scatterplot' , ylab = 'Endurance' , xlab = 'Active Years' ) # Add a regression line abline ( model_1 , col = 'red' ) abline ( lm ( predicted ~ PE $ activeyears ), col = 'green' , lty = 2 ) Executing a multiple regression in R 1 2 3 4 5 # Multiple Regression model_2 <- lm ( PE $ endurance ~ PE $ activeyear + PE $ age ) # Examine the results of the regression summary ( model_2 ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 ## ## Call: ## lm(formula = PE$endurance ~ PE$activeyear + PE$age) ## ## Residuals: ## Min 1Q Median 3Q Max ## -21.4598 -7.7398 0.6984 5.5364 27.9230 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 27.7035 3.4779 7.966 1.29e-13 *** ## PE$activeyear 0.9192 0.1610 5.708 4.16e-08 *** ## PE$age -0.2229 0.0720 -3.096 0.00225 ** ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## Residual standard error: 10.06 on 197 degrees of freedom ## Multiple R-squared: 0.1481, Adjusted R-squared: 0.1394 ## F-statistic: 17.12 on 2 and 197 DF, p-value: 1.394e-07 1 2 3 4 5 6 # Extract the predicted values predicted <- fitted ( model_2 ) # Plotting predicted scores against observed scores plot ( predicted ~ PE $ activeyears , main = 'Scatterplot' , ylab = 'Endurance' , xlab = 'Active Years' ) abline ( lm ( predicted ~ PE $ activeyears ), col = 'green' )","title":"An introduction to Linear Regression Models"},{"location":"a_hands-on_introduction_to_statistics_with_r_course_five_correlation_and_regression/#linear-regression-models-continued","text":"Calculating the sum of squared residuals 1 2 3 4 5 6 7 8 9 10 11 12 # Create a linear regression with `ic2` and `vismem2` as regressors model_1 <- lm ( PE $ endurance ~ PE $ activeyear + PE $ age ) # Extract the predicted values predicted_1 <- fitted ( model_1 ) # Calculate the squared deviation of the predicted values from the observed values deviation_1 <- ( predicted_1 - PE $ endurance ) ^2 # Sum the squared deviations SSR_1 <- sum ( deviation_1 ) SSR_1 1 ## [1] 19919.55 Standardized linear regression 1 2 3 4 5 # Create a standardized simple linear regression model_1_z <- lm ( scale ( PE $ endurance ) ~ scale ( PE $ activeyear )) # Look at the output of this regression model summary ( model_1_z ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 ## ## Call: ## lm(formula = scale(PE$endurance) ~ scale(PE$activeyear)) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.89126 -0.72019 0.04893 0.53184 2.86459 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 4.871e-17 6.700e-02 0.000 1 ## scale(PE$activeyear) 3.265e-01 6.717e-02 4.861 2.37e-06 *** ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## Residual standard error: 0.9476 on 198 degrees of freedom ## Multiple R-squared: 0.1066, Adjusted R-squared: 0.1021 ## F-statistic: 23.63 on 1 and 198 DF, p-value: 2.37e-06 1 2 3 # Extract the R-Squared value for this regression r_square_1 <- summary ( model_1_z ) $ r.squared r_square_1 1 ## [1] 0.1066285 1 2 3 # Calculate the correlation coefficient corr_coef_1 <- sqrt ( r_square_1 ) corr_coef_1 1 ## [1] 0.3265402 1 2 3 4 5 # Create a standardized multiple linear regression model_2_z <- lm ( scale ( PE $ endurance ) ~ scale ( PE $ activeyear ) + scale ( PE $ age )) # Look at the output of this regression model summary ( model_2_z ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 ## ## Call: ## lm(formula = scale(PE$endurance) ~ scale(PE$activeyear) + scale(PE$age)) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.97975 -0.71403 0.06443 0.51076 2.57601 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 5.590e-17 6.560e-02 0.000 1.00000 ## scale(PE$activeyear) 3.975e-01 6.964e-02 5.708 4.16e-08 *** ## scale(PE$age) -2.156e-01 6.964e-02 -3.096 0.00225 ** ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## Residual standard error: 0.9277 on 197 degrees of freedom ## Multiple R-squared: 0.1481, Adjusted R-squared: 0.1394 ## F-statistic: 17.12 on 2 and 197 DF, p-value: 1.394e-07 1 2 3 # Extract the R-Squared value for this regression r_square_2 <- summary ( model_2_z ) $ r.squared r_square_2 1 ## [1] 0.1480817 1 2 3 # Calculate the correlation coefficient corr_coef_2 <- sqrt ( r_square_2 ) corr_coef_2 1 ## [1] 0.3848139 Assumptions of linear regression: Normal distribution for Y. Linear relationship between X and Y. Homoscedasticity. Reliability of X and Y. Validity of X and Y. Random and representative sampling. Check it out with Anscombe\u2019s quartet plots. Plotting residuals 1 2 3 4 5 # Extract the residuals from the model residual <- resid ( model_2 ) # Draw a histogram of the residuals hist ( residual ) 1 2 3 4 5 6 # Extract the predicted symptom scores from the model predicted <- fitted ( model_2 ) # Plot the residuals against the predicted symptom scores plot ( residual ~ predicted , main = 'Scatterplot' , ylab = 'Model 2 Residuals' , xlab = 'Model 2 Predicted Scores' ) abline ( lm ( residual ~ predicted ), col = 'red' )","title":"Linear Regression Models continued"},{"location":"a_hands-on_introduction_to_statistics_with_r_course_four_repeated_measures_anova/","text":"Foreword Output options: the \u2018tango\u2019 syntax and the \u2018readable\u2019 theme. Snippets and results. An introduction to repeated measures \u00b6 The independent t-test is analogous to between-groups ANOVA and the paired-sample t-test is analogous to repeated measures ANOVA. In a between-groups design, each subject is exposed to two or more treatments or conditions over time. In a within-subjects design, each subject is allocated to exactly one treatment or condition. Between: Experiment 1: You want to test the effect of alcohol on test scores of students. There are three conditions: the student consumed no alcohol, two glasses of beer, or five glasses of beer. Alcohol tolerance and time spent studying should also be considered somehow. Experiment 2: You want to investigate the effects of certain fertilizers on plant growth. Assume you have two different fertilizers, A and B. Consider three conditions: you give the plant no fertilizer, fertilizer A, or fertilizer B. You measure the height of the plant after a specific period of time to see whether the fertilizers had an effect. Use a within-subjects design for for Experiment 1 and between-groups design for Experiment 2. Is it always either manipulation between-groups or manipulation within-groups, or are there experiments where you could use either approach? In some cases, either approach is possible. Explore the working memory data 1 2 # Print the data set in the console str ( wm ) 1 2 3 4 ## 'data.frame': 80 obs. of 3 variables: ## $ subject : num 1 2 3 4 5 6 7 8 9 10 ... ## $ condition: Factor w/ 4 levels \"12 days\",\"17 days\",..: 4 4 4 4 4 4 4 4 4 4 ... ## $ iq : num 12.4 11.8 14.6 7.7 15.7 11.6 7 8.4 10.7 10.6 ... 1 2 3 4 5 6 7 8 library ( psych ) library ( ggplot2 ) # Define the variable subject as a categorical variable wm $ subject <- factor ( wm $ subject ) # Summary statistics by all groups (8 sessions, 12 sessions, 17 sessions, 19 sessions) describeBy ( wm , wm $ condition ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 ## ## Descriptive statistics by group ## group: 12 days ## vars n mean sd median trimmed mad min max range skew ## subject* 1 20 10.5 5.92 10.50 10.50 7.41 1.0 20.0 19.0 0.00 ## condition* 2 20 1.0 0.00 1.00 1.00 0.00 1.0 1.0 0.0 NaN ## iq 3 20 11.7 2.58 11.65 11.69 2.89 6.9 16.1 9.2 0.05 ## kurtosis se ## subject* -1.38 1.32 ## condition* NaN 0.00 ## iq -1.06 0.58 ## -------------------------------------------------------- ## group: 17 days ## vars n mean sd median trimmed mad min max range skew ## subject* 1 20 10.5 5.92 10.5 10.5 7.41 1.0 20.0 19.0 0.00 ## condition* 2 20 2.0 0.00 2.0 2.0 0.00 2.0 2.0 0.0 NaN ## iq 3 20 13.9 2.26 13.6 13.9 2.00 9.8 18.1 8.3 0.11 ## kurtosis se ## subject* -1.38 1.32 ## condition* NaN 0.00 ## iq -0.85 0.50 ## -------------------------------------------------------- ## group: 19 days ## vars n mean sd median trimmed mad min max range skew ## subject* 1 20 10.50 5.92 10.5 10.50 7.41 1.0 20.0 19.0 0.00 ## condition* 2 20 3.00 0.00 3.0 3.00 0.00 3.0 3.0 0.0 NaN ## iq 3 20 14.75 2.50 15.3 14.71 2.15 10.4 19.2 8.8 -0.09 ## kurtosis se ## subject* -1.38 1.32 ## condition* NaN 0.00 ## iq -0.99 0.56 ## -------------------------------------------------------- ## group: 8 days ## vars n mean sd median trimmed mad min max range skew ## subject* 1 20 10.50 5.92 10.5 10.50 7.41 1.0 20.0 19.0 0.00 ## condition* 2 20 4.00 0.00 4.0 4.00 0.00 4.0 4.0 0.0 NaN ## iq 3 20 10.91 2.63 11.3 10.97 2.67 5.4 15.7 10.3 -0.21 ## kurtosis se ## subject* -1.38 1.32 ## condition* NaN 0.00 ## iq -0.70 0.59 1 2 # Boxplot IQ versus condition boxplot ( wm $ iq ~ wm $ condition , main = 'Boxplot' , xlab = 'Training sessions' , ylab = 'IQ' ) 1 2 # Illustration data, each line represents the development of each subject by number of trainings ggplot ( data = wm , aes ( x = wm $ condition , y = wm $ iq , group = wm $ subject , colour = wm $ subject )) + geom_line () + geom_point () Reduced cost The cost advantage of using manipulation within groups verses manipulation between groups for the working memory experiment is you need 60 subjects fewer. Statistically more powerful Repeated measures analysis accounts for individual differences across the experiment. This reduces the error term, which increases statistical power. Counterbalancing Suppose you have three levels of an independent variable A (i.e. A1, A2, A3) and a blocked design. You want to use full counterbalancing to take into account order effects. What are all the possible orders that you need to use? In other words, what are the order conditions? (A1, A2, A3), (A1, A3, A2) , (A2, A1, A3), (A2, A3, A1) , (A3, A2, A1), (A3, A1, A2) Number of order conditions? Assume the number of levels of the independent variable goes up and you want to completely counterbalance. What will happen to the number of order conditions you\u2019ll need? The number becomes really large. An independent variable with n levels will have n!=n*(n???1)*(n???2)*... order conditions. Latin Squares design As you hopefully realized in the previous exercise, completely counterbalancing is not always a practical solution for taking into account order effects. This is because the number of different orders required gets really large as the number of possible conditions increases. The most common workaround to this problem is the Latin Squares design, in which you do not completely counterbalance, but instead put each condition at every position (at least) once. Which of the following examples has been constructed according to the Latin Squares design? (A1, A2, A3), (A2, A3, A1), (A3, A1, A2) More on Latin Squares The number of order conditions is always equal to the number of levels of your independent variable. Why is missing data a problem? In a between-groups design, it is okay to have a slightly different number of subjects in each group, so if one subject drops out in one of the conditions then that group has just one less subject. Now you want to look at how subjects change over time and the different scores between two or more conditions for each subject. Understanding sphericity The variances of the differences between all possible pairs of groups (i.e. levels of the independent variable) are equal. Mauchly\u2019s test 1 2 3 4 5 6 7 8 # Define iq as a data frame where each column represents a condition iq <- cbind ( wm $ iq[wm $ condition == '8 days' ] , wm $ iq[wm $ condition == '12 days' ] , wm $ iq[wm $ condition == '17 days' ] , wm $ iq[wm $ condition == '19 days' ] ) # Make an mlm object mlm <- lm ( iq ~ 1 ) # Mauchly's test mauchly.test ( mlm , x = ~ 1 ) 1 2 3 4 5 ## ## Mauchly's test of sphericity ## ## data: SSD matrix from lm(formula = iq ~ 1) ## W = 0.81725, p-value = 0.9407 Based on the results, the sphericity assumption holds. Pros of repeated measures Less cost and statistically more powerful Cons of repeated measures Order effects, counterbalancing, missing data, and an extra assumption Repeated measures ANOVA \u00b6 The systematic between groups variance To understand everything a bit better, we will calculate the F-ratio for a repeated measures design by ourself in the next exercises. First, we will need the systematic between-groups variance. This is the same as in the between-groups design\u2013the variance due to grouping by condition. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 # Define number of subjects for each condition n <- 20 # Calculate group means y_j <- tapply ( wm $ iq , wm $ condition , mean ) # Calculate the grand mean y_t <- mean ( y_j ) # Calculate the sum of squares ss_cond <- sum (( y_j - y_t ) ^2 ) * n # Define the degrees of freedom for conditions df <- ( 4 - 1 ) # Calculate the mean squares (variance) ms_cond <- ss_cond / df The subject variance We will also need the error term of the repeated measures design. This can be calculated in a few steps. First calculate the systematic variance due to subjects. Below, we will calculate the unsystematic variance, like we did with the between-groups design. If we subtract these two results, we will get the error term of the repeated measures design. The systematic (stable) subject variance will be taken out of the error term, so the error term is reduced in comparison with the between-groups design. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 # Define number of conditions for each subject n <- 4 # Calculate subject means y_j <- tapply ( wm $ iq , wm $ subject , mean ) # Calculate the grand mean y_t <- mean ( y_j ) # Calculate the sum of squares ss_subjects <- sum (( y_j - y_t ) ^2 ) * n # Define the degrees of freedom for subjects df <- ( 20 - 1 ) # Calculate the mean squares (variance) ms_subjects <- ss_subjects / df The unsystematic within groups variance To calculate the error term of the repeated measures design, we need the unsystematic within-groups variance: the unsystematic variance or the error term of the between-groups design. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 # Create four subsets of the four groups, containing the IQ results # Make the subset for the group condition = \"8 days\" y_i1 <- subset ( wm $ iq , wm $ condition == \"8 days\" ) # Make the subset for the group condition = \"12 days\" y_i2 <- subset ( wm $ iq , wm $ condition == \"12 days\" ) # Make the subset for the group condition = \"17 days\" y_i3 <- subset ( wm $ iq , wm $ condition == \"17 days\" ) # Make the subset for the group condition = \"19 days\" y_i4 <- subset ( wm $ iq , wm $ condition == \"19 days\" ) # Subtract the individual values by their group means s_1 <- y_i1 - mean ( y_i1 ) s_2 <- y_i2 - mean ( y_i2 ) s_3 <- y_i3 - mean ( y_i3 ) s_4 <- y_i4 - mean ( y_i4 ) # Put everything back into one vector s_t <- c ( s_1 , s_2 , s_3 , s_4 ) # Calculate the within sum of squares by using the vector s_t ss_sa <- sum ( s_t^2 ) # Define the degrees of freedom df <- 4 * ( 20-1 ) # Calculate the mean squares (variances) ms_sa <- ss_sa / df The unsystematic variance for the repeated measures design Now we can easily calculate the unsystematic variance for the repeated measures design, also called the error term. 1 2 3 4 5 6 7 8 # ss_sa = ss_subjects + ss_rm ss_rm <- ss_sa - ss_subjects # Define the degrees of freedom df <- ( 20 - 1 ) * ( 4 - 1 ) # Calculate the mean squares (variances) ms_rm <- ss_rm / df Now we\u2019ve calculated the error term of the repeated measures design, which is clearly smaller than the error term of the between-groups design because you reduced this one. To complete you can now calculate the F-ratio and the corresponding p-value. F-ratio and p-value To do the ANOVA analysis we actually need the F-ratio and the corresponding p-value. 1 2 3 4 5 6 7 8 9 10 11 # Calculate the F-ratio f_rat <- ms_cond / ms_rm # Define the degrees of freedom of the F-distribution # df1 for freedom of conditions (ss_cond) # df2 for (freedom of) conditions and subjects (sa_sa) df1 <- ( 4 - 1 ) df2 <- ( 4 - 1 ) * ( 20 - 1 ) # Calculate the p-value p <- 1 - pf ( f_rat , df1 , df2 ) Error term in a repeated measures design? The inconsistent individual differences across conditions, the effect of subjects that differ across conditions. So it is an interaction between subjects and condition. Anova in R ANOVA in R is usually done with the aov function. 1 2 3 4 5 # anova model model <- aov ( wm $ iq ~ wm $ condition + Error ( wm $ subject / wm $ condition )) # summary model summary ( model ) 1 2 3 4 5 6 7 8 9 10 11 ## ## Error: wm$subject ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Residuals 19 175.6 9.242 ## ## Error: wm$subject:wm$condition ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## wm$condition 3 196.1 65.36 12.51 2.16e-06 *** ## Residuals 57 297.8 5.22 ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 The F-ratio is significant. Therefore, the number of training days does affect the IQ scores. Effect size Calculate eta-squared, which helps you estimate effect size. 1 2 3 4 5 6 7 8 9 # Define the total sum of squares # ss_cond (syst. between groups or of the effect), # ss_sa (unsyst. within groups) = # ss_rm (unsyst. for repeated measures design) + # ss_subjects (ok) ss_total <- ss_cond + ss_rm # Calculate the effect size eta_sq <- ss_cond / ss_total Post-hoc test one We will now look at a few different procedures for the post-hoc test. Let\u2019s start with the Holm procedure. 1 2 # Post-hoc test: default procedure with ( wm , pairwise.t.test ( iq , condition , paired = T )) 1 2 3 4 5 6 7 8 9 10 11 ## ## Pairwise comparisons using paired t tests ## ## data: iq and condition ## ## 12 days 17 days 19 days ## 17 days 0.01955 - - ## 19 days 0.00270 0.40038 - ## 8 days 0.40038 0.00244 0.00054 ## ## P value adjustment method: holm We get a table with some values as the output for the post-hoc test and a line saying that we have used the Holm procedure. What are the values in the table of the output? p-values. Recall that the hypotheses are tested at a 5% significance level. We can conclude that all pairwise comparisons are significant, except for the comparison between 19 and 17 and the comparison between 12 and 8. Post-hoc test: Bonferroni Now we\u2019ll take a look at the most conservative procedure, Bonferroni. This procedure will apply the most extreme adjustments to the p-values. 1 2 # Post-hoc test: Bonferroni procedure with ( wm , pairwise.t.test ( iq , condition , paired = TRUE , p.adjust.method = 'bonferroni' )) 1 2 3 4 5 6 7 8 9 10 11 ## ## Pairwise comparisons using paired t tests ## ## data: iq and condition ## ## 12 days 17 days 19 days ## 17 days 0.03910 - - ## 19 days 0.00405 1.00000 - ## 8 days 1.00000 0.00293 0.00054 ## ## P value adjustment method: bonferroni Notice the change in p-values in comparison with the previous procedure. Paired t-test Assume that you do not know how to perform an analysis of variance (ANOVA), we may do a number of paired t-tests instead. Have a look at just one paired t-test and take, for example, the comparison between 12 days and 17 days. 1 2 3 4 # Define two subsets containing the IQ scores for the condition group '12 days' and '17 days' cond_12days <- subset ( wm , condition == '12 days' ) $ iq cond_17days <- subset ( wm , condition == '17 days' ) $ iq cond_12days 1 2 ## [1] 12.5 11.6 8.9 8.3 10.9 13.4 12.3 8.7 9.7 9.7 6.9 10.5 11.6 13.8 ## [15] 15.6 11.7 16.1 11.7 15.0 15.1 1 2 # t-test t.test ( cond_12days , cond_17days , paired = TRUE ) 1 2 3 4 5 6 7 8 9 10 11 ## ## Paired t-test ## ## data: cond_12days and cond_17days ## t = -3.0549, df = 19, p-value = 0.006517 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -3.7157116 -0.6942884 ## sample estimates: ## mean of the differences ## -2.205 It is clear that we can apply different procedures for post-hoc tests. These procedures differ with respect to how they handle inflation of the possibility of a type I error and will therefore give us different p-values. However, these p-values will always be in a certain range. What is the (smallest) range of p-values for the comparison between 12 days and 17 days? Look at the results from applying the Bonferroni procedure as well as the paired t-test. p-values: Bonferroni 12 days-17 days: 0.03910 Paired t-test (cond_12days, cond_17days): 0.006517 Therefore: [0.0065, 0.0391].","title":"Statistics with R, Course Four, Repeated Measures ANOVA"},{"location":"a_hands-on_introduction_to_statistics_with_r_course_four_repeated_measures_anova/#an-introduction-to-repeated-measures","text":"The independent t-test is analogous to between-groups ANOVA and the paired-sample t-test is analogous to repeated measures ANOVA. In a between-groups design, each subject is exposed to two or more treatments or conditions over time. In a within-subjects design, each subject is allocated to exactly one treatment or condition. Between: Experiment 1: You want to test the effect of alcohol on test scores of students. There are three conditions: the student consumed no alcohol, two glasses of beer, or five glasses of beer. Alcohol tolerance and time spent studying should also be considered somehow. Experiment 2: You want to investigate the effects of certain fertilizers on plant growth. Assume you have two different fertilizers, A and B. Consider three conditions: you give the plant no fertilizer, fertilizer A, or fertilizer B. You measure the height of the plant after a specific period of time to see whether the fertilizers had an effect. Use a within-subjects design for for Experiment 1 and between-groups design for Experiment 2. Is it always either manipulation between-groups or manipulation within-groups, or are there experiments where you could use either approach? In some cases, either approach is possible. Explore the working memory data 1 2 # Print the data set in the console str ( wm ) 1 2 3 4 ## 'data.frame': 80 obs. of 3 variables: ## $ subject : num 1 2 3 4 5 6 7 8 9 10 ... ## $ condition: Factor w/ 4 levels \"12 days\",\"17 days\",..: 4 4 4 4 4 4 4 4 4 4 ... ## $ iq : num 12.4 11.8 14.6 7.7 15.7 11.6 7 8.4 10.7 10.6 ... 1 2 3 4 5 6 7 8 library ( psych ) library ( ggplot2 ) # Define the variable subject as a categorical variable wm $ subject <- factor ( wm $ subject ) # Summary statistics by all groups (8 sessions, 12 sessions, 17 sessions, 19 sessions) describeBy ( wm , wm $ condition ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 ## ## Descriptive statistics by group ## group: 12 days ## vars n mean sd median trimmed mad min max range skew ## subject* 1 20 10.5 5.92 10.50 10.50 7.41 1.0 20.0 19.0 0.00 ## condition* 2 20 1.0 0.00 1.00 1.00 0.00 1.0 1.0 0.0 NaN ## iq 3 20 11.7 2.58 11.65 11.69 2.89 6.9 16.1 9.2 0.05 ## kurtosis se ## subject* -1.38 1.32 ## condition* NaN 0.00 ## iq -1.06 0.58 ## -------------------------------------------------------- ## group: 17 days ## vars n mean sd median trimmed mad min max range skew ## subject* 1 20 10.5 5.92 10.5 10.5 7.41 1.0 20.0 19.0 0.00 ## condition* 2 20 2.0 0.00 2.0 2.0 0.00 2.0 2.0 0.0 NaN ## iq 3 20 13.9 2.26 13.6 13.9 2.00 9.8 18.1 8.3 0.11 ## kurtosis se ## subject* -1.38 1.32 ## condition* NaN 0.00 ## iq -0.85 0.50 ## -------------------------------------------------------- ## group: 19 days ## vars n mean sd median trimmed mad min max range skew ## subject* 1 20 10.50 5.92 10.5 10.50 7.41 1.0 20.0 19.0 0.00 ## condition* 2 20 3.00 0.00 3.0 3.00 0.00 3.0 3.0 0.0 NaN ## iq 3 20 14.75 2.50 15.3 14.71 2.15 10.4 19.2 8.8 -0.09 ## kurtosis se ## subject* -1.38 1.32 ## condition* NaN 0.00 ## iq -0.99 0.56 ## -------------------------------------------------------- ## group: 8 days ## vars n mean sd median trimmed mad min max range skew ## subject* 1 20 10.50 5.92 10.5 10.50 7.41 1.0 20.0 19.0 0.00 ## condition* 2 20 4.00 0.00 4.0 4.00 0.00 4.0 4.0 0.0 NaN ## iq 3 20 10.91 2.63 11.3 10.97 2.67 5.4 15.7 10.3 -0.21 ## kurtosis se ## subject* -1.38 1.32 ## condition* NaN 0.00 ## iq -0.70 0.59 1 2 # Boxplot IQ versus condition boxplot ( wm $ iq ~ wm $ condition , main = 'Boxplot' , xlab = 'Training sessions' , ylab = 'IQ' ) 1 2 # Illustration data, each line represents the development of each subject by number of trainings ggplot ( data = wm , aes ( x = wm $ condition , y = wm $ iq , group = wm $ subject , colour = wm $ subject )) + geom_line () + geom_point () Reduced cost The cost advantage of using manipulation within groups verses manipulation between groups for the working memory experiment is you need 60 subjects fewer. Statistically more powerful Repeated measures analysis accounts for individual differences across the experiment. This reduces the error term, which increases statistical power. Counterbalancing Suppose you have three levels of an independent variable A (i.e. A1, A2, A3) and a blocked design. You want to use full counterbalancing to take into account order effects. What are all the possible orders that you need to use? In other words, what are the order conditions? (A1, A2, A3), (A1, A3, A2) , (A2, A1, A3), (A2, A3, A1) , (A3, A2, A1), (A3, A1, A2) Number of order conditions? Assume the number of levels of the independent variable goes up and you want to completely counterbalance. What will happen to the number of order conditions you\u2019ll need? The number becomes really large. An independent variable with n levels will have n!=n*(n???1)*(n???2)*... order conditions. Latin Squares design As you hopefully realized in the previous exercise, completely counterbalancing is not always a practical solution for taking into account order effects. This is because the number of different orders required gets really large as the number of possible conditions increases. The most common workaround to this problem is the Latin Squares design, in which you do not completely counterbalance, but instead put each condition at every position (at least) once. Which of the following examples has been constructed according to the Latin Squares design? (A1, A2, A3), (A2, A3, A1), (A3, A1, A2) More on Latin Squares The number of order conditions is always equal to the number of levels of your independent variable. Why is missing data a problem? In a between-groups design, it is okay to have a slightly different number of subjects in each group, so if one subject drops out in one of the conditions then that group has just one less subject. Now you want to look at how subjects change over time and the different scores between two or more conditions for each subject. Understanding sphericity The variances of the differences between all possible pairs of groups (i.e. levels of the independent variable) are equal. Mauchly\u2019s test 1 2 3 4 5 6 7 8 # Define iq as a data frame where each column represents a condition iq <- cbind ( wm $ iq[wm $ condition == '8 days' ] , wm $ iq[wm $ condition == '12 days' ] , wm $ iq[wm $ condition == '17 days' ] , wm $ iq[wm $ condition == '19 days' ] ) # Make an mlm object mlm <- lm ( iq ~ 1 ) # Mauchly's test mauchly.test ( mlm , x = ~ 1 ) 1 2 3 4 5 ## ## Mauchly's test of sphericity ## ## data: SSD matrix from lm(formula = iq ~ 1) ## W = 0.81725, p-value = 0.9407 Based on the results, the sphericity assumption holds. Pros of repeated measures Less cost and statistically more powerful Cons of repeated measures Order effects, counterbalancing, missing data, and an extra assumption","title":"An introduction to repeated measures"},{"location":"a_hands-on_introduction_to_statistics_with_r_course_four_repeated_measures_anova/#repeated-measures-anova","text":"The systematic between groups variance To understand everything a bit better, we will calculate the F-ratio for a repeated measures design by ourself in the next exercises. First, we will need the systematic between-groups variance. This is the same as in the between-groups design\u2013the variance due to grouping by condition. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 # Define number of subjects for each condition n <- 20 # Calculate group means y_j <- tapply ( wm $ iq , wm $ condition , mean ) # Calculate the grand mean y_t <- mean ( y_j ) # Calculate the sum of squares ss_cond <- sum (( y_j - y_t ) ^2 ) * n # Define the degrees of freedom for conditions df <- ( 4 - 1 ) # Calculate the mean squares (variance) ms_cond <- ss_cond / df The subject variance We will also need the error term of the repeated measures design. This can be calculated in a few steps. First calculate the systematic variance due to subjects. Below, we will calculate the unsystematic variance, like we did with the between-groups design. If we subtract these two results, we will get the error term of the repeated measures design. The systematic (stable) subject variance will be taken out of the error term, so the error term is reduced in comparison with the between-groups design. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 # Define number of conditions for each subject n <- 4 # Calculate subject means y_j <- tapply ( wm $ iq , wm $ subject , mean ) # Calculate the grand mean y_t <- mean ( y_j ) # Calculate the sum of squares ss_subjects <- sum (( y_j - y_t ) ^2 ) * n # Define the degrees of freedom for subjects df <- ( 20 - 1 ) # Calculate the mean squares (variance) ms_subjects <- ss_subjects / df The unsystematic within groups variance To calculate the error term of the repeated measures design, we need the unsystematic within-groups variance: the unsystematic variance or the error term of the between-groups design. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 # Create four subsets of the four groups, containing the IQ results # Make the subset for the group condition = \"8 days\" y_i1 <- subset ( wm $ iq , wm $ condition == \"8 days\" ) # Make the subset for the group condition = \"12 days\" y_i2 <- subset ( wm $ iq , wm $ condition == \"12 days\" ) # Make the subset for the group condition = \"17 days\" y_i3 <- subset ( wm $ iq , wm $ condition == \"17 days\" ) # Make the subset for the group condition = \"19 days\" y_i4 <- subset ( wm $ iq , wm $ condition == \"19 days\" ) # Subtract the individual values by their group means s_1 <- y_i1 - mean ( y_i1 ) s_2 <- y_i2 - mean ( y_i2 ) s_3 <- y_i3 - mean ( y_i3 ) s_4 <- y_i4 - mean ( y_i4 ) # Put everything back into one vector s_t <- c ( s_1 , s_2 , s_3 , s_4 ) # Calculate the within sum of squares by using the vector s_t ss_sa <- sum ( s_t^2 ) # Define the degrees of freedom df <- 4 * ( 20-1 ) # Calculate the mean squares (variances) ms_sa <- ss_sa / df The unsystematic variance for the repeated measures design Now we can easily calculate the unsystematic variance for the repeated measures design, also called the error term. 1 2 3 4 5 6 7 8 # ss_sa = ss_subjects + ss_rm ss_rm <- ss_sa - ss_subjects # Define the degrees of freedom df <- ( 20 - 1 ) * ( 4 - 1 ) # Calculate the mean squares (variances) ms_rm <- ss_rm / df Now we\u2019ve calculated the error term of the repeated measures design, which is clearly smaller than the error term of the between-groups design because you reduced this one. To complete you can now calculate the F-ratio and the corresponding p-value. F-ratio and p-value To do the ANOVA analysis we actually need the F-ratio and the corresponding p-value. 1 2 3 4 5 6 7 8 9 10 11 # Calculate the F-ratio f_rat <- ms_cond / ms_rm # Define the degrees of freedom of the F-distribution # df1 for freedom of conditions (ss_cond) # df2 for (freedom of) conditions and subjects (sa_sa) df1 <- ( 4 - 1 ) df2 <- ( 4 - 1 ) * ( 20 - 1 ) # Calculate the p-value p <- 1 - pf ( f_rat , df1 , df2 ) Error term in a repeated measures design? The inconsistent individual differences across conditions, the effect of subjects that differ across conditions. So it is an interaction between subjects and condition. Anova in R ANOVA in R is usually done with the aov function. 1 2 3 4 5 # anova model model <- aov ( wm $ iq ~ wm $ condition + Error ( wm $ subject / wm $ condition )) # summary model summary ( model ) 1 2 3 4 5 6 7 8 9 10 11 ## ## Error: wm$subject ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Residuals 19 175.6 9.242 ## ## Error: wm$subject:wm$condition ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## wm$condition 3 196.1 65.36 12.51 2.16e-06 *** ## Residuals 57 297.8 5.22 ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 The F-ratio is significant. Therefore, the number of training days does affect the IQ scores. Effect size Calculate eta-squared, which helps you estimate effect size. 1 2 3 4 5 6 7 8 9 # Define the total sum of squares # ss_cond (syst. between groups or of the effect), # ss_sa (unsyst. within groups) = # ss_rm (unsyst. for repeated measures design) + # ss_subjects (ok) ss_total <- ss_cond + ss_rm # Calculate the effect size eta_sq <- ss_cond / ss_total Post-hoc test one We will now look at a few different procedures for the post-hoc test. Let\u2019s start with the Holm procedure. 1 2 # Post-hoc test: default procedure with ( wm , pairwise.t.test ( iq , condition , paired = T )) 1 2 3 4 5 6 7 8 9 10 11 ## ## Pairwise comparisons using paired t tests ## ## data: iq and condition ## ## 12 days 17 days 19 days ## 17 days 0.01955 - - ## 19 days 0.00270 0.40038 - ## 8 days 0.40038 0.00244 0.00054 ## ## P value adjustment method: holm We get a table with some values as the output for the post-hoc test and a line saying that we have used the Holm procedure. What are the values in the table of the output? p-values. Recall that the hypotheses are tested at a 5% significance level. We can conclude that all pairwise comparisons are significant, except for the comparison between 19 and 17 and the comparison between 12 and 8. Post-hoc test: Bonferroni Now we\u2019ll take a look at the most conservative procedure, Bonferroni. This procedure will apply the most extreme adjustments to the p-values. 1 2 # Post-hoc test: Bonferroni procedure with ( wm , pairwise.t.test ( iq , condition , paired = TRUE , p.adjust.method = 'bonferroni' )) 1 2 3 4 5 6 7 8 9 10 11 ## ## Pairwise comparisons using paired t tests ## ## data: iq and condition ## ## 12 days 17 days 19 days ## 17 days 0.03910 - - ## 19 days 0.00405 1.00000 - ## 8 days 1.00000 0.00293 0.00054 ## ## P value adjustment method: bonferroni Notice the change in p-values in comparison with the previous procedure. Paired t-test Assume that you do not know how to perform an analysis of variance (ANOVA), we may do a number of paired t-tests instead. Have a look at just one paired t-test and take, for example, the comparison between 12 days and 17 days. 1 2 3 4 # Define two subsets containing the IQ scores for the condition group '12 days' and '17 days' cond_12days <- subset ( wm , condition == '12 days' ) $ iq cond_17days <- subset ( wm , condition == '17 days' ) $ iq cond_12days 1 2 ## [1] 12.5 11.6 8.9 8.3 10.9 13.4 12.3 8.7 9.7 9.7 6.9 10.5 11.6 13.8 ## [15] 15.6 11.7 16.1 11.7 15.0 15.1 1 2 # t-test t.test ( cond_12days , cond_17days , paired = TRUE ) 1 2 3 4 5 6 7 8 9 10 11 ## ## Paired t-test ## ## data: cond_12days and cond_17days ## t = -3.0549, df = 19, p-value = 0.006517 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -3.7157116 -0.6942884 ## sample estimates: ## mean of the differences ## -2.205 It is clear that we can apply different procedures for post-hoc tests. These procedures differ with respect to how they handle inflation of the possibility of a type I error and will therefore give us different p-values. However, these p-values will always be in a certain range. What is the (smallest) range of p-values for the comparison between 12 days and 17 days? Look at the results from applying the Bonferroni procedure as well as the paired t-test. p-values: Bonferroni 12 days-17 days: 0.03910 Paired t-test (cond_12days, cond_17days): 0.006517 Therefore: [0.0065, 0.0391].","title":"Repeated measures ANOVA"},{"location":"a_hands-on_introduction_to_statistics_with_r_course_one_introduction/","text":"Foreword Output options: the \u2018tango\u2019 syntax and the \u2018readable\u2019 theme. Snippets and results. Variables \u00b6 Nominal variables in R 1 2 3 4 5 # Create a numeric vector with the identifiers of the participants of your survey participants_1 <- c ( 2 , 3 , 5 , 7 , 11 , 13 , 17 ) # Check what type of values R thinks the vector consists of class ( participants_1 ) 1 ## [1] \"numeric\" 1 2 3 4 5 # Transform the numeric vector to a factor vector participants_2 <- factor ( participants_1 ) # Check what type of values R thinks the vector consists of now class ( participants_2 ) 1 ## [1] \"factor\" Ordinal variables in R 1 2 3 # Create a vector of temperature observations temperature_vector <- c ( 'High' , 'Low' , 'High' , 'Low' , 'Medium' ) temperature_vector 1 ## [1] \"High\" \"Low\" \"High\" \"Low\" \"Medium\" 1 2 3 # Specify that they are ordinal variables with the given levels factor_temperature_vector <- factor ( temperature_vector , order = TRUE , levels = c ( 'Low' , 'Medium' , 'High' )) factor_temperature_vector 1 2 ## [ 1 ] High Low High Low Medium ## Levels : Low & lt ; Medium & lt ; High Interval and Ratio variables in R 1 2 3 4 5 6 7 # Assign to the variable 'longitudes' a vector with the longitudes # This is an interval variable. longitudes <- c ( 10 , 20 , 30 , 40 ) # Assign the times it takes for an athlete to run 100 meters to the variable 'chronos' # This is a ratio variable. chronos <- c ( 10.60 , 10.12 , 9.58 , 11.1 ) Histograms and Distributions \u00b6 Creating histograms in R 1 2 # Print the data set in the console head ( impact ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 ## subject condition verbal_memory_baseline visual_memory_baseline ## 1 1 control 95 88 ## 2 2 control 90 82 ## 3 3 control 87 77 ## 4 4 control 84 72 ## 5 5 control 92 77 ## 6 6 control 89 79 ## visual.motor_speed_baseline reaction_time_baseline ## 1 35.29 0.42 ## 2 31.47 0.63 ## 3 30.87 0.56 ## 4 41.87 0.66 ## 5 33.28 0.56 ## 6 40.73 0.81 ## impulse_control_baseline total_symptom_baseline verbal_memory_retest ## 1 11 0 97 ## 2 7 0 86 ## 3 8 0 90 ## 4 7 0 85 ## 5 7 1 87 ## 6 6 0 91 ## visual_memory_retest visual.motor_speed_retest reaction_time_retest ## 1 86 35.61 0.65 ## 2 80 37.01 0.49 ## 3 79 20.15 0.75 ## 4 70 33.26 0.19 ## 5 77 28.34 0.59 ## 6 85 33.47 0.48 ## impulse_control_retest total_symptom_retest ## 1 10 0 ## 2 7 0 ## 3 9 0 ## 4 8 0 ## 5 8 1 ## 6 5 0 1 2 3 # Use the describe() function to see some summary information per variable #describe(impact) summary ( impact ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 ## subject condition verbal_memory_baseline ## Min. : 1.00 Length:40 Min. :75.00 ## 1st Qu.:10.75 Class :character 1st Qu.:85.00 ## Median :20.50 Mode :character Median :91.00 ## Mean :20.50 Mean :89.75 ## 3rd Qu.:30.25 3rd Qu.:95.00 ## Max. :40.00 Max. :98.00 ## visual_memory_baseline visual.motor_speed_baseline reaction_time_baseline ## Min. :59.00 Min. :26.29 Min. :0.4200 ## 1st Qu.:68.75 1st Qu.:31.59 1st Qu.:0.5675 ## Median :75.00 Median :33.50 Median :0.6500 ## Mean :74.88 Mean :34.03 Mean :0.6670 ## 3rd Qu.:81.25 3rd Qu.:36.44 3rd Qu.:0.7325 ## Max. :91.00 Max. :41.87 Max. :1.2000 ## impulse_control_baseline total_symptom_baseline verbal_memory_retest ## Min. : 2.000 Min. :0.00 Min. :59 ## 1st Qu.: 7.000 1st Qu.:0.00 1st Qu.:74 ## Median : 8.500 Median :0.00 Median :85 ## Mean : 8.275 Mean :0.05 Mean :82 ## 3rd Qu.:10.000 3rd Qu.:0.00 3rd Qu.:91 ## Max. :12.000 Max. :1.00 Max. :97 ## visual_memory_retest visual.motor_speed_retest reaction_time_retest ## Min. :54.00 Min. :20.15 Min. :0.1900 ## 1st Qu.:66.75 1st Qu.:30.33 1st Qu.:0.5575 ## Median :72.00 Median :35.15 Median :0.6500 ## Mean :71.90 Mean :35.83 Mean :0.6730 ## 3rd Qu.:79.00 3rd Qu.:39.41 3rd Qu.:0.7325 ## Max. :86.00 Max. :60.77 Max. :1.3000 ## impulse_control_retest total_symptom_retest ## Min. : 1.00 Min. : 0.00 ## 1st Qu.: 5.00 1st Qu.: 0.00 ## Median : 7.00 Median : 7.00 ## Mean : 6.75 Mean :13.88 ## 3rd Qu.: 9.00 3rd Qu.:27.00 ## Max. :12.00 Max. :43.00 1 2 3 # Select the variable 'verbal_memory_baseline' from the 'impact' data.frame and assign it to the variable 'verbal_baseline' verbal_baseline <- impact $ verbal_memory_baseline verbal_baseline 1 2 ## [1] 95 90 87 84 92 89 78 97 93 90 89 97 79 86 85 85 98 95 96 92 79 85 97 ## [24] 89 75 75 84 93 88 97 93 96 84 89 95 95 97 95 92 95 1 2 # Plot a histogram of the verbal_baseline variable that you have just created hist ( verbal_baseline , main = 'Distribution of verbal memory baseline scores' , xlab = 'score' , ylab = 'frequency' ) Let us go wine tasting (red wine) 1 2 3 4 5 # Read in the data set and assign to the object red_wine_data <- readWorksheetFromFile ( 'A Hands-on Introduction to Statistics with R.xls' , sheet = 'red_wine_data' , header = TRUE , startCol = 1 , startRow = 1 ) # This will print the data set in the console head ( red_wine_data ) 1 2 3 4 5 6 7 ## subject condition Ratings ## 1 1 Australia 77 ## 2 2 Australia 82 ## 3 3 Australia 75 ## 4 4 Australia 92 ## 5 5 Australia 83 ## 6 6 Australia 75 1 2 3 # Print basic statistical properties of the red_wine_data data.frame. Use the describe() function #describe(red_wine_data) summary ( red_wine_data ) 1 2 3 4 5 6 7 ## subject condition Ratings ## Min. : 1.0 Length:400 Min. :39.00 ## 1st Qu.:100.8 Class :character 1st Qu.:67.00 ## Median :200.5 Mode :character Median :74.00 ## Mean :200.5 Mean :73.94 ## 3rd Qu.:300.2 3rd Qu.:81.00 ## Max. :400.0 Max. :98.00 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 # Split the data.frame in subsets for each country and assign these subsets to the variables below red_usa <- subset ( red_wine_data , red_wine_data $ condition == 'USA' ) red_france <- subset ( red_wine_data , red_wine_data $ condition == 'France' ) red_australia <- subset ( red_wine_data , red_wine_data $ condition == 'Australia' ) red_argentina <- subset ( red_wine_data , red_wine_data $ condition == 'Argentina' ) # Select only the Ratings variable for each of these subsets and assign them to the variables below red_ratings_usa <- red_usa $ Ratings red_ratings_france <- red_france $ Ratings red_ratings_australia <- red_australia $ Ratings red_ratings_argentina <- red_argentina $ Ratings ## Create a 2 by 2 matrix of histograms # Organize the histograms so that they are structured in a 2 by 2 matrix. par ( mfrow = c ( 2 , 2 )) # Plot four histograms, one for each subject hist ( red_ratings_usa ) hist ( red_ratings_france ) hist ( red_ratings_australia ) hist ( red_ratings_argentina ) Let us go wine tasting (white wine) 1 2 3 4 5 # Read in the data set and assign to the object white_wine_data <- readWorksheetFromFile ( 'A Hands-on Introduction to Statistics with R.xls' , sheet = 'white_wine_data' , header = TRUE , startCol = 1 , startRow = 1 ) # This will print the data set in the console head ( white_wine_data ) 1 2 3 4 5 6 7 ## condition Ratings ## 1 Australia 85.6 ## 2 Australia 85.6 ## 3 Australia 85.6 ## 4 Australia 85.6 ## 5 Australia 85.6 ## 6 Australia 85.6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 # Assign the scores for each country to a variable white_ratings_france <- subset ( white_wine_data , white_wine_data $ condition == 'France' ) $ Ratings white_ratings_argentina <- subset ( white_wine_data , white_wine_data $ condition == 'Argentina' ) $ Ratings white_ratings_australia <- subset ( white_wine_data , white_wine_data $ condition == 'Australia' ) $ Ratings white_ratings_usa <- subset ( white_wine_data , white_wine_data $ condition == 'USA' ) $ Ratings # Plot a histogram for each of the countries # Organize the histograms so that they are structured in a 2 by 2 matrix. par ( mfrow = c ( 2 , 2 )) hist ( white_ratings_usa , main = 'USA white ratings' , xlab = 'score' ) hist ( white_ratings_australia , main = 'Australia white ratings' , xlab = 'score' ) hist ( white_ratings_argentina , main = 'Argentina white ratings' , xlab = 'score' ) hist ( white_ratings_france , main = 'France white ratings' , xlab = 'score' ) Scales of Measurement \u00b6 Converting a distribution to Z-scale 1 2 3 4 # Read in the data set and assign to the object ratings_australia <- readWorksheetFromFile ( 'A Hands-on Introduction to Statistics with R.xls' , sheet = 'ratings_australia' , header = TRUE , startCol = 1 , startRow = 1 ) ratings_australia <- as.vector ( ratings_australia $ ratings_australia ) 1 2 # Print the ratings for the Australian red wine ratings_australia 1 2 3 4 5 ## [1] 77 82 75 92 83 75 84 86 85 79 92 84 77 65 89 81 81 88 87 85 87 86 82 ## [24] 67 85 81 80 71 78 84 91 80 84 81 71 78 78 81 89 86 80 79 86 85 76 76 ## [47] 84 86 80 87 84 77 83 73 91 95 78 74 85 80 98 81 86 81 76 82 68 91 82 ## [70] 96 84 76 85 74 72 83 78 81 82 77 77 80 89 70 85 83 88 79 84 83 77 89 ## [93] 89 86 92 85 72 77 72 78 1 2 3 4 5 6 7 8 9 10 11 # Convert these ratings to Z-scores. Use the `scale()` function z_scores_australia <- scale ( ratings_australia ) # Plot both the original data and the scaled data in histograms next to each other par ( mfrow = c ( 1 , 2 )) # Plot the histogram for the original scores hist ( ratings_australia ) # Plot the histogram for the Z-scores hist ( z_scores_australia ) Measures of Central Tendency \u00b6 The mean of a Fibonacci sequence 1 2 3 4 5 6 # create a vector that contains the Fibonacci elements fibonacci <- c ( 0 , 1 , 1 , 2 , 3 , 5 , 8 , 13 ) # calculate the mean manually. Use the sum() and the length() functions mean <- sum ( fibonacci ) / length ( fibonacci ) mean 1 ## [1] 4.125 1 2 3 # calculate the mean the easy way mean_check <- mean ( fibonacci ) mean_check 1 ## [1] 4.125 Setting up histograms 1 2 3 4 # Read in the data set and assign to the object wine_data <- readWorksheetFromFile ( 'A Hands-on Introduction to Statistics with R.xls' , sheet = 'wine_data' , header = TRUE , startCol = 1 , startRow = 1 ) head ( wine_data ) 1 2 3 4 5 6 7 ## condition Ratings ## 1 Red 77 ## 2 Red 82 ## 3 Red 75 ## 4 Red 92 ## 5 Red 83 ## 6 Red 75 1 2 3 4 5 6 7 8 # create the two subsets red_wine <- subset ( wine_data , wine_data $ condition == 'Red' ) white_wine <- subset ( wine_data , wine_data $ condition == 'White' ) # Plot the histograms of the ratings of both subsets par ( mfrow = c ( 1 , 2 )) hist ( red_wine $ Ratings , main = 'Shiraz' , xlab = 'Ratings' ) hist ( white_wine $ Ratings , main = 'Pinot Grigio' , xlab = 'Ratings' ) Robustness to outliers 1 2 3 4 5 6 7 8 9 # create the outlier and add it to the dataset outlier <- data.frame ( condition = 'Red' , Ratings = 0 ) red_wine_extreme <- rbind ( red_wine , outlier ) # calculate the difference in means and display it afterwards diff_means <- mean ( red_wine $ Ratings ) - mean ( red_wine_extreme $ Ratings ) diff_means 1 ## [1] 0.8093069 1 2 3 4 # calculate the difference in medians and display it afterwards diff_medians <- median ( red_wine $ Ratings ) - median ( red_wine_extreme $ Ratings ) diff_medians 1 ## [1] 0 Measures of Variability \u00b6 Michael Jordan\u2019s first NBA season - Global overview 1 2 3 4 # Read in the data set and assign to the object data_jordan <- readWorksheetFromFile ( 'A Hands-on Introduction to Statistics with R.xls' , sheet = 'data_jordan' , header = TRUE , startCol = 1 , startRow = 1 ) head ( data_jordan ) 1 2 3 4 5 6 7 ## game points ## 1 1 16 ## 2 2 21 ## 3 3 37 ## 4 4 25 ## 5 5 17 ## 6 6 25 1 2 3 4 5 # Make a scatterplot of the data on which a horizontal line with height equal to the mean is drawn. mean_jordan <- mean ( data_jordan $ points ) plot ( data_jordan $ game , data_jordan $ points , main = '1st NBA season of Michael Jordan' ) abline ( h = mean_jordan ) Michael Jordan\u2019s first NBA season - Calculate the variance manually 1 2 3 4 5 6 7 8 9 # Calculate the differences with respect to the mean diff <- data_jordan $ points - mean ( data_jordan $ points ) # Calculate the squared differences squared_diff <- diff^2 # Combine all pieces of the puzzle in order to acquire the variance variance <- sum ( squared_diff ) / ( length ( data_jordan $ points ) - 1 ) variance 1 ## [1] 66.73427 1 2 # Compare your result to the correct solution. You can find the correct solution by calculating it with the `var()` function. var ( data_jordan $ points ) 1 ## [1] 66.73427","title":"Statistics with R, Course One, Introduction"},{"location":"a_hands-on_introduction_to_statistics_with_r_course_one_introduction/#variables","text":"Nominal variables in R 1 2 3 4 5 # Create a numeric vector with the identifiers of the participants of your survey participants_1 <- c ( 2 , 3 , 5 , 7 , 11 , 13 , 17 ) # Check what type of values R thinks the vector consists of class ( participants_1 ) 1 ## [1] \"numeric\" 1 2 3 4 5 # Transform the numeric vector to a factor vector participants_2 <- factor ( participants_1 ) # Check what type of values R thinks the vector consists of now class ( participants_2 ) 1 ## [1] \"factor\" Ordinal variables in R 1 2 3 # Create a vector of temperature observations temperature_vector <- c ( 'High' , 'Low' , 'High' , 'Low' , 'Medium' ) temperature_vector 1 ## [1] \"High\" \"Low\" \"High\" \"Low\" \"Medium\" 1 2 3 # Specify that they are ordinal variables with the given levels factor_temperature_vector <- factor ( temperature_vector , order = TRUE , levels = c ( 'Low' , 'Medium' , 'High' )) factor_temperature_vector 1 2 ## [ 1 ] High Low High Low Medium ## Levels : Low & lt ; Medium & lt ; High Interval and Ratio variables in R 1 2 3 4 5 6 7 # Assign to the variable 'longitudes' a vector with the longitudes # This is an interval variable. longitudes <- c ( 10 , 20 , 30 , 40 ) # Assign the times it takes for an athlete to run 100 meters to the variable 'chronos' # This is a ratio variable. chronos <- c ( 10.60 , 10.12 , 9.58 , 11.1 )","title":"Variables"},{"location":"a_hands-on_introduction_to_statistics_with_r_course_one_introduction/#histograms-and-distributions","text":"Creating histograms in R 1 2 # Print the data set in the console head ( impact ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 ## subject condition verbal_memory_baseline visual_memory_baseline ## 1 1 control 95 88 ## 2 2 control 90 82 ## 3 3 control 87 77 ## 4 4 control 84 72 ## 5 5 control 92 77 ## 6 6 control 89 79 ## visual.motor_speed_baseline reaction_time_baseline ## 1 35.29 0.42 ## 2 31.47 0.63 ## 3 30.87 0.56 ## 4 41.87 0.66 ## 5 33.28 0.56 ## 6 40.73 0.81 ## impulse_control_baseline total_symptom_baseline verbal_memory_retest ## 1 11 0 97 ## 2 7 0 86 ## 3 8 0 90 ## 4 7 0 85 ## 5 7 1 87 ## 6 6 0 91 ## visual_memory_retest visual.motor_speed_retest reaction_time_retest ## 1 86 35.61 0.65 ## 2 80 37.01 0.49 ## 3 79 20.15 0.75 ## 4 70 33.26 0.19 ## 5 77 28.34 0.59 ## 6 85 33.47 0.48 ## impulse_control_retest total_symptom_retest ## 1 10 0 ## 2 7 0 ## 3 9 0 ## 4 8 0 ## 5 8 1 ## 6 5 0 1 2 3 # Use the describe() function to see some summary information per variable #describe(impact) summary ( impact ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 ## subject condition verbal_memory_baseline ## Min. : 1.00 Length:40 Min. :75.00 ## 1st Qu.:10.75 Class :character 1st Qu.:85.00 ## Median :20.50 Mode :character Median :91.00 ## Mean :20.50 Mean :89.75 ## 3rd Qu.:30.25 3rd Qu.:95.00 ## Max. :40.00 Max. :98.00 ## visual_memory_baseline visual.motor_speed_baseline reaction_time_baseline ## Min. :59.00 Min. :26.29 Min. :0.4200 ## 1st Qu.:68.75 1st Qu.:31.59 1st Qu.:0.5675 ## Median :75.00 Median :33.50 Median :0.6500 ## Mean :74.88 Mean :34.03 Mean :0.6670 ## 3rd Qu.:81.25 3rd Qu.:36.44 3rd Qu.:0.7325 ## Max. :91.00 Max. :41.87 Max. :1.2000 ## impulse_control_baseline total_symptom_baseline verbal_memory_retest ## Min. : 2.000 Min. :0.00 Min. :59 ## 1st Qu.: 7.000 1st Qu.:0.00 1st Qu.:74 ## Median : 8.500 Median :0.00 Median :85 ## Mean : 8.275 Mean :0.05 Mean :82 ## 3rd Qu.:10.000 3rd Qu.:0.00 3rd Qu.:91 ## Max. :12.000 Max. :1.00 Max. :97 ## visual_memory_retest visual.motor_speed_retest reaction_time_retest ## Min. :54.00 Min. :20.15 Min. :0.1900 ## 1st Qu.:66.75 1st Qu.:30.33 1st Qu.:0.5575 ## Median :72.00 Median :35.15 Median :0.6500 ## Mean :71.90 Mean :35.83 Mean :0.6730 ## 3rd Qu.:79.00 3rd Qu.:39.41 3rd Qu.:0.7325 ## Max. :86.00 Max. :60.77 Max. :1.3000 ## impulse_control_retest total_symptom_retest ## Min. : 1.00 Min. : 0.00 ## 1st Qu.: 5.00 1st Qu.: 0.00 ## Median : 7.00 Median : 7.00 ## Mean : 6.75 Mean :13.88 ## 3rd Qu.: 9.00 3rd Qu.:27.00 ## Max. :12.00 Max. :43.00 1 2 3 # Select the variable 'verbal_memory_baseline' from the 'impact' data.frame and assign it to the variable 'verbal_baseline' verbal_baseline <- impact $ verbal_memory_baseline verbal_baseline 1 2 ## [1] 95 90 87 84 92 89 78 97 93 90 89 97 79 86 85 85 98 95 96 92 79 85 97 ## [24] 89 75 75 84 93 88 97 93 96 84 89 95 95 97 95 92 95 1 2 # Plot a histogram of the verbal_baseline variable that you have just created hist ( verbal_baseline , main = 'Distribution of verbal memory baseline scores' , xlab = 'score' , ylab = 'frequency' ) Let us go wine tasting (red wine) 1 2 3 4 5 # Read in the data set and assign to the object red_wine_data <- readWorksheetFromFile ( 'A Hands-on Introduction to Statistics with R.xls' , sheet = 'red_wine_data' , header = TRUE , startCol = 1 , startRow = 1 ) # This will print the data set in the console head ( red_wine_data ) 1 2 3 4 5 6 7 ## subject condition Ratings ## 1 1 Australia 77 ## 2 2 Australia 82 ## 3 3 Australia 75 ## 4 4 Australia 92 ## 5 5 Australia 83 ## 6 6 Australia 75 1 2 3 # Print basic statistical properties of the red_wine_data data.frame. Use the describe() function #describe(red_wine_data) summary ( red_wine_data ) 1 2 3 4 5 6 7 ## subject condition Ratings ## Min. : 1.0 Length:400 Min. :39.00 ## 1st Qu.:100.8 Class :character 1st Qu.:67.00 ## Median :200.5 Mode :character Median :74.00 ## Mean :200.5 Mean :73.94 ## 3rd Qu.:300.2 3rd Qu.:81.00 ## Max. :400.0 Max. :98.00 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 # Split the data.frame in subsets for each country and assign these subsets to the variables below red_usa <- subset ( red_wine_data , red_wine_data $ condition == 'USA' ) red_france <- subset ( red_wine_data , red_wine_data $ condition == 'France' ) red_australia <- subset ( red_wine_data , red_wine_data $ condition == 'Australia' ) red_argentina <- subset ( red_wine_data , red_wine_data $ condition == 'Argentina' ) # Select only the Ratings variable for each of these subsets and assign them to the variables below red_ratings_usa <- red_usa $ Ratings red_ratings_france <- red_france $ Ratings red_ratings_australia <- red_australia $ Ratings red_ratings_argentina <- red_argentina $ Ratings ## Create a 2 by 2 matrix of histograms # Organize the histograms so that they are structured in a 2 by 2 matrix. par ( mfrow = c ( 2 , 2 )) # Plot four histograms, one for each subject hist ( red_ratings_usa ) hist ( red_ratings_france ) hist ( red_ratings_australia ) hist ( red_ratings_argentina ) Let us go wine tasting (white wine) 1 2 3 4 5 # Read in the data set and assign to the object white_wine_data <- readWorksheetFromFile ( 'A Hands-on Introduction to Statistics with R.xls' , sheet = 'white_wine_data' , header = TRUE , startCol = 1 , startRow = 1 ) # This will print the data set in the console head ( white_wine_data ) 1 2 3 4 5 6 7 ## condition Ratings ## 1 Australia 85.6 ## 2 Australia 85.6 ## 3 Australia 85.6 ## 4 Australia 85.6 ## 5 Australia 85.6 ## 6 Australia 85.6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 # Assign the scores for each country to a variable white_ratings_france <- subset ( white_wine_data , white_wine_data $ condition == 'France' ) $ Ratings white_ratings_argentina <- subset ( white_wine_data , white_wine_data $ condition == 'Argentina' ) $ Ratings white_ratings_australia <- subset ( white_wine_data , white_wine_data $ condition == 'Australia' ) $ Ratings white_ratings_usa <- subset ( white_wine_data , white_wine_data $ condition == 'USA' ) $ Ratings # Plot a histogram for each of the countries # Organize the histograms so that they are structured in a 2 by 2 matrix. par ( mfrow = c ( 2 , 2 )) hist ( white_ratings_usa , main = 'USA white ratings' , xlab = 'score' ) hist ( white_ratings_australia , main = 'Australia white ratings' , xlab = 'score' ) hist ( white_ratings_argentina , main = 'Argentina white ratings' , xlab = 'score' ) hist ( white_ratings_france , main = 'France white ratings' , xlab = 'score' )","title":"Histograms and Distributions"},{"location":"a_hands-on_introduction_to_statistics_with_r_course_one_introduction/#scales-of-measurement","text":"Converting a distribution to Z-scale 1 2 3 4 # Read in the data set and assign to the object ratings_australia <- readWorksheetFromFile ( 'A Hands-on Introduction to Statistics with R.xls' , sheet = 'ratings_australia' , header = TRUE , startCol = 1 , startRow = 1 ) ratings_australia <- as.vector ( ratings_australia $ ratings_australia ) 1 2 # Print the ratings for the Australian red wine ratings_australia 1 2 3 4 5 ## [1] 77 82 75 92 83 75 84 86 85 79 92 84 77 65 89 81 81 88 87 85 87 86 82 ## [24] 67 85 81 80 71 78 84 91 80 84 81 71 78 78 81 89 86 80 79 86 85 76 76 ## [47] 84 86 80 87 84 77 83 73 91 95 78 74 85 80 98 81 86 81 76 82 68 91 82 ## [70] 96 84 76 85 74 72 83 78 81 82 77 77 80 89 70 85 83 88 79 84 83 77 89 ## [93] 89 86 92 85 72 77 72 78 1 2 3 4 5 6 7 8 9 10 11 # Convert these ratings to Z-scores. Use the `scale()` function z_scores_australia <- scale ( ratings_australia ) # Plot both the original data and the scaled data in histograms next to each other par ( mfrow = c ( 1 , 2 )) # Plot the histogram for the original scores hist ( ratings_australia ) # Plot the histogram for the Z-scores hist ( z_scores_australia )","title":"Scales of Measurement"},{"location":"a_hands-on_introduction_to_statistics_with_r_course_one_introduction/#measures-of-central-tendency","text":"The mean of a Fibonacci sequence 1 2 3 4 5 6 # create a vector that contains the Fibonacci elements fibonacci <- c ( 0 , 1 , 1 , 2 , 3 , 5 , 8 , 13 ) # calculate the mean manually. Use the sum() and the length() functions mean <- sum ( fibonacci ) / length ( fibonacci ) mean 1 ## [1] 4.125 1 2 3 # calculate the mean the easy way mean_check <- mean ( fibonacci ) mean_check 1 ## [1] 4.125 Setting up histograms 1 2 3 4 # Read in the data set and assign to the object wine_data <- readWorksheetFromFile ( 'A Hands-on Introduction to Statistics with R.xls' , sheet = 'wine_data' , header = TRUE , startCol = 1 , startRow = 1 ) head ( wine_data ) 1 2 3 4 5 6 7 ## condition Ratings ## 1 Red 77 ## 2 Red 82 ## 3 Red 75 ## 4 Red 92 ## 5 Red 83 ## 6 Red 75 1 2 3 4 5 6 7 8 # create the two subsets red_wine <- subset ( wine_data , wine_data $ condition == 'Red' ) white_wine <- subset ( wine_data , wine_data $ condition == 'White' ) # Plot the histograms of the ratings of both subsets par ( mfrow = c ( 1 , 2 )) hist ( red_wine $ Ratings , main = 'Shiraz' , xlab = 'Ratings' ) hist ( white_wine $ Ratings , main = 'Pinot Grigio' , xlab = 'Ratings' ) Robustness to outliers 1 2 3 4 5 6 7 8 9 # create the outlier and add it to the dataset outlier <- data.frame ( condition = 'Red' , Ratings = 0 ) red_wine_extreme <- rbind ( red_wine , outlier ) # calculate the difference in means and display it afterwards diff_means <- mean ( red_wine $ Ratings ) - mean ( red_wine_extreme $ Ratings ) diff_means 1 ## [1] 0.8093069 1 2 3 4 # calculate the difference in medians and display it afterwards diff_medians <- median ( red_wine $ Ratings ) - median ( red_wine_extreme $ Ratings ) diff_medians 1 ## [1] 0","title":"Measures of Central Tendency"},{"location":"a_hands-on_introduction_to_statistics_with_r_course_one_introduction/#measures-of-variability","text":"Michael Jordan\u2019s first NBA season - Global overview 1 2 3 4 # Read in the data set and assign to the object data_jordan <- readWorksheetFromFile ( 'A Hands-on Introduction to Statistics with R.xls' , sheet = 'data_jordan' , header = TRUE , startCol = 1 , startRow = 1 ) head ( data_jordan ) 1 2 3 4 5 6 7 ## game points ## 1 1 16 ## 2 2 21 ## 3 3 37 ## 4 4 25 ## 5 5 17 ## 6 6 25 1 2 3 4 5 # Make a scatterplot of the data on which a horizontal line with height equal to the mean is drawn. mean_jordan <- mean ( data_jordan $ points ) plot ( data_jordan $ game , data_jordan $ points , main = '1st NBA season of Michael Jordan' ) abline ( h = mean_jordan ) Michael Jordan\u2019s first NBA season - Calculate the variance manually 1 2 3 4 5 6 7 8 9 # Calculate the differences with respect to the mean diff <- data_jordan $ points - mean ( data_jordan $ points ) # Calculate the squared differences squared_diff <- diff^2 # Combine all pieces of the puzzle in order to acquire the variance variance <- sum ( squared_diff ) / ( length ( data_jordan $ points ) - 1 ) variance 1 ## [1] 66.73427 1 2 # Compare your result to the correct solution. You can find the correct solution by calculating it with the `var()` function. var ( data_jordan $ points ) 1 ## [1] 66.73427","title":"Measures of Variability"},{"location":"a_hands-on_introduction_to_statistics_with_r_course_seven_moderation_and_mediation/","text":"Foreword Output options: the \u2018tango\u2019 syntax and the \u2018readable\u2019 theme. Snippets only. An introduction to moderation \u00b6 Data exploration 1 2 3 4 5 6 7 library ( psych ) # Summary statistics describeBy ( mod , mod $ condition ) # Create a boxplot of the data boxplot ( mod $ iq ~ mod $ condition , main = 'Boxplot' , ylab = 'IQ' , xlab = 'Group condition' ) Calculate correlations 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 # Create subsets of the three groups # Make the subset for the group condition = 'control' mod_control <- subset ( mod , condition == 'control' ) # Make the subset for the group condition = 'threat1' mod_threat1 <- subset ( mod , condition == 'threat1' ) # Make the subset for the group condition = 'threat2' mod_threat2 <- subset ( mod , condition == 'threat2' ) # Calculate the correlations cor ( mod_control $ iq , mod_control $ wm , method = 'pearson' ) cor ( mod_threat1 $ iq , mod_threat1 $ wm , method = 'pearson' ) cor ( mod_threat2 $ iq , mod_threat2 $ wm , method = 'pearson' ) Model with and without moderation A moderator variable (Z) will enhance a regression model if the relationship between X and Y varies as a function of Z. Experimental research The manipulation of an X causes change in a Y. A moderator variable (Z) implies that the effect of the X on the Y is NOT consistent across the distribution of Z. Correlational research Assume a correlation between X and Y. A moderator variable (Z) implies that the correlation between X and Y is NOT consistent across the distribution of Z. If both X and Z are continuous: Y \u2004=\u2004 \u03b2 0 \u2005+\u2005 \u03b2 1 X \u2005+\u2005 \u03b2 2 Z \u2005+\u2005 \u03b2 3 ( X \u2005*\u2005 Z )+ \u03f5 If X is categorical and Z is continuous: Y \u2004=\u2004 \u03b2 0 \u2005+\u2005 \u03b2 1 ( D 1 )+ \u03b2 2 ( D 2 )+ \u03b2 3 Z \u2005+\u2005 \u03b2 4 ( D 1 \u2005*\u2005 Z )+ \u03b2 5 ( D 2 \u2005*\u2005 Z )+ \u03f5 Consult the PDF for performing tests. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 # Model without moderation (tests for 'first-order effects') model_1 <- lm ( mod $ iq ~ mod $ wm + mod $ d1 + mod $ d2 ) # Make a summary of model_1 summary ( model_1 ) # Create new predictor variables wm_d1 <- mod $ wm * mod $ d1 wm_d2 <- mod $ wm * mod $ d2 # Model with moderation model_2 <- lm ( mod $ iq ~ mod $ wm + mod $ d1 + mod $ d2 + wm_d1 + wm_d2 ) # Make a summary of model_2 summary ( model_2 ) Model comparison 1 2 # Compare model_1 and model_2 anova ( model_1 , model_2 ) Scatterplot 1 2 3 4 5 6 7 8 9 # Choose colors to represent the points by group color <- c ( 'red' , 'green' , 'blue' ) # Illustration of the first-order effects of working memory on IQ ggplot ( mod , aes ( x = wm , y = iq )) + geom_smooth ( method = 'lm' , color = 'black' ) + geom_point ( aes ( color = condition )) # Illustration of the moderation effect of working memory on IQ ggplot ( mod , aes ( x = wm , y = iq )) + geom_smooth ( aes ( group = condition ), method = 'lm' , se = T , color = 'black' , fullrange = T ) + geom_point ( aes ( color = condition )) Centering data 1 2 3 4 5 # Define wm_center wm_center <- mod $ wm - mean ( mod $ wm ) # Compare with the variable wm.centered all.equal ( wm_center , mod $ wm.centered ) An introduction to centering predictors \u00b6 Centering versus no centering To center means to put in deviation form: X C \u2004=\u2004 X \u2005\u2212\u2005 M . Convert raw scores to deviation scores. Two reason: Conceptual: regression constant will be more meaningful. Statistical: avoid multicolinearity. Conceptual. The intercept, \u03b2 0 , is the predicted score on Y (child\u2019s verbal ability) when all predictors (X = mother\u2019s vocabulary, Z = child\u2019s age) are zero. If X = zero or Z = zero is meaningless, or impossible, then \u03b2 0 will be difficult to interpret. In contrast, if X = zero and Z = zero, are the average then \u03b2 0 is easy to interpret. The regression coefficient \u03b2 1 is the slope for X assuming an average score on Z. No moderation effect implies that \u03b2 1 is consistent across the entire distribution of Z. In contrast, a moderation effect implies that \u03b2 1 is NOT consistent across the entire distribution of Z. Where in the distribution of Z is \u03b2 1 most representative of the relationship between X & Y? Statistical The predictors, X and Z, can become highly correlated with the product, (X*Z). Multicolinearity: when two predictor variables in a GLM are so highly correlated that they are essentially redundant and it becomes difficult to estimate \u03b2 values associated with each predictor. 1 2 3 4 5 # Model without moderation and with centered data model_1_centered <- lm ( mod $ iq ~ mod $ wm.centered + mod $ d1 + mod $ d2 ) # Make a summary of model_1_centered summary ( model_1_centered ) Centering versus no centering with moderation 1 2 3 4 5 6 7 8 9 # Create new predictor variables wm_d1_centered <- mod $ wm.centered * mod $ d1 wm_d2_centered <- mod $ wm.centered * mod $ d2 # Define model_2_centered model_2_centered <- lm ( mod $ iq ~ mod $ wm.centered + mod $ d1 + mod $ d2 + wm_d1_centered + wm_d2_centered ) # Make a summary of model_2_centered summary ( model_2_centered ) Model comparison 1 2 3 4 5 # Compare model_1_centered and model_2_centered anova ( model_1_centered , model_2_centered ) # Compare model_1 and model_2 anova ( model_1 , model_2 ) Some correlations 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 # Calculate the correlations between working memory capacity and the product terms cor_wmd1 <- cor ( mod $ wm , wm_d1 ) cor_wmd2 <- cor ( mod $ wm , wm_d2 ) cor_wmd1_centered <- cor ( mod $ wm.centered , wm_d1_centered ) cor_wmd2_centered <- cor ( mod $ wm.centered , wm_d2_centered ) # Calculate the correlations between the dummy variables and the product terms cor_d1d1 <- cor ( mod $ d1 , wm_d1 ) cor_d2d2 <- cor ( mod $ d2 , wm_d2 ) cor_d1d1_centered <- cor ( mod $ d1 , wm_d1_centered ) cor_d2d2_centered <- cor ( mod $ d2 , wm_d2_centered ) # correlations rbind ( c ( cor_wmd1 , cor_wmd2 ), c ( cor_wmd1_centered , cor_wmd2_centered )) rbind ( c ( cor_d1d1 , cor_d2d2 ), c ( cor_d1d1_centered , cor_d2d2_centered )) An introduction to mediation \u00b6 Model with and without mediation X: Experimental manipulation (Stereotype threat). Y: Behavioral outcome (IQ score). M: Mediator (Mechanism = Working memory capacity). A mediation analysis is typically conducted to better understand an observed effect of a correlation between X and Y. Why, and how, does stereotype threat influence IQ test performance? If X and Y are correlated then we can use regression to predict Y from X Y \u2004=\u2004 \u03b2 0 \u2005+\u2005 \u03b2 1 X \u2005+\u2005 \u03f5 If X and Y are correlated BECAUSE of the mediator M, then (X \u00e0 M \u00e0 Y): Y \u2004=\u2004 \u03b2 0 \u2005+\u2005 \u03b2 1 M \u2005+\u2005 \u03f5 M \u2004=\u2004 \u03b2 0 \u2005+\u2005 \u03b2 1 X \u2005+\u2005 \u03f5 If X and Y are correlated BECAUSE of the mediator M, and: Y \u2004=\u2004 \u03b2 0 \u2005+\u2005 \u03b2 1 M \u2005+\u2005 \u03b2 2 X \u2005+\u2005 \u03f5 A mediator variable (M) accounts for some (partial) or all of the relationship between X and Y. CAUTION: Correlation does not imply causation! In other words, there is a BIG difference between statistical mediation and true causal mediation. Data exploration 1 2 3 4 5 # Summary statistics describeBy ( med , med $ condition ) # Create a boxplot of the data boxplot ( med $ iq ~ med $ condition , main = 'Boxplot' , xlab = 'Group condition' , ylab = 'IQ' ) Run 3 regression models on the data 1 2 3 4 5 6 7 8 9 10 11 12 13 14 # Run the three regression models # outcome ~ predictor model_yx <- lm ( med $ iq ~ med $ condition ) # mediator ~ predictor model_mx <- lm ( med $ wm ~ med $ condition ) # outcome ~ predictor + mediator model_yxm <- lm ( med $ iq ~ med $ condition + med $ wm ) # Make a summary of the three models summary ( model_yx ) summary ( model_mx ) summary ( model_yxm ) Sobel test 1 2 3 4 5 # Compare the previous results to the output of the sobel function # sobel(pred,med,out) model_all <- sobel ( med $ condition , med $ wm , med $ iq ) model_all The Sobel test is a method of testing the significance of a mediation effect. Consult the PDF.","title":"Statistics with R, Course Seven, Moderation and Mediation"},{"location":"a_hands-on_introduction_to_statistics_with_r_course_seven_moderation_and_mediation/#an-introduction-to-moderation","text":"Data exploration 1 2 3 4 5 6 7 library ( psych ) # Summary statistics describeBy ( mod , mod $ condition ) # Create a boxplot of the data boxplot ( mod $ iq ~ mod $ condition , main = 'Boxplot' , ylab = 'IQ' , xlab = 'Group condition' ) Calculate correlations 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 # Create subsets of the three groups # Make the subset for the group condition = 'control' mod_control <- subset ( mod , condition == 'control' ) # Make the subset for the group condition = 'threat1' mod_threat1 <- subset ( mod , condition == 'threat1' ) # Make the subset for the group condition = 'threat2' mod_threat2 <- subset ( mod , condition == 'threat2' ) # Calculate the correlations cor ( mod_control $ iq , mod_control $ wm , method = 'pearson' ) cor ( mod_threat1 $ iq , mod_threat1 $ wm , method = 'pearson' ) cor ( mod_threat2 $ iq , mod_threat2 $ wm , method = 'pearson' ) Model with and without moderation A moderator variable (Z) will enhance a regression model if the relationship between X and Y varies as a function of Z. Experimental research The manipulation of an X causes change in a Y. A moderator variable (Z) implies that the effect of the X on the Y is NOT consistent across the distribution of Z. Correlational research Assume a correlation between X and Y. A moderator variable (Z) implies that the correlation between X and Y is NOT consistent across the distribution of Z. If both X and Z are continuous: Y \u2004=\u2004 \u03b2 0 \u2005+\u2005 \u03b2 1 X \u2005+\u2005 \u03b2 2 Z \u2005+\u2005 \u03b2 3 ( X \u2005*\u2005 Z )+ \u03f5 If X is categorical and Z is continuous: Y \u2004=\u2004 \u03b2 0 \u2005+\u2005 \u03b2 1 ( D 1 )+ \u03b2 2 ( D 2 )+ \u03b2 3 Z \u2005+\u2005 \u03b2 4 ( D 1 \u2005*\u2005 Z )+ \u03b2 5 ( D 2 \u2005*\u2005 Z )+ \u03f5 Consult the PDF for performing tests. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 # Model without moderation (tests for 'first-order effects') model_1 <- lm ( mod $ iq ~ mod $ wm + mod $ d1 + mod $ d2 ) # Make a summary of model_1 summary ( model_1 ) # Create new predictor variables wm_d1 <- mod $ wm * mod $ d1 wm_d2 <- mod $ wm * mod $ d2 # Model with moderation model_2 <- lm ( mod $ iq ~ mod $ wm + mod $ d1 + mod $ d2 + wm_d1 + wm_d2 ) # Make a summary of model_2 summary ( model_2 ) Model comparison 1 2 # Compare model_1 and model_2 anova ( model_1 , model_2 ) Scatterplot 1 2 3 4 5 6 7 8 9 # Choose colors to represent the points by group color <- c ( 'red' , 'green' , 'blue' ) # Illustration of the first-order effects of working memory on IQ ggplot ( mod , aes ( x = wm , y = iq )) + geom_smooth ( method = 'lm' , color = 'black' ) + geom_point ( aes ( color = condition )) # Illustration of the moderation effect of working memory on IQ ggplot ( mod , aes ( x = wm , y = iq )) + geom_smooth ( aes ( group = condition ), method = 'lm' , se = T , color = 'black' , fullrange = T ) + geom_point ( aes ( color = condition )) Centering data 1 2 3 4 5 # Define wm_center wm_center <- mod $ wm - mean ( mod $ wm ) # Compare with the variable wm.centered all.equal ( wm_center , mod $ wm.centered )","title":"An introduction to moderation"},{"location":"a_hands-on_introduction_to_statistics_with_r_course_seven_moderation_and_mediation/#an-introduction-to-centering-predictors","text":"Centering versus no centering To center means to put in deviation form: X C \u2004=\u2004 X \u2005\u2212\u2005 M . Convert raw scores to deviation scores. Two reason: Conceptual: regression constant will be more meaningful. Statistical: avoid multicolinearity. Conceptual. The intercept, \u03b2 0 , is the predicted score on Y (child\u2019s verbal ability) when all predictors (X = mother\u2019s vocabulary, Z = child\u2019s age) are zero. If X = zero or Z = zero is meaningless, or impossible, then \u03b2 0 will be difficult to interpret. In contrast, if X = zero and Z = zero, are the average then \u03b2 0 is easy to interpret. The regression coefficient \u03b2 1 is the slope for X assuming an average score on Z. No moderation effect implies that \u03b2 1 is consistent across the entire distribution of Z. In contrast, a moderation effect implies that \u03b2 1 is NOT consistent across the entire distribution of Z. Where in the distribution of Z is \u03b2 1 most representative of the relationship between X & Y? Statistical The predictors, X and Z, can become highly correlated with the product, (X*Z). Multicolinearity: when two predictor variables in a GLM are so highly correlated that they are essentially redundant and it becomes difficult to estimate \u03b2 values associated with each predictor. 1 2 3 4 5 # Model without moderation and with centered data model_1_centered <- lm ( mod $ iq ~ mod $ wm.centered + mod $ d1 + mod $ d2 ) # Make a summary of model_1_centered summary ( model_1_centered ) Centering versus no centering with moderation 1 2 3 4 5 6 7 8 9 # Create new predictor variables wm_d1_centered <- mod $ wm.centered * mod $ d1 wm_d2_centered <- mod $ wm.centered * mod $ d2 # Define model_2_centered model_2_centered <- lm ( mod $ iq ~ mod $ wm.centered + mod $ d1 + mod $ d2 + wm_d1_centered + wm_d2_centered ) # Make a summary of model_2_centered summary ( model_2_centered ) Model comparison 1 2 3 4 5 # Compare model_1_centered and model_2_centered anova ( model_1_centered , model_2_centered ) # Compare model_1 and model_2 anova ( model_1 , model_2 ) Some correlations 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 # Calculate the correlations between working memory capacity and the product terms cor_wmd1 <- cor ( mod $ wm , wm_d1 ) cor_wmd2 <- cor ( mod $ wm , wm_d2 ) cor_wmd1_centered <- cor ( mod $ wm.centered , wm_d1_centered ) cor_wmd2_centered <- cor ( mod $ wm.centered , wm_d2_centered ) # Calculate the correlations between the dummy variables and the product terms cor_d1d1 <- cor ( mod $ d1 , wm_d1 ) cor_d2d2 <- cor ( mod $ d2 , wm_d2 ) cor_d1d1_centered <- cor ( mod $ d1 , wm_d1_centered ) cor_d2d2_centered <- cor ( mod $ d2 , wm_d2_centered ) # correlations rbind ( c ( cor_wmd1 , cor_wmd2 ), c ( cor_wmd1_centered , cor_wmd2_centered )) rbind ( c ( cor_d1d1 , cor_d2d2 ), c ( cor_d1d1_centered , cor_d2d2_centered ))","title":"An introduction to centering predictors"},{"location":"a_hands-on_introduction_to_statistics_with_r_course_seven_moderation_and_mediation/#an-introduction-to-mediation","text":"Model with and without mediation X: Experimental manipulation (Stereotype threat). Y: Behavioral outcome (IQ score). M: Mediator (Mechanism = Working memory capacity). A mediation analysis is typically conducted to better understand an observed effect of a correlation between X and Y. Why, and how, does stereotype threat influence IQ test performance? If X and Y are correlated then we can use regression to predict Y from X Y \u2004=\u2004 \u03b2 0 \u2005+\u2005 \u03b2 1 X \u2005+\u2005 \u03f5 If X and Y are correlated BECAUSE of the mediator M, then (X \u00e0 M \u00e0 Y): Y \u2004=\u2004 \u03b2 0 \u2005+\u2005 \u03b2 1 M \u2005+\u2005 \u03f5 M \u2004=\u2004 \u03b2 0 \u2005+\u2005 \u03b2 1 X \u2005+\u2005 \u03f5 If X and Y are correlated BECAUSE of the mediator M, and: Y \u2004=\u2004 \u03b2 0 \u2005+\u2005 \u03b2 1 M \u2005+\u2005 \u03b2 2 X \u2005+\u2005 \u03f5 A mediator variable (M) accounts for some (partial) or all of the relationship between X and Y. CAUTION: Correlation does not imply causation! In other words, there is a BIG difference between statistical mediation and true causal mediation. Data exploration 1 2 3 4 5 # Summary statistics describeBy ( med , med $ condition ) # Create a boxplot of the data boxplot ( med $ iq ~ med $ condition , main = 'Boxplot' , xlab = 'Group condition' , ylab = 'IQ' ) Run 3 regression models on the data 1 2 3 4 5 6 7 8 9 10 11 12 13 14 # Run the three regression models # outcome ~ predictor model_yx <- lm ( med $ iq ~ med $ condition ) # mediator ~ predictor model_mx <- lm ( med $ wm ~ med $ condition ) # outcome ~ predictor + mediator model_yxm <- lm ( med $ iq ~ med $ condition + med $ wm ) # Make a summary of the three models summary ( model_yx ) summary ( model_mx ) summary ( model_yxm ) Sobel test 1 2 3 4 5 # Compare the previous results to the output of the sobel function # sobel(pred,med,out) model_all <- sobel ( med $ condition , med $ wm , med $ iq ) model_all The Sobel test is a method of testing the significance of a mediation effect. Consult the PDF.","title":"An introduction to mediation"},{"location":"a_hands-on_introduction_to_statistics_with_r_course_six_multiple_regression/","text":"Foreword Output options: the \u2018tango\u2019 syntax and the \u2018readable\u2019 theme. Snippets only. A gentle introduction to the principles of multiple regression \u00b6 Multiple regression: visualization of the relationships 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 # Perform the two single regressions and save them in a variable #model_years <- lm(salary ~ years, data = fs) #model_pubs <- lm(salary ~ pubs, data = fs) model_years <- lm ( fs $ salary ~ fs $ years ) model_pubs <- lm ( fs $ salary ~ fs $ pubs ) # Plot both enhanced scatter plots in one plot matrix of 1 by 2 par ( mfrow = c ( 1 , 2 )) #plot(fs$years, fs$salary, main = 'plot_years', xlab = 'years', ylab = 'salary') plot ( fs $ salary ~ fs $ years , main = 'plot_years' , xlab = 'years' , ylab = 'salary' ) abline ( model_years ) #plot(fs$pubs, fs$salary, main = 'plot_pubs', xlab = 'pubs', ylab = 'salary') plot ( fs $ salary ~ fs $ pubs , main = 'plot_pubs' , xlab = 'pubs' , ylab = 'salary' ) abline ( model_pubs ) Multiple regression: model selection 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 # Do a single regression of salary onto years of experience and check the output model_1 <- lm ( fs $ salary ~ fs $ years ) summary ( model_1 ) # Do a multiple regression of salary onto years of experience and numbers of publications and check the output model_2 <- lm ( fs $ salary ~ fs $ years + fs $ pubs ) summary ( model_2 ) # Save the R squared of both models in preliminary variables preliminary_model_1 <- summary ( model_1 ) $ r.squared preliminary_model_2 <- summary ( model_2 ) $ r.squared # Round them off while you save them in new variables r_squared <- c () r_squared[1] <- round ( preliminary_model_1 , 3 ) r_squared[2] <- round ( preliminary_model_2 , 3 ) # Print out the vector to see both R squared coefficients r_squared Multiple regression: beware of redundancy 1 2 3 4 5 6 7 8 9 # Do multiple regression and check the regression output model_3 <- lm ( fs $ salary ~ fs $ years + fs $ pubs + fs $ age ) summary ( model_3 ) # Round off the R squared coefficients and save the result in the vector (in one step!) r_squared[3] <- round ( summary ( model_3 ) $ r.squared , 3 ) # Print out the vector in order to display all R squared coefficients simultaneously r_squared Intuition behind estimation of multiple regression coefficients \u00b6 Definition of matrices 1 2 3 4 5 6 7 8 # Construction of 3 by 8 matrix r that contains the numbers 1 up to 24 r <- matrix ( seq ( 1 , 24 ), 3 ) # Construction of 3 by 8 matrix s that contains the numbers 21 up to 44 s <- matrix ( seq ( 21 , 44 ), 3 ) # Take the transpose t of matrix r t <- t ( r ) Addition, subtraction and multiplication of matrices 1 2 3 4 5 6 7 8 # Compute the sum of matrices r and s operation_1 <- r + s # Compute the difference between matrices r and s operation_2 <- r - s # Multiply matrices t and s operation_3 <- t %*% s Row vector of sums 1 2 3 4 5 6 7 8 # The raw dataframe `X` is already loaded in. X # Construction of 1 by 10 matrix I of which the elements are all 1 I <- matrix ( rep ( 1 , 10 ), 1 , 10 ) # Compute the row vector of sums t_mat <- I %*% X Row vector of means and matrix of means 1 2 3 4 5 6 7 8 9 10 11 12 13 # The data matrix `X` and the row vector of sums (`T`) are saved and can be used. # Number of observations n = 10 # Compute the row vector of means # you summed up the row, you divide by the nrow to compute the average M <- t_mat / 10 # Construction of 10 by 1 matrix J of which the elements are all 1 J <- matrix ( rep ( 1 , 10 ), 10 , 1 ) # Compute the matrix of means MM <- J %*% M Matrix of deviation scores 1 2 3 4 # The previously generated matrices X, M and MM do not need to be constructed again but are saved and can be used. # Matrix of deviation scores D D <- X - MM Sum of squares and sum of cross products matrix 1 2 3 4 # The previously generated matrices X, M, MM and D do not need to be constructed again but are saved and can be used. # Sum of squares and sum of cross products matrix S <- t ( D ) %*% D Calculating the correlation matrix 1 2 3 4 5 6 7 8 9 10 11 # The previously generated matrices X, M, MM, D and S do not need to be constructed again but are saved and can be used. n = 10 # Construct the variance-covariance matrix C <- S * 1 / n # Generate the standard deviations matrix SD <- diag ( x = diag ( C ) ^ ( 1 / 2 ), nrow = 3 , ncol = 3 ) # Compute the correlation matrix R <- solve ( SD ) %*% C %*% solve ( SD ) Dummy coding \u00b6 Starting off 1 2 # Summary statistics describeBy ( fs , fs $ dept ) A system to code categorical predictors in a regression analysis Suppose we have a categorical vector of observations. The vector counts 4 distinct groups. Here is how to assign the dummy variables: ProfID Group Pubs D1 D2 D3 NU Cognitive 83 0 0 0 ZH Clinical 74 1 0 0 MK Developmental 80 0 1 0 RH Social 68 0 0 1 Summary statistics: Group M SD N Cognitive 93.31 29.48 13 Clinical 60.67 11.12 8 Developmental 103.5 23.64 6 Social 70.13 21.82 9 Model: Y \u2004=\u2004 \u03b2 0 \u2005+\u2005 \u03b2 1 ( D 1 )+ \u03b2 2 ( D 2 )+ \u03b2 3 ( D 3 )+ \u03f5 Coefficient: B SE B t p 93.31 6.5 0 14.37 <.001 D1 (Clinical) -32.64 10.16 -0.51 -3.21 0.003 D2 (Devel) 10.19 11.56 0.14 0.88 0.384 D3 (Social) -23.18 10.52 -0.35 -2.2 0.035 Creating dummy variables (1) 1 2 3 4 5 6 7 8 9 10 11 12 # Create the dummy variables dept_code <- dummy.code ( fs $ dept ) dept_code # Merge the dataset in an extended dataframe extended_fs <- cbind ( fs , dept_code ) # Look at the extended dataframe extended_fs # Provide summary statistics summary ( extended_fs ) Creating dummy variables (2) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 # Regress salary against years and publications model <- lm ( fs $ salary ~ fs $ years + fs $ pubs ) # Apply the summary function to get summarized results for model summary ( model ) # Compute the confidence intervals for model confint ( model ) # Create dummies for the categorical variable fs$dept by using the C() function dept_code <- C ( fs $ dept , treatment ) # Regress salary against years, publications and department model_dummy <- lm ( fs $ salary ~ fs $ years + fs $ pubs + dept_code ) # Apply the summary function to get summarized results for model_dummy summary ( model_dummy ) # Compute the confidence intervals for model_dummy confint ( model_dummy ) Model selection: ANOVA 1 2 # Compare model 4 with model3 anova ( model , model_dummy ) Discrepancy between actual and predicted means 1 2 # Actual means of fs$salary tapply ( fs $ salary , fs $ dept , mean ) Unweighted effects coding Consult the PDF for \u2018Unweighted Effects Coding\u2019. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 # Number of levels fs $ dept # Factorize the categorical variable fs$dept and name the factorized variable dept.f dept.f <- factor ( fs $ dept ) # Assign the 3 levels generated in step 2 to dept.f contrasts ( dept.f ) <- contr.sum ( 3 ) # Regress salary against dept.f model_unweighted <- lm ( fs $ salary ~ dept.f ) # Apply the summary() function summary ( model_unweighted ) Weighted effects coding Consult \u2018Weighted Effects Coding\u2019. 1 2 3 4 5 6 7 8 9 10 11 # Factorize the categorical variable fs$dept and name the factorized variable dept.g dept.g <- factor ( fs $ dept ) # Assign the weights matrix to dept.g contrasts ( dept.g ) <- weights # Regress salary against dept.f and apply the summary() function model_weighted <- lm ( fs $ salary ~ dept.g ) # Apply the summary() function summary ( model_weighted )","title":"Statistics with R, Course Six, Multiple Regression"},{"location":"a_hands-on_introduction_to_statistics_with_r_course_six_multiple_regression/#a-gentle-introduction-to-the-principles-of-multiple-regression","text":"Multiple regression: visualization of the relationships 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 # Perform the two single regressions and save them in a variable #model_years <- lm(salary ~ years, data = fs) #model_pubs <- lm(salary ~ pubs, data = fs) model_years <- lm ( fs $ salary ~ fs $ years ) model_pubs <- lm ( fs $ salary ~ fs $ pubs ) # Plot both enhanced scatter plots in one plot matrix of 1 by 2 par ( mfrow = c ( 1 , 2 )) #plot(fs$years, fs$salary, main = 'plot_years', xlab = 'years', ylab = 'salary') plot ( fs $ salary ~ fs $ years , main = 'plot_years' , xlab = 'years' , ylab = 'salary' ) abline ( model_years ) #plot(fs$pubs, fs$salary, main = 'plot_pubs', xlab = 'pubs', ylab = 'salary') plot ( fs $ salary ~ fs $ pubs , main = 'plot_pubs' , xlab = 'pubs' , ylab = 'salary' ) abline ( model_pubs ) Multiple regression: model selection 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 # Do a single regression of salary onto years of experience and check the output model_1 <- lm ( fs $ salary ~ fs $ years ) summary ( model_1 ) # Do a multiple regression of salary onto years of experience and numbers of publications and check the output model_2 <- lm ( fs $ salary ~ fs $ years + fs $ pubs ) summary ( model_2 ) # Save the R squared of both models in preliminary variables preliminary_model_1 <- summary ( model_1 ) $ r.squared preliminary_model_2 <- summary ( model_2 ) $ r.squared # Round them off while you save them in new variables r_squared <- c () r_squared[1] <- round ( preliminary_model_1 , 3 ) r_squared[2] <- round ( preliminary_model_2 , 3 ) # Print out the vector to see both R squared coefficients r_squared Multiple regression: beware of redundancy 1 2 3 4 5 6 7 8 9 # Do multiple regression and check the regression output model_3 <- lm ( fs $ salary ~ fs $ years + fs $ pubs + fs $ age ) summary ( model_3 ) # Round off the R squared coefficients and save the result in the vector (in one step!) r_squared[3] <- round ( summary ( model_3 ) $ r.squared , 3 ) # Print out the vector in order to display all R squared coefficients simultaneously r_squared","title":"A gentle introduction to the principles of multiple regression"},{"location":"a_hands-on_introduction_to_statistics_with_r_course_six_multiple_regression/#intuition-behind-estimation-of-multiple-regression-coefficients","text":"Definition of matrices 1 2 3 4 5 6 7 8 # Construction of 3 by 8 matrix r that contains the numbers 1 up to 24 r <- matrix ( seq ( 1 , 24 ), 3 ) # Construction of 3 by 8 matrix s that contains the numbers 21 up to 44 s <- matrix ( seq ( 21 , 44 ), 3 ) # Take the transpose t of matrix r t <- t ( r ) Addition, subtraction and multiplication of matrices 1 2 3 4 5 6 7 8 # Compute the sum of matrices r and s operation_1 <- r + s # Compute the difference between matrices r and s operation_2 <- r - s # Multiply matrices t and s operation_3 <- t %*% s Row vector of sums 1 2 3 4 5 6 7 8 # The raw dataframe `X` is already loaded in. X # Construction of 1 by 10 matrix I of which the elements are all 1 I <- matrix ( rep ( 1 , 10 ), 1 , 10 ) # Compute the row vector of sums t_mat <- I %*% X Row vector of means and matrix of means 1 2 3 4 5 6 7 8 9 10 11 12 13 # The data matrix `X` and the row vector of sums (`T`) are saved and can be used. # Number of observations n = 10 # Compute the row vector of means # you summed up the row, you divide by the nrow to compute the average M <- t_mat / 10 # Construction of 10 by 1 matrix J of which the elements are all 1 J <- matrix ( rep ( 1 , 10 ), 10 , 1 ) # Compute the matrix of means MM <- J %*% M Matrix of deviation scores 1 2 3 4 # The previously generated matrices X, M and MM do not need to be constructed again but are saved and can be used. # Matrix of deviation scores D D <- X - MM Sum of squares and sum of cross products matrix 1 2 3 4 # The previously generated matrices X, M, MM and D do not need to be constructed again but are saved and can be used. # Sum of squares and sum of cross products matrix S <- t ( D ) %*% D Calculating the correlation matrix 1 2 3 4 5 6 7 8 9 10 11 # The previously generated matrices X, M, MM, D and S do not need to be constructed again but are saved and can be used. n = 10 # Construct the variance-covariance matrix C <- S * 1 / n # Generate the standard deviations matrix SD <- diag ( x = diag ( C ) ^ ( 1 / 2 ), nrow = 3 , ncol = 3 ) # Compute the correlation matrix R <- solve ( SD ) %*% C %*% solve ( SD )","title":"Intuition behind estimation of multiple regression coefficients"},{"location":"a_hands-on_introduction_to_statistics_with_r_course_six_multiple_regression/#dummy-coding","text":"Starting off 1 2 # Summary statistics describeBy ( fs , fs $ dept ) A system to code categorical predictors in a regression analysis Suppose we have a categorical vector of observations. The vector counts 4 distinct groups. Here is how to assign the dummy variables: ProfID Group Pubs D1 D2 D3 NU Cognitive 83 0 0 0 ZH Clinical 74 1 0 0 MK Developmental 80 0 1 0 RH Social 68 0 0 1 Summary statistics: Group M SD N Cognitive 93.31 29.48 13 Clinical 60.67 11.12 8 Developmental 103.5 23.64 6 Social 70.13 21.82 9 Model: Y \u2004=\u2004 \u03b2 0 \u2005+\u2005 \u03b2 1 ( D 1 )+ \u03b2 2 ( D 2 )+ \u03b2 3 ( D 3 )+ \u03f5 Coefficient: B SE B t p 93.31 6.5 0 14.37 <.001 D1 (Clinical) -32.64 10.16 -0.51 -3.21 0.003 D2 (Devel) 10.19 11.56 0.14 0.88 0.384 D3 (Social) -23.18 10.52 -0.35 -2.2 0.035 Creating dummy variables (1) 1 2 3 4 5 6 7 8 9 10 11 12 # Create the dummy variables dept_code <- dummy.code ( fs $ dept ) dept_code # Merge the dataset in an extended dataframe extended_fs <- cbind ( fs , dept_code ) # Look at the extended dataframe extended_fs # Provide summary statistics summary ( extended_fs ) Creating dummy variables (2) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 # Regress salary against years and publications model <- lm ( fs $ salary ~ fs $ years + fs $ pubs ) # Apply the summary function to get summarized results for model summary ( model ) # Compute the confidence intervals for model confint ( model ) # Create dummies for the categorical variable fs$dept by using the C() function dept_code <- C ( fs $ dept , treatment ) # Regress salary against years, publications and department model_dummy <- lm ( fs $ salary ~ fs $ years + fs $ pubs + dept_code ) # Apply the summary function to get summarized results for model_dummy summary ( model_dummy ) # Compute the confidence intervals for model_dummy confint ( model_dummy ) Model selection: ANOVA 1 2 # Compare model 4 with model3 anova ( model , model_dummy ) Discrepancy between actual and predicted means 1 2 # Actual means of fs$salary tapply ( fs $ salary , fs $ dept , mean ) Unweighted effects coding Consult the PDF for \u2018Unweighted Effects Coding\u2019. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 # Number of levels fs $ dept # Factorize the categorical variable fs$dept and name the factorized variable dept.f dept.f <- factor ( fs $ dept ) # Assign the 3 levels generated in step 2 to dept.f contrasts ( dept.f ) <- contr.sum ( 3 ) # Regress salary against dept.f model_unweighted <- lm ( fs $ salary ~ dept.f ) # Apply the summary() function summary ( model_unweighted ) Weighted effects coding Consult \u2018Weighted Effects Coding\u2019. 1 2 3 4 5 6 7 8 9 10 11 # Factorize the categorical variable fs$dept and name the factorized variable dept.g dept.g <- factor ( fs $ dept ) # Assign the weights matrix to dept.g contrasts ( dept.g ) <- weights # Regress salary against dept.f and apply the summary() function model_weighted <- lm ( fs $ salary ~ dept.g ) # Apply the summary() function summary ( model_weighted )","title":"Dummy coding"},{"location":"a_hands-on_introduction_to_statistics_with_r_course_three_analysis_of_variance/","text":"Foreword Output options: the \u2018tango\u2019 syntax and the \u2018readable\u2019 theme. Snippets and results. An introduction to ANOVA \u00b6 Working memory experiment We\u2019ll use data from the working memory experiment, which investigates the relationship between the number of training days and a change in IQ. There are four independent groups, each of which trained for a different period of time: 8, 12, 17, or 19 days. The independent variable is the number of training days and the dependent variable is the IQ gain. 1 2 # Print the data set in the console head ( wm ) 1 2 3 4 5 6 7 ## subject condition iq ## 1 1 8 days 12.4 ## 2 2 8 days 11.8 ## 3 3 8 days 14.6 ## 4 4 8 days 7.7 ## 5 5 8 days 15.7 ## 6 6 8 days 11.6 1 2 3 4 library ( psych ) # Summary statistics by all groups (8 sessions, 12 sessions, 17 sessions, 19 sessions) describeBy ( wm , wm $ condition ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 ## ## Descriptive statistics by group ## group: 12 days ## vars n mean sd median trimmed mad min max range skew ## subject 1 20 30.5 5.92 30.50 30.50 7.41 21.0 40.0 19.0 0.00 ## condition* 2 20 NaN NA NA NaN NA Inf -Inf -Inf NA ## iq 3 20 11.7 2.58 11.65 11.69 2.89 6.9 16.1 9.2 0.05 ## kurtosis se ## subject -1.38 1.32 ## condition* NA NA ## iq -1.06 0.58 ## -------------------------------------------------------- ## group: 17 days ## vars n mean sd median trimmed mad min max range skew ## subject 1 20 50.5 5.92 50.5 50.5 7.41 41.0 60.0 19.0 0.00 ## condition* 2 20 NaN NA NA NaN NA Inf -Inf -Inf NA ## iq 3 20 13.9 2.26 13.6 13.9 2.00 9.8 18.1 8.3 0.11 ## kurtosis se ## subject -1.38 1.32 ## condition* NA NA ## iq -0.85 0.50 ## -------------------------------------------------------- ## group: 19 days ## vars n mean sd median trimmed mad min max range skew ## subject 1 20 70.50 5.92 70.5 70.50 7.41 61.0 80.0 19.0 0.00 ## condition* 2 20 NaN NA NA NaN NA Inf -Inf -Inf NA ## iq 3 20 14.75 2.50 15.3 14.71 2.15 10.4 19.2 8.8 -0.09 ## kurtosis se ## subject -1.38 1.32 ## condition* NA NA ## iq -0.99 0.56 ## -------------------------------------------------------- ## group: 8 days ## vars n mean sd median trimmed mad min max range skew ## subject 1 20 10.50 5.92 10.5 10.50 7.41 1.0 20.0 19.0 0.00 ## condition* 2 20 NaN NA NA NaN NA Inf -Inf -Inf NA ## iq 3 20 10.91 2.63 11.3 10.97 2.67 5.4 15.7 10.3 -0.21 ## kurtosis se ## subject -1.38 1.32 ## condition* NA NA ## iq -0.70 0.59 1 2 # Boxplot IQ versus cond boxplot ( wm $ iq ~ wm $ condition , main = \"Boxplot\" , xlab = \"Group (cond)\" , ylab = \"IQ\" ) Notice that the IQ increases as the amount of training sessions increases. t-test vs ANOVA ANOVA is used when more than two group means are compared, whereas a t-test can only compare two group means. Generate density plot of the F-distribution The test statistic associated with ANOVA is the F-test (or F-ratio). Recall that when carrying out a t-test, you computed an observed t-value, then compared that with a critical value derived from the relevant t-distribution. That t-distribution came from a family of t-distributions, each of which was defined entirely by its degrees of freedom. ANOVA uses the same principle, but instead an observed F-value is computed and compared to the relevant F-distribution. That F-distribution comes from a family of F-distributions, each of which is defined by two numbers (i.e. degrees of freedom). F-distribution has a different shape than the t-distribution. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 # Create the vector x x <- seq ( from = 0 , to = 2 , length = 100 ) # Simulate the F-distributions y_1 <- df ( x , 1 , 1 ) y_2 <- df ( x , 3 , 1 ) y_3 <- df ( x , 6 , 1 ) y_4 <- df ( x , 3 , 3 ) y_5 <- df ( x , 6 , 3 ) y_6 <- df ( x , 3 , 6 ) y_7 <- df ( x , 6 , 6 ) # Plot the F-distributions plot ( x , y_1 , col = 1 , 'l' ) lines ( x , y_2 , col = 2 , 'l' ) lines ( x , y_3 , col = 3 , 'l' ) lines ( x , y_4 , col = 4 , 'l' ) lines ( x , y_5 , col = 5 , 'l' ) lines ( x , y_6 , col = 6 , 'l' ) lines ( x , y_7 , col = 7 , 'l' ) # Add the legend in the top right corner and with the title 'F distributions' legend ( 'topright' , title = 'F distributions' , c ( 'df = (1,1)' , 'df = (3,1)' , 'df = (6,1)' , 'df = (3,3)' , 'df = (6,3)' , 'df = (3,6)' , 'df = (6,6)' ), col = c ( 1 , 2 , 3 , 4 , 5 , 6 , 7 ), lty = 1 ) The F-distribution cannot take negative values, because it is a ratio of variances and variances are always non-negative numbers. The distribution represents the ratio between the variance between groups and the variance within groups. Between group sum of squares To calculate the F-value, you need to calculate the ratio between the variance between groups and the variance within groups. Furthermore, to calculate the variance (i.e. mean of squares), you first have to calculate the sum of squares. Now, remember that the working memory experiment investigates the relationship between the change in IQ and the number of training sessions. Calculate the between group sum of squares for the data from this experiment. 1 2 3 4 5 6 # Define number of subjects in each group n <- 20 # Calculate group means Y_j <- as.numeric ( tapply ( wm $ iq , wm $ condition , mean )) Y_j 1 ## [1] 11.700 13.905 14.750 10.910 1 2 3 # Calculate the grand mean Y_T <- mean ( wm $ iq ) Y_T 1 ## [1] 12.81625 1 2 3 # Calculate the sum of squares SS_A <- sum (( Y_j - Y_T ) ^2 ) * n SS_A 1 ## [1] 196.0914 Within groups sum of squares To calculate the F-value, you also need the variance within groups. Similar to the last exercise, we\u2019ll start by computing the within groups sum of squares. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 # Create four subsets of the four groups, containing the IQ results # Make the subset for the group cond = '8 days' Y_i1 <- subset ( wm $ iq , wm $ condition == '8 days' ) # Make the subset for the group cond = '12 days' Y_i2 <- subset ( wm $ iq , wm $ condition == '12 days' ) # Make the subset for the group cond = '17 days' Y_i3 <- subset ( wm $ iq , wm $ condition == '17 days' ) # Make the subset for the group cond = '19 days' Y_i4 <- subset ( wm $ iq , wm $ condition == '19 days' ) # subtract the individual values by their group means # You have already calculated the group means in the previous exercise so use this result, the vector that contains these group means was called Y_j S_1 <- Y_i1 - Y_j[1] S_2 <- Y_i2 - Y_j[2] # Do it without the vector Y_j, so calculate the group means again. S_3 <- Y_i3 - mean ( Y_i3 ) S_4 <- Y_i4 - mean ( Y_i4 ) #Put everything back in one vector S_T <- c ( S_1 , S_2 , S_3 , S_4 ) #Calculate the sum of squares by using the vector S_T SS_SA <- sum ( S_T^2 ) Calculating the F-ratio Calculate the F-ratio. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 # Number of groups a <- 4 # Number of subject in each group n <- 20 # Define the degrees of freedom df_A <- a - 1 df_SA <- a * ( n - 1 ) # Calculate the mean squares (variances) by using the sum of squares SS_A and SS_SA MS_A <- SS_A / df_A MS_SA <- SS_SA / df_SA # Calculate the F-ratio F <- MS_A / MS_SA A faster way: ANOVA in R Normally, we do not have to do all calculations. 1 2 3 # Apply the aov function anova.wm <- aov ( wm $ iq ~ wm $ condition ) anova.wm 1 2 3 4 5 6 7 8 9 10 ## Call: ## aov(formula = wm$iq ~ wm$condition) ## ## Terms: ## wm$condition Residuals ## Sum of Squares 196.0914 473.4175 ## Deg. of Freedom 3 76 ## ## Residual standard error: 2.495832 ## Estimated effects may be unbalanced 1 2 # Look at the summary table of the result summary ( anova.wm ) 1 2 3 4 5 ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## wm$condition 3 196.1 65.36 10.49 7.47e-06 *** ## Residuals 76 473.4 6.23 ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 F-value is significant. Levene\u2019s test The assumptions of ANOVA are relatively simple. Similar to an independent t-test, we have a continuous dependent variable, which we assume to be normally distributed. Furthermore, we assume homogeneity of variance, which can be tested with Levene\u2019s test. 1 2 3 4 library ( car ) # Levene's test leveneTest ( wm $ iq ~ wm $ condition ) 1 2 3 4 ## Levene's Test for Homogeneity of Variance (center = median) ## Df F value Pr(&gt;F) ## group 3 0.1405 0.9355 ## 76 1 2 # Levene's test with the change for the default, namely center = mean leveneTest ( wm $ iq ~ wm $ condition , center = mean ) 1 2 3 4 ## Levene's Test for Homogeneity of Variance (center = mean) ## Df F value Pr(&gt;F) ## group 3 0.1598 0.923 ## 76 The assumption of homogeneity of variance hold: the within group variance equivalent for all groups. Post-hoc analysis \u00b6 Post-hoc tests help finding out which groups differ significantly from one other and which do not. More formally, post-hoc tests allow for multiple pairwise comparisons without inflating the type I error. What does it mean to inflate the type I error? Suppose the post-hoc test involves performing three pairwise comparisons, each with the probability of a type I error set at 5%. The probability of making at least one type I error: if you assume independence of these three events, the maximum familywise error rate is then equal to: 1 - (0.95 x 0.95 x 0.95) = 14.26 %. In other words, the probability of having at least one false alarm (i.e. type I error) is 14.26%. What is the maximum familywise error rate for the working memory experiment, assuming that you do all possible pairwise comparisons with a type I error of 5%? 26.49%. Null Hypothesis Significance Testing (NHST) is a statistical method used to test whether or not you are able to reject or retain the null hypothesis. This type of test can confront you with a type I error. This happens when the test rejects the null hypothesis, while it is actually true in reality. Furthermore, the test can also deliver a type II error. This is the failure to reject a null hypothesis when it is false. All hypothesis tests have a probability of making type I and II errors. Sensitivity and specificity are two concepts that statisticians use to measure the performance of a statistical test. The sensitivity of a test is its true positive rate: sensitivity = \\frac{number~of~true~positives}{number~ of~true~positives + number~of~false~negatives} sensitivity = \\frac{number~of~true~positives}{number~ of~true~positives + number~of~false~negatives} The specificity of a test is its true negative rate: specificity = \\frac{number~of~true~negatives}{number~ of~true~negatives + number~of~false~positives} specificity = \\frac{number~of~true~negatives}{number~ of~true~negatives + number~of~false~positives} Calculate both the sensitivity and specificity of the test based on numbers displayed in the NHST table? The sensitivity is 0.89 and the specificity is 0.85. Calculate and interpret the results of Tukey In a situation were you do multiple pairwise comparisons, the probability of type I errors in the process inflates substantially. Therefore, it is better to build in adjustments to take this into account. This is what Tukey tests and other post-hoc procedures do. They adjust the p-value to prevent inflation of the type I error rate. Use Tukey\u2019s procedure. 1 2 3 4 5 # Read in the data set and assign to the object wm <- readWorksheetFromFile ( 'A Hands-on Introduction to Statistics with R.xls' , sheet = 'wm' , header = TRUE , startCol = 1 , startRow = 1 ) # This will print the data set in the console head ( wm ) 1 2 3 4 5 6 7 ## cond pre post gain train ## 1 t08 8 9 1 1 ## 2 t08 8 10 2 1 ## 3 t08 8 8 0 1 ## 4 t08 8 7 -1 1 ## 5 t08 9 11 2 1 ## 6 t08 9 10 1 1 1 2 3 4 5 # Revision: Analysis of variance anova_wm <- aov ( wm $ gain ~ wm $ cond ) # Summary Analysis of Variance summary ( anova_wm ) 1 2 3 4 5 ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## wm$cond 4 274.0 68.51 34.57 &lt;2e-16 *** ## Residuals 115 227.9 1.98 ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 1 2 # Post-hoc (Tukey) TukeyHSD ( anova_wm ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 ## Tukey multiple comparisons of means ## 95% family-wise confidence level ## ## Fit: aov(formula = wm$gain ~ wm$cond) ## ## $`wm$cond` ## diff lwr upr p adj ## t08-control -0.625 -1.69355785 0.4435578 0.4871216 ## t12-control 0.625 -0.44355785 1.6935578 0.4871216 ## t17-control 2.425 1.35644215 3.4935578 0.0000001 ## t19-control 3.625 2.55644215 4.6935578 0.0000000 ## t12-t08 1.250 0.01613568 2.4838643 0.0454650 ## t17-t08 3.050 1.81613568 4.2838643 0.0000000 ## t19-t08 4.250 3.01613568 5.4838643 0.0000000 ## t17-t12 1.800 0.56613568 3.0338643 0.0008953 ## t19-t12 3.000 1.76613568 4.2338643 0.0000000 ## t19-t17 1.200 -0.03386432 2.4338643 0.0607853 1 2 3 # Plot confidence intervals #plot(c(TukeyHSD(anova_wm)$lwr,TukeyHSD(anova_wm)$upr)) plot ( TukeyHSD ( anova_wm )) Bonferroni adjusted p-values Just like Tukey\u2019s procedure, the Bonferroni correction is a method that is used to counteract the problem of inflated type I errors while engaging in multiple pairwise comparisons between subgroups. Bonferroni is generally known as the most conservative method to control the familywise error rate. Bonferroni is based on the idea that if you test N dependent or independent hypotheses, one way of maintaining the familywise error rate is to test each individual hypothesis at a statistical significance level that is deflated by a factor of \\frac{1}{n} \\frac{1}{n} . So, for a significance level for the whole family of tests of \u03b1 , the Bonferroni correction would be to test each of the individual tests at a significance level of \\frac{\\alpha}{n} \\frac{\\alpha}{n} . The Bonferroni correction is controversial. It is a strict measure to limit false positives and generates conservative p-value. Alternative: increase the sample size, compute the false discovery rate (the expected percent of false predictions in the set of predictions. For example if the algorithm returns 100 results with a false discovery rate of .3 then we should expect 70 of them to be correct.), and the Holm-Bonferroni method. 1 2 3 4 # Use `p.adjust` bonferroni_ex <- p.adjust ( .005 , method = 'bonferroni' , n = 8 ) bonferroni_ex 1 ## [1] 0.04 1 2 # Pairwise T-test pairwise.t.test ( wm $ gain , wm $ cond , p.adjust = 'bonferroni' ) 1 2 3 4 5 6 7 8 9 10 11 12 ## ## Pairwise comparisons using t tests with pooled SD ## ## data: wm$gain and wm$cond ## ## control t08 t12 t17 ## t08 1.00000 - - - ## t12 1.00000 0.05862 - - ## t17 5.9e-08 3.8e-09 0.00096 - ## t19 6.4e-15 2.9e-15 6.7e-09 0.08084 ## ## P value adjustment method: bonferroni Between groups factorial ANOVA \u00b6 Data exploration with a barplot We\u2019ll use in this chapter is a randomized controlled experiment investigating the effects of talking on a cell phone while driving. The dependent variable in the experiment is the number of driving errors that subjects made in a driving simulator. There are two independent variables: Conversation difficulty: None, Low, High Driving difficulty: Easy, Difficult 1 2 3 4 5 6 7 8 9 10 # Read in the data set and assign to the object ab <- readWorksheetFromFile ( 'A Hands-on Introduction to Statistics with R.xls' , sheet = 'ab' , header = TRUE , startCol = 1 , startRow = 1 ) ab $ subject <- as.integer ( ab $ subject ) ab $ conversation <- as.factor ( ab $ conversation ) ab $ driving <- as.factor ( ab $ driving ) ab $ error <- as.integer ( ab $ error ) # This will print the data set in the console head ( ab ) 1 2 3 4 5 6 7 ## subject conversation driving errors error ## 1 1 None Easy 20 20 ## 2 2 None Easy 19 19 ## 3 3 None Easy 31 31 ## 4 4 None Easy 27 27 ## 5 5 None Easy 31 31 ## 6 6 None Easy 17 17 Each of the subjects was randomly assigned to one of six conditions formed by combining different values of the independent variables. subject : unique identifier for each subject conversation: level of conversation difficulty. driving : level of driving difficulty in the simulator. errors : number of driving errors made. 1 2 3 4 5 6 7 8 # Use the tapply function to create your groups ab_groups <- tapply ( ab $ errors , list ( ab $ driving , ab $ conversation ), sum ) # Make the required barplot barplot ( ab_groups , beside = TRUE , col = c ( 'orange' , 'blue' ), main = 'Driving Errors' , xlab = 'Conversation Demands' , ylab = 'Errors' ) # Add the legend legend ( 'topright' , c ( 'Difficult' , 'Easy' ), title = 'Driving' , fill = c ( 'orange' , 'blue' )) The driving errors made during different driving conditions are influenced by the level of conversation demand. In other words, the driving conditions have a different effect on the number of errors made, depending on the level of conversation demand. The homogeneity of variance assumption Before we do factorial ANOVA, we need to test the homogeneity of the variance assumption. When studying one-way ANOVA, we tested this assumption with the leveneTest function. We now have two independent variables instead of just one. 1 2 # Test the homogeneity of variance assumption leveneTest ( ab $ errors ~ ab $ conversation * ab $ driving ) 1 2 3 4 ## Levene's Test for Homogeneity of Variance (center = median) ## Df F value Pr(&gt;F) ## group 5 0.5206 0.7602 ## 114 The homogeneity of variance assumption holds. By performing a leveneTest , we can check whether or not the homogeneity of variance assumption holds for a given dataset. The assumption must hold for the results of an ANOVA analysis to be valid. Recall from the first chapter that ANOVA makes use of F-statistics, or F-ratios, in which two types of degrees of freedom are involved. 1 dim ( ab ) 1 ## [1] 120 5 1 str ( ab $ conversation ) 1 ## Factor w/ 3 levels \"High demand\",..: 3 3 3 3 3 3 3 3 3 3 ... 1 str ( ab $ driving ) 1 ## Factor w/ 2 levels \"Difficult\",\"Easy\": 2 2 2 2 2 2 2 2 2 2 ... There are 120 subjets and 2 (Easy, Difficult) * 3 (High Demand, Low Demand, None) = 6 groups. The factorial ANOVA 1 2 3 4 5 # Factorial ANOVA ab_model <- aov ( ab $ errors ~ ab $ conversation * ab $ driving ) # Get the summary table summary ( ab_model ) 1 2 3 4 5 6 7 ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## ab$conversation 2 4416 2208 36.14 6.98e-13 *** ## ab$driving 1 5782 5782 94.64 &lt; 2e-16 *** ## ab$conversation:ab$driving 2 1639 820 13.41 5.86e-06 *** ## Residuals 114 6965 61 ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 Based on the summary table of the factorial ANOVA, the main effect for driving difficulty, the main effect for conversation difficulty and the interaction effect are all significant. The interaction effect Now it\u2019s time to explore the interaction effect. You will do this with the help of a simple effects analysis. Why a simple effects analysis? Well, remember what we had to do when you had a significant main effect in a one-way ANOVA? There, you just had to perform some post-hoc tests to see from which level of the categorical variable the main effect was coming. With an interaction effect, it is quite similar. Conduct a simple effects analysis of the variable conversation on the outcome variable errors at each level of driving . 1 2 3 4 5 6 7 8 9 10 # Create the two subsets ab_1 <- subset ( ab , ab $ driving == 'Easy' ) ab_2 <- subset ( ab , ab $ driving == 'Difficult' ) # Perform the one-way ANOVA for both subsets aov_ab_1 <- aov ( ab_1 $ errors ~ ab_1 $ conversation ) aov_ab_2 <- aov ( ab_2 $ errors ~ ab_2 $ conversation ) # Get the summary tables for both aov_ab_1 and aov_ab_2 summary ( aov_ab_1 ) 1 2 3 4 5 ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## ab_1$conversation 2 504.7 252.3 4.928 0.0106 * ## Residuals 57 2918.5 51.2 ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 1 summary ( aov_ab_2 ) 1 2 3 4 5 ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## ab_2$conversation 2 5551 2776 39.09 2.05e-11 *** ## Residuals 57 4047 71 ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 There a significant simple effect for the easy driving condition based on the summary table of aov_ab_1 . There a significant simple effect for the difficult driving condition based on the summary table of aov_ab_2 . The effect sizes The definition of an interaction effect states that the effect of one variable changes across levels of the other variable. For example, we might expect the effect of conversation to be greater when driving conditions are difficult than when they are relatively easy. Unfortunately, it is not quite that simple. In order to really understand the different effect sizes, you should make use of the etaSquared function. 1 2 3 4 5 6 library ( lsr ) # Calculate the etaSquared for the easy driving case #easy is ab_1 #difficult is ab_2 etaSquared ( aov_ab_1 , anova = TRUE ) 1 2 3 4 5 6 ## eta.sq eta.sq.part SS df MS F ## ab_1$conversation 0.147433 0.147433 504.70 2 252.35000 4.928458 ## Residuals 0.852567 NA 2918.55 57 51.20263 NA ## p ## ab_1$conversation 0.01061116 ## Residuals NA 1 2 # Calculate the etaSquared for the difficult driving case etaSquared ( aov_ab_2 , anova = TRUE ) 1 2 3 4 5 6 ## eta.sq eta.sq.part SS df MS F ## ab_2$conversation 0.5783571 0.5783571 5551.033 2 2775.51667 39.09275 ## Residuals 0.4216429 NA 4046.900 57 70.99825 NA ## p ## ab_2$conversation 2.046097e-11 ## Residuals NA Based on the output of the etaSquared function for the easy driving condition, the percentage of variance explained by the conversation variable is 14.7%; the percentage of variance explained by the conversation variable is 57.8%. Pairwise comparisons Finally, let us look at pairwise comparisons for the simple effects. You can do this with the Tukey post-hoc test. 1 2 # Tukey for easy driving TukeyHSD ( aov_ab_1 ) 1 2 3 4 5 6 7 8 9 10 ## Tukey multiple comparisons of means ## 95% family-wise confidence level ## ## Fit: aov(formula = ab_1$errors ~ ab_1$conversation) ## ## $`ab_1$conversation` ## diff lwr upr p adj ## Low demand-High demand -6.05 -11.495243 -0.6047574 0.0260458 ## None-High demand -6.25 -11.695243 -0.8047574 0.0207614 ## None-Low demand -0.20 -5.645243 5.2452426 0.9957026 1 2 # Tukey for difficult driving TukeyHSD ( aov_ab_2 ) 1 2 3 4 5 6 7 8 9 10 ## Tukey multiple comparisons of means ## 95% family-wise confidence level ## ## Fit: aov(formula = ab_2$errors ~ ab_2$conversation) ## ## $`ab_2$conversation` ## diff lwr upr p adj ## Low demand-High demand -9.75 -16.16202 -3.337979 0.0015849 ## None-High demand -23.45 -29.86202 -17.037979 0.0000000 ## None-Low demand -13.70 -20.11202 -7.287979 0.0000103 For \u2018Easy Driving\u2019, two mean differences in terms of number of errors are significant (below 0.05). For \u2018Difficult Driving\u2019, three mean differences in terms of number of errors are significant (below 0.05).","title":"Statistics with R, Course Three, Analysis of Variance"},{"location":"a_hands-on_introduction_to_statistics_with_r_course_three_analysis_of_variance/#an-introduction-to-anova","text":"Working memory experiment We\u2019ll use data from the working memory experiment, which investigates the relationship between the number of training days and a change in IQ. There are four independent groups, each of which trained for a different period of time: 8, 12, 17, or 19 days. The independent variable is the number of training days and the dependent variable is the IQ gain. 1 2 # Print the data set in the console head ( wm ) 1 2 3 4 5 6 7 ## subject condition iq ## 1 1 8 days 12.4 ## 2 2 8 days 11.8 ## 3 3 8 days 14.6 ## 4 4 8 days 7.7 ## 5 5 8 days 15.7 ## 6 6 8 days 11.6 1 2 3 4 library ( psych ) # Summary statistics by all groups (8 sessions, 12 sessions, 17 sessions, 19 sessions) describeBy ( wm , wm $ condition ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 ## ## Descriptive statistics by group ## group: 12 days ## vars n mean sd median trimmed mad min max range skew ## subject 1 20 30.5 5.92 30.50 30.50 7.41 21.0 40.0 19.0 0.00 ## condition* 2 20 NaN NA NA NaN NA Inf -Inf -Inf NA ## iq 3 20 11.7 2.58 11.65 11.69 2.89 6.9 16.1 9.2 0.05 ## kurtosis se ## subject -1.38 1.32 ## condition* NA NA ## iq -1.06 0.58 ## -------------------------------------------------------- ## group: 17 days ## vars n mean sd median trimmed mad min max range skew ## subject 1 20 50.5 5.92 50.5 50.5 7.41 41.0 60.0 19.0 0.00 ## condition* 2 20 NaN NA NA NaN NA Inf -Inf -Inf NA ## iq 3 20 13.9 2.26 13.6 13.9 2.00 9.8 18.1 8.3 0.11 ## kurtosis se ## subject -1.38 1.32 ## condition* NA NA ## iq -0.85 0.50 ## -------------------------------------------------------- ## group: 19 days ## vars n mean sd median trimmed mad min max range skew ## subject 1 20 70.50 5.92 70.5 70.50 7.41 61.0 80.0 19.0 0.00 ## condition* 2 20 NaN NA NA NaN NA Inf -Inf -Inf NA ## iq 3 20 14.75 2.50 15.3 14.71 2.15 10.4 19.2 8.8 -0.09 ## kurtosis se ## subject -1.38 1.32 ## condition* NA NA ## iq -0.99 0.56 ## -------------------------------------------------------- ## group: 8 days ## vars n mean sd median trimmed mad min max range skew ## subject 1 20 10.50 5.92 10.5 10.50 7.41 1.0 20.0 19.0 0.00 ## condition* 2 20 NaN NA NA NaN NA Inf -Inf -Inf NA ## iq 3 20 10.91 2.63 11.3 10.97 2.67 5.4 15.7 10.3 -0.21 ## kurtosis se ## subject -1.38 1.32 ## condition* NA NA ## iq -0.70 0.59 1 2 # Boxplot IQ versus cond boxplot ( wm $ iq ~ wm $ condition , main = \"Boxplot\" , xlab = \"Group (cond)\" , ylab = \"IQ\" ) Notice that the IQ increases as the amount of training sessions increases. t-test vs ANOVA ANOVA is used when more than two group means are compared, whereas a t-test can only compare two group means. Generate density plot of the F-distribution The test statistic associated with ANOVA is the F-test (or F-ratio). Recall that when carrying out a t-test, you computed an observed t-value, then compared that with a critical value derived from the relevant t-distribution. That t-distribution came from a family of t-distributions, each of which was defined entirely by its degrees of freedom. ANOVA uses the same principle, but instead an observed F-value is computed and compared to the relevant F-distribution. That F-distribution comes from a family of F-distributions, each of which is defined by two numbers (i.e. degrees of freedom). F-distribution has a different shape than the t-distribution. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 # Create the vector x x <- seq ( from = 0 , to = 2 , length = 100 ) # Simulate the F-distributions y_1 <- df ( x , 1 , 1 ) y_2 <- df ( x , 3 , 1 ) y_3 <- df ( x , 6 , 1 ) y_4 <- df ( x , 3 , 3 ) y_5 <- df ( x , 6 , 3 ) y_6 <- df ( x , 3 , 6 ) y_7 <- df ( x , 6 , 6 ) # Plot the F-distributions plot ( x , y_1 , col = 1 , 'l' ) lines ( x , y_2 , col = 2 , 'l' ) lines ( x , y_3 , col = 3 , 'l' ) lines ( x , y_4 , col = 4 , 'l' ) lines ( x , y_5 , col = 5 , 'l' ) lines ( x , y_6 , col = 6 , 'l' ) lines ( x , y_7 , col = 7 , 'l' ) # Add the legend in the top right corner and with the title 'F distributions' legend ( 'topright' , title = 'F distributions' , c ( 'df = (1,1)' , 'df = (3,1)' , 'df = (6,1)' , 'df = (3,3)' , 'df = (6,3)' , 'df = (3,6)' , 'df = (6,6)' ), col = c ( 1 , 2 , 3 , 4 , 5 , 6 , 7 ), lty = 1 ) The F-distribution cannot take negative values, because it is a ratio of variances and variances are always non-negative numbers. The distribution represents the ratio between the variance between groups and the variance within groups. Between group sum of squares To calculate the F-value, you need to calculate the ratio between the variance between groups and the variance within groups. Furthermore, to calculate the variance (i.e. mean of squares), you first have to calculate the sum of squares. Now, remember that the working memory experiment investigates the relationship between the change in IQ and the number of training sessions. Calculate the between group sum of squares for the data from this experiment. 1 2 3 4 5 6 # Define number of subjects in each group n <- 20 # Calculate group means Y_j <- as.numeric ( tapply ( wm $ iq , wm $ condition , mean )) Y_j 1 ## [1] 11.700 13.905 14.750 10.910 1 2 3 # Calculate the grand mean Y_T <- mean ( wm $ iq ) Y_T 1 ## [1] 12.81625 1 2 3 # Calculate the sum of squares SS_A <- sum (( Y_j - Y_T ) ^2 ) * n SS_A 1 ## [1] 196.0914 Within groups sum of squares To calculate the F-value, you also need the variance within groups. Similar to the last exercise, we\u2019ll start by computing the within groups sum of squares. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 # Create four subsets of the four groups, containing the IQ results # Make the subset for the group cond = '8 days' Y_i1 <- subset ( wm $ iq , wm $ condition == '8 days' ) # Make the subset for the group cond = '12 days' Y_i2 <- subset ( wm $ iq , wm $ condition == '12 days' ) # Make the subset for the group cond = '17 days' Y_i3 <- subset ( wm $ iq , wm $ condition == '17 days' ) # Make the subset for the group cond = '19 days' Y_i4 <- subset ( wm $ iq , wm $ condition == '19 days' ) # subtract the individual values by their group means # You have already calculated the group means in the previous exercise so use this result, the vector that contains these group means was called Y_j S_1 <- Y_i1 - Y_j[1] S_2 <- Y_i2 - Y_j[2] # Do it without the vector Y_j, so calculate the group means again. S_3 <- Y_i3 - mean ( Y_i3 ) S_4 <- Y_i4 - mean ( Y_i4 ) #Put everything back in one vector S_T <- c ( S_1 , S_2 , S_3 , S_4 ) #Calculate the sum of squares by using the vector S_T SS_SA <- sum ( S_T^2 ) Calculating the F-ratio Calculate the F-ratio. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 # Number of groups a <- 4 # Number of subject in each group n <- 20 # Define the degrees of freedom df_A <- a - 1 df_SA <- a * ( n - 1 ) # Calculate the mean squares (variances) by using the sum of squares SS_A and SS_SA MS_A <- SS_A / df_A MS_SA <- SS_SA / df_SA # Calculate the F-ratio F <- MS_A / MS_SA A faster way: ANOVA in R Normally, we do not have to do all calculations. 1 2 3 # Apply the aov function anova.wm <- aov ( wm $ iq ~ wm $ condition ) anova.wm 1 2 3 4 5 6 7 8 9 10 ## Call: ## aov(formula = wm$iq ~ wm$condition) ## ## Terms: ## wm$condition Residuals ## Sum of Squares 196.0914 473.4175 ## Deg. of Freedom 3 76 ## ## Residual standard error: 2.495832 ## Estimated effects may be unbalanced 1 2 # Look at the summary table of the result summary ( anova.wm ) 1 2 3 4 5 ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## wm$condition 3 196.1 65.36 10.49 7.47e-06 *** ## Residuals 76 473.4 6.23 ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 F-value is significant. Levene\u2019s test The assumptions of ANOVA are relatively simple. Similar to an independent t-test, we have a continuous dependent variable, which we assume to be normally distributed. Furthermore, we assume homogeneity of variance, which can be tested with Levene\u2019s test. 1 2 3 4 library ( car ) # Levene's test leveneTest ( wm $ iq ~ wm $ condition ) 1 2 3 4 ## Levene's Test for Homogeneity of Variance (center = median) ## Df F value Pr(&gt;F) ## group 3 0.1405 0.9355 ## 76 1 2 # Levene's test with the change for the default, namely center = mean leveneTest ( wm $ iq ~ wm $ condition , center = mean ) 1 2 3 4 ## Levene's Test for Homogeneity of Variance (center = mean) ## Df F value Pr(&gt;F) ## group 3 0.1598 0.923 ## 76 The assumption of homogeneity of variance hold: the within group variance equivalent for all groups.","title":"An introduction to ANOVA"},{"location":"a_hands-on_introduction_to_statistics_with_r_course_three_analysis_of_variance/#post-hoc-analysis","text":"Post-hoc tests help finding out which groups differ significantly from one other and which do not. More formally, post-hoc tests allow for multiple pairwise comparisons without inflating the type I error. What does it mean to inflate the type I error? Suppose the post-hoc test involves performing three pairwise comparisons, each with the probability of a type I error set at 5%. The probability of making at least one type I error: if you assume independence of these three events, the maximum familywise error rate is then equal to: 1 - (0.95 x 0.95 x 0.95) = 14.26 %. In other words, the probability of having at least one false alarm (i.e. type I error) is 14.26%. What is the maximum familywise error rate for the working memory experiment, assuming that you do all possible pairwise comparisons with a type I error of 5%? 26.49%. Null Hypothesis Significance Testing (NHST) is a statistical method used to test whether or not you are able to reject or retain the null hypothesis. This type of test can confront you with a type I error. This happens when the test rejects the null hypothesis, while it is actually true in reality. Furthermore, the test can also deliver a type II error. This is the failure to reject a null hypothesis when it is false. All hypothesis tests have a probability of making type I and II errors. Sensitivity and specificity are two concepts that statisticians use to measure the performance of a statistical test. The sensitivity of a test is its true positive rate: sensitivity = \\frac{number~of~true~positives}{number~ of~true~positives + number~of~false~negatives} sensitivity = \\frac{number~of~true~positives}{number~ of~true~positives + number~of~false~negatives} The specificity of a test is its true negative rate: specificity = \\frac{number~of~true~negatives}{number~ of~true~negatives + number~of~false~positives} specificity = \\frac{number~of~true~negatives}{number~ of~true~negatives + number~of~false~positives} Calculate both the sensitivity and specificity of the test based on numbers displayed in the NHST table? The sensitivity is 0.89 and the specificity is 0.85. Calculate and interpret the results of Tukey In a situation were you do multiple pairwise comparisons, the probability of type I errors in the process inflates substantially. Therefore, it is better to build in adjustments to take this into account. This is what Tukey tests and other post-hoc procedures do. They adjust the p-value to prevent inflation of the type I error rate. Use Tukey\u2019s procedure. 1 2 3 4 5 # Read in the data set and assign to the object wm <- readWorksheetFromFile ( 'A Hands-on Introduction to Statistics with R.xls' , sheet = 'wm' , header = TRUE , startCol = 1 , startRow = 1 ) # This will print the data set in the console head ( wm ) 1 2 3 4 5 6 7 ## cond pre post gain train ## 1 t08 8 9 1 1 ## 2 t08 8 10 2 1 ## 3 t08 8 8 0 1 ## 4 t08 8 7 -1 1 ## 5 t08 9 11 2 1 ## 6 t08 9 10 1 1 1 2 3 4 5 # Revision: Analysis of variance anova_wm <- aov ( wm $ gain ~ wm $ cond ) # Summary Analysis of Variance summary ( anova_wm ) 1 2 3 4 5 ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## wm$cond 4 274.0 68.51 34.57 &lt;2e-16 *** ## Residuals 115 227.9 1.98 ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 1 2 # Post-hoc (Tukey) TukeyHSD ( anova_wm ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 ## Tukey multiple comparisons of means ## 95% family-wise confidence level ## ## Fit: aov(formula = wm$gain ~ wm$cond) ## ## $`wm$cond` ## diff lwr upr p adj ## t08-control -0.625 -1.69355785 0.4435578 0.4871216 ## t12-control 0.625 -0.44355785 1.6935578 0.4871216 ## t17-control 2.425 1.35644215 3.4935578 0.0000001 ## t19-control 3.625 2.55644215 4.6935578 0.0000000 ## t12-t08 1.250 0.01613568 2.4838643 0.0454650 ## t17-t08 3.050 1.81613568 4.2838643 0.0000000 ## t19-t08 4.250 3.01613568 5.4838643 0.0000000 ## t17-t12 1.800 0.56613568 3.0338643 0.0008953 ## t19-t12 3.000 1.76613568 4.2338643 0.0000000 ## t19-t17 1.200 -0.03386432 2.4338643 0.0607853 1 2 3 # Plot confidence intervals #plot(c(TukeyHSD(anova_wm)$lwr,TukeyHSD(anova_wm)$upr)) plot ( TukeyHSD ( anova_wm )) Bonferroni adjusted p-values Just like Tukey\u2019s procedure, the Bonferroni correction is a method that is used to counteract the problem of inflated type I errors while engaging in multiple pairwise comparisons between subgroups. Bonferroni is generally known as the most conservative method to control the familywise error rate. Bonferroni is based on the idea that if you test N dependent or independent hypotheses, one way of maintaining the familywise error rate is to test each individual hypothesis at a statistical significance level that is deflated by a factor of \\frac{1}{n} \\frac{1}{n} . So, for a significance level for the whole family of tests of \u03b1 , the Bonferroni correction would be to test each of the individual tests at a significance level of \\frac{\\alpha}{n} \\frac{\\alpha}{n} . The Bonferroni correction is controversial. It is a strict measure to limit false positives and generates conservative p-value. Alternative: increase the sample size, compute the false discovery rate (the expected percent of false predictions in the set of predictions. For example if the algorithm returns 100 results with a false discovery rate of .3 then we should expect 70 of them to be correct.), and the Holm-Bonferroni method. 1 2 3 4 # Use `p.adjust` bonferroni_ex <- p.adjust ( .005 , method = 'bonferroni' , n = 8 ) bonferroni_ex 1 ## [1] 0.04 1 2 # Pairwise T-test pairwise.t.test ( wm $ gain , wm $ cond , p.adjust = 'bonferroni' ) 1 2 3 4 5 6 7 8 9 10 11 12 ## ## Pairwise comparisons using t tests with pooled SD ## ## data: wm$gain and wm$cond ## ## control t08 t12 t17 ## t08 1.00000 - - - ## t12 1.00000 0.05862 - - ## t17 5.9e-08 3.8e-09 0.00096 - ## t19 6.4e-15 2.9e-15 6.7e-09 0.08084 ## ## P value adjustment method: bonferroni","title":"Post-hoc analysis"},{"location":"a_hands-on_introduction_to_statistics_with_r_course_three_analysis_of_variance/#between-groups-factorial-anova","text":"Data exploration with a barplot We\u2019ll use in this chapter is a randomized controlled experiment investigating the effects of talking on a cell phone while driving. The dependent variable in the experiment is the number of driving errors that subjects made in a driving simulator. There are two independent variables: Conversation difficulty: None, Low, High Driving difficulty: Easy, Difficult 1 2 3 4 5 6 7 8 9 10 # Read in the data set and assign to the object ab <- readWorksheetFromFile ( 'A Hands-on Introduction to Statistics with R.xls' , sheet = 'ab' , header = TRUE , startCol = 1 , startRow = 1 ) ab $ subject <- as.integer ( ab $ subject ) ab $ conversation <- as.factor ( ab $ conversation ) ab $ driving <- as.factor ( ab $ driving ) ab $ error <- as.integer ( ab $ error ) # This will print the data set in the console head ( ab ) 1 2 3 4 5 6 7 ## subject conversation driving errors error ## 1 1 None Easy 20 20 ## 2 2 None Easy 19 19 ## 3 3 None Easy 31 31 ## 4 4 None Easy 27 27 ## 5 5 None Easy 31 31 ## 6 6 None Easy 17 17 Each of the subjects was randomly assigned to one of six conditions formed by combining different values of the independent variables. subject : unique identifier for each subject conversation: level of conversation difficulty. driving : level of driving difficulty in the simulator. errors : number of driving errors made. 1 2 3 4 5 6 7 8 # Use the tapply function to create your groups ab_groups <- tapply ( ab $ errors , list ( ab $ driving , ab $ conversation ), sum ) # Make the required barplot barplot ( ab_groups , beside = TRUE , col = c ( 'orange' , 'blue' ), main = 'Driving Errors' , xlab = 'Conversation Demands' , ylab = 'Errors' ) # Add the legend legend ( 'topright' , c ( 'Difficult' , 'Easy' ), title = 'Driving' , fill = c ( 'orange' , 'blue' )) The driving errors made during different driving conditions are influenced by the level of conversation demand. In other words, the driving conditions have a different effect on the number of errors made, depending on the level of conversation demand. The homogeneity of variance assumption Before we do factorial ANOVA, we need to test the homogeneity of the variance assumption. When studying one-way ANOVA, we tested this assumption with the leveneTest function. We now have two independent variables instead of just one. 1 2 # Test the homogeneity of variance assumption leveneTest ( ab $ errors ~ ab $ conversation * ab $ driving ) 1 2 3 4 ## Levene's Test for Homogeneity of Variance (center = median) ## Df F value Pr(&gt;F) ## group 5 0.5206 0.7602 ## 114 The homogeneity of variance assumption holds. By performing a leveneTest , we can check whether or not the homogeneity of variance assumption holds for a given dataset. The assumption must hold for the results of an ANOVA analysis to be valid. Recall from the first chapter that ANOVA makes use of F-statistics, or F-ratios, in which two types of degrees of freedom are involved. 1 dim ( ab ) 1 ## [1] 120 5 1 str ( ab $ conversation ) 1 ## Factor w/ 3 levels \"High demand\",..: 3 3 3 3 3 3 3 3 3 3 ... 1 str ( ab $ driving ) 1 ## Factor w/ 2 levels \"Difficult\",\"Easy\": 2 2 2 2 2 2 2 2 2 2 ... There are 120 subjets and 2 (Easy, Difficult) * 3 (High Demand, Low Demand, None) = 6 groups. The factorial ANOVA 1 2 3 4 5 # Factorial ANOVA ab_model <- aov ( ab $ errors ~ ab $ conversation * ab $ driving ) # Get the summary table summary ( ab_model ) 1 2 3 4 5 6 7 ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## ab$conversation 2 4416 2208 36.14 6.98e-13 *** ## ab$driving 1 5782 5782 94.64 &lt; 2e-16 *** ## ab$conversation:ab$driving 2 1639 820 13.41 5.86e-06 *** ## Residuals 114 6965 61 ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 Based on the summary table of the factorial ANOVA, the main effect for driving difficulty, the main effect for conversation difficulty and the interaction effect are all significant. The interaction effect Now it\u2019s time to explore the interaction effect. You will do this with the help of a simple effects analysis. Why a simple effects analysis? Well, remember what we had to do when you had a significant main effect in a one-way ANOVA? There, you just had to perform some post-hoc tests to see from which level of the categorical variable the main effect was coming. With an interaction effect, it is quite similar. Conduct a simple effects analysis of the variable conversation on the outcome variable errors at each level of driving . 1 2 3 4 5 6 7 8 9 10 # Create the two subsets ab_1 <- subset ( ab , ab $ driving == 'Easy' ) ab_2 <- subset ( ab , ab $ driving == 'Difficult' ) # Perform the one-way ANOVA for both subsets aov_ab_1 <- aov ( ab_1 $ errors ~ ab_1 $ conversation ) aov_ab_2 <- aov ( ab_2 $ errors ~ ab_2 $ conversation ) # Get the summary tables for both aov_ab_1 and aov_ab_2 summary ( aov_ab_1 ) 1 2 3 4 5 ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## ab_1$conversation 2 504.7 252.3 4.928 0.0106 * ## Residuals 57 2918.5 51.2 ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 1 summary ( aov_ab_2 ) 1 2 3 4 5 ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## ab_2$conversation 2 5551 2776 39.09 2.05e-11 *** ## Residuals 57 4047 71 ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 There a significant simple effect for the easy driving condition based on the summary table of aov_ab_1 . There a significant simple effect for the difficult driving condition based on the summary table of aov_ab_2 . The effect sizes The definition of an interaction effect states that the effect of one variable changes across levels of the other variable. For example, we might expect the effect of conversation to be greater when driving conditions are difficult than when they are relatively easy. Unfortunately, it is not quite that simple. In order to really understand the different effect sizes, you should make use of the etaSquared function. 1 2 3 4 5 6 library ( lsr ) # Calculate the etaSquared for the easy driving case #easy is ab_1 #difficult is ab_2 etaSquared ( aov_ab_1 , anova = TRUE ) 1 2 3 4 5 6 ## eta.sq eta.sq.part SS df MS F ## ab_1$conversation 0.147433 0.147433 504.70 2 252.35000 4.928458 ## Residuals 0.852567 NA 2918.55 57 51.20263 NA ## p ## ab_1$conversation 0.01061116 ## Residuals NA 1 2 # Calculate the etaSquared for the difficult driving case etaSquared ( aov_ab_2 , anova = TRUE ) 1 2 3 4 5 6 ## eta.sq eta.sq.part SS df MS F ## ab_2$conversation 0.5783571 0.5783571 5551.033 2 2775.51667 39.09275 ## Residuals 0.4216429 NA 4046.900 57 70.99825 NA ## p ## ab_2$conversation 2.046097e-11 ## Residuals NA Based on the output of the etaSquared function for the easy driving condition, the percentage of variance explained by the conversation variable is 14.7%; the percentage of variance explained by the conversation variable is 57.8%. Pairwise comparisons Finally, let us look at pairwise comparisons for the simple effects. You can do this with the Tukey post-hoc test. 1 2 # Tukey for easy driving TukeyHSD ( aov_ab_1 ) 1 2 3 4 5 6 7 8 9 10 ## Tukey multiple comparisons of means ## 95% family-wise confidence level ## ## Fit: aov(formula = ab_1$errors ~ ab_1$conversation) ## ## $`ab_1$conversation` ## diff lwr upr p adj ## Low demand-High demand -6.05 -11.495243 -0.6047574 0.0260458 ## None-High demand -6.25 -11.695243 -0.8047574 0.0207614 ## None-Low demand -0.20 -5.645243 5.2452426 0.9957026 1 2 # Tukey for difficult driving TukeyHSD ( aov_ab_2 ) 1 2 3 4 5 6 7 8 9 10 ## Tukey multiple comparisons of means ## 95% family-wise confidence level ## ## Fit: aov(formula = ab_2$errors ~ ab_2$conversation) ## ## $`ab_2$conversation` ## diff lwr upr p adj ## Low demand-High demand -9.75 -16.16202 -3.337979 0.0015849 ## None-High demand -23.45 -29.86202 -17.037979 0.0000000 ## None-Low demand -13.70 -20.11202 -7.287979 0.0000103 For \u2018Easy Driving\u2019, two mean differences in terms of number of errors are significant (below 0.05). For \u2018Difficult Driving\u2019, three mean differences in terms of number of errors are significant (below 0.05).","title":"Between groups factorial ANOVA"},{"location":"a_hands-on_introduction_to_statistics_with_r_course_two_student_s_t-test/","text":"Foreword Output options: the \u2018tango\u2019 syntax and the \u2018readable\u2019 theme. Snippets and results. Introduction to t-tests \u00b6 Test p-values for significance with z-tests, t-tests. Single sample t-test: group of people from a particular geographic region perform on a well-known test of intelligence. In particular, you are interested in finding out whether or not this group scores significantly higher than the overall population on an IQ test. This is a form of Null Hypothesis Significance Testing (NHST), where the null hypothesis is that there\u2019s no difference between this group and the overall population. Dependent t-test: single group of voters to rate their likelihood of voting for the candidate before the speech and again after the speech; understand if voters from a particular neighborhood are likely to vote differently when compared to the overall voting population. Independent t-test: significant difference in preferences between these two groups; compare liberals and convervatives. t-distribution, observed value, expected value, standard error. Generate density plots of different t-distributions 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 # Generate a vector of 100 values between -4 and 4 x <- seq ( -4 , 4 , length = 100 ) # Simulate the t-distribution y_1 <- dt ( x , 4 ) y_2 <- dt ( x , 6 ) y_3 <- dt ( x , 8 ) y_4 <- dt ( x , 10 ) y_5 <- dt ( x , 12 ) # Plot the t-distributions plot ( x , y_1 , type = 'l' , lwd = 2 , xlab = 'T value' , ylab = 'Density' , main = 'Comparison of t-distributions' ) lines ( x , y_2 , col = 'red' ) #lines(x, y_3, col = 'orange') #lines(x, y_4, col = 'green') #lines(x, y_5, col = 'blue') # Add a legend legend ( 'topright' , c ( 'df = 4' , 'df = 6' , 'df = 8' , 'df = 10' , 'df = 12' ), title = 'T distributions' , col = c ( 'black' , 'red' , 'orange' , 'green' , 'blue' ), lty = 1 ) The working memory dataset Conduct a dependent (or paired) t-test on the \u201cworking memory\u201d dataset. This dataset consists of the intelligence scores for subjects before and after training, as well as for a control group. Our goal is to assess whether intelligence training results in significantly different intelligence scores for the individuals. The observations of individuals before and after training are two samples from the same group at different points in time, which calls for a dependent t-test. This will test whether or not the difference in mean intelligence scores before and after training are significant. 1 2 # Print the data set in the console head ( wm ) 1 2 3 4 5 6 7 ## cond pre post gain train ## 1 t08 8 9 1 1 ## 2 t08 8 10 2 1 ## 3 t08 8 8 0 1 ## 4 t08 8 7 -1 1 ## 5 t08 9 11 2 1 ## 6 t08 9 10 1 1 1 2 3 4 5 6 7 library ( Hmisc ) # Create a subset for the data that contains information on those subject who trained wm_t <- subset ( wm , wm $ train == 1 ) # Summary statistics describe ( wm_t ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 ## wm_t ## ## 5 Variables 80 Observations ## --------------------------------------------------------------------------- ## cond ## n missing distinct ## 80 0 4 ## ## Value t08 t12 t17 t19 ## Frequency 20 20 20 20 ## Proportion 0.25 0.25 0.25 0.25 ## --------------------------------------------------------------------------- ## pre ## n missing distinct Info Mean Gmd ## 80 0 5 0.955 10.03 1.551 ## ## Value 8 9 10 11 12 ## Frequency 12 20 19 12 17 ## Proportion 0.150 0.250 0.238 0.150 0.212 ## --------------------------------------------------------------------------- ## post ## n missing distinct Info Mean Gmd .05 .10 ## 80 0 13 0.984 13.51 2.87 9.95 10.00 ## .25 .50 .75 .90 .95 ## 12.00 14.00 15.00 17.00 18.00 ## ## Value 7 8 9 10 11 12 13 14 15 16 ## Frequency 1 1 2 5 8 11 11 15 8 9 ## Proportion 0.012 0.012 0.025 0.062 0.100 0.138 0.138 0.188 0.100 0.112 ## ## Value 17 18 19 ## Frequency 4 2 3 ## Proportion 0.050 0.025 0.038 ## --------------------------------------------------------------------------- ## gain ## n missing distinct Info Mean Gmd .05 .10 ## 80 0 10 0.975 3.487 2.415 0.0 1.0 ## .25 .50 .75 .90 .95 ## 2.0 3.0 5.0 6.1 7.0 ## ## Value -1 0 1 2 3 4 5 6 7 9 ## Frequency 2 3 7 18 12 16 6 8 6 2 ## Proportion 0.025 0.038 0.088 0.225 0.150 0.200 0.075 0.100 0.075 0.025 ## --------------------------------------------------------------------------- ## train ## n missing distinct Info Mean Gmd ## 80 0 1 0 1 0 ## ## Value 1 ## Frequency 80 ## Proportion 1 ## --------------------------------------------------------------------------- 1 2 # Create a boxplot with pre- and post-training groups boxplot ( wm_t $ pre , wm_t $ post , main = \"Boxplot\" , xlab = \"Pre and Post Training\" , ylab = \"Intelligence Score\" , col = c ( \"red\" , \"green\" )) Performing dependent t-tests manually in R (1) Conducting a dependent t-test, also known as a paired t-test, requires the following steps: Define null and alternative hypotheses Decide significance level \u03b1 Compute observed t-value Find critical value Compare observed value to critical value We\u2019re performing a Null Hypothesis Significance Test (NHST), so our null hypothesis is that there\u2019s no effect (i.e. training has no impact on intelligence scores). The alternative hypothesis is that training results in signficantly different intelligence scores. We\u2019ll use a significance level of 0.05, which is very common in statistics. Compute the observed t-value. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 # Define the sample size n <- dim ( wm_t ) [1] # Calculate the degrees of freedom df <- n - 1 # Find the critical t-value t_crit <- abs ( qt ( 0.025 , df )) # Calculate the mean of the difference in scores. The differences are already in the dataset under the column 'gain'. mean_diff <- sum ( wm_t $ gain ) / n # Calculate the standard deviation xdt <- sum ( wm_t $ gain^2 ) xdt2 <- xdt / n sd_diff2 <- sqrt (( xdt - xdt2 ) / ( n - 1 )) sd_diff <- sqrt (( sum ( wm_t $ gain^2 ) - (( sum ( wm_t $ gain )) ^2 / n )) / ( n - 1 )) sd_diff2 1 ## [1] 4.091149 1 sd_diff 1 ## [1] 2.152383 Performing dependent t-tests manually in R (2) Now that we\u2019ve determined our null and alternative hypotheses, decided on a significance level, and computed our observed t-value, all that remains is to calculate the critical value for this test and compare it to our observed t-value. This will tell us whether we have sufficient evidence to reject our null hypothesis. We\u2019ll even go one step further and compute an effect size with Cohen\u2019s d! The critical value is the point on the relevant t-distribution that determines whether the value we observed is extreme enough to warrant rejecting the null hypothesis. Recall that a t-distribution is defined by its degrees of freedom, which in turn is equal to the sample size minus 1. In this example, we have 80 subjects so the relevant t-distribution has 79 degrees of freedom. We\u2019re performing a two-tailed t-test in this situation since we care about detecting a significant effect in either the positive or negative direction. In other words, we want to know if training significantly increases or decreases intelligence, however, given that our observed t-value is positive (14.49) the right-hand is the only relevant value here. Furthermore, since our desired significance level (i.e. alpha) is 0.05, our critical value is the point on our t-distribution at which 0.025 (0.05 / 2) of its total area of 1 is to the right and thus 0.975 (1 - 0.025) of its total area is to the left. This point is called the 0.975 quantile and is computed for a t-distrbution. 1 2 3 4 5 6 7 8 9 10 11 12 # The variables from the previous exercise are still preloaded, type ls() in the console to see them n <- dim ( wm_t ) [1] df <- n - 1 t_crit <- abs ( qt ( 0.025 , df )) mean_diff <- sum ( wm_t $ gain ) / n sd_diff <- sqrt (( sum ( wm_t $ gain^2 ) - (( sum ( wm_t $ gain )) ^2 / n )) / ( n - 1 )) # Calculate the t-value for this test t_value <- mean_diff / ( sd_diff / sqrt ( n )) # Check whether or not the mean difference is statistically significant t_value 1 ## [1] 14.49238 1 t_crit 1 ## [1] 1.99045 1 2 3 4 # Calculate the confidence interval conf_upper <- mean_diff + t_crit * ( sd_diff / sqrt ( n )) conf_lower <- mean_diff - t_crit * ( sd_diff / sqrt ( n )) conf_upper 1 ## [1] 3.966489 1 conf_lower 1 ## [1] 3.008511 1 2 3 # Calculate Cohen's d cohens_d <- mean_diff / sd_diff cohens_d 1 ## [1] 1.620297 Letting R do all the dirty work: Dependent t-tests The CohensD function (not showns). Cohen\u2019s d Determine that the difference pre- and post-training is statistically significant; and the effect size, meaning the effect of training on intelligence gains particularly strong or not. Cohen\u2019s d is unbiased by sample size. Cohen\u2019s d provides a standardized difference between two means. Cohen\u2019s d is calculated by subtracting one group mean from the other, then dividing by the pooled standard deviation. 1 2 # Conduct a paired t-test using the t.test function t.test ( wm_t $ post , wm_t $ pre , paired = TRUE ) 1 2 3 4 5 6 7 8 9 10 11 ## ## Paired t-test ## ## data: wm_t$post and wm_t$pre ## t = 14.492, df = 79, p-value &lt; 2.2e-16 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## 3.008511 3.966489 ## sample estimates: ## mean of the differences ## 3.4875 1 2 3 4 # Calculate Cohen's d library ( lsr ) cohensD ( wm_t $ post , wm_t $ pre , method = 'paired' ) 1 ## [1] 1.620297 Independent t-tests \u00b6 An independent t-test is appropriate when you want to compare the the means for two independent groups. Preliminary statistics For independent t-tests you will revisit the working memory dataset from the previous chapter. In this dataset, subjects were randomly assigned to four different training groups that trained for 8, 12, 17 and 19 days. 1 2 3 4 5 # Read in the data set and assign to the object wm_t <- readWorksheetFromFile ( 'A Hands-on Introduction to Statistics with R.xls' , sheet = 'wm_t' , header = TRUE , startCol = 1 , startRow = 1 ) # This will print the data set in the console head ( wm_t ) 1 2 3 4 5 6 7 ## cond pre post gain train ## 1 t08 8 9 1 1 ## 2 t08 8 10 2 1 ## 3 t08 8 8 0 1 ## 4 t08 8 7 -1 1 ## 5 t08 9 11 2 1 ## 6 t08 9 10 1 1 Add statistical functions. 1 2 # Load the psych(ology) package library ( psych ) Levene\u2019s test for homogeneity of variance. 1 2 3 4 5 6 7 8 # Create subsets for each training time wm_t08 <- subset ( wm_t , cond == 't08' ) wm_t12 <- subset ( wm_t , cond == 't12' ) wm_t17 <- subset ( wm_t , cond == 't17' ) wm_t19 <- subset ( wm_t , cond == 't19' ) # Summary statistics of the change in training scores before and after exercise describe ( wm_t08 ) 1 2 3 4 5 6 7 8 9 10 11 12 ## vars n mean sd median trimmed mad min max range skew kurtosis ## cond* 1 20 NaN NA NA NaN NA Inf -Inf -Inf NA NA ## pre 2 20 10.05 1.50 10.0 10.06 1.48 8 12 4 0.01 -1.53 ## post 3 20 11.40 2.14 11.5 11.50 2.22 7 15 8 -0.25 -0.84 ## gain 4 20 1.35 1.23 1.0 1.44 1.48 -1 3 4 -0.32 -0.82 ## train 5 20 1.00 0.00 1.0 1.00 0.00 1 1 0 NaN NaN ## se ## cond* NA ## pre 0.34 ## post 0.48 ## gain 0.27 ## train 0.00 1 describe ( wm_t12 ) 1 2 3 4 5 6 7 8 9 10 11 12 ## vars n mean sd median trimmed mad min max range skew kurtosis ## cond* 1 20 NaN NA NA NaN NA Inf -Inf -Inf NA NA ## pre 2 20 9.9 1.45 10 9.88 1.48 8 12 4 0.16 -1.43 ## post 3 20 12.5 1.88 12 12.38 2.22 10 17 7 0.48 -0.54 ## gain 4 20 2.6 1.27 2 2.50 0.00 0 5 5 0.44 -0.54 ## train 5 20 1.0 0.00 1 1.00 0.00 1 1 0 NaN NaN ## se ## cond* NA ## pre 0.32 ## post 0.42 ## gain 0.28 ## train 0.00 1 describe ( wm_t17 ) 1 2 3 4 5 6 7 8 9 10 11 12 ## vars n mean sd median trimmed mad min max range skew kurtosis ## cond* 1 20 NaN NA NA NaN NA Inf -Inf -Inf NA NA ## pre 2 20 10.0 1.34 10 10.00 1.48 8 12 4 0.25 -1.34 ## post 3 20 14.4 1.85 14 14.25 1.48 12 19 7 0.63 -0.27 ## gain 4 20 4.4 1.39 4 4.25 1.48 3 7 4 0.64 -1.12 ## train 5 20 1.0 0.00 1 1.00 0.00 1 1 0 NaN NaN ## se ## cond* NA ## pre 0.30 ## post 0.41 ## gain 0.31 ## train 0.00 1 describe ( wm_t19 ) 1 2 3 4 5 6 7 8 9 10 11 12 ## vars n mean sd median trimmed mad min max range skew kurtosis ## cond* 1 20 NaN NA NA NaN NA Inf -Inf -Inf NA NA ## pre 2 20 10.15 1.27 10.0 10.19 1.48 8 12 4 0.03 -1.10 ## post 3 20 15.75 1.86 16.0 15.69 1.48 13 19 6 0.16 -1.03 ## gain 4 20 5.60 1.73 5.5 5.50 2.22 3 9 6 0.36 -0.76 ## train 5 20 1.00 0.00 1.0 1.00 0.00 1 1 0 NaN NaN ## se ## cond* NA ## pre 0.28 ## post 0.42 ## gain 0.39 ## train 0.00 1 2 # Create a boxplot of the different training times ggplot ( wm_t , aes ( x = cond , y = gain , fill = cond )) + geom_boxplot () 1 2 3 4 # Levene's test library ( car ) leveneTest ( wm_t $ gain ~ wm_t $ cond ) 1 2 3 4 ## Levene's Test for Homogeneity of Variance (center = median) ## Df F value Pr(&gt;F) ## group 3 1.3134 0.2763 ## 76 Conducting an independent t-test manually (1) Perform an independent t-test the same way we did for the dependent t-test in the previous chapter. Continuing with the working memory example, our null hypothesis is that the difference in intelligence score gain between the group that trained for 8 days and the group that trained for 19 days is equal to zero. If our observed t-value is sufficiently large, we can reject the null in favor of the alternative hypothesis, which would imply a significant difference in intelligence gain between the two training groups. Calculation of the observed t-value for an independent t-test is similar to the dependent t-test, but involves slightly different formulas. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 # Calculate mean difference by subtracting the gain for t08 by the gain for t19 mean_t08 <- mean ( wm_t08 $ gain ) mean_t19 <- mean ( wm_t19 $ gain ) mean_diff <- ( mean_t19 - mean_t08 ) # Calculate degrees of freedom n_t08 <- dim ( wm_t08 ) [1] n_t19 <- dim ( wm_t19 ) [1] df <- n_t08 + n_t19 - 2 # Calculate the pooled standard error var_t08 <- ( sum (( wm_t08 $ gain - mean_t08 ) ^2 )) / ( n_t08 - 1 ) var_t19 <- ( sum (( wm_t19 $ gain - mean_t19 ) ^2 )) / ( n_t19 - 1 ) se_pooled <- sqrt (( var_t08 / n_t08 ) + ( var_t19 / n_t19 )) Conducting an independent t-test manually (2) Compute the observed t-value. Then we will determine the p-value using the relevant t-distribution. If you recall, in the last chapter we calculated the critical value. The p-value is simply an alternative approach to hypothesis testing and determining the significance of your results. It\u2019s good to practice both! Finally, we will finish by calculating effect size via Cohen\u2019s d. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 # All variables from the previous exercises are preloaded in your workspace # Type ls() to see them # Calculate the t-value t_value <- mean_diff / se_pooled # Calculate p-value #two-tail test, 0.05/2 = 0.025 p_value <- 2 * ( 1 - pt ( t_value , df = df )) # Calculate Cohen's d sd_t08 <- sd ( wm_t08 $ gain ) sd_t19 <- sd ( wm_t19 $ gain ) pooled_sd <- ( sd_t08 + sd_t19 ) / 2 cohens_d <- mean_diff / pooled_sd Letting R do all the dirty work: Independent t-tests 1 2 # Conduct an independent t-test t.test ( wm_t19 $ gain , wm_t08 $ gain , var.equal = TRUE ) 1 2 3 4 5 6 7 8 9 10 11 ## ## Two Sample t-test ## ## data: wm_t19$gain and wm_t08$gain ## t = 8.9677, df = 38, p-value = 6.443e-11 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## 3.290588 5.209412 ## sample estimates: ## mean of x mean of y ## 5.60 1.35 1 2 # Calculate Cohen's d cohensD ( wm_t19 $ gain , wm_t08 $ gain , method = 'pooled' ) 1 ## [1] 2.835822","title":"Statistics with R, Course Two, Student's t-test"},{"location":"a_hands-on_introduction_to_statistics_with_r_course_two_student_s_t-test/#introduction-to-t-tests","text":"Test p-values for significance with z-tests, t-tests. Single sample t-test: group of people from a particular geographic region perform on a well-known test of intelligence. In particular, you are interested in finding out whether or not this group scores significantly higher than the overall population on an IQ test. This is a form of Null Hypothesis Significance Testing (NHST), where the null hypothesis is that there\u2019s no difference between this group and the overall population. Dependent t-test: single group of voters to rate their likelihood of voting for the candidate before the speech and again after the speech; understand if voters from a particular neighborhood are likely to vote differently when compared to the overall voting population. Independent t-test: significant difference in preferences between these two groups; compare liberals and convervatives. t-distribution, observed value, expected value, standard error. Generate density plots of different t-distributions 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 # Generate a vector of 100 values between -4 and 4 x <- seq ( -4 , 4 , length = 100 ) # Simulate the t-distribution y_1 <- dt ( x , 4 ) y_2 <- dt ( x , 6 ) y_3 <- dt ( x , 8 ) y_4 <- dt ( x , 10 ) y_5 <- dt ( x , 12 ) # Plot the t-distributions plot ( x , y_1 , type = 'l' , lwd = 2 , xlab = 'T value' , ylab = 'Density' , main = 'Comparison of t-distributions' ) lines ( x , y_2 , col = 'red' ) #lines(x, y_3, col = 'orange') #lines(x, y_4, col = 'green') #lines(x, y_5, col = 'blue') # Add a legend legend ( 'topright' , c ( 'df = 4' , 'df = 6' , 'df = 8' , 'df = 10' , 'df = 12' ), title = 'T distributions' , col = c ( 'black' , 'red' , 'orange' , 'green' , 'blue' ), lty = 1 ) The working memory dataset Conduct a dependent (or paired) t-test on the \u201cworking memory\u201d dataset. This dataset consists of the intelligence scores for subjects before and after training, as well as for a control group. Our goal is to assess whether intelligence training results in significantly different intelligence scores for the individuals. The observations of individuals before and after training are two samples from the same group at different points in time, which calls for a dependent t-test. This will test whether or not the difference in mean intelligence scores before and after training are significant. 1 2 # Print the data set in the console head ( wm ) 1 2 3 4 5 6 7 ## cond pre post gain train ## 1 t08 8 9 1 1 ## 2 t08 8 10 2 1 ## 3 t08 8 8 0 1 ## 4 t08 8 7 -1 1 ## 5 t08 9 11 2 1 ## 6 t08 9 10 1 1 1 2 3 4 5 6 7 library ( Hmisc ) # Create a subset for the data that contains information on those subject who trained wm_t <- subset ( wm , wm $ train == 1 ) # Summary statistics describe ( wm_t ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 ## wm_t ## ## 5 Variables 80 Observations ## --------------------------------------------------------------------------- ## cond ## n missing distinct ## 80 0 4 ## ## Value t08 t12 t17 t19 ## Frequency 20 20 20 20 ## Proportion 0.25 0.25 0.25 0.25 ## --------------------------------------------------------------------------- ## pre ## n missing distinct Info Mean Gmd ## 80 0 5 0.955 10.03 1.551 ## ## Value 8 9 10 11 12 ## Frequency 12 20 19 12 17 ## Proportion 0.150 0.250 0.238 0.150 0.212 ## --------------------------------------------------------------------------- ## post ## n missing distinct Info Mean Gmd .05 .10 ## 80 0 13 0.984 13.51 2.87 9.95 10.00 ## .25 .50 .75 .90 .95 ## 12.00 14.00 15.00 17.00 18.00 ## ## Value 7 8 9 10 11 12 13 14 15 16 ## Frequency 1 1 2 5 8 11 11 15 8 9 ## Proportion 0.012 0.012 0.025 0.062 0.100 0.138 0.138 0.188 0.100 0.112 ## ## Value 17 18 19 ## Frequency 4 2 3 ## Proportion 0.050 0.025 0.038 ## --------------------------------------------------------------------------- ## gain ## n missing distinct Info Mean Gmd .05 .10 ## 80 0 10 0.975 3.487 2.415 0.0 1.0 ## .25 .50 .75 .90 .95 ## 2.0 3.0 5.0 6.1 7.0 ## ## Value -1 0 1 2 3 4 5 6 7 9 ## Frequency 2 3 7 18 12 16 6 8 6 2 ## Proportion 0.025 0.038 0.088 0.225 0.150 0.200 0.075 0.100 0.075 0.025 ## --------------------------------------------------------------------------- ## train ## n missing distinct Info Mean Gmd ## 80 0 1 0 1 0 ## ## Value 1 ## Frequency 80 ## Proportion 1 ## --------------------------------------------------------------------------- 1 2 # Create a boxplot with pre- and post-training groups boxplot ( wm_t $ pre , wm_t $ post , main = \"Boxplot\" , xlab = \"Pre and Post Training\" , ylab = \"Intelligence Score\" , col = c ( \"red\" , \"green\" )) Performing dependent t-tests manually in R (1) Conducting a dependent t-test, also known as a paired t-test, requires the following steps: Define null and alternative hypotheses Decide significance level \u03b1 Compute observed t-value Find critical value Compare observed value to critical value We\u2019re performing a Null Hypothesis Significance Test (NHST), so our null hypothesis is that there\u2019s no effect (i.e. training has no impact on intelligence scores). The alternative hypothesis is that training results in signficantly different intelligence scores. We\u2019ll use a significance level of 0.05, which is very common in statistics. Compute the observed t-value. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 # Define the sample size n <- dim ( wm_t ) [1] # Calculate the degrees of freedom df <- n - 1 # Find the critical t-value t_crit <- abs ( qt ( 0.025 , df )) # Calculate the mean of the difference in scores. The differences are already in the dataset under the column 'gain'. mean_diff <- sum ( wm_t $ gain ) / n # Calculate the standard deviation xdt <- sum ( wm_t $ gain^2 ) xdt2 <- xdt / n sd_diff2 <- sqrt (( xdt - xdt2 ) / ( n - 1 )) sd_diff <- sqrt (( sum ( wm_t $ gain^2 ) - (( sum ( wm_t $ gain )) ^2 / n )) / ( n - 1 )) sd_diff2 1 ## [1] 4.091149 1 sd_diff 1 ## [1] 2.152383 Performing dependent t-tests manually in R (2) Now that we\u2019ve determined our null and alternative hypotheses, decided on a significance level, and computed our observed t-value, all that remains is to calculate the critical value for this test and compare it to our observed t-value. This will tell us whether we have sufficient evidence to reject our null hypothesis. We\u2019ll even go one step further and compute an effect size with Cohen\u2019s d! The critical value is the point on the relevant t-distribution that determines whether the value we observed is extreme enough to warrant rejecting the null hypothesis. Recall that a t-distribution is defined by its degrees of freedom, which in turn is equal to the sample size minus 1. In this example, we have 80 subjects so the relevant t-distribution has 79 degrees of freedom. We\u2019re performing a two-tailed t-test in this situation since we care about detecting a significant effect in either the positive or negative direction. In other words, we want to know if training significantly increases or decreases intelligence, however, given that our observed t-value is positive (14.49) the right-hand is the only relevant value here. Furthermore, since our desired significance level (i.e. alpha) is 0.05, our critical value is the point on our t-distribution at which 0.025 (0.05 / 2) of its total area of 1 is to the right and thus 0.975 (1 - 0.025) of its total area is to the left. This point is called the 0.975 quantile and is computed for a t-distrbution. 1 2 3 4 5 6 7 8 9 10 11 12 # The variables from the previous exercise are still preloaded, type ls() in the console to see them n <- dim ( wm_t ) [1] df <- n - 1 t_crit <- abs ( qt ( 0.025 , df )) mean_diff <- sum ( wm_t $ gain ) / n sd_diff <- sqrt (( sum ( wm_t $ gain^2 ) - (( sum ( wm_t $ gain )) ^2 / n )) / ( n - 1 )) # Calculate the t-value for this test t_value <- mean_diff / ( sd_diff / sqrt ( n )) # Check whether or not the mean difference is statistically significant t_value 1 ## [1] 14.49238 1 t_crit 1 ## [1] 1.99045 1 2 3 4 # Calculate the confidence interval conf_upper <- mean_diff + t_crit * ( sd_diff / sqrt ( n )) conf_lower <- mean_diff - t_crit * ( sd_diff / sqrt ( n )) conf_upper 1 ## [1] 3.966489 1 conf_lower 1 ## [1] 3.008511 1 2 3 # Calculate Cohen's d cohens_d <- mean_diff / sd_diff cohens_d 1 ## [1] 1.620297 Letting R do all the dirty work: Dependent t-tests The CohensD function (not showns). Cohen\u2019s d Determine that the difference pre- and post-training is statistically significant; and the effect size, meaning the effect of training on intelligence gains particularly strong or not. Cohen\u2019s d is unbiased by sample size. Cohen\u2019s d provides a standardized difference between two means. Cohen\u2019s d is calculated by subtracting one group mean from the other, then dividing by the pooled standard deviation. 1 2 # Conduct a paired t-test using the t.test function t.test ( wm_t $ post , wm_t $ pre , paired = TRUE ) 1 2 3 4 5 6 7 8 9 10 11 ## ## Paired t-test ## ## data: wm_t$post and wm_t$pre ## t = 14.492, df = 79, p-value &lt; 2.2e-16 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## 3.008511 3.966489 ## sample estimates: ## mean of the differences ## 3.4875 1 2 3 4 # Calculate Cohen's d library ( lsr ) cohensD ( wm_t $ post , wm_t $ pre , method = 'paired' ) 1 ## [1] 1.620297","title":"Introduction to t-tests"},{"location":"a_hands-on_introduction_to_statistics_with_r_course_two_student_s_t-test/#independent-t-tests","text":"An independent t-test is appropriate when you want to compare the the means for two independent groups. Preliminary statistics For independent t-tests you will revisit the working memory dataset from the previous chapter. In this dataset, subjects were randomly assigned to four different training groups that trained for 8, 12, 17 and 19 days. 1 2 3 4 5 # Read in the data set and assign to the object wm_t <- readWorksheetFromFile ( 'A Hands-on Introduction to Statistics with R.xls' , sheet = 'wm_t' , header = TRUE , startCol = 1 , startRow = 1 ) # This will print the data set in the console head ( wm_t ) 1 2 3 4 5 6 7 ## cond pre post gain train ## 1 t08 8 9 1 1 ## 2 t08 8 10 2 1 ## 3 t08 8 8 0 1 ## 4 t08 8 7 -1 1 ## 5 t08 9 11 2 1 ## 6 t08 9 10 1 1 Add statistical functions. 1 2 # Load the psych(ology) package library ( psych ) Levene\u2019s test for homogeneity of variance. 1 2 3 4 5 6 7 8 # Create subsets for each training time wm_t08 <- subset ( wm_t , cond == 't08' ) wm_t12 <- subset ( wm_t , cond == 't12' ) wm_t17 <- subset ( wm_t , cond == 't17' ) wm_t19 <- subset ( wm_t , cond == 't19' ) # Summary statistics of the change in training scores before and after exercise describe ( wm_t08 ) 1 2 3 4 5 6 7 8 9 10 11 12 ## vars n mean sd median trimmed mad min max range skew kurtosis ## cond* 1 20 NaN NA NA NaN NA Inf -Inf -Inf NA NA ## pre 2 20 10.05 1.50 10.0 10.06 1.48 8 12 4 0.01 -1.53 ## post 3 20 11.40 2.14 11.5 11.50 2.22 7 15 8 -0.25 -0.84 ## gain 4 20 1.35 1.23 1.0 1.44 1.48 -1 3 4 -0.32 -0.82 ## train 5 20 1.00 0.00 1.0 1.00 0.00 1 1 0 NaN NaN ## se ## cond* NA ## pre 0.34 ## post 0.48 ## gain 0.27 ## train 0.00 1 describe ( wm_t12 ) 1 2 3 4 5 6 7 8 9 10 11 12 ## vars n mean sd median trimmed mad min max range skew kurtosis ## cond* 1 20 NaN NA NA NaN NA Inf -Inf -Inf NA NA ## pre 2 20 9.9 1.45 10 9.88 1.48 8 12 4 0.16 -1.43 ## post 3 20 12.5 1.88 12 12.38 2.22 10 17 7 0.48 -0.54 ## gain 4 20 2.6 1.27 2 2.50 0.00 0 5 5 0.44 -0.54 ## train 5 20 1.0 0.00 1 1.00 0.00 1 1 0 NaN NaN ## se ## cond* NA ## pre 0.32 ## post 0.42 ## gain 0.28 ## train 0.00 1 describe ( wm_t17 ) 1 2 3 4 5 6 7 8 9 10 11 12 ## vars n mean sd median trimmed mad min max range skew kurtosis ## cond* 1 20 NaN NA NA NaN NA Inf -Inf -Inf NA NA ## pre 2 20 10.0 1.34 10 10.00 1.48 8 12 4 0.25 -1.34 ## post 3 20 14.4 1.85 14 14.25 1.48 12 19 7 0.63 -0.27 ## gain 4 20 4.4 1.39 4 4.25 1.48 3 7 4 0.64 -1.12 ## train 5 20 1.0 0.00 1 1.00 0.00 1 1 0 NaN NaN ## se ## cond* NA ## pre 0.30 ## post 0.41 ## gain 0.31 ## train 0.00 1 describe ( wm_t19 ) 1 2 3 4 5 6 7 8 9 10 11 12 ## vars n mean sd median trimmed mad min max range skew kurtosis ## cond* 1 20 NaN NA NA NaN NA Inf -Inf -Inf NA NA ## pre 2 20 10.15 1.27 10.0 10.19 1.48 8 12 4 0.03 -1.10 ## post 3 20 15.75 1.86 16.0 15.69 1.48 13 19 6 0.16 -1.03 ## gain 4 20 5.60 1.73 5.5 5.50 2.22 3 9 6 0.36 -0.76 ## train 5 20 1.00 0.00 1.0 1.00 0.00 1 1 0 NaN NaN ## se ## cond* NA ## pre 0.28 ## post 0.42 ## gain 0.39 ## train 0.00 1 2 # Create a boxplot of the different training times ggplot ( wm_t , aes ( x = cond , y = gain , fill = cond )) + geom_boxplot () 1 2 3 4 # Levene's test library ( car ) leveneTest ( wm_t $ gain ~ wm_t $ cond ) 1 2 3 4 ## Levene's Test for Homogeneity of Variance (center = median) ## Df F value Pr(&gt;F) ## group 3 1.3134 0.2763 ## 76 Conducting an independent t-test manually (1) Perform an independent t-test the same way we did for the dependent t-test in the previous chapter. Continuing with the working memory example, our null hypothesis is that the difference in intelligence score gain between the group that trained for 8 days and the group that trained for 19 days is equal to zero. If our observed t-value is sufficiently large, we can reject the null in favor of the alternative hypothesis, which would imply a significant difference in intelligence gain between the two training groups. Calculation of the observed t-value for an independent t-test is similar to the dependent t-test, but involves slightly different formulas. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 # Calculate mean difference by subtracting the gain for t08 by the gain for t19 mean_t08 <- mean ( wm_t08 $ gain ) mean_t19 <- mean ( wm_t19 $ gain ) mean_diff <- ( mean_t19 - mean_t08 ) # Calculate degrees of freedom n_t08 <- dim ( wm_t08 ) [1] n_t19 <- dim ( wm_t19 ) [1] df <- n_t08 + n_t19 - 2 # Calculate the pooled standard error var_t08 <- ( sum (( wm_t08 $ gain - mean_t08 ) ^2 )) / ( n_t08 - 1 ) var_t19 <- ( sum (( wm_t19 $ gain - mean_t19 ) ^2 )) / ( n_t19 - 1 ) se_pooled <- sqrt (( var_t08 / n_t08 ) + ( var_t19 / n_t19 )) Conducting an independent t-test manually (2) Compute the observed t-value. Then we will determine the p-value using the relevant t-distribution. If you recall, in the last chapter we calculated the critical value. The p-value is simply an alternative approach to hypothesis testing and determining the significance of your results. It\u2019s good to practice both! Finally, we will finish by calculating effect size via Cohen\u2019s d. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 # All variables from the previous exercises are preloaded in your workspace # Type ls() to see them # Calculate the t-value t_value <- mean_diff / se_pooled # Calculate p-value #two-tail test, 0.05/2 = 0.025 p_value <- 2 * ( 1 - pt ( t_value , df = df )) # Calculate Cohen's d sd_t08 <- sd ( wm_t08 $ gain ) sd_t19 <- sd ( wm_t19 $ gain ) pooled_sd <- ( sd_t08 + sd_t19 ) / 2 cohens_d <- mean_diff / pooled_sd Letting R do all the dirty work: Independent t-tests 1 2 # Conduct an independent t-test t.test ( wm_t19 $ gain , wm_t08 $ gain , var.equal = TRUE ) 1 2 3 4 5 6 7 8 9 10 11 ## ## Two Sample t-test ## ## data: wm_t19$gain and wm_t08$gain ## t = 8.9677, df = 38, p-value = 6.443e-11 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## 3.290588 5.209412 ## sample estimates: ## mean of x mean of y ## 5.60 1.35 1 2 # Calculate Cohen's d cohensD ( wm_t19 $ gain , wm_t08 $ gain , method = 'pooled' ) 1 ## [1] 2.835822","title":"Independent t-tests"},{"location":"a_hands-on_introduction_to_statistics_with_r_notes/","text":"Foreword Output options: the \u2018tango\u2019 syntax and the \u2018readable\u2019 theme. Codes and snippets. Descriptive Statistics \u00b6 Extract basic, exploratory statistics from datasets with apply , summary , fivenum , describe , and stat.desc . 1 2 # dataset head ( longley , 3 ) 1 2 3 4 ## GNP.deflator GNP Unemployed Armed.Forces Population Year Employed ## 1947 83.0 234.289 235.6 159.0 107.608 1947 60.323 ## 1948 88.5 259.426 232.5 145.6 108.632 1948 61.122 ## 1949 88.2 258.054 368.2 161.6 109.773 1949 60.171 1 2 3 # apply a function # excluding missing values sapply ( longley , mean , na.rm = TRUE ) 1 2 3 4 ## GNP.deflator GNP Unemployed Armed.Forces Population ## 101.6813 387.6984 319.3313 260.6687 117.4240 ## Year Employed ## 1954.5000 65.3170 1 2 # mean, median, 25th and 75th quartiles, min, max summary ( longley ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 ## GNP.deflator GNP Unemployed Armed.Forces ## Min. : 83.00 Min. :234.3 Min. :187.0 Min. :145.6 ## 1st Qu.: 94.53 1st Qu.:317.9 1st Qu.:234.8 1st Qu.:229.8 ## Median :100.60 Median :381.4 Median :314.4 Median :271.8 ## Mean :101.68 Mean :387.7 Mean :319.3 Mean :260.7 ## 3rd Qu.:111.25 3rd Qu.:454.1 3rd Qu.:384.2 3rd Qu.:306.1 ## Max. :116.90 Max. :554.9 Max. :480.6 Max. :359.4 ## Population Year Employed ## Min. :107.6 Min. :1947 Min. :60.17 ## 1st Qu.:111.8 1st Qu.:1951 1st Qu.:62.71 ## Median :116.8 Median :1954 Median :65.50 ## Mean :117.4 Mean :1954 Mean :65.32 ## 3rd Qu.:122.3 3rd Qu.:1958 3rd Qu.:68.29 ## Max. :130.1 Max. :1962 Max. :70.55 1 2 # Tukey min, lower-hinge, median, upper-hinge, max fivenum ( longley $ GNP ) 1 ## [1] 234.289 306.787 381.427 463.625 554.894 1 2 3 4 5 # n, nmiss, unique, mean, 5, 10, 25, 50, 75, 90, 95th percentiles # 5 lowest and 5 highest scores library ( Hmisc ) describe ( longley ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 ## longley ## ## 7 Variables 16 Observations ## --------------------------------------------------------------------------- ## GNP.deflator ## n missing distinct Info Mean Gmd .05 .10 ## 16 0 16 1 101.7 12.74 86.90 88.35 ## .25 .50 .75 .90 .95 ## 94.53 100.60 111.25 114.95 116.00 ## ## Value 83.0 88.2 88.5 89.5 96.2 98.1 99.0 100.0 101.2 104.6 ## Frequency 1 1 1 1 1 1 1 1 1 1 ## Proportion 0.062 0.062 0.062 0.062 0.062 0.062 0.062 0.062 0.062 0.062 ## ## Value 108.4 110.8 112.6 114.2 115.7 116.9 ## Frequency 1 1 1 1 1 1 ## Proportion 0.062 0.062 0.062 0.062 0.062 0.062 ## --------------------------------------------------------------------------- ## GNP ## n missing distinct Info Mean Gmd .05 .10 ## 16 0 16 1 387.7 117.8 252.1 258.7 ## .25 .50 .75 .90 .95 ## 317.9 381.4 454.1 510.4 527.4 ## ## Value 234.289 258.054 259.426 284.599 328.975 346.999 363.112 365.385 ## Frequency 1 1 1 1 1 1 1 1 ## Proportion 0.062 0.062 0.062 0.062 0.062 0.062 0.062 0.062 ## ## Value 397.469 419.180 442.769 444.546 482.704 502.601 518.173 554.894 ## Frequency 1 1 1 1 1 1 1 1 ## Proportion 0.062 0.062 0.062 0.062 0.062 0.062 0.062 0.062 ## --------------------------------------------------------------------------- ## Unemployed ## n missing distinct Info Mean Gmd .05 .10 ## 16 0 16 1 319.3 110.1 191.6 201.6 ## .25 .50 .75 .90 .95 ## 234.8 314.4 384.2 434.4 471.2 ## ## Value 187.0 193.2 209.9 232.5 235.6 282.2 290.4 293.6 335.1 357.8 ## Frequency 1 1 1 1 1 1 1 1 1 1 ## Proportion 0.062 0.062 0.062 0.062 0.062 0.062 0.062 0.062 0.062 0.062 ## ## Value 368.2 381.3 393.1 400.7 468.1 480.6 ## Frequency 1 1 1 1 1 1 ## Proportion 0.062 0.062 0.062 0.062 0.062 0.062 ## --------------------------------------------------------------------------- ## Armed.Forces ## n missing distinct Info Mean Gmd .05 .10 ## 16 0 16 1 260.7 79.85 155.7 160.3 ## .25 .50 .75 .90 .95 ## 229.8 271.8 306.1 344.9 355.9 ## ## Value 145.6 159.0 161.6 165.0 251.4 255.2 257.2 263.7 279.8 282.7 ## Frequency 1 1 1 1 1 1 1 1 1 1 ## Proportion 0.062 0.062 0.062 0.062 0.062 0.062 0.062 0.062 0.062 0.062 ## ## Value 285.7 304.8 309.9 335.0 354.7 359.4 ## Frequency 1 1 1 1 1 1 ## Proportion 0.062 0.062 0.062 0.062 0.062 0.062 ## --------------------------------------------------------------------------- ## Population ## n missing distinct Info Mean Gmd .05 .10 ## 16 0 16 1 117.4 8.229 108.4 109.2 ## .25 .50 .75 .90 .95 ## 111.8 116.8 122.3 126.6 128.4 ## ## Value 107.608 108.632 109.773 110.929 112.075 113.270 115.094 116.219 ## Frequency 1 1 1 1 1 1 1 1 ## Proportion 0.062 0.062 0.062 0.062 0.062 0.062 0.062 0.062 ## ## Value 117.388 118.734 120.445 121.950 123.366 125.368 127.852 130.081 ## Frequency 1 1 1 1 1 1 1 1 ## Proportion 0.062 0.062 0.062 0.062 0.062 0.062 0.062 0.062 ## --------------------------------------------------------------------------- ## Year ## n missing distinct Info Mean Gmd .05 .10 ## 16 0 16 1 1954 5.667 1948 1948 ## .25 .50 .75 .90 .95 ## 1951 1954 1958 1960 1961 ## ## Value 1947 1948 1949 1950 1951 1952 1953 1954 1955 1956 ## Frequency 1 1 1 1 1 1 1 1 1 1 ## Proportion 0.062 0.062 0.062 0.062 0.062 0.062 0.062 0.062 0.062 0.062 ## ## Value 1957 1958 1959 1960 1961 1962 ## Frequency 1 1 1 1 1 1 ## Proportion 0.062 0.062 0.062 0.062 0.062 0.062 ## --------------------------------------------------------------------------- ## Employed ## n missing distinct Info Mean Gmd .05 .10 ## 16 0 16 1 65.32 4.153 60.28 60.72 ## .25 .50 .75 .90 .95 ## 62.71 65.50 68.29 69.45 69.81 ## ## Value 60.171 60.323 61.122 61.187 63.221 63.639 63.761 64.989 66.019 ## Frequency 1 1 1 1 1 1 1 1 1 ## Proportion 0.062 0.062 0.062 0.062 0.062 0.062 0.062 0.062 0.062 ## ## Value 66.513 67.857 68.169 68.655 69.331 69.564 70.551 ## Frequency 1 1 1 1 1 1 1 ## Proportion 0.062 0.062 0.062 0.062 0.062 0.062 0.062 ## --------------------------------------------------------------------------- 1 2 3 4 5 # nbr.val, nbr.null, nbr.na, min max, range, sum, # median, mean, SE.mean, CI.mean, var, std.dev, coef.var library ( pastecs ) stat.desc ( longley ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 ## GNP.deflator GNP Unemployed Armed.Forces ## nbr.val 16.0000000 16.0000000 16.0000000 16.0000000 ## nbr.null 0.0000000 0.0000000 0.0000000 0.0000000 ## nbr.na 0.0000000 0.0000000 0.0000000 0.0000000 ## min 83.0000000 234.2890000 187.0000000 145.6000000 ## max 116.9000000 554.8940000 480.6000000 359.4000000 ## range 33.9000000 320.6050000 293.6000000 213.8000000 ## sum 1626.9000000 6203.1750000 5109.3000000 4170.7000000 ## median 100.6000000 381.4270000 314.3500000 271.7500000 ## mean 101.6812500 387.6984375 319.3312500 260.6687500 ## SE.mean 2.6978884 24.8487344 23.3616062 17.3979901 ## CI.mean.0.95 5.7504129 52.9638237 49.7940849 37.0829381 ## var 116.4576250 9879.3536593 8732.2342917 4843.0409583 ## std.dev 10.7915534 99.3949378 93.4464247 69.5919604 ## coef.var 0.1061312 0.2563718 0.2926316 0.2669747 ## Population Year Employed ## nbr.val 1.600000e+01 1.600000e+01 1.600000e+01 ## nbr.null 0.000000e+00 0.000000e+00 0.000000e+00 ## nbr.na 0.000000e+00 0.000000e+00 0.000000e+00 ## min 1.076080e+02 1.947000e+03 6.017100e+01 ## max 1.300810e+02 1.962000e+03 7.055100e+01 ## range 2.247300e+01 1.500000e+01 1.038000e+01 ## sum 1.878784e+03 3.127200e+04 1.045072e+03 ## median 1.168035e+02 1.954500e+03 6.550400e+01 ## mean 1.174240e+02 1.954500e+03 6.531700e+01 ## SE.mean 1.739025e+00 1.190238e+00 8.779921e-01 ## CI.mean.0.95 3.706645e+00 2.536932e+00 1.871396e+00 ## var 4.838735e+01 2.266667e+01 1.233392e+01 ## std.dev 6.956102e+00 4.760952e+00 3.511968e+00 ## coef.var 5.923918e-02 2.435893e-03 5.376806e-02 1 2 3 4 5 # item name, item number, nvalid, mean, sd, # median, mad, min, max, skew, kurtosis, se library ( psych ) describe ( longley ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 ## vars n mean sd median trimmed mad min max ## GNP.deflator 1 16 101.68 10.79 100.60 101.93 15.79 83.00 116.90 ## GNP 2 16 387.70 99.39 381.43 386.71 118.57 234.29 554.89 ## Unemployed 3 16 319.33 93.45 314.35 317.26 116.75 187.00 480.60 ## Armed.Forces 4 16 260.67 69.59 271.75 261.84 52.78 145.60 359.40 ## Population 5 16 117.42 6.96 116.80 117.22 8.17 107.61 130.08 ## Year 6 16 1954.50 4.76 1954.50 1954.50 5.93 1947.00 1962.00 ## Employed 7 16 65.32 3.51 65.50 65.31 4.31 60.17 70.55 ## range skew kurtosis se ## GNP.deflator 33.90 -0.13 -1.40 2.70 ## GNP 320.61 0.02 -1.35 24.85 ## Unemployed 293.60 0.14 -1.30 23.36 ## Armed.Forces 213.80 -0.37 -1.20 17.40 ## Population 22.47 0.26 -1.27 1.74 ## Year 15.00 0.00 -1.43 1.19 ## Employed 10.38 -0.09 -1.55 0.88 1 2 # with with ( longley , median ( GNP )) 1 ## [1] 381.427 1 2 # vs. median ( longley $ GNP ) 1 ## [1] 381.427 Group data \u00b6 1 2 # dataset head ( mtcars , 3 ) 1 2 3 4 ## mpg cyl disp hp drat wt qsec vs am gear carb ## Mazda RX4 21.0 6 160 110 3.90 2.620 16.46 0 1 4 4 ## Mazda RX4 Wag 21.0 6 160 110 3.90 2.875 17.02 0 1 4 4 ## Datsun 710 22.8 4 108 93 3.85 2.320 18.61 1 1 4 1 1 2 # variable grouped by one factor to show a function by ( mtcars $ mpg , mtcars $ cyl , FUN = function ( x ) { c ( m = mean ( x ), s = sd ( x )) }) 1 2 3 4 5 6 7 8 9 10 11 ## mtcars$cyl: 4 ## m s ## 26.663636 4.509828 ## -------------------------------------------------------- ## mtcars$cyl: 6 ## m s ## 19.742857 1.453567 ## -------------------------------------------------------- ## mtcars$cyl: 8 ## m s ## 15.100000 2.560048 1 2 3 4 # description statistics by group library ( psych ) describeBy ( mtcars , group = mtcars $ am ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 ## ## Descriptive statistics by group ## group: 0 ## vars n mean sd median trimmed mad min max range ## mpg 1 19 17.15 3.83 17.30 17.12 3.11 10.40 24.40 14.00 ## cyl 2 19 6.95 1.54 8.00 7.06 0.00 4.00 8.00 4.00 ## disp 3 19 290.38 110.17 275.80 289.71 124.83 120.10 472.00 351.90 ## hp 4 19 160.26 53.91 175.00 161.06 77.10 62.00 245.00 183.00 ## drat 5 19 3.29 0.39 3.15 3.28 0.22 2.76 3.92 1.16 ## wt 6 19 3.77 0.78 3.52 3.75 0.45 2.46 5.42 2.96 ## qsec 7 19 18.18 1.75 17.82 18.07 1.19 15.41 22.90 7.49 ## vs 8 19 0.37 0.50 0.00 0.35 0.00 0.00 1.00 1.00 ## am 9 19 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 ## gear 10 19 3.21 0.42 3.00 3.18 0.00 3.00 4.00 1.00 ## carb 11 19 2.74 1.15 3.00 2.76 1.48 1.00 4.00 3.00 ## skew kurtosis se ## mpg 0.01 -0.80 0.88 ## cyl -0.95 -0.74 0.35 ## disp 0.05 -1.26 25.28 ## hp -0.01 -1.21 12.37 ## drat 0.50 -1.30 0.09 ## wt 0.98 0.14 0.18 ## qsec 0.85 0.55 0.40 ## vs 0.50 -1.84 0.11 ## am NaN NaN 0.00 ## gear 1.31 -0.29 0.10 ## carb -0.14 -1.57 0.26 ## -------------------------------------------------------- ## group: 1 ## vars n mean sd median trimmed mad min max range skew ## mpg 1 13 24.39 6.17 22.80 24.38 6.67 15.00 33.90 18.90 0.05 ## cyl 2 13 5.08 1.55 4.00 4.91 0.00 4.00 8.00 4.00 0.87 ## disp 3 13 143.53 87.20 120.30 131.25 58.86 71.10 351.00 279.90 1.33 ## hp 4 13 126.85 84.06 109.00 114.73 63.75 52.00 335.00 283.00 1.36 ## drat 5 13 4.05 0.36 4.08 4.02 0.27 3.54 4.93 1.39 0.79 ## wt 6 13 2.41 0.62 2.32 2.39 0.68 1.51 3.57 2.06 0.21 ## qsec 7 13 17.36 1.79 17.02 17.39 2.34 14.50 19.90 5.40 -0.23 ## vs 8 13 0.54 0.52 1.00 0.55 0.00 0.00 1.00 1.00 -0.14 ## am 9 13 1.00 0.00 1.00 1.00 0.00 1.00 1.00 0.00 NaN ## gear 10 13 4.38 0.51 4.00 4.36 0.00 4.00 5.00 1.00 0.42 ## carb 11 13 2.92 2.18 2.00 2.64 1.48 1.00 8.00 7.00 0.98 ## kurtosis se ## mpg -1.46 1.71 ## cyl -0.90 0.43 ## disp 0.40 24.19 ## hp 0.56 23.31 ## drat 0.21 0.10 ## wt -1.17 0.17 ## qsec -1.42 0.50 ## vs -2.13 0.14 ## am NaN 0.00 ## gear -1.96 0.14 ## carb -0.21 0.60 1 2 3 4 5 # description statistics by group library ( doBy ) summaryBy ( mpg + wt ~ cyl + vs , data = mtcars , FUN = function ( x ) { c ( m = mean ( x ), s = sd ( x )) } ) 1 2 3 4 5 6 ## cyl vs mpg.m mpg.s wt.m wt.s ## 1 4 0 26.00000 NA 2.140000 NA ## 2 4 1 26.73000 4.7481107 2.300300 0.5982073 ## 3 6 0 20.56667 0.7505553 2.755000 0.1281601 ## 4 6 1 19.12500 1.6317169 3.388750 0.1162164 ## 5 8 0 15.10000 2.5600481 3.999214 0.7594047 Frequency Tables, CrossTables, and Independence \u00b6 Create frequency and contingency tables from categorical variables. Perform tests of independence, measures of association, and graphically display results. 2D frequency tables \u00b6 mytable <- table ( A , B ) where A are rows, B are columns. 1 2 3 # dataset mytable <- matrix ( c ( 1 , 2 , 3 , 4 ), nrow = 2 ) mytable 1 2 3 ## [,1] [,2] ## [1,] 1 3 ## [2,] 2 4 1 2 # A frequencies (summed over columns = 1) margin.table ( mytable , 1 ) 1 ## [1] 4 6 1 2 # B frequencies (summed over rows = 2) margin.table ( mytable , 2 ) 1 ## [1] 3 7 1 2 3 # A/(A + B) # cell percentages prop.table ( mytable ) 1 2 3 ## [,1] [,2] ## [1,] 0.1 0.3 ## [2,] 0.2 0.4 1 2 # row percentages prop.table ( mytable , 1 ) 1 2 3 ## [,1] [,2] ## [1,] 0.2500000 0.7500000 ## [2,] 0.3333333 0.6666667 1 2 # column percentages prop.table ( mytable , 2 ) 1 2 3 ## [,1] [,2] ## [1,] 0.3333333 0.4285714 ## [2,] 0.6666667 0.5714286 3D frequency tables \u00b6 1 2 # dataset head ( CO2 , 3 ) 1 2 3 4 5 ## Grouped Data: uptake ~ conc | Plant ## Plant Type Treatment conc uptake ## 1 Qn1 Quebec nonchilled 95 16.0 ## 2 Qn1 Quebec nonchilled 175 30.4 ## 3 Qn1 Quebec nonchilled 250 34.8 1 2 3 4 A <- as.numeric ( CO2[ , 'Plant' ] ) B <- CO2[ , 'conc' ] C <- CO2[ , 'uptake' ] mytable <- table ( A , B , C ) 1 2 # several arrays (3D) dim ( mytable ) # the printout is immense 1 ## [1] 12 7 76 1 2 # folded table (2D) dim ( ftable ( mytable )) # the printout is immense 1 ## [1] 84 76 1 2 3 4 # 3-Way frequency Table mytable <- xtabs ( ~ A + B + C , data = mytable , na.action = na.omit ) dim ( ftable ( mytable )) # the printout is immense 1 ## [1] 84 76 CrossTable \u00b6 1 2 A <- as.numeric ( CO2[1 : 8 , 'Plant' ] ) A 1 ## [1] 1 1 1 1 1 1 1 2 1 2 B <- CO2[1 : 8 , 'conc' ] B 1 ## [1] 95 175 250 350 500 675 1000 95 1 2 3 4 # 2-way cross tabulation library ( gmodels ) CrossTable ( A , B ) # mydata$myrowvar x mydata$mycolvar 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 ## ## ## Cell Contents ## |-------------------------| ## | N | ## | Chi-square contribution | ## | N / Row Total | ## | N / Col Total | ## | N / Table Total | ## |-------------------------| ## ## ## Total Observations in Table: 8 ## ## ## | B ## A | 95 | 175 | 250 | 350 | 500 | 675 | 1000 | Row Total | ## -------------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------| ## 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 7 | ## | 0.321 | 0.018 | 0.018 | 0.018 | 0.018 | 0.018 | 0.018 | | ## | 0.143 | 0.143 | 0.143 | 0.143 | 0.143 | 0.143 | 0.143 | 0.875 | ## | 0.500 | 1.000 | 1.000 | 1.000 | 1.000 | 1.000 | 1.000 | | ## | 0.125 | 0.125 | 0.125 | 0.125 | 0.125 | 0.125 | 0.125 | | ## -------------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------| ## 2 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | ## | 2.250 | 0.125 | 0.125 | 0.125 | 0.125 | 0.125 | 0.125 | | ## | 1.000 | 0.000 | 0.000 | 0.000 | 0.000 | 0.000 | 0.000 | 0.125 | ## | 0.500 | 0.000 | 0.000 | 0.000 | 0.000 | 0.000 | 0.000 | | ## | 0.125 | 0.000 | 0.000 | 0.000 | 0.000 | 0.000 | 0.000 | | ## -------------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------| ## Column Total | 2 | 1 | 1 | 1 | 1 | 1 | 1 | 8 | ## | 0.250 | 0.125 | 0.125 | 0.125 | 0.125 | 0.125 | 0.125 | | ## -------------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------| ## ## Tests of independence \u00b6 There are more tests of independence in section 10, Resampling Statistics. 1 2 # dataset, a contingency table colors 1 2 3 4 5 ## fair.hair red.hair medium.hair dark.hair black.hair ## blue eyes 326 38 241 110 3 ## light eyes 688 116 584 188 4 ## medium eyes 343 84 909 412 26 ## dark eyes 98 48 403 681 85 1 2 3 # chi-square test on 2-way tables # test independence of the row and column variables, p-value is calculated from the asymptotic chi-squared distribution of the test statistic chisq.test ( colors ) 1 2 3 4 5 ## ## Pearson's Chi-squared test ## ## data: colors ## X-squared = 1240, df = 12, p-value &lt; 2.2e-16 1 2 3 4 5 6 7 # dataset, a contingency table in 2x2 matrix form TeaTasting <- matrix ( c ( 3 , 1 , 1 , 3 ), nrow = 2 , dimnames = list ( Guess = c ( \"Milk\" , \"Tea\" ), Truth = c ( \"Milk\" , \"Tea\" ))) TeaTasting 1 2 3 4 ## Truth ## Guess Milk Tea ## Milk 3 1 ## Tea 1 3 1 2 # Fisher exact test fisher.test ( TeaTasting , alternative = \"greater\" ) 1 2 3 4 5 6 7 8 9 10 11 ## ## Fisher's Exact Test for Count Data ## ## data: TeaTasting ## p-value = 0.2429 ## alternative hypothesis: true odds ratio is greater than 1 ## 95 percent confidence interval: ## 0.3135693 Inf ## sample estimates: ## odds ratio ## 6.408309 1 2 3 4 5 6 7 8 9 10 11 12 13 # 3D contingency table, where the last dimension refers to the strata Rabbits <- array ( c ( 0 , 0 , 6 , 5 , 3 , 0 , 3 , 6 , 6 , 2 , 0 , 4 , 5 , 6 , 1 , 0 , 2 , 5 , 0 , 0 ), dim = c ( 2 , 2 , 5 ), dimnames = list ( Delay = c ( \"None\" , \"1.5h\" ), Response = c ( \"Cured\" , \"Died\" ), Penicillin.Level = c ( \"1/8\" , \"1/4\" , \"1/2\" , \"1\" , \"4\" ))) Rabbits 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 ## , , Penicillin.Level = 1/8 ## ## Response ## Delay Cured Died ## None 0 6 ## 1.5h 0 5 ## ## , , Penicillin.Level = 1/4 ## ## Response ## Delay Cured Died ## None 3 3 ## 1.5h 0 6 ## ## , , Penicillin.Level = 1/2 ## ## Response ## Delay Cured Died ## None 6 0 ## 1.5h 2 4 ## ## , , Penicillin.Level = 1 ## ## Response ## Delay Cured Died ## None 5 1 ## 1.5h 6 0 ## ## , , Penicillin.Level = 4 ## ## Response ## Delay Cured Died ## None 2 0 ## 1.5h 5 0 1 2 # Mantel-Haenszel test / Cochran-Mantel-Haenszel chi-squared test, hypothesis that two nominal variables are conditionally independent in each stratum, assuming that there is no three-way interaction. mantelhaen.test ( Rabbits ) 1 2 3 4 5 6 7 8 9 10 11 ## ## Mantel-Haenszel chi-squared test with continuity correction ## ## data: Rabbits ## Mantel-Haenszel X-squared = 3.9286, df = 1, p-value = 0.04747 ## alternative hypothesis: true common odds ratio is not equal to 1 ## 95 percent confidence interval: ## 1.026713 47.725133 ## sample estimates: ## common odds ratio ## 7 1 2 3 4 5 6 # dataset # 3-way contingency table based on variables A, B, and C A <- CO2[ , 'Plant' ] B <- CO2[ , 'conc' ] C <- CO2[ , 'uptake' ] mytable <- xtabs ( ~ A + B + C ) # a 3D array 1 2 3 4 5 # loglinear Models # mutual independence: A, B, and C are pairwise independent library ( MASS ) loglm ( ~ A + B + C , mytable ) 1 2 3 4 5 6 7 ## Call: ## loglm(formula = ~A + B + C, data = mytable) ## ## Statistics: ## X^2 df P(&gt; X^2) ## Likelihood Ratio 720.1035 6291 1.0000000 ## Pearson 6300.0000 6291 0.4656775 1 2 # conditional independence: A is independent of B, given C loglm ( ~ A + B + C + A * C + B * C , mytable ) 1 2 3 4 5 6 7 ## Call: ## loglm(formula = ~A + B + C + A * C + B * C, data = mytable) ## ## Statistics: ## X^2 df P(&gt; X^2) ## Likelihood Ratio 12.13685 5016 1 ## Pearson NaN 5016 NaN 1 2 # no three-way interaction loglm ( ~ A + B + C + A * B + A * C + B * C , mytable ) 1 2 3 4 5 6 7 ## Call: ## loglm(formula = ~A + B + C + A * B + A * C + B * C, data = mytable) ## ## Statistics: ## X^2 df P(&gt; X^2) ## Likelihood Ratio 1.038376 4950 1 ## Pearson NaN 4950 NaN Measures of association \u00b6 Association between two nominal variables, giving a value between 0 and +1 (inclusive). It is based on Pearson\u2019s chi-squared statistic. 1 2 # Dataset str ( Arthritis ) 1 2 3 4 5 6 ## 'data.frame': 84 obs. of 5 variables: ## $ ID : int 57 46 77 17 36 23 75 39 33 55 ... ## $ Treatment: Factor w/ 2 levels \"Placebo\",\"Treated\": 2 2 2 2 2 2 2 2 2 2 ... ## $ Sex : Factor w/ 2 levels \"Female\",\"Male\": 2 2 2 2 2 2 2 2 2 2 ... ## $ Age : int 27 29 30 32 46 58 59 59 63 63 ... ## $ Improved : Ord.factor w/ 3 levels \"None\"&lt;\"Some\"&lt;..: 2 1 1 3 3 3 1 3 1 1 ... 1 2 tab <- xtabs ( ~ Improved + Treatment , data = Arthritis ) tab 1 2 3 4 5 ## Treatment ## Improved Placebo Treated ## None 29 13 ## Some 7 7 ## Marked 7 21 1 summary ( assocstats ( tab )) 1 2 3 4 5 6 7 8 9 10 11 12 13 ## ## Call: xtabs(formula = ~Improved + Treatment, data = Arthritis) ## Number of cases in table: 84 ## Number of factors: 2 ## Test for independence of all factors: ## Chisq = 13.055, df = 2, p-value = 0.001463 ## X^2 df P(&gt; X^2) ## Likelihood Ratio 13.530 2 0.0011536 ## Pearson 13.055 2 0.0014626 ## ## Phi-Coefficient : NA ## Contingency Coeff.: 0.367 ## Cramer's V : 0.394 1 2 3 4 # phi coefficient, contingency coefficient, and Cram\u00e9r's V for an 2D table library ( vcd ) assocstats ( tab ) 1 2 3 4 5 6 7 ## X^2 df P(&gt; X^2) ## Likelihood Ratio 13.530 2 0.0011536 ## Pearson 13.055 2 0.0014626 ## ## Phi-Coefficient : NA ## Contingency Coeff.: 0.367 ## Cramer's V : 0.394 1 2 # Dataset TeaTasting 1 2 3 4 ## Truth ## Guess Milk Tea ## Milk 3 1 ## Tea 1 3 1 2 3 4 # Cohen's kappa and weighted kappa for a confusion matrix library ( vcd ) kappa ( TeaTasting ) 1 ## [1] 2.333333 Correlations \u00b6 1 2 # dataset head ( mtcars , 3 ) 1 2 3 4 ## mpg cyl disp hp drat wt qsec vs am gear carb ## Mazda RX4 21.0 6 160 110 3.90 2.620 16.46 0 1 4 4 ## Mazda RX4 Wag 21.0 6 160 110 3.90 2.875 17.02 0 1 4 4 ## Datsun 710 22.8 4 108 93 3.85 2.320 18.61 1 1 4 1 1 2 3 # correlations/covariances among numeric variables in # a data frame cor ( mtcars , use = \"complete.obs\" , method = \"kendall\" ) # method = \"pearson\", \"spearman\" or \"kendall\" 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 ## mpg cyl disp hp drat wt ## mpg 1.0000000 -0.7953134 -0.7681311 -0.7428125 0.46454879 -0.7278321 ## cyl -0.7953134 1.0000000 0.8144263 0.7851865 -0.55131785 0.7282611 ## disp -0.7681311 0.8144263 1.0000000 0.6659987 -0.49898277 0.7433824 ## hp -0.7428125 0.7851865 0.6659987 1.0000000 -0.38262689 0.6113081 ## drat 0.4645488 -0.5513178 -0.4989828 -0.3826269 1.00000000 -0.5471495 ## wt -0.7278321 0.7282611 0.7433824 0.6113081 -0.54714953 1.0000000 ## qsec 0.3153652 -0.4489698 -0.3008155 -0.4729061 0.03272155 -0.1419881 ## vs 0.5896790 -0.7710007 -0.6033059 -0.6305926 0.37510111 -0.4884787 ## am 0.4690128 -0.4946212 -0.5202739 -0.3039956 0.57554849 -0.6138790 ## gear 0.4331509 -0.5125435 -0.4759795 -0.2794458 0.58392476 -0.5435956 ## carb -0.5043945 0.4654299 0.4137360 0.5959842 -0.09535193 0.3713741 ## qsec vs am gear carb ## mpg 0.31536522 0.5896790 0.46901280 0.43315089 -0.50439455 ## cyl -0.44896982 -0.7710007 -0.49462115 -0.51254349 0.46542994 ## disp -0.30081549 -0.6033059 -0.52027392 -0.47597955 0.41373600 ## hp -0.47290613 -0.6305926 -0.30399557 -0.27944584 0.59598416 ## drat 0.03272155 0.3751011 0.57554849 0.58392476 -0.09535193 ## wt -0.14198812 -0.4884787 -0.61387896 -0.54359562 0.37137413 ## qsec 1.00000000 0.6575431 -0.16890405 -0.09126069 -0.50643945 ## vs 0.65754312 1.0000000 0.16834512 0.26974788 -0.57692729 ## am -0.16890405 0.1683451 1.00000000 0.77078758 -0.05859929 ## gear -0.09126069 0.2697479 0.77078758 1.00000000 0.09801487 ## carb -0.50643945 -0.5769273 -0.05859929 0.09801487 1.00000000 1 cov ( mtcars , use = \"complete.obs\" ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 ## mpg cyl disp hp drat ## mpg 36.324103 -9.1723790 -633.09721 -320.732056 2.19506351 ## cyl -9.172379 3.1895161 199.66028 101.931452 -0.66836694 ## disp -633.097208 199.6602823 15360.79983 6721.158669 -47.06401915 ## hp -320.732056 101.9314516 6721.15867 4700.866935 -16.45110887 ## drat 2.195064 -0.6683669 -47.06402 -16.451109 0.28588135 ## wt -5.116685 1.3673710 107.68420 44.192661 -0.37272073 ## qsec 4.509149 -1.8868548 -96.05168 -86.770081 0.08714073 ## vs 2.017137 -0.7298387 -44.37762 -24.987903 0.11864919 ## am 1.803931 -0.4657258 -36.56401 -8.320565 0.19015121 ## gear 2.135685 -0.6491935 -50.80262 -6.358871 0.27598790 ## carb -5.363105 1.5201613 79.06875 83.036290 -0.07840726 ## wt qsec vs am gear ## mpg -5.1166847 4.50914919 2.01713710 1.80393145 2.1356855 ## cyl 1.3673710 -1.88685484 -0.72983871 -0.46572581 -0.6491935 ## disp 107.6842040 -96.05168145 -44.37762097 -36.56401210 -50.8026210 ## hp 44.1926613 -86.77008065 -24.98790323 -8.32056452 -6.3588710 ## drat -0.3727207 0.08714073 0.11864919 0.19015121 0.2759879 ## wt 0.9573790 -0.30548161 -0.27366129 -0.33810484 -0.4210806 ## qsec -0.3054816 3.19316613 0.67056452 -0.20495968 -0.2804032 ## vs -0.2736613 0.67056452 0.25403226 0.04233871 0.0766129 ## am -0.3381048 -0.20495968 0.04233871 0.24899194 0.2923387 ## gear -0.4210806 -0.28040323 0.07661290 0.29233871 0.5443548 ## carb 0.6757903 -1.89411290 -0.46370968 0.04637097 0.3266129 ## carb ## mpg -5.36310484 ## cyl 1.52016129 ## disp 79.06875000 ## hp 83.03629032 ## drat -0.07840726 ## wt 0.67579032 ## qsec -1.89411290 ## vs -0.46370968 ## am 0.04637097 ## gear 0.32661290 ## carb 2.60887097 1 2 3 4 # correlations with significance levels library ( Hmisc ) rcorr ( as.matrix ( mtcars ), type = \"pearson\" ) # type can be pearson or spearman 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 ## mpg cyl disp hp drat wt qsec vs am gear carb ## mpg 1.00 -0.85 -0.85 -0.78 0.68 -0.87 0.42 0.66 0.60 0.48 -0.55 ## cyl -0.85 1.00 0.90 0.83 -0.70 0.78 -0.59 -0.81 -0.52 -0.49 0.53 ## disp -0.85 0.90 1.00 0.79 -0.71 0.89 -0.43 -0.71 -0.59 -0.56 0.39 ## hp -0.78 0.83 0.79 1.00 -0.45 0.66 -0.71 -0.72 -0.24 -0.13 0.75 ## drat 0.68 -0.70 -0.71 -0.45 1.00 -0.71 0.09 0.44 0.71 0.70 -0.09 ## wt -0.87 0.78 0.89 0.66 -0.71 1.00 -0.17 -0.55 -0.69 -0.58 0.43 ## qsec 0.42 -0.59 -0.43 -0.71 0.09 -0.17 1.00 0.74 -0.23 -0.21 -0.66 ## vs 0.66 -0.81 -0.71 -0.72 0.44 -0.55 0.74 1.00 0.17 0.21 -0.57 ## am 0.60 -0.52 -0.59 -0.24 0.71 -0.69 -0.23 0.17 1.00 0.79 0.06 ## gear 0.48 -0.49 -0.56 -0.13 0.70 -0.58 -0.21 0.21 0.79 1.00 0.27 ## carb -0.55 0.53 0.39 0.75 -0.09 0.43 -0.66 -0.57 0.06 0.27 1.00 ## ## n= 32 ## ## ## P ## mpg cyl disp hp drat wt qsec vs am gear ## mpg 0.0000 0.0000 0.0000 0.0000 0.0000 0.0171 0.0000 0.0003 0.0054 ## cyl 0.0000 0.0000 0.0000 0.0000 0.0000 0.0004 0.0000 0.0022 0.0042 ## disp 0.0000 0.0000 0.0000 0.0000 0.0000 0.0131 0.0000 0.0004 0.0010 ## hp 0.0000 0.0000 0.0000 0.0100 0.0000 0.0000 0.0000 0.1798 0.4930 ## drat 0.0000 0.0000 0.0000 0.0100 0.0000 0.6196 0.0117 0.0000 0.0000 ## wt 0.0000 0.0000 0.0000 0.0000 0.0000 0.3389 0.0010 0.0000 0.0005 ## qsec 0.0171 0.0004 0.0131 0.0000 0.6196 0.3389 0.0000 0.2057 0.2425 ## vs 0.0000 0.0000 0.0000 0.0000 0.0117 0.0010 0.0000 0.3570 0.2579 ## am 0.0003 0.0022 0.0004 0.1798 0.0000 0.0000 0.2057 0.3570 0.0000 ## gear 0.0054 0.0042 0.0010 0.4930 0.0000 0.0005 0.2425 0.2579 0.0000 ## carb 0.0011 0.0019 0.0253 0.0000 0.6212 0.0146 0.0000 0.0007 0.7545 0.1290 ## carb ## mpg 0.0011 ## cyl 0.0019 ## disp 0.0253 ## hp 0.0000 ## drat 0.6212 ## wt 0.0146 ## qsec 0.0000 ## vs 0.0007 ## am 0.7545 ## gear 0.1290 ## carb 1 2 3 # dataset x <- mtcars[1 : 3 ] y <- mtcars[4 : 6 ] 1 2 # correlation between two vectors cor ( x , y ) 1 2 3 4 ## hp drat wt ## mpg -0.7761684 0.6811719 -0.8676594 ## cyl 0.8324475 -0.6999381 0.7824958 ## disp 0.7909486 -0.7102139 0.8879799 Polychoric correlation The correlation between two theorised normally distributed continuous latent variables, from two observed ordinal variables. 1 2 3 # dataset # 2-way contingency table of counts colors 1 2 3 4 5 ## fair.hair red.hair medium.hair dark.hair black.hair ## blue eyes 326 38 241 110 3 ## light eyes 688 116 584 188 4 ## medium eyes 343 84 909 412 26 ## dark eyes 98 48 403 681 85 1 2 3 4 # polychoric correlation library ( polycor ) polychor ( colors ) 1 ## [1] 0.4743984 1 2 3 4 5 6 # heterogeneous correlations in one matrix # pearson (numeric-numeric), # polyserial (numeric-ordinal), # and polychoric (ordinal-ordinal) # a data frame with ordered factors and numeric variables hetcor ( colors ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 ## ## Two-Step Estimates ## ## Correlations/Type of Correlation: ## fair.hair red.hair medium.hair dark.hair black.hair ## fair.hair 1 Pearson Pearson Pearson Pearson ## red.hair 0.8333 1 Pearson Pearson Pearson ## medium.hair 0.2597 0.6467 1 Pearson Pearson ## dark.hair -0.7091 -0.2251 0.1911 1 Pearson ## black.hair -0.781 -0.3875 -0.06329 0.9674 1 ## ## Standard Errors: ## fair.hair red.hair medium.hair dark.hair ## fair.hair ## red.hair 0.03628 ## medium.hair 0.3607 0.1383 ## dark.hair 0.09977 0.3733 0.3841 ## black.hair 0.06019 0.3004 0.4092 0.001491 ## ## n = 4 ## ## P-values for Tests of Bivariate Normality: ## fair.hair red.hair medium.hair dark.hair ## fair.hair ## red.hair 0.8306 ## medium.hair 0.6939 0.8609 ## dark.hair 0.7465 0.6335 0.7161 ## black.hair 0.6582 0.5927 0.6711 0.9873 t-tests \u00b6 1 2 3 4 5 6 7 8 9 10 11 # independent 2-group t-test t.test ( y ~ x ) # where y is numeric and x is a binary factor # independent 2-group t-test t.test ( y1 , y2 ) # where y1 and y2 are numeric # paired t-test t.test ( y1 , y2 , paired = TRUE ) # where y1 & y2 are numeric # one sample t-test t.test ( y , mu = 3 ) # Ho: mu=3 1 2 3 # dataset # 2-way contingency table colors 1 2 3 4 5 ## fair.hair red.hair medium.hair dark.hair black.hair ## blue eyes 326 38 241 110 3 ## light eyes 688 116 584 188 4 ## medium eyes 343 84 909 412 26 ## dark eyes 98 48 403 681 85 1 mean ( as.numeric ( colors[1 , ] )) # row 1 average 1 ## [1] 143.6 1 2 # independent 2-group t-test t.test ( as.numeric ( colors[1 , ] ), as.numeric ( colors[2 , ] )) # where y1 and y2 are numeric 1 2 3 4 5 6 7 8 9 10 11 ## ## Welch Two Sample t-test ## ## data: as.numeric(colors[1, ]) and as.numeric(colors[2, ]) ## t = -1.164, df = 5.5777, p-value = 0.2918 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -541.5725 196.7725 ## sample estimates: ## mean of x mean of y ## 143.6 316.0 1 2 # one sample t-test t.test ( as.numeric ( colors[1 , ] ), mu = 0 ) # Ho: mu=0 1 2 3 4 5 6 7 8 9 10 11 ## ## One Sample t-test ## ## data: as.numeric(colors[1, ]) ## t = 2.348, df = 4, p-value = 0.07868 ## alternative hypothesis: true mean is not equal to 0 ## 95 percent confidence interval: ## -26.2009 313.4009 ## sample estimates: ## mean of x ## 143.6 1 t.test ( as.numeric ( colors[1 , ] ), mu = 0 , alternative = \"greater\" ) # Ho: mu=<0 1 2 3 4 5 6 7 8 9 10 11 ## ## One Sample t-test ## ## data: as.numeric(colors[1, ]) ## t = 2.348, df = 4, p-value = 0.03934 ## alternative hypothesis: true mean is greater than 0 ## 95 percent confidence interval: ## 13.22123 Inf ## sample estimates: ## mean of x ## 143.6 alternative=\"less\" or alternative=\"greater\" option to specify a one tailed test. var.equal = TRUE option to specify equal variances and a pooled variance estimate. For multivariate tests and ANOVA, see sections 8 and 9. Nonparametric statistics \u00b6 Nonnormal distributions. Bivariate tests \u00b6 1 2 3 4 5 6 7 8 # independent 2-group Mann-Whitney U test wilcox.test ( y ~ A ) # where y is numeric and A is A binary factor # independent 2-group Mann-Whitney U test wilcox.test ( y , x ) # where y and x are numeric # dependent 2-group Wilcoxon signed rank test wilcox.test ( y1 , y2 , paired = TRUE ) # where y1 and y2 are numeric 1 2 # 2-way contingency table colors 1 2 3 4 5 ## fair.hair red.hair medium.hair dark.hair black.hair ## blue eyes 326 38 241 110 3 ## light eyes 688 116 584 188 4 ## medium eyes 343 84 909 412 26 ## dark eyes 98 48 403 681 85 1 2 # independent 2-group Mann-Whitney U Test wilcox.test ( as.numeric ( colors[1 , ] ), as.numeric ( colors[2 , ] )) # where y and x are numeric 1 2 3 4 5 6 ## ## Wilcoxon rank sum test ## ## data: as.numeric(colors[1, ]) and as.numeric(colors[2, ]) ## W = 8, p-value = 0.4206 ## alternative hypothesis: true location shift is not equal to 0 alternative=\"less\" or alternative=\"greater\" option to specify a one tailed test. ANOVA \u00b6 1 2 3 4 5 6 7 8 # Kruskal Wallis test one-Way ANOVA by ranks kruskal.test ( y ~ A ) # where y1 is numeric and A is a factor # randomized block design - Friedman test friedman.test ( y ~ A | B ) # where y are the data values, A is a grouping factor and B is a blocking factor # Kruskal Wallis test one-way ANOVA by ranks kruskal.test ( y , x ) # where y and x are numeric 1 2 3 # dataset # 2-way contingency table colors 1 2 3 4 5 ## fair.hair red.hair medium.hair dark.hair black.hair ## blue eyes 326 38 241 110 3 ## light eyes 688 116 584 188 4 ## medium eyes 343 84 909 412 26 ## dark eyes 98 48 403 681 85 1 2 # Kruskal Wallis test one-way ANOVA by ranks kruskal.test ( as.numeric ( colors[1 , ] ), as.numeric ( colors[2 , ] )) # where y and x are numeric 1 2 3 4 5 ## ## Kruskal-Wallis rank sum test ## ## data: as.numeric(colors[1, ]) and as.numeric(colors[2, ]) ## Kruskal-Wallis chi-squared = 4, df = 4, p-value = 0.406 Multiple Regressions \u00b6 Fitting the Model \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 # multiple linear regression fit <- lm ( y ~ x1 + x2 + x3 , data = mydata ) summary ( fit ) # show results # useful functions coefficients ( fit ) # model coefficients confint ( fit , level = 0.95 ) # CIs for model parameters fitted ( fit ) # predicted values residuals ( fit ) # residuals anova ( fit ) # ANOVA table vcov ( fit ) # covariance matrix for model parameters influence ( fit ) # regression diagnostics 1 2 # dataset str ( longley ) 1 2 3 4 5 6 7 8 ## 'data.frame': 16 obs. of 7 variables: ## $ GNP.deflator: num 83 88.5 88.2 89.5 96.2 ... ## $ GNP : num 234 259 258 285 329 ... ## $ Unemployed : num 236 232 368 335 210 ... ## $ Armed.Forces: num 159 146 162 165 310 ... ## $ Population : num 108 109 110 111 112 ... ## $ Year : int 1947 1948 1949 1950 1951 1952 1953 1954 1955 1956 ... ## $ Employed : num 60.3 61.1 60.2 61.2 63.2 ... 1 2 3 # multiple linear regression example fit <- lm ( Armed.Forces ~ GNP + Population , data = longley ) summary ( fit ) # show results 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 ## ## Call: ## lm(formula = Armed.Forces ~ GNP + Population, data = longley) ## ## Residuals: ## Min 1Q Median 3Q Max ## -70.349 -33.569 5.076 16.409 104.037 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 4123.922 1276.579 3.230 0.00657 ** ## GNP 3.365 0.986 3.413 0.00463 ** ## Population -44.011 14.089 -3.124 0.00807 ** ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## Residual standard error: 50.56 on 13 degrees of freedom ## Multiple R-squared: 0.5426, Adjusted R-squared: 0.4723 ## F-statistic: 7.712 on 2 and 13 DF, p-value: 0.006191 1 2 # other useful functions coefficients ( fit ) # model coefficients 1 2 ## (Intercept) GNP Population ## 4123.922484 3.365215 -44.010955 1 confint ( fit , level = 0.95 ) # CIs for model parameters 1 2 3 4 ## 2.5 % 97.5 % ## (Intercept) 1366.042123 6881.802846 ## GNP 1.235097 5.495333 ## Population -74.447969 -13.573942 1 fitted ( fit ) # predicted values 1 2 3 4 ## 1947 1948 1949 1950 1951 1952 1953 1954 ## 176.4245 215.9487 161.1151 199.5681 298.4663 306.5279 288.1248 230.9633 ## 1955 1956 1957 1958 1959 1960 1961 1962 ## 295.1332 308.9566 313.0359 252.7794 318.8698 297.7176 240.7975 266.2711 1 residuals ( fit ) # residuals 1 2 3 4 5 6 ## 1947 1948 1949 1950 1951 1952 ## -17.4245114 -70.3487085 0.4848669 -34.5681071 11.4336564 52.8721086 ## 1953 1954 1955 1956 1957 1958 ## 66.5752438 104.0367029 9.6668098 -23.2566323 -33.2359499 10.9205505 ## 1959 1960 1961 1962 ## -63.6698197 -46.3175746 16.4025069 16.4288577 1 anova ( fit ) # ANOVA table 1 2 3 4 5 6 7 8 9 ## Analysis of Variance Table ## ## Response: Armed.Forces ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## GNP 1 14479 14478.7 5.6649 0.033301 * ## Population 1 24941 24940.8 9.7583 0.008068 ** ## Residuals 13 33226 2555.9 ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 1 vcov ( fit ) # covariance matrix for model parameters 1 2 3 4 ## (Intercept) GNP Population ## (Intercept) 1629652.882 1239.7478355 -17970.27388 ## GNP 1239.748 0.9721911 -13.76775 ## Population -17970.274 -13.7677545 198.49444 1 influence ( fit ) # regression diagnostics 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 ## $hat ## 1947 1948 1949 1950 1951 1952 ## 0.27412252 0.17438942 0.31563199 0.16765687 0.21219668 0.21127212 ## 1953 1954 1955 1956 1957 1958 ## 0.11339116 0.08602104 0.10270245 0.12845683 0.13251347 0.11070418 ## 1959 1960 1961 1962 ## 0.15598638 0.15164367 0.32488437 0.33842686 ## ## $coefficients ## (Intercept) GNP Population ## 1947 128.042531 0.131479474 -1.53731106 ## 1948 29.040635 0.121992502 -0.69544934 ## 1949 -6.396740 -0.005738655 0.07379997 ## 1950 177.778426 0.175668519 -2.11609661 ## 1951 133.333008 0.093997366 -1.43810938 ## 1952 638.680267 0.462229807 -6.92955762 ## 1953 422.107967 0.305133565 -4.56222462 ## 1954 -385.998493 -0.325674565 4.42308458 ## 1955 54.458118 0.042127997 -0.59713302 ## 1956 -163.371695 -0.131240587 1.81041089 ## 1957 -212.039316 -0.179084306 2.37664757 ## 1958 -51.395691 -0.033854337 0.55600613 ## 1959 -329.488802 -0.311550864 3.79446941 ## 1960 3.116881 -0.049904757 0.10916689 ## 1961 -242.199361 -0.158976865 2.60043035 ## 1962 -194.416431 -0.113799395 2.04462752 ## ## $sigma ## 1947 1948 1949 1950 1951 1952 1953 1954 ## 52.28757 47.63741 52.61955 51.47046 52.48826 49.73420 48.50003 42.21357 ## 1955 1956 1957 1958 1959 1960 1961 1962 ## 52.53729 52.12610 51.60167 52.51353 48.66817 50.57779 52.30331 52.29577 ## ## $wt.res ## 1947 1948 1949 1950 1951 1952 ## -17.4245114 -70.3487085 0.4848669 -34.5681071 11.4336564 52.8721086 ## 1953 1954 1955 1956 1957 1958 ## 66.5752438 104.0367029 9.6668098 -23.2566323 -33.2359499 10.9205505 ## 1959 1960 1961 1962 ## -63.6698197 -46.3175746 16.4025069 16.4288577 Diagnostic plots \u00b6 1 2 3 # diagnostic plots layout ( matrix ( c ( 1 , 2 , 3 , 4 ), 2 , 2 )) # optional 4 graphs/page plot ( fit ) 1 layout ( matrix ( c ( 1 , 1 , 1 , 1 ), 2 , 2 )) Comparing two models with ANOVA \u00b6 1 2 3 4 5 # compare models fit1 <- lm ( y ~ x1 + x2 + x3 + x4 , data = mydata ) fit2 <- lm ( y ~ x1 + x2 ) anova ( fit1 , fit2 ) 1 2 3 4 5 # compare models fit1 <- lm ( Armed.Forces ~ GNP + Population , data = longley ) fit2 <- lm ( Armed.Forces ~ GNP , data = longley ) anova ( fit1 , fit2 ) 1 2 3 4 5 6 7 8 9 ## Analysis of Variance Table ## ## Model 1: Armed.Forces ~ GNP + Population ## Model 2: Armed.Forces ~ GNP ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 13 33226 ## 2 14 58167 -1 -24941 9.7583 0.008068 ** ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 Cross validation \u00b6 1 2 3 4 5 # dataset library ( DAAG ) data ( houseprices ) head ( houseprices , 3 ) 1 2 3 4 ## area bedrooms sale.price ## 9 694 4 192 ## 10 905 4 215 ## 11 802 4 215 1 2 3 4 5 # k-fold cross-validation library ( DAAG ) # case 1, # 3 fold cross-validation CVlm ( houseprices , form.lm = formula ( sale.price ~ area ), m = 3 , dots = FALSE , seed = 29 , plotit = c ( \"Observed\" , \"Residual\" ), main = \"Small symbols show cross-validation predicted values\" , legend.pos = \"topleft\" , printit = TRUE ) 1 2 3 4 5 6 7 8 ## Analysis of Variance Table ## ## Response: sale.price ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## area 1 18566 18566 8 0.014 * ## Residuals 13 30179 2321 ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 ## ## fold 1 ## Observations in test set: 5 ## 11 20 21 22 23 ## area 802 696 771.0 1006.0 1191 ## cvpred 204 188 199.3 234.7 262 ## sale.price 215 255 260.0 293.0 375 ## CV residual 11 67 60.7 58.3 113 ## ## Sum of squares = 24351 Mean square = 4870 n = 5 ## ## fold 2 ## Observations in test set: 5 ## 10 13 14 17 18 ## area 905 716 963.0 1018.00 887.00 ## cvpred 255 224 264.4 273.38 252.06 ## sale.price 215 113 185.0 276.00 260.00 ## CV residual -40 -112 -79.4 2.62 7.94 ## ## Sum of squares = 20416 Mean square = 4083 n = 5 ## ## fold 3 ## Observations in test set: 5 ## 9 12 15 16 19 ## area 694.0 1366 821.00 714.0 790.00 ## cvpred 183.2 388 221.94 189.3 212.49 ## sale.price 192.0 274 212.00 220.0 221.50 ## CV residual 8.8 -114 -9.94 30.7 9.01 ## ## Sum of squares = 14241 Mean square = 2848 n = 5 ## ## Overall (Sum over all 5 folds) ## ms ## 3934 1 2 # dataset head ( longley , 3 ) 1 2 3 4 ## GNP.deflator GNP Unemployed Armed.Forces Population Year Employed ## 1947 83.0 234 236 159 108 1947 60.3 ## 1948 88.5 259 232 146 109 1948 61.1 ## 1949 88.2 258 368 162 110 1949 60.2 1 2 # case 2, # 3 fold cross-validation CVlm ( longley , form.lm = formula ( Armed.Forces ~ GNP + Population ), m = 3 , dots = FALSE , seed = 29 , plotit = c ( \"Observed\" , \"Residual\" ), main = \"Small symbols show cross-validation predicted values\" , legend.pos = \"topleft\" , printit = TRUE ) 1 2 3 4 5 6 7 8 9 ## Analysis of Variance Table ## ## Response: Armed.Forces ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## GNP 1 14479 14479 5.66 0.0333 * ## Population 1 24941 24941 9.76 0.0081 ** ## Residuals 13 33226 2556 ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 ## ## fold 1 ## Observations in test set: 5 ## 1953 1954 1957 1960 1962 ## Predicted 288.1 231 313.0 297.7 266.3 ## cvpred 281.4 219 310.6 295.7 263.1 ## Armed.Forces 354.7 335 279.8 251.4 282.7 ## CV residual 73.3 116 -30.8 -44.3 19.6 ## ## Sum of squares = 22004 Mean square = 4401 n = 5 ## ## fold 2 ## Observations in test set: 6 ## 1948 1949 1955 1958 1959 1961 ## Predicted 215.9 161.12 295.13 252.78 318.9 240.8 ## cvpred 231.9 171.41 306.33 255.07 324.5 234.9 ## Armed.Forces 145.6 161.60 304.80 263.70 255.2 257.2 ## CV residual -86.3 -9.81 -1.53 8.63 -69.3 22.3 ## ## Sum of squares = 12917 Mean square = 2153 n = 6 ## ## fold 3 ## Observations in test set: 5 ## 1947 1950 1951 1952 1956 ## Predicted 176.4 199.6 298.5 306.5 308.96 ## cvpred 192.8 211.9 282.3 288.9 295.17 ## Armed.Forces 159.0 165.0 309.9 359.4 285.70 ## CV residual -33.8 -46.9 27.6 70.5 -9.47 ## ## Sum of squares = 9161 Mean square = 1832 n = 5 ## ## Overall (Sum over all 5 folds) ## ms ## 2755 Variable selection \u2013 Heuristic methods \u00b6 1 2 # dataset head ( longley , 3 ) 1 2 3 4 ## GNP.deflator GNP Unemployed Armed.Forces Population Year Employed ## 1947 83.0 234 236 159 108 1947 60.3 ## 1948 88.5 259 232 146 109 1948 61.1 ## 1949 88.2 258 368 162 110 1949 60.2 1 2 3 4 5 # Stepwise Regression library ( MASS ) fit <- lm ( Armed.Forces ~ GNP + Population + Employed + Unemployed , data = longley ) step <- stepAIC ( fit , direction = \"both\" ) 1 2 3 4 5 6 7 8 9 ## Start: AIC=125 ## Armed.Forces ~ GNP + Population + Employed + Unemployed ## ## Df Sum of Sq RSS AIC ## &lt;none&gt; 21655 125 ## - Unemployed 1 4508 26163 126 ## - Population 1 5522 27177 127 ## - Employed 1 10208 31863 130 ## - GNP 1 15323 36978 132 1 step $ anova # display results 1 2 3 4 5 6 7 8 9 10 11 12 ## Stepwise Model Path ## Analysis of Deviance Table ## ## Initial Model: ## Armed.Forces ~ GNP + Population + Employed + Unemployed ## ## Final Model: ## Armed.Forces ~ GNP + Population + Employed + Unemployed ## ## ## Step Df Deviance Resid. Df Resid. Dev AIC ## 1 11 21655 125 The goal is to reduce the AIC. Adding variable does not improve the model. Let\u2019s opt for the most valuable variable. 1 2 fit <- lm ( Armed.Forces ~ Unemployed , data = longley ) step <- stepAIC ( fit , direction = \"both\" ) 1 2 3 4 5 6 7 8 9 10 11 12 13 ## Start: AIC=138 ## Armed.Forces ~ Unemployed ## ## Df Sum of Sq RSS AIC ## - Unemployed 1 2287 72646 137 ## &lt;none&gt; 70359 138 ## ## Step: AIC=137 ## Armed.Forces ~ 1 ## ## Df Sum of Sq RSS AIC ## &lt;none&gt; 72646 137 ## + Unemployed 1 2287 70359 138 1 step $ anova # display results 1 2 3 4 5 6 7 8 9 10 11 12 13 ## Stepwise Model Path ## Analysis of Deviance Table ## ## Initial Model: ## Armed.Forces ~ Unemployed ## ## Final Model: ## Armed.Forces ~ 1 ## ## ## Step Df Deviance Resid. Df Resid. Dev AIC ## 1 14 70359 138 ## 2 - Unemployed 1 2287 15 72646 137 Variable selection \u2013 Graphical methods \u00b6 1 2 # model fit <- lm ( Armed.Forces ~ GNP + Population + Employed + Unemployed , data = longley ) 1 2 3 4 5 6 7 # all Subsets Regression library ( leaps ) leaps <- regsubsets ( Armed.Forces ~ GNP + Population + Employed + Unemployed , data = longley , nbest = 10 ) # view results summary ( leaps ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 ## Subset selection object ## Call: regsubsets.formula(Armed.Forces ~ GNP + Population + Employed + ## Unemployed, data = longley, nbest = 10) ## 4 Variables (and intercept) ## Forced in Forced out ## GNP FALSE FALSE ## Population FALSE FALSE ## Employed FALSE FALSE ## Unemployed FALSE FALSE ## 10 subsets of each size up to 4 ## Selection Algorithm: exhaustive ## GNP Population Employed Unemployed ## 1 ( 1 ) \" \" \" \" \"*\" \" \" ## 1 ( 2 ) \"*\" \" \" \" \" \" \" ## 1 ( 3 ) \" \" \"*\" \" \" \" \" ## 1 ( 4 ) \" \" \" \" \" \" \"*\" ## 2 ( 1 ) \"*\" \"*\" \" \" \" \" ## 2 ( 2 ) \"*\" \" \" \" \" \"*\" ## 2 ( 3 ) \" \" \"*\" \" \" \"*\" ## 2 ( 4 ) \" \" \" \" \"*\" \"*\" ## 2 ( 5 ) \" \" \"*\" \"*\" \" \" ## 2 ( 6 ) \"*\" \" \" \"*\" \" \" ## 3 ( 1 ) \"*\" \"*\" \"*\" \" \" ## 3 ( 2 ) \"*\" \" \" \"*\" \"*\" ## 3 ( 3 ) \"*\" \"*\" \" \" \"*\" ## 3 ( 4 ) \" \" \"*\" \"*\" \"*\" ## 4 ( 1 ) \"*\" \"*\" \"*\" \"*\" 1 2 3 # plot a table of models showing variables in each model # models are ordered by the selection statistic plot ( leaps , scale = \"r2\" ) 1 2 3 4 # plot statistic by subset size library ( car ) subsets ( leaps , statistic = \"rsq\" , legend = FALSE ) # available criteria are rsq, rss, adjr2, cp, bic 1 2 3 4 5 ## Abbreviation ## GNP G ## Population P ## Employed E ## Unemployed U 1 subsets ( leaps , statistic = \"bic\" , legend = FALSE ) # available criteria are rsq, rss, adjr2, cp, bic 1 2 3 4 5 ## Abbreviation ## GNP G ## Population P ## Employed E ## Unemployed U Variable selection \u2013 Relative importance \u00b6 Model: fit <- lm ( Armed.Forces ~ GNP + Population + Employed + Unemployed , data = longley ) . Warning: eval=FALSE . 1 2 3 4 # calculate the relative importance of each predictor library ( relaimpo ) calc.relimp ( fit , type = c ( \"lmg\" , \"last\" , \"first\" , \"pratt\" ), rela = TRUE ) Results. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 Response variable: Armed.Forces Total response variance: 4843 Analysis based on 16 observations 4 Regressors: GNP Population Employed Unemployed Proportion of variance explained by model: 70.2% Metrics are normalized to sum to 100% (rela=TRUE). Relative importance metrics: lmg last first pratt GNP 0.328 0.431 0.348 4.566 Population 0.241 0.155 0.232 -1.934 Employed 0.217 0.287 0.365 -1.780 Unemployed 0.215 0.127 0.055 0.148 Average coefficients for different model sizes: 1X 2Xs 3Xs 4Xs GNP 0.313 1.301 3.641 5.026 Population 3.646 -14.815 -24.637 -37.260 Employed 9.062 17.646 -34.249 -54.144 Unemployed -0.132 -0.511 -0.584 -0.436 Bootstrapping Model: fit <- lm ( Armed.Forces ~ GNP + Population + Employed + Unemployed , data = longley ) . Warning: eval=FALSE . 1 2 3 # bootstrap measures of relative importance (1000 samples) boot <- boot.relimp ( fit , b = 1000 , type = c ( \"lmg\" , \"last\" , \"first\" , \"pratt\" ), rank = TRUE , diff = TRUE , rela = TRUE ) booteval.relimp ( boot ) # print result Results. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 Response variable: Armed.Forces Total response variance: 4843 Analysis based on 16 observations 4 Regressors: GNP Population Employed Unemployed Proportion of variance explained by model: 70.2% Metrics are normalized to sum to 100% (rela=TRUE). Relative importance metrics: lmg last first pratt GNP 0.328 0.431 0.348 4.566 Population 0.241 0.155 0.232 -1.934 Employed 0.217 0.287 0.365 -1.780 Unemployed 0.215 0.127 0.055 0.148 Average coefficients for different model sizes: 1X 2Xs 3Xs 4Xs GNP 0.313 1.301 3.641 5.026 Population 3.646 -14.815 -24.637 -37.260 Employed 9.062 17.646 -34.249 -54.144 Unemployed -0.132 -0.511 -0.584 -0.436 Confidence interval information ( 1000 bootstrap replicates, bty= perc ): Relative Contributions with confidence intervals: Lower Upper percentage 0.95 0.95 0.95 GNP.lmg 0.3280 ABC_ 0.1488 0.4030 Population.lmg 0.2410 _BCD 0.1572 0.3060 Employed.lmg 0.2170 ABCD 0.1107 0.3240 Unemployed.lmg 0.2150 ABCD 0.0586 0.5550 GNP.last 0.4310 ABCD 0.0101 0.5660 Population.last 0.1550 _BCD 0.0003 0.3850 Employed.last 0.2870 ABCD 0.0476 0.6280 Unemployed.last 0.1270 ABCD 0.0009 0.7810 GNP.first 0.3480 ABCD 0.0151 0.3960 Population.first 0.2320 _BCD 0.0070 0.3060 Employed.first 0.3650 ABCD 0.0159 0.4020 Unemployed.first 0.0550 ABCD 0.0004 0.9280 GNP.pratt 4.5660 ABCD -1.0160 10.5380 Population.pratt -1.9340 ABCD -6.0800 2.2050 Employed.pratt -1.7800 _BCD -5.0290 0.8510 Unemployed.pratt 0.1480 ABC_ -0.3250 1.0910 Letters indicate the ranks covered by bootstrap CIs. (Rank bootstrap confidence intervals always obtained by percentile method) CAUTION: Bootstrap confidence intervals can be somewhat liberal. Differences between Relative Contributions: Lower Upper difference 0.95 0.95 0.95 GNP-Population.lmg 0.0871 -0.0188 0.1521 GNP-Employed.lmg 0.1106 -0.0479 0.1949 GNP-Unemployed.lmg 0.1130 -0.4021 0.3281 Population-Employed.lmg 0.0236 -0.1091 0.1124 Population-Unemployed.lmg 0.0259 -0.3904 0.2417 Employed-Unemployed.lmg 0.0023 -0.4315 0.2295 GNP-Population.last 0.2756 -0.1062 0.4502 GNP-Employed.last 0.1438 -0.5564 0.4011 GNP-Unemployed.last 0.3041 -0.7408 0.5502 Population-Employed.last -0.1318 -0.6191 0.2653 Population-Unemployed.last 0.0285 -0.7382 0.3622 Employed-Unemployed.last 0.1603 -0.6784 0.4852 GNP-Population.first 0.1161 -0.0590 0.1736 GNP-Employed.first -0.0172 -0.0893 0.0934 GNP-Unemployed.first 0.2930 -0.9047 0.3766 Population-Employed.first -0.1333 -0.2134 0.0688 Population-Unemployed.first 0.1769 -0.8967 0.2969 Employed-Unemployed.first 0.3102 -0.8937 0.3823 GNP-Population.pratt 6.4995 -2.2069 15.7978 GNP-Employed.pratt 6.3461 -1.3879 15.0093 GNP-Unemployed.pratt 4.4180 -1.9134 10.6456 Population-Employed.pratt -0.1534 -4.1860 5.9691 Population-Unemployed.pratt -2.0815 -6.1439 2.2348 Employed-Unemployed.pratt -1.9281 -5.2057 0.1725 * indicates that CI for difference does not include 0. CAUTION: Bootstrap confidence intervals can be somewhat liberal. Warning: eval=FALSE . 1 plot ( booteval.relimp ( boot , sort = TRUE )) # to plot the results The .png file. Going further \u00b6 The nls package provides functions for nonlinear regression. Perform robust regression with the rlm function in the MASS package. The robust package provides a comprehensive library of robust methods, including regression. The robustbase package also provides basic robust statistics including model selection methods. Regression diagnostics \u00b6 1 2 3 # assume that we are fitting a multiple linear regression fit <- lm ( mpg ~ disp + hp + wt + drat , data = mtcars ) fit 1 2 3 4 5 6 7 ## ## Call: ## lm(formula = mpg ~ disp + hp + wt + drat, data = mtcars) ## ## Coefficients: ## (Intercept) disp hp wt drat ## 29.14874 0.00382 -0.03478 -3.47967 1.76805 Outliers \u00b6 1 2 3 4 library ( car ) # assessing outliers outlierTest ( fit ) # Bonferonni p-value for most extreme obs 1 2 3 4 5 ## ## No Studentized residuals with Bonferonni p &lt; 0.05 ## Largest |rstudent|: ## rstudent unadjusted p-value Bonferonni p ## Toyota Corolla 2.52 0.0184 0.588 1 qqPlot ( fit , main = \"QQ Plot\" ) #qq plot for studentized resid 1 leveragePlots ( fit ) # leverage plots Influential observations \u00b6 Model: fit <- lm ( mpg ~ disp + hp + wt + drat , data = mtcars ) . 1 2 3 4 5 # influential observations # added variable plots library ( car ) avPlots ( fit ) 1 2 3 4 5 6 par ( mfrow = c ( 1 , 1 )) # Cook's D plot # identify D values > 4/(n-k-1) cutoff <- 4 / (( nrow ( mtcars ) - length ( fit $ coefficients ) - 2 )) plot ( fit , which = 4 , cook.levels = cutoff ) Warning: eval=FALSE ; interactive function. 1 2 # influence plot influencePlot ( fit , id.method = \"identify\" , main = \"Influence Plot\" , sub = \"Circle size is proportial to Cook's Distance\" ) Nonnormality \u00b6 Model: fit <- lm ( mpg ~ disp + hp + wt + drat , data = mtcars ) . 1 2 3 4 5 # normality of residuals # qq plot for studentized resid library ( car ) qqPlot ( fit , main = \"QQ Plot\" ) 1 2 3 4 5 6 7 8 # distribution of studentized residuals library ( MASS ) sresid <- studres ( fit ) hist ( sresid , freq = FALSE , main = \"Distribution of Studentized Residuals\" ) xfit <- seq ( min ( sresid ), max ( sresid ), length = 40 ) yfit <- dnorm ( xfit ) lines ( xfit , yfit ) Heteroscedasticity \u00b6 Nonconstant error variance. Model: fit <- lm ( mpg ~ disp + hp + wt + drat , data = mtcars ) . 1 2 3 # evaluate homoscedasticity # non-constant error variance test ncvTest ( fit ) 1 2 3 ## Non-constant Variance Score Test ## Variance formula: ~ fitted.values ## Chisquare = 1.43 Df = 1 p = 0.232 1 2 # plot studentized residuals vs. fitted values spreadLevelPlot ( fit ) 1 2 ## ## Suggested power transformation: 0.662 Multicollinearity \u00b6 Model: fit <- lm ( mpg ~ disp + hp + wt + drat , data = mtcars ) . 1 2 # evaluatecCollinearity vif ( fit ) 1 2 ## disp hp wt drat ## 8.21 2.89 5.10 2.28 1 sqrt ( vif ( fit )) > 2 # benchmark = 1.96, rounded to 2 1 2 ## disp hp wt drat ## TRUE FALSE TRUE FALSE Nonlinearity \u00b6 Model: fit <- lm ( mpg ~ disp + hp + wt + drat , data = mtcars ) . 1 2 3 # evaluate nonlinearity # component + residual plot crPlots ( fit ) 1 2 # Ceres plots ceresPlots ( fit ) Autocorrelation \u00b6 Serial correlation or non-independence of errors. Model: fit <- lm ( mpg ~ disp + hp + wt + drat , data = mtcars ) . 1 2 # test for autocorrelated errors durbinWatsonTest ( fit ) 1 2 3 ## lag Autocorrelation D-W Statistic p-value ## 1 0.101 1.74 0.29 ## Alternative hypothesis: rho != 0 Global diagnostic \u00b6 Model: fit <- lm ( mpg ~ disp + hp + wt + drat , data = mtcars ) . 1 2 3 4 5 # global test of model assumptions library ( gvlma ) gvmodel <- gvlma ( fit ) summary ( gvmodel ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 ## ## Call: ## lm(formula = mpg ~ disp + hp + wt + drat, data = mtcars) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.508 -1.905 -0.506 0.982 5.688 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 29.14874 6.29359 4.63 8.2e-05 *** ## disp 0.00382 0.01080 0.35 0.7268 ## hp -0.03478 0.01160 -3.00 0.0058 ** ## wt -3.47967 1.07837 -3.23 0.0033 ** ## drat 1.76805 1.31978 1.34 0.1915 ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## Residual standard error: 2.6 on 27 degrees of freedom ## Multiple R-squared: 0.838, Adjusted R-squared: 0.814 ## F-statistic: 34.8 on 4 and 27 DF, p-value: 2.7e-10 ## ## ## ASSESSMENT OF THE LINEAR MODEL ASSUMPTIONS ## USING THE GLOBAL TEST ON 4 DEGREES-OF-FREEDOM: ## Level of Significance = 0.05 ## ## Call: ## gvlma(x = fit) ## ## Value p-value Decision ## Global Stat 13.9382 0.00750 Assumptions NOT satisfied! ## Skewness 4.3131 0.03782 Assumptions NOT satisfied! ## Kurtosis 0.0138 0.90654 Assumptions acceptable. ## Link Function 8.7166 0.00315 Assumptions NOT satisfied! ## Heteroscedasticity 0.8947 0.34421 Assumptions acceptable. ANOVA \u00b6 Analysis of variance (ANOVA) is an alternative to regressions among other applications. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 # lower case letters are numeric variables # upper case letters are factors # one-way ANOVA (completely randomized design) fit <- aov ( y ~ A , data = mydataframe ) # randomized block design (B is the blocking factor) fit <- aov ( y ~ A + B , data = mydataframe ) # two-way factorial design fit <- aov ( y ~ A + B + A : B , data = mydataframe ) fit <- aov ( y ~ A * B , data = mydataframe ) # same thing # analysis of covariance fit <- aov ( y ~ A + x , data = mydataframe ) 1 2 # dataset head ( CO2 , 3 ) 1 2 3 4 5 ## Grouped Data: uptake ~ conc | Plant ## Plant Type Treatment conc uptake ## 1 Qn1 Quebec nonchilled 95 16.0 ## 2 Qn1 Quebec nonchilled 175 30.4 ## 3 Qn1 Quebec nonchilled 250 34.8 1 2 3 # one-way ANOVA (completely randomized design) fit <- aov ( uptake ~ Plant , data = CO2 ) fit 1 2 3 4 5 6 7 8 9 10 ## Call: ## aov(formula = uptake ~ Plant, data = CO2) ## ## Terms: ## Plant Residuals ## Sum of Squares 4862 4845 ## Deg. of Freedom 11 72 ## ## Residual standard error: 8.2 ## Estimated effects are balanced 1 summary ( fit ) 1 2 3 4 5 ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Plant 11 4862 442 6.57 1.8e-07 *** ## Residuals 72 4845 67 ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 1 2 3 # randomized block design (B is the blocking factor) fit <- aov ( uptake ~ Plant + Type , data = CO2 ) fit 1 2 3 4 5 6 7 8 9 10 11 ## Call: ## aov(formula = uptake ~ Plant + Type, data = CO2) ## ## Terms: ## Plant Residuals ## Sum of Squares 4862 4845 ## Deg. of Freedom 11 72 ## ## Residual standard error: 8.2 ## 1 out of 13 effects not estimable ## Estimated effects are balanced 1 summary ( fit ) 1 2 3 4 5 ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Plant 11 4862 442 6.57 1.8e-07 *** ## Residuals 72 4845 67 ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 1 2 3 # two-way factorial design fit <- aov ( uptake ~ Plant + Type + Plant : Type , data = CO2 ) fit 1 2 3 4 5 6 7 8 9 10 11 ## Call: ## aov(formula = uptake ~ Plant + Type + Plant:Type, data = CO2) ## ## Terms: ## Plant Residuals ## Sum of Squares 4862 4845 ## Deg. of Freedom 11 72 ## ## Residual standard error: 8.2 ## 12 out of 24 effects not estimable ## Estimated effects are balanced 1 summary ( fit ) 1 2 3 4 5 ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Plant 11 4862 442 6.57 1.8e-07 *** ## Residuals 72 4845 67 ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 1 2 fit <- aov ( uptake ~ Plant * Type , data = CO2 ) # same thing fit 1 2 3 4 5 6 7 8 9 10 11 ## Call: ## aov(formula = uptake ~ Plant * Type, data = CO2) ## ## Terms: ## Plant Residuals ## Sum of Squares 4862 4845 ## Deg. of Freedom 11 72 ## ## Residual standard error: 8.2 ## 12 out of 24 effects not estimable ## Estimated effects are balanced 1 summary ( fit ) 1 2 3 4 5 ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Plant 11 4862 442 6.57 1.8e-07 *** ## Residuals 72 4845 67 ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 1 2 3 # Analysis of Covariance fit <- aov ( uptake ~ uptake + conc , data = CO2 ) fit 1 2 3 4 5 6 7 8 9 10 ## Call: ## aov(formula = uptake ~ uptake + conc, data = CO2) ## ## Terms: ## conc Residuals ## Sum of Squares 2285 7422 ## Deg. of Freedom 1 82 ## ## Residual standard error: 9.51 ## Estimated effects may be unbalanced 1 summary ( fit ) 1 2 3 4 5 ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## conc 1 2285 2285 25.2 2.9e-06 *** ## Residuals 82 7422 91 ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 Evaluate model effects \u00b6 Type I sequential SS: A+B and B+A will produce different results! Use the drop1 to produce the familiar Type III results; compare each term with the full model. 1 2 3 # display Type I ANOVA table fit1 <- aov ( uptake ~ Plant + Type , data = CO2 ) summary ( fit1 ) 1 2 3 4 5 ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Plant 11 4862 442 6.57 1.8e-07 *** ## Residuals 72 4845 67 ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 1 2 3 # display Type I ANOVA table fit2 <- aov ( uptake ~ Type + Plant , data = CO2 ) summary ( fit2 ) 1 2 3 4 5 6 ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Type 1 3366 3366 50.02 8.1e-10 *** ## Plant 10 1497 150 2.22 0.026 * ## Residuals 72 4845 67 ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 1 2 # display type III SS and F tests drop1 ( fit1 , ~ ., test = \"F\" ) 1 2 3 4 5 6 7 8 9 10 ## Single term deletions ## ## Model: ## uptake ~ Plant + Type ## Df Sum of Sq RSS AIC F value Pr(&gt;F) ## &lt;none&gt; 4845 365 ## Plant 10 1497 6341 367 2.22 0.026 * ## Type 0 0 4845 365 ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 1 drop1 ( fit2 , ~ ., test = \"F\" ) 1 2 3 4 5 6 7 8 9 10 ## Single term deletions ## ## Model: ## uptake ~ Type + Plant ## Df Sum of Sq RSS AIC F value Pr(&gt;F) ## &lt;none&gt; 4845 365 ## Type 0 0 4845 365 ## Plant 10 1497 6341 367 2.22 0.026 * ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 Compare nested models directly \u00b6 1 2 3 4 fit1 <- aov ( uptake ~ Plant + Type , data = CO2 ) fit2 <- aov ( uptake ~ Plant , data = CO2 ) anova ( fit1 , fit2 ) 1 2 3 4 5 6 7 ## Analysis of Variance Table ## ## Model 1: uptake ~ Plant + Type ## Model 2: uptake ~ Plant ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 72 4845 ## 2 72 4845 0 0 1 2 3 4 fit2 <- aov ( uptake ~ Plant + Type , data = CO2 ) fit1 <- aov ( uptake ~ Plant , data = CO2 ) anova ( fit1 , fit2 ) 1 2 3 4 5 6 7 ## Analysis of Variance Table ## ## Model 1: uptake ~ Plant ## Model 2: uptake ~ Plant + Type ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 72 4845 ## 2 72 4845 0 0 Multiple comparisons \u00b6 Tukey HSD tests for post hoc comparisons on each factor in the model. Factors as an option. Type I SS. 1 2 3 4 5 # model fit <- aov ( uptake ~ Plant + Type , data = CO2 ) # Tukey honestly significant differences TukeyHSD ( fit ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 ## Tukey multiple comparisons of means ## 95% family-wise confidence level ## ## Fit: aov(formula = uptake ~ Plant + Type, data = CO2) ## ## $Plant ## diff lwr upr p adj ## Qn2-Qn1 1.929 -12.88 16.739 1.000 ## Qn3-Qn1 4.386 -10.42 19.196 0.997 ## Qc1-Qn1 -3.257 -18.07 11.553 1.000 ## Qc3-Qn1 -0.643 -15.45 14.168 1.000 ## Qc2-Qn1 -0.529 -15.34 14.282 1.000 ## Mn3-Qn1 -9.114 -23.92 5.696 0.639 ## Mn2-Qn1 -5.886 -20.70 8.925 0.970 ## Mn1-Qn1 -6.829 -21.64 7.982 0.918 ## Mc2-Qn1 -21.086 -35.90 -6.275 0.000 ## Mc3-Qn1 -15.929 -30.74 -1.118 0.024 ## Mc1-Qn1 -15.229 -30.04 -0.418 0.038 ## Qn3-Qn2 2.457 -12.35 17.268 1.000 ## Qc1-Qn2 -5.186 -20.00 9.625 0.989 ## Qc3-Qn2 -2.571 -17.38 12.239 1.000 ## Qc2-Qn2 -2.457 -17.27 12.353 1.000 ## Mn3-Qn2 -11.043 -25.85 3.768 0.346 ## Mn2-Qn2 -7.814 -22.62 6.996 0.822 ## Mn1-Qn2 -8.757 -23.57 6.053 0.694 ## Mc2-Qn2 -23.014 -37.82 -8.204 0.000 ## Mc3-Qn2 -17.857 -32.67 -3.047 0.006 ## Mc1-Qn2 -17.157 -31.97 -2.347 0.010 ## Qc1-Qn3 -7.643 -22.45 7.168 0.842 ## Qc3-Qn3 -5.029 -19.84 9.782 0.991 ## Qc2-Qn3 -4.914 -19.72 9.896 0.993 ## Mn3-Qn3 -13.500 -28.31 1.311 0.108 ## Mn2-Qn3 -10.271 -25.08 4.539 0.457 ## Mn1-Qn3 -11.214 -26.02 3.596 0.323 ## Mc2-Qn3 -25.471 -40.28 -10.661 0.000 ## Mc3-Qn3 -20.314 -35.12 -5.504 0.001 ## Mc1-Qn3 -19.614 -34.42 -4.804 0.002 ## Qc3-Qc1 2.614 -12.20 17.425 1.000 ## Qc2-Qc1 2.729 -12.08 17.539 1.000 ## Mn3-Qc1 -5.857 -20.67 8.953 0.971 ## Mn2-Qc1 -2.629 -17.44 12.182 1.000 ## Mn1-Qc1 -3.571 -18.38 11.239 1.000 ## Mc2-Qc1 -17.829 -32.64 -3.018 0.006 ## Mc3-Qc1 -12.671 -27.48 2.139 0.167 ## Mc1-Qc1 -11.971 -26.78 2.839 0.233 ## Qc2-Qc3 0.114 -14.70 14.925 1.000 ## Mn3-Qc3 -8.471 -23.28 6.339 0.735 ## Mn2-Qc3 -5.243 -20.05 9.568 0.988 ## Mn1-Qc3 -6.186 -21.00 8.625 0.958 ## Mc2-Qc3 -20.443 -35.25 -5.632 0.001 ## Mc3-Qc3 -15.286 -30.10 -0.475 0.037 ## Mc1-Qc3 -14.586 -29.40 0.225 0.057 ## Mn3-Qc2 -8.586 -23.40 6.225 0.719 ## Mn2-Qc2 -5.357 -20.17 9.453 0.985 ## Mn1-Qc2 -6.300 -21.11 8.511 0.952 ## Mc2-Qc2 -20.557 -35.37 -5.747 0.001 ## Mc3-Qc2 -15.400 -30.21 -0.589 0.034 ## Mc1-Qc2 -14.700 -29.51 0.111 0.054 ## Mn2-Mn3 3.229 -11.58 18.039 1.000 ## Mn1-Mn3 2.286 -12.52 17.096 1.000 ## Mc2-Mn3 -11.971 -26.78 2.839 0.233 ## Mc3-Mn3 -6.814 -21.62 7.996 0.919 ## Mc1-Mn3 -6.114 -20.92 8.696 0.961 ## Mn1-Mn2 -0.943 -15.75 13.868 1.000 ## Mc2-Mn2 -15.200 -30.01 -0.389 0.039 ## Mc3-Mn2 -10.043 -24.85 4.768 0.493 ## Mc1-Mn2 -9.343 -24.15 5.468 0.603 ## Mc2-Mn1 -14.257 -29.07 0.553 0.070 ## Mc3-Mn1 -9.100 -23.91 5.711 0.641 ## Mc1-Mn1 -8.400 -23.21 6.411 0.746 ## Mc3-Mc2 5.157 -9.65 19.968 0.989 ## Mc1-Mc2 5.857 -8.95 20.668 0.971 ## Mc1-Mc3 0.700 -14.11 15.511 1.000 Visualizing results \u00b6 1 2 3 4 5 6 7 8 9 10 # two-way interaction plot attach ( mtcars ) gear <- factor ( gear ) cyl <- factor ( cyl ) # two-way interactions library ( car ) interaction.plot ( cyl , gear , mpg , type = \"b\" , col = c ( 1 : 3 ), leg.bty = \"o\" , leg.bg = \"beige\" , lwd = 2 , pch = c ( 18 , 24 , 22 ), xlab = \"Number of Cylinders\" , ylab = \"Mean Miles Per Gallon\" , main = \"Interaction Plot\" ) 1 2 3 4 # mean plots for single factors, and includes confidence intervals library ( gplots ) plotmeans ( mpg ~ cyl , xlab = \"Number of Cylinders\" , ylab = \"Miles Per Gallon\" , main = \"Mean Plot\\nwith 95% CI\" ) 1 detach ( mtcars ) MANOVA \u00b6 Multivariate analysis of variance (MANOVA) With more than one dependent variable Y. We can run several ANOVA over different Y, or one MANOVA with one Y built with several variables. 1 head ( longley , 3 ) 1 2 3 4 ## GNP.deflator GNP Unemployed Armed.Forces Population Year Employed ## 1947 83.0 234 236 159 108 1947 60.3 ## 1948 88.5 259 232 146 109 1948 61.1 ## 1949 88.2 258 368 162 110 1949 60.2 1 2 3 4 5 6 7 8 attach ( longley ) # 2x2 factorial MANOVA with 3 dependent variables, Y Y <- cbind ( Unemployed , Armed.Forces , Employed ) # Y fit <- manova ( Y ~ Population * GNP ) # Y ~ X # display type I SS summary ( fit , test = \"Pillai\" ) 1 2 3 4 5 6 7 ## Df Pillai approx F num Df den Df Pr(&gt;F) ## Population 1 0.997 1074 3 10 7.7e-13 *** ## GNP 1 0.888 26 3 10 4.6e-05 *** ## Population:GNP 1 0.870 22 3 10 9.4e-05 *** ## Residuals 12 ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 1 2 3 4 # test = \"Wilks\", \"Hotelling-Lawley\", and \"Roy\" # display univariate statistics summary.aov ( fit ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 ## Response Unemployed : ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Population 1 61739 61739 45.92 2e-05 *** ## GNP 1 42841 42841 31.87 0.00011 *** ## Population:GNP 1 10270 10270 7.64 0.01715 * ## Residuals 12 16133 1344 ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## Response Armed.Forces : ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Population 1 9647 9647 4.25 0.0617 . ## GNP 1 29772 29772 13.10 0.0035 ** ## Population:GNP 1 5958 5958 2.62 0.1314 ## Residuals 12 27268 2272 ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## Response Employed : ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Population 1 170.6 170.6 546.45 2.2e-11 *** ## GNP 1 10.5 10.5 33.60 8.5e-05 *** ## Population:GNP 1 0.1 0.1 0.41 0.54 ## Residuals 12 3.7 0.3 ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 1 2 3 4 5 # display type III SS fit1 <- manova ( Y ~ Population * GNP ) fit2 <- manova ( Y ~ GNP * Population ) # type III GNP effect summary ( fit1 ) 1 2 3 4 5 6 7 ## Df Pillai approx F num Df den Df Pr(&gt;F) ## Population 1 0.997 1074 3 10 7.7e-13 *** ## GNP 1 0.888 26 3 10 4.6e-05 *** ## Population:GNP 1 0.870 22 3 10 9.4e-05 *** ## Residuals 12 ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 1 2 # type III Population effect summary ( fit2 ) 1 2 3 4 5 6 7 ## Df Pillai approx F num Df den Df Pr(&gt;F) ## GNP 1 0.997 1088 3 10 7.2e-13 *** ## Population 1 0.786 12 3 10 0.0011 ** ## GNP:Population 1 0.870 22 3 10 9.4e-05 *** ## Residuals 12 ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 Multiple comparisons TukeyHSD and plot do work with a MANOVA fit. Run each dependent variable separately to obtain them\u2026 or proceed with ANOVA on each dependent variable. Going further \u00b6 Package lme4 has excellent facilities for fitting linear and generalized linear mixed-effects models. (M)ANOVA Assumptions \u00b6 Outliers \u00b6 Outliers can severely affect normality and homogeneity of variance. 1 2 # dataset head ( mtcars , 3 ) 1 2 3 4 ## mpg cyl disp hp drat wt qsec vs am gear carb ## Mazda RX4 21.0 6 160 110 3.90 2.62 16.5 0 1 4 4 ## Mazda RX4 Wag 21.0 6 160 110 3.90 2.88 17.0 0 1 4 4 ## Datsun 710 22.8 4 108 93 3.85 2.32 18.6 1 1 4 1 1 2 3 4 5 6 # detect outliers # ordered squared robust Mahalanobis distances of the observations against the empirical distribution function of the MD2i library ( mvoutlier ) outliers <- aq.plot ( mtcars [c ( \"mpg\" , \"disp\" , \"hp\" , \"drat\" , \"wt\" , \"qsec\" ) ] ) 1 2 ## Projection to the first and second robust principal components. ## Proportion of total variation (explained variance): 0.974 1 outliers # show list of outliers 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 ## $outliers ## Mazda RX4 Mazda RX4 Wag Datsun 710 ## FALSE FALSE FALSE ## Hornet 4 Drive Hornet Sportabout Valiant ## FALSE FALSE FALSE ## Duster 360 Merc 240D Merc 230 ## TRUE TRUE TRUE ## Merc 280 Merc 280C Merc 450SE ## FALSE FALSE FALSE ## Merc 450SL Merc 450SLC Cadillac Fleetwood ## FALSE FALSE FALSE ## Lincoln Continental Chrysler Imperial Fiat 128 ## FALSE FALSE TRUE ## Honda Civic Toyota Corolla Toyota Corona ## FALSE FALSE FALSE ## Dodge Challenger AMC Javelin Camaro Z28 ## FALSE FALSE TRUE ## Pontiac Firebird Fiat X1-9 Porsche 914-2 ## FALSE FALSE FALSE ## Lotus Europa Ford Pantera L Ferrari Dino ## FALSE TRUE FALSE ## Maserati Bora Volvo 142E ## TRUE FALSE 1 par ( mfrow = c ( 1 , 1 )) Univariate normality \u00b6 1 2 3 # Q-Q plot qqnorm ( mtcars $ mpg ) qqline ( mtcars $ mpg ) 1 2 # Shapiro-Wilk test of normality shapiro.test ( mtcars $ mpg ) 1 2 3 4 5 ## ## Shapiro-Wilk normality test ## ## data: mtcars$mpg ## W = 0.9, p-value = 0.1 1 2 3 4 library ( nortest ) # Anderson-Darling test for normality ad.test ( mtcars $ mpg ) 1 2 3 4 5 ## ## Anderson-Darling normality test ## ## data: mtcars$mpg ## A = 0.6, p-value = 0.1 1 2 # Cramer-von Mises test for normality cvm.test ( mtcars $ mpg ) 1 2 3 4 5 ## ## Cramer-von Mises normality test ## ## data: mtcars$mpg ## W = 0.09, p-value = 0.2 1 2 # Lilliefors (Kolmogorov-Smirnov) test for normality lillie.test ( mtcars $ mpg ) 1 2 3 4 5 ## ## Lilliefors (Kolmogorov-Smirnov) normality test ## ## data: mtcars$mpg ## D = 0.1, p-value = 0.2 1 2 # Pearson chi-square test for normality pearson.test ( mtcars $ mpg ) 1 2 3 4 5 ## ## Pearson chi-square normality test ## ## data: mtcars$mpg ## P = 8, p-value = 0.2 1 2 # Shapiro-Francia test for normality sf.test ( mtcars $ mpg ) 1 2 3 4 5 ## ## Shapiro-Francia normality test ## ## data: mtcars$mpg ## W = 1, p-value = 0.1 Multivariate normality \u00b6 1 head ( EuStockMarkets , 3 ) 1 2 3 4 ## DAX SMI CAC FTSE ## [1,] 1629 1678 1773 2444 ## [2,] 1614 1688 1750 2460 ## [3,] 1607 1679 1718 2448 1 2 3 4 library ( mvnormtest ) # Shapiro-Wilk test for multivariate normality of numeric matrix mshapiro.test ( t ( EuStockMarkets )) 1 2 3 4 5 ## ## Shapiro-Wilk normality test ## ## data: Z ## W = 0.9, p-value &lt;2e-16 With a p x 1 multivariate normal random vector x vector, the squared Mahalanobis distance between x and ?? is going to be chi-square distributed with p degrees of freedom. 1 2 3 4 5 6 7 8 9 # graphical assessment of multivariate normality x <- as.matrix ( EuStockMarkets ) # n x p numeric matrix center <- colMeans ( x ) # centroid n <- nrow ( x ); p <- ncol ( x ) cov <- cov ( x ) d <- mahalanobis ( x , center , cov ) # distances qqplot ( qchisq ( ppoints ( n ), df = p ), d , main = \"QQ Plot Assessing Multivariate Normality\" , ylab = \"Mahalanobis D2\" ) abline ( a = 0 , b = 1 ) Heteroscedasticity \u00b6 Nonconstant error variance or non-homogeneity of variances. 1 2 # dataset head ( mtcars , 3 ) 1 2 3 4 ## mpg cyl disp hp drat wt qsec vs am gear carb ## Mazda RX4 21.0 6 160 110 3.90 2.62 16.5 0 1 4 4 ## Mazda RX4 Wag 21.0 6 160 110 3.90 2.88 17.0 0 1 4 4 ## Datsun 710 22.8 4 108 93 3.85 2.32 18.6 1 1 4 1 1 2 3 # y is a numeric variable and G is the grouping variable # Bartlett parametric test of homogeneity of variances bartlett.test ( mpg ~ cyl , data = mtcars ) 1 2 3 4 5 ## ## Bartlett test of homogeneity of variances ## ## data: mpg by cyl ## Bartlett's K-squared = 8, df = 2, p-value = 0.02 1 2 # Figner-Killeen non-parametric test of homogeneity of variances fligner.test ( mpg ~ cyl , data = mtcars ) 1 2 3 4 5 ## ## Fligner-Killeen test of homogeneity of variances ## ## data: mpg by cyl ## Fligner-Killeen:med chi-squared = 7, df = 2, p-value = 0.03 1 2 3 4 5 library ( HH ) # y is numeric and G is a grouping factor # G must be of type factor hov ( mpg ~ factor ( cyl ), data = mtcars ) 1 2 3 4 5 6 ## ## hov: Brown-Forsyth ## ## data: mpg ## F = 6, df:factor(cyl) = 2, df:Residuals = 30, p-value = 0.009 ## alternative hypothesis: variances are not identical 1 2 3 # homogeneity of variance plot # graphic test of homogeneity of variances based on Brown-Forsyth hovPlot ( mpg ~ factor ( cyl ), data = mtcars ) Non-homogeneity of covariance matrices 1 2 # dataset head ( iris , 3 ) 1 2 3 4 ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3.0 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa 1 library ( biotools ) 1 2 ## --- ## biotools version 3.0 1 2 3 # Box's M test # very sensitive to violations of normality, leading to rejection in most typical cases boxM ( iris[ , -5 ] , iris[ , 5 ] ) 1 2 3 4 5 ## ## Box's M-test for Homogeneity of Covariance Matrices ## ## data: iris[, -5] ## Chi-Sq (approx.) = 100, df = 20, p-value &lt;2e-16 Resampling Statistics \u00b6 Independent k-sample location tests \u00b6 1 2 # dataset head ( mtcars , 3 ) 1 2 3 4 ## mpg cyl disp hp drat wt qsec vs am gear carb ## Mazda RX4 21.0 6 160 110 3.90 2.62 16.5 0 1 4 4 ## Mazda RX4 Wag 21.0 6 160 110 3.90 2.88 17.0 0 1 4 4 ## Datsun 710 22.8 4 108 93 3.85 2.32 18.6 1 1 4 1 1 2 3 4 5 6 # exact Wilcoxon Mann Whitney rank sum test # re-randomization or permutation based statistical tests # where y is numeric and A is a binary factor library ( coin ) wilcox_test ( mpg ~ factor ( am ), data = mtcars , distribution = \"exact\" ) 1 2 3 4 5 6 ## ## Exact Wilcoxon-Mann-Whitney Test ## ## data: mpg by factor(am) (0, 1) ## Z = -3, p-value = 0.001 ## alternative hypothesis: true mu is not equal to 0 1 # lower case letters represent numerical variables and upper case letters represent categorical factors Monte-Carlo simulations are available for all tests. Exact tests are available for 2 group procedures. These tests do not assume random sampling from well-defined populations. They can be a reasonable alternative to classical procedures when test assumptions can not be met. 1 2 3 4 5 6 # one-way permutation test based on 9999 Monte-Carlo # resamplings # y is numeric and A is a categorical factor library ( coin ) oneway_test ( mpg ~ factor ( am ), data = mtcars , distribution = approximate ( B = 9999 )) 1 2 3 4 5 6 ## ## Approximative Two-Sample Fisher-Pitman Permutation Test ## ## data: mpg by factor(am) (0, 1) ## Z = -3, p-value = 5e-04 ## alternative hypothesis: true mu is not equal to 0 Symmetry of a response for repeated measurements \u00b6 1 2 3 4 5 # exact Wilcoxon signed rank test # where y1 and y2 are repeated measures library ( coin ) wilcoxsign_test ( mpg ~ factor ( am ), data = mtcars , distribution = \"exact\" ) 1 2 3 4 5 6 7 ## ## Exact Wilcoxon-Pratt Signed-Rank Test ## ## data: y by x (pos, neg) ## stratified by block ## Z = 5, p-value = 5e-10 ## alternative hypothesis: true mu is not equal to 0 1 2 3 4 # Freidman Test based on 9999 Monte-Carlo resamplings. # y is numeric, A is a grouping factor, and B is a # blocking factor friedman_test ( y ~ A | B , data = mydata , distribution = approximate ( B = 9999 )) Independence of two numeric variables \u00b6 1 2 3 4 5 # Spearman Test of independence based on 9999 Monte-Carlo # resamplings. x and y are numeric variables library ( coin ) spearman_test ( mpg ~ am , data = mtcars , distribution = approximate ( B = 9999 )) 1 2 3 4 5 6 ## ## Approximative Spearman Correlation Test ## ## data: mpg by am ## Z = 3, p-value = 0.001 ## alternative hypothesis: true rho is not equal to 0 Independence in contingency tables \u00b6 1 2 # dataset head ( CO2 ) 1 2 3 4 5 6 7 8 ## Grouped Data: uptake ~ conc | Plant ## Plant Type Treatment conc uptake ## 1 Qn1 Quebec nonchilled 95 16.0 ## 2 Qn1 Quebec nonchilled 175 30.4 ## 3 Qn1 Quebec nonchilled 250 34.8 ## 4 Qn1 Quebec nonchilled 350 37.2 ## 5 Qn1 Quebec nonchilled 500 35.3 ## 6 Qn1 Quebec nonchilled 675 39.2 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 cases <- c ( 4 , 2 , 3 , 1 , 59 ) n <- sum ( cases ) cochran <- data.frame ( diphtheria = factor ( unlist ( rep ( list ( c ( 1 , 1 , 1 , 1 ), c ( 1 , 1 , 0 , 1 ), c ( 0 , 1 , 1 , 1 ), c ( 0 , 1 , 0 , 1 ), c ( 0 , 0 , 0 , 0 )), cases )) ), media = factor ( rep ( LETTERS [1 : 4 ] , n )), case = factor ( rep ( seq_len ( n ), each = 4 )) ) head ( cochran ) 1 2 3 4 5 6 7 ## diphtheria media case ## 1 1 A 1 ## 2 1 B 1 ## 3 1 C 1 ## 4 1 D 1 ## 5 1 A 2 ## 6 1 B 2 1 2 3 4 5 # independence in 2-way contingency table based on # 9999 Monte-Carlo resamplings. A and B are factors library ( coin ) chisq_test ( Plant ~ Type , data = CO2 , distribution = approximate ( B = 9999 )) 1 2 3 4 5 6 ## ## Approximative Linear-by-Linear Association Test ## ## data: Plant (ordered) by Type (Quebec, Mississippi) ## Z = -8, p-value &lt;2e-16 ## alternative hypothesis: two.sided 1 2 3 4 # Cochran-Mantel-Haenzsel Test of 3-way Contingency Table # based on 9999 Monte-Carlo resamplings. A, B, are factors # and C is a stratefying factor mh_test ( diphtheria ~ media | case , data = cochran , distribution = approximate ( B = 9999 )) 1 2 3 4 5 6 7 ## ## Approximative Marginal Homogeneity Test ## ## data: diphtheria by ## media (A, B, C, D) ## stratified by case ## chi-squared = 8, p-value = 0.05 1 2 3 4 5 6 # linear by linear association test based on 9999 # Monte-Carlo resamplings # A and B are ordered factors library ( coin ) lbl_test ( Plant ~ Type , data = CO2 , distribution = approximate ( B = 9999 )) 1 2 3 4 5 6 ## ## Approximative Linear-by-Linear Association Test ## ## data: Plant (ordered) by Type (Quebec, Mississippi) ## Z = -8, p-value &lt;2e-16 ## alternative hypothesis: two.sided Many other univariate and multivariate tests are possible using the functions in the coin package.","title":"Statistics with R, Notes"},{"location":"a_hands-on_introduction_to_statistics_with_r_notes/#descriptive-statistics","text":"Extract basic, exploratory statistics from datasets with apply , summary , fivenum , describe , and stat.desc . 1 2 # dataset head ( longley , 3 ) 1 2 3 4 ## GNP.deflator GNP Unemployed Armed.Forces Population Year Employed ## 1947 83.0 234.289 235.6 159.0 107.608 1947 60.323 ## 1948 88.5 259.426 232.5 145.6 108.632 1948 61.122 ## 1949 88.2 258.054 368.2 161.6 109.773 1949 60.171 1 2 3 # apply a function # excluding missing values sapply ( longley , mean , na.rm = TRUE ) 1 2 3 4 ## GNP.deflator GNP Unemployed Armed.Forces Population ## 101.6813 387.6984 319.3313 260.6687 117.4240 ## Year Employed ## 1954.5000 65.3170 1 2 # mean, median, 25th and 75th quartiles, min, max summary ( longley ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 ## GNP.deflator GNP Unemployed Armed.Forces ## Min. : 83.00 Min. :234.3 Min. :187.0 Min. :145.6 ## 1st Qu.: 94.53 1st Qu.:317.9 1st Qu.:234.8 1st Qu.:229.8 ## Median :100.60 Median :381.4 Median :314.4 Median :271.8 ## Mean :101.68 Mean :387.7 Mean :319.3 Mean :260.7 ## 3rd Qu.:111.25 3rd Qu.:454.1 3rd Qu.:384.2 3rd Qu.:306.1 ## Max. :116.90 Max. :554.9 Max. :480.6 Max. :359.4 ## Population Year Employed ## Min. :107.6 Min. :1947 Min. :60.17 ## 1st Qu.:111.8 1st Qu.:1951 1st Qu.:62.71 ## Median :116.8 Median :1954 Median :65.50 ## Mean :117.4 Mean :1954 Mean :65.32 ## 3rd Qu.:122.3 3rd Qu.:1958 3rd Qu.:68.29 ## Max. :130.1 Max. :1962 Max. :70.55 1 2 # Tukey min, lower-hinge, median, upper-hinge, max fivenum ( longley $ GNP ) 1 ## [1] 234.289 306.787 381.427 463.625 554.894 1 2 3 4 5 # n, nmiss, unique, mean, 5, 10, 25, 50, 75, 90, 95th percentiles # 5 lowest and 5 highest scores library ( Hmisc ) describe ( longley ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 ## longley ## ## 7 Variables 16 Observations ## --------------------------------------------------------------------------- ## GNP.deflator ## n missing distinct Info Mean Gmd .05 .10 ## 16 0 16 1 101.7 12.74 86.90 88.35 ## .25 .50 .75 .90 .95 ## 94.53 100.60 111.25 114.95 116.00 ## ## Value 83.0 88.2 88.5 89.5 96.2 98.1 99.0 100.0 101.2 104.6 ## Frequency 1 1 1 1 1 1 1 1 1 1 ## Proportion 0.062 0.062 0.062 0.062 0.062 0.062 0.062 0.062 0.062 0.062 ## ## Value 108.4 110.8 112.6 114.2 115.7 116.9 ## Frequency 1 1 1 1 1 1 ## Proportion 0.062 0.062 0.062 0.062 0.062 0.062 ## --------------------------------------------------------------------------- ## GNP ## n missing distinct Info Mean Gmd .05 .10 ## 16 0 16 1 387.7 117.8 252.1 258.7 ## .25 .50 .75 .90 .95 ## 317.9 381.4 454.1 510.4 527.4 ## ## Value 234.289 258.054 259.426 284.599 328.975 346.999 363.112 365.385 ## Frequency 1 1 1 1 1 1 1 1 ## Proportion 0.062 0.062 0.062 0.062 0.062 0.062 0.062 0.062 ## ## Value 397.469 419.180 442.769 444.546 482.704 502.601 518.173 554.894 ## Frequency 1 1 1 1 1 1 1 1 ## Proportion 0.062 0.062 0.062 0.062 0.062 0.062 0.062 0.062 ## --------------------------------------------------------------------------- ## Unemployed ## n missing distinct Info Mean Gmd .05 .10 ## 16 0 16 1 319.3 110.1 191.6 201.6 ## .25 .50 .75 .90 .95 ## 234.8 314.4 384.2 434.4 471.2 ## ## Value 187.0 193.2 209.9 232.5 235.6 282.2 290.4 293.6 335.1 357.8 ## Frequency 1 1 1 1 1 1 1 1 1 1 ## Proportion 0.062 0.062 0.062 0.062 0.062 0.062 0.062 0.062 0.062 0.062 ## ## Value 368.2 381.3 393.1 400.7 468.1 480.6 ## Frequency 1 1 1 1 1 1 ## Proportion 0.062 0.062 0.062 0.062 0.062 0.062 ## --------------------------------------------------------------------------- ## Armed.Forces ## n missing distinct Info Mean Gmd .05 .10 ## 16 0 16 1 260.7 79.85 155.7 160.3 ## .25 .50 .75 .90 .95 ## 229.8 271.8 306.1 344.9 355.9 ## ## Value 145.6 159.0 161.6 165.0 251.4 255.2 257.2 263.7 279.8 282.7 ## Frequency 1 1 1 1 1 1 1 1 1 1 ## Proportion 0.062 0.062 0.062 0.062 0.062 0.062 0.062 0.062 0.062 0.062 ## ## Value 285.7 304.8 309.9 335.0 354.7 359.4 ## Frequency 1 1 1 1 1 1 ## Proportion 0.062 0.062 0.062 0.062 0.062 0.062 ## --------------------------------------------------------------------------- ## Population ## n missing distinct Info Mean Gmd .05 .10 ## 16 0 16 1 117.4 8.229 108.4 109.2 ## .25 .50 .75 .90 .95 ## 111.8 116.8 122.3 126.6 128.4 ## ## Value 107.608 108.632 109.773 110.929 112.075 113.270 115.094 116.219 ## Frequency 1 1 1 1 1 1 1 1 ## Proportion 0.062 0.062 0.062 0.062 0.062 0.062 0.062 0.062 ## ## Value 117.388 118.734 120.445 121.950 123.366 125.368 127.852 130.081 ## Frequency 1 1 1 1 1 1 1 1 ## Proportion 0.062 0.062 0.062 0.062 0.062 0.062 0.062 0.062 ## --------------------------------------------------------------------------- ## Year ## n missing distinct Info Mean Gmd .05 .10 ## 16 0 16 1 1954 5.667 1948 1948 ## .25 .50 .75 .90 .95 ## 1951 1954 1958 1960 1961 ## ## Value 1947 1948 1949 1950 1951 1952 1953 1954 1955 1956 ## Frequency 1 1 1 1 1 1 1 1 1 1 ## Proportion 0.062 0.062 0.062 0.062 0.062 0.062 0.062 0.062 0.062 0.062 ## ## Value 1957 1958 1959 1960 1961 1962 ## Frequency 1 1 1 1 1 1 ## Proportion 0.062 0.062 0.062 0.062 0.062 0.062 ## --------------------------------------------------------------------------- ## Employed ## n missing distinct Info Mean Gmd .05 .10 ## 16 0 16 1 65.32 4.153 60.28 60.72 ## .25 .50 .75 .90 .95 ## 62.71 65.50 68.29 69.45 69.81 ## ## Value 60.171 60.323 61.122 61.187 63.221 63.639 63.761 64.989 66.019 ## Frequency 1 1 1 1 1 1 1 1 1 ## Proportion 0.062 0.062 0.062 0.062 0.062 0.062 0.062 0.062 0.062 ## ## Value 66.513 67.857 68.169 68.655 69.331 69.564 70.551 ## Frequency 1 1 1 1 1 1 1 ## Proportion 0.062 0.062 0.062 0.062 0.062 0.062 0.062 ## --------------------------------------------------------------------------- 1 2 3 4 5 # nbr.val, nbr.null, nbr.na, min max, range, sum, # median, mean, SE.mean, CI.mean, var, std.dev, coef.var library ( pastecs ) stat.desc ( longley ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 ## GNP.deflator GNP Unemployed Armed.Forces ## nbr.val 16.0000000 16.0000000 16.0000000 16.0000000 ## nbr.null 0.0000000 0.0000000 0.0000000 0.0000000 ## nbr.na 0.0000000 0.0000000 0.0000000 0.0000000 ## min 83.0000000 234.2890000 187.0000000 145.6000000 ## max 116.9000000 554.8940000 480.6000000 359.4000000 ## range 33.9000000 320.6050000 293.6000000 213.8000000 ## sum 1626.9000000 6203.1750000 5109.3000000 4170.7000000 ## median 100.6000000 381.4270000 314.3500000 271.7500000 ## mean 101.6812500 387.6984375 319.3312500 260.6687500 ## SE.mean 2.6978884 24.8487344 23.3616062 17.3979901 ## CI.mean.0.95 5.7504129 52.9638237 49.7940849 37.0829381 ## var 116.4576250 9879.3536593 8732.2342917 4843.0409583 ## std.dev 10.7915534 99.3949378 93.4464247 69.5919604 ## coef.var 0.1061312 0.2563718 0.2926316 0.2669747 ## Population Year Employed ## nbr.val 1.600000e+01 1.600000e+01 1.600000e+01 ## nbr.null 0.000000e+00 0.000000e+00 0.000000e+00 ## nbr.na 0.000000e+00 0.000000e+00 0.000000e+00 ## min 1.076080e+02 1.947000e+03 6.017100e+01 ## max 1.300810e+02 1.962000e+03 7.055100e+01 ## range 2.247300e+01 1.500000e+01 1.038000e+01 ## sum 1.878784e+03 3.127200e+04 1.045072e+03 ## median 1.168035e+02 1.954500e+03 6.550400e+01 ## mean 1.174240e+02 1.954500e+03 6.531700e+01 ## SE.mean 1.739025e+00 1.190238e+00 8.779921e-01 ## CI.mean.0.95 3.706645e+00 2.536932e+00 1.871396e+00 ## var 4.838735e+01 2.266667e+01 1.233392e+01 ## std.dev 6.956102e+00 4.760952e+00 3.511968e+00 ## coef.var 5.923918e-02 2.435893e-03 5.376806e-02 1 2 3 4 5 # item name, item number, nvalid, mean, sd, # median, mad, min, max, skew, kurtosis, se library ( psych ) describe ( longley ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 ## vars n mean sd median trimmed mad min max ## GNP.deflator 1 16 101.68 10.79 100.60 101.93 15.79 83.00 116.90 ## GNP 2 16 387.70 99.39 381.43 386.71 118.57 234.29 554.89 ## Unemployed 3 16 319.33 93.45 314.35 317.26 116.75 187.00 480.60 ## Armed.Forces 4 16 260.67 69.59 271.75 261.84 52.78 145.60 359.40 ## Population 5 16 117.42 6.96 116.80 117.22 8.17 107.61 130.08 ## Year 6 16 1954.50 4.76 1954.50 1954.50 5.93 1947.00 1962.00 ## Employed 7 16 65.32 3.51 65.50 65.31 4.31 60.17 70.55 ## range skew kurtosis se ## GNP.deflator 33.90 -0.13 -1.40 2.70 ## GNP 320.61 0.02 -1.35 24.85 ## Unemployed 293.60 0.14 -1.30 23.36 ## Armed.Forces 213.80 -0.37 -1.20 17.40 ## Population 22.47 0.26 -1.27 1.74 ## Year 15.00 0.00 -1.43 1.19 ## Employed 10.38 -0.09 -1.55 0.88 1 2 # with with ( longley , median ( GNP )) 1 ## [1] 381.427 1 2 # vs. median ( longley $ GNP ) 1 ## [1] 381.427","title":"Descriptive Statistics"},{"location":"a_hands-on_introduction_to_statistics_with_r_notes/#group-data","text":"1 2 # dataset head ( mtcars , 3 ) 1 2 3 4 ## mpg cyl disp hp drat wt qsec vs am gear carb ## Mazda RX4 21.0 6 160 110 3.90 2.620 16.46 0 1 4 4 ## Mazda RX4 Wag 21.0 6 160 110 3.90 2.875 17.02 0 1 4 4 ## Datsun 710 22.8 4 108 93 3.85 2.320 18.61 1 1 4 1 1 2 # variable grouped by one factor to show a function by ( mtcars $ mpg , mtcars $ cyl , FUN = function ( x ) { c ( m = mean ( x ), s = sd ( x )) }) 1 2 3 4 5 6 7 8 9 10 11 ## mtcars$cyl: 4 ## m s ## 26.663636 4.509828 ## -------------------------------------------------------- ## mtcars$cyl: 6 ## m s ## 19.742857 1.453567 ## -------------------------------------------------------- ## mtcars$cyl: 8 ## m s ## 15.100000 2.560048 1 2 3 4 # description statistics by group library ( psych ) describeBy ( mtcars , group = mtcars $ am ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 ## ## Descriptive statistics by group ## group: 0 ## vars n mean sd median trimmed mad min max range ## mpg 1 19 17.15 3.83 17.30 17.12 3.11 10.40 24.40 14.00 ## cyl 2 19 6.95 1.54 8.00 7.06 0.00 4.00 8.00 4.00 ## disp 3 19 290.38 110.17 275.80 289.71 124.83 120.10 472.00 351.90 ## hp 4 19 160.26 53.91 175.00 161.06 77.10 62.00 245.00 183.00 ## drat 5 19 3.29 0.39 3.15 3.28 0.22 2.76 3.92 1.16 ## wt 6 19 3.77 0.78 3.52 3.75 0.45 2.46 5.42 2.96 ## qsec 7 19 18.18 1.75 17.82 18.07 1.19 15.41 22.90 7.49 ## vs 8 19 0.37 0.50 0.00 0.35 0.00 0.00 1.00 1.00 ## am 9 19 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 ## gear 10 19 3.21 0.42 3.00 3.18 0.00 3.00 4.00 1.00 ## carb 11 19 2.74 1.15 3.00 2.76 1.48 1.00 4.00 3.00 ## skew kurtosis se ## mpg 0.01 -0.80 0.88 ## cyl -0.95 -0.74 0.35 ## disp 0.05 -1.26 25.28 ## hp -0.01 -1.21 12.37 ## drat 0.50 -1.30 0.09 ## wt 0.98 0.14 0.18 ## qsec 0.85 0.55 0.40 ## vs 0.50 -1.84 0.11 ## am NaN NaN 0.00 ## gear 1.31 -0.29 0.10 ## carb -0.14 -1.57 0.26 ## -------------------------------------------------------- ## group: 1 ## vars n mean sd median trimmed mad min max range skew ## mpg 1 13 24.39 6.17 22.80 24.38 6.67 15.00 33.90 18.90 0.05 ## cyl 2 13 5.08 1.55 4.00 4.91 0.00 4.00 8.00 4.00 0.87 ## disp 3 13 143.53 87.20 120.30 131.25 58.86 71.10 351.00 279.90 1.33 ## hp 4 13 126.85 84.06 109.00 114.73 63.75 52.00 335.00 283.00 1.36 ## drat 5 13 4.05 0.36 4.08 4.02 0.27 3.54 4.93 1.39 0.79 ## wt 6 13 2.41 0.62 2.32 2.39 0.68 1.51 3.57 2.06 0.21 ## qsec 7 13 17.36 1.79 17.02 17.39 2.34 14.50 19.90 5.40 -0.23 ## vs 8 13 0.54 0.52 1.00 0.55 0.00 0.00 1.00 1.00 -0.14 ## am 9 13 1.00 0.00 1.00 1.00 0.00 1.00 1.00 0.00 NaN ## gear 10 13 4.38 0.51 4.00 4.36 0.00 4.00 5.00 1.00 0.42 ## carb 11 13 2.92 2.18 2.00 2.64 1.48 1.00 8.00 7.00 0.98 ## kurtosis se ## mpg -1.46 1.71 ## cyl -0.90 0.43 ## disp 0.40 24.19 ## hp 0.56 23.31 ## drat 0.21 0.10 ## wt -1.17 0.17 ## qsec -1.42 0.50 ## vs -2.13 0.14 ## am NaN 0.00 ## gear -1.96 0.14 ## carb -0.21 0.60 1 2 3 4 5 # description statistics by group library ( doBy ) summaryBy ( mpg + wt ~ cyl + vs , data = mtcars , FUN = function ( x ) { c ( m = mean ( x ), s = sd ( x )) } ) 1 2 3 4 5 6 ## cyl vs mpg.m mpg.s wt.m wt.s ## 1 4 0 26.00000 NA 2.140000 NA ## 2 4 1 26.73000 4.7481107 2.300300 0.5982073 ## 3 6 0 20.56667 0.7505553 2.755000 0.1281601 ## 4 6 1 19.12500 1.6317169 3.388750 0.1162164 ## 5 8 0 15.10000 2.5600481 3.999214 0.7594047","title":"Group data"},{"location":"a_hands-on_introduction_to_statistics_with_r_notes/#frequency-tables-crosstables-and-independence","text":"Create frequency and contingency tables from categorical variables. Perform tests of independence, measures of association, and graphically display results.","title":"Frequency Tables, CrossTables, and Independence"},{"location":"a_hands-on_introduction_to_statistics_with_r_notes/#2d-frequency-tables","text":"mytable <- table ( A , B ) where A are rows, B are columns. 1 2 3 # dataset mytable <- matrix ( c ( 1 , 2 , 3 , 4 ), nrow = 2 ) mytable 1 2 3 ## [,1] [,2] ## [1,] 1 3 ## [2,] 2 4 1 2 # A frequencies (summed over columns = 1) margin.table ( mytable , 1 ) 1 ## [1] 4 6 1 2 # B frequencies (summed over rows = 2) margin.table ( mytable , 2 ) 1 ## [1] 3 7 1 2 3 # A/(A + B) # cell percentages prop.table ( mytable ) 1 2 3 ## [,1] [,2] ## [1,] 0.1 0.3 ## [2,] 0.2 0.4 1 2 # row percentages prop.table ( mytable , 1 ) 1 2 3 ## [,1] [,2] ## [1,] 0.2500000 0.7500000 ## [2,] 0.3333333 0.6666667 1 2 # column percentages prop.table ( mytable , 2 ) 1 2 3 ## [,1] [,2] ## [1,] 0.3333333 0.4285714 ## [2,] 0.6666667 0.5714286","title":"2D frequency tables"},{"location":"a_hands-on_introduction_to_statistics_with_r_notes/#3d-frequency-tables","text":"1 2 # dataset head ( CO2 , 3 ) 1 2 3 4 5 ## Grouped Data: uptake ~ conc | Plant ## Plant Type Treatment conc uptake ## 1 Qn1 Quebec nonchilled 95 16.0 ## 2 Qn1 Quebec nonchilled 175 30.4 ## 3 Qn1 Quebec nonchilled 250 34.8 1 2 3 4 A <- as.numeric ( CO2[ , 'Plant' ] ) B <- CO2[ , 'conc' ] C <- CO2[ , 'uptake' ] mytable <- table ( A , B , C ) 1 2 # several arrays (3D) dim ( mytable ) # the printout is immense 1 ## [1] 12 7 76 1 2 # folded table (2D) dim ( ftable ( mytable )) # the printout is immense 1 ## [1] 84 76 1 2 3 4 # 3-Way frequency Table mytable <- xtabs ( ~ A + B + C , data = mytable , na.action = na.omit ) dim ( ftable ( mytable )) # the printout is immense 1 ## [1] 84 76","title":"3D frequency tables"},{"location":"a_hands-on_introduction_to_statistics_with_r_notes/#crosstable","text":"1 2 A <- as.numeric ( CO2[1 : 8 , 'Plant' ] ) A 1 ## [1] 1 1 1 1 1 1 1 2 1 2 B <- CO2[1 : 8 , 'conc' ] B 1 ## [1] 95 175 250 350 500 675 1000 95 1 2 3 4 # 2-way cross tabulation library ( gmodels ) CrossTable ( A , B ) # mydata$myrowvar x mydata$mycolvar 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 ## ## ## Cell Contents ## |-------------------------| ## | N | ## | Chi-square contribution | ## | N / Row Total | ## | N / Col Total | ## | N / Table Total | ## |-------------------------| ## ## ## Total Observations in Table: 8 ## ## ## | B ## A | 95 | 175 | 250 | 350 | 500 | 675 | 1000 | Row Total | ## -------------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------| ## 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 7 | ## | 0.321 | 0.018 | 0.018 | 0.018 | 0.018 | 0.018 | 0.018 | | ## | 0.143 | 0.143 | 0.143 | 0.143 | 0.143 | 0.143 | 0.143 | 0.875 | ## | 0.500 | 1.000 | 1.000 | 1.000 | 1.000 | 1.000 | 1.000 | | ## | 0.125 | 0.125 | 0.125 | 0.125 | 0.125 | 0.125 | 0.125 | | ## -------------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------| ## 2 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | ## | 2.250 | 0.125 | 0.125 | 0.125 | 0.125 | 0.125 | 0.125 | | ## | 1.000 | 0.000 | 0.000 | 0.000 | 0.000 | 0.000 | 0.000 | 0.125 | ## | 0.500 | 0.000 | 0.000 | 0.000 | 0.000 | 0.000 | 0.000 | | ## | 0.125 | 0.000 | 0.000 | 0.000 | 0.000 | 0.000 | 0.000 | | ## -------------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------| ## Column Total | 2 | 1 | 1 | 1 | 1 | 1 | 1 | 8 | ## | 0.250 | 0.125 | 0.125 | 0.125 | 0.125 | 0.125 | 0.125 | | ## -------------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------| ## ##","title":"CrossTable"},{"location":"a_hands-on_introduction_to_statistics_with_r_notes/#tests-of-independence","text":"There are more tests of independence in section 10, Resampling Statistics. 1 2 # dataset, a contingency table colors 1 2 3 4 5 ## fair.hair red.hair medium.hair dark.hair black.hair ## blue eyes 326 38 241 110 3 ## light eyes 688 116 584 188 4 ## medium eyes 343 84 909 412 26 ## dark eyes 98 48 403 681 85 1 2 3 # chi-square test on 2-way tables # test independence of the row and column variables, p-value is calculated from the asymptotic chi-squared distribution of the test statistic chisq.test ( colors ) 1 2 3 4 5 ## ## Pearson's Chi-squared test ## ## data: colors ## X-squared = 1240, df = 12, p-value &lt; 2.2e-16 1 2 3 4 5 6 7 # dataset, a contingency table in 2x2 matrix form TeaTasting <- matrix ( c ( 3 , 1 , 1 , 3 ), nrow = 2 , dimnames = list ( Guess = c ( \"Milk\" , \"Tea\" ), Truth = c ( \"Milk\" , \"Tea\" ))) TeaTasting 1 2 3 4 ## Truth ## Guess Milk Tea ## Milk 3 1 ## Tea 1 3 1 2 # Fisher exact test fisher.test ( TeaTasting , alternative = \"greater\" ) 1 2 3 4 5 6 7 8 9 10 11 ## ## Fisher's Exact Test for Count Data ## ## data: TeaTasting ## p-value = 0.2429 ## alternative hypothesis: true odds ratio is greater than 1 ## 95 percent confidence interval: ## 0.3135693 Inf ## sample estimates: ## odds ratio ## 6.408309 1 2 3 4 5 6 7 8 9 10 11 12 13 # 3D contingency table, where the last dimension refers to the strata Rabbits <- array ( c ( 0 , 0 , 6 , 5 , 3 , 0 , 3 , 6 , 6 , 2 , 0 , 4 , 5 , 6 , 1 , 0 , 2 , 5 , 0 , 0 ), dim = c ( 2 , 2 , 5 ), dimnames = list ( Delay = c ( \"None\" , \"1.5h\" ), Response = c ( \"Cured\" , \"Died\" ), Penicillin.Level = c ( \"1/8\" , \"1/4\" , \"1/2\" , \"1\" , \"4\" ))) Rabbits 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 ## , , Penicillin.Level = 1/8 ## ## Response ## Delay Cured Died ## None 0 6 ## 1.5h 0 5 ## ## , , Penicillin.Level = 1/4 ## ## Response ## Delay Cured Died ## None 3 3 ## 1.5h 0 6 ## ## , , Penicillin.Level = 1/2 ## ## Response ## Delay Cured Died ## None 6 0 ## 1.5h 2 4 ## ## , , Penicillin.Level = 1 ## ## Response ## Delay Cured Died ## None 5 1 ## 1.5h 6 0 ## ## , , Penicillin.Level = 4 ## ## Response ## Delay Cured Died ## None 2 0 ## 1.5h 5 0 1 2 # Mantel-Haenszel test / Cochran-Mantel-Haenszel chi-squared test, hypothesis that two nominal variables are conditionally independent in each stratum, assuming that there is no three-way interaction. mantelhaen.test ( Rabbits ) 1 2 3 4 5 6 7 8 9 10 11 ## ## Mantel-Haenszel chi-squared test with continuity correction ## ## data: Rabbits ## Mantel-Haenszel X-squared = 3.9286, df = 1, p-value = 0.04747 ## alternative hypothesis: true common odds ratio is not equal to 1 ## 95 percent confidence interval: ## 1.026713 47.725133 ## sample estimates: ## common odds ratio ## 7 1 2 3 4 5 6 # dataset # 3-way contingency table based on variables A, B, and C A <- CO2[ , 'Plant' ] B <- CO2[ , 'conc' ] C <- CO2[ , 'uptake' ] mytable <- xtabs ( ~ A + B + C ) # a 3D array 1 2 3 4 5 # loglinear Models # mutual independence: A, B, and C are pairwise independent library ( MASS ) loglm ( ~ A + B + C , mytable ) 1 2 3 4 5 6 7 ## Call: ## loglm(formula = ~A + B + C, data = mytable) ## ## Statistics: ## X^2 df P(&gt; X^2) ## Likelihood Ratio 720.1035 6291 1.0000000 ## Pearson 6300.0000 6291 0.4656775 1 2 # conditional independence: A is independent of B, given C loglm ( ~ A + B + C + A * C + B * C , mytable ) 1 2 3 4 5 6 7 ## Call: ## loglm(formula = ~A + B + C + A * C + B * C, data = mytable) ## ## Statistics: ## X^2 df P(&gt; X^2) ## Likelihood Ratio 12.13685 5016 1 ## Pearson NaN 5016 NaN 1 2 # no three-way interaction loglm ( ~ A + B + C + A * B + A * C + B * C , mytable ) 1 2 3 4 5 6 7 ## Call: ## loglm(formula = ~A + B + C + A * B + A * C + B * C, data = mytable) ## ## Statistics: ## X^2 df P(&gt; X^2) ## Likelihood Ratio 1.038376 4950 1 ## Pearson NaN 4950 NaN","title":"Tests of independence"},{"location":"a_hands-on_introduction_to_statistics_with_r_notes/#measures-of-association","text":"Association between two nominal variables, giving a value between 0 and +1 (inclusive). It is based on Pearson\u2019s chi-squared statistic. 1 2 # Dataset str ( Arthritis ) 1 2 3 4 5 6 ## 'data.frame': 84 obs. of 5 variables: ## $ ID : int 57 46 77 17 36 23 75 39 33 55 ... ## $ Treatment: Factor w/ 2 levels \"Placebo\",\"Treated\": 2 2 2 2 2 2 2 2 2 2 ... ## $ Sex : Factor w/ 2 levels \"Female\",\"Male\": 2 2 2 2 2 2 2 2 2 2 ... ## $ Age : int 27 29 30 32 46 58 59 59 63 63 ... ## $ Improved : Ord.factor w/ 3 levels \"None\"&lt;\"Some\"&lt;..: 2 1 1 3 3 3 1 3 1 1 ... 1 2 tab <- xtabs ( ~ Improved + Treatment , data = Arthritis ) tab 1 2 3 4 5 ## Treatment ## Improved Placebo Treated ## None 29 13 ## Some 7 7 ## Marked 7 21 1 summary ( assocstats ( tab )) 1 2 3 4 5 6 7 8 9 10 11 12 13 ## ## Call: xtabs(formula = ~Improved + Treatment, data = Arthritis) ## Number of cases in table: 84 ## Number of factors: 2 ## Test for independence of all factors: ## Chisq = 13.055, df = 2, p-value = 0.001463 ## X^2 df P(&gt; X^2) ## Likelihood Ratio 13.530 2 0.0011536 ## Pearson 13.055 2 0.0014626 ## ## Phi-Coefficient : NA ## Contingency Coeff.: 0.367 ## Cramer's V : 0.394 1 2 3 4 # phi coefficient, contingency coefficient, and Cram\u00e9r's V for an 2D table library ( vcd ) assocstats ( tab ) 1 2 3 4 5 6 7 ## X^2 df P(&gt; X^2) ## Likelihood Ratio 13.530 2 0.0011536 ## Pearson 13.055 2 0.0014626 ## ## Phi-Coefficient : NA ## Contingency Coeff.: 0.367 ## Cramer's V : 0.394 1 2 # Dataset TeaTasting 1 2 3 4 ## Truth ## Guess Milk Tea ## Milk 3 1 ## Tea 1 3 1 2 3 4 # Cohen's kappa and weighted kappa for a confusion matrix library ( vcd ) kappa ( TeaTasting ) 1 ## [1] 2.333333","title":"Measures of association"},{"location":"a_hands-on_introduction_to_statistics_with_r_notes/#correlations","text":"1 2 # dataset head ( mtcars , 3 ) 1 2 3 4 ## mpg cyl disp hp drat wt qsec vs am gear carb ## Mazda RX4 21.0 6 160 110 3.90 2.620 16.46 0 1 4 4 ## Mazda RX4 Wag 21.0 6 160 110 3.90 2.875 17.02 0 1 4 4 ## Datsun 710 22.8 4 108 93 3.85 2.320 18.61 1 1 4 1 1 2 3 # correlations/covariances among numeric variables in # a data frame cor ( mtcars , use = \"complete.obs\" , method = \"kendall\" ) # method = \"pearson\", \"spearman\" or \"kendall\" 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 ## mpg cyl disp hp drat wt ## mpg 1.0000000 -0.7953134 -0.7681311 -0.7428125 0.46454879 -0.7278321 ## cyl -0.7953134 1.0000000 0.8144263 0.7851865 -0.55131785 0.7282611 ## disp -0.7681311 0.8144263 1.0000000 0.6659987 -0.49898277 0.7433824 ## hp -0.7428125 0.7851865 0.6659987 1.0000000 -0.38262689 0.6113081 ## drat 0.4645488 -0.5513178 -0.4989828 -0.3826269 1.00000000 -0.5471495 ## wt -0.7278321 0.7282611 0.7433824 0.6113081 -0.54714953 1.0000000 ## qsec 0.3153652 -0.4489698 -0.3008155 -0.4729061 0.03272155 -0.1419881 ## vs 0.5896790 -0.7710007 -0.6033059 -0.6305926 0.37510111 -0.4884787 ## am 0.4690128 -0.4946212 -0.5202739 -0.3039956 0.57554849 -0.6138790 ## gear 0.4331509 -0.5125435 -0.4759795 -0.2794458 0.58392476 -0.5435956 ## carb -0.5043945 0.4654299 0.4137360 0.5959842 -0.09535193 0.3713741 ## qsec vs am gear carb ## mpg 0.31536522 0.5896790 0.46901280 0.43315089 -0.50439455 ## cyl -0.44896982 -0.7710007 -0.49462115 -0.51254349 0.46542994 ## disp -0.30081549 -0.6033059 -0.52027392 -0.47597955 0.41373600 ## hp -0.47290613 -0.6305926 -0.30399557 -0.27944584 0.59598416 ## drat 0.03272155 0.3751011 0.57554849 0.58392476 -0.09535193 ## wt -0.14198812 -0.4884787 -0.61387896 -0.54359562 0.37137413 ## qsec 1.00000000 0.6575431 -0.16890405 -0.09126069 -0.50643945 ## vs 0.65754312 1.0000000 0.16834512 0.26974788 -0.57692729 ## am -0.16890405 0.1683451 1.00000000 0.77078758 -0.05859929 ## gear -0.09126069 0.2697479 0.77078758 1.00000000 0.09801487 ## carb -0.50643945 -0.5769273 -0.05859929 0.09801487 1.00000000 1 cov ( mtcars , use = \"complete.obs\" ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 ## mpg cyl disp hp drat ## mpg 36.324103 -9.1723790 -633.09721 -320.732056 2.19506351 ## cyl -9.172379 3.1895161 199.66028 101.931452 -0.66836694 ## disp -633.097208 199.6602823 15360.79983 6721.158669 -47.06401915 ## hp -320.732056 101.9314516 6721.15867 4700.866935 -16.45110887 ## drat 2.195064 -0.6683669 -47.06402 -16.451109 0.28588135 ## wt -5.116685 1.3673710 107.68420 44.192661 -0.37272073 ## qsec 4.509149 -1.8868548 -96.05168 -86.770081 0.08714073 ## vs 2.017137 -0.7298387 -44.37762 -24.987903 0.11864919 ## am 1.803931 -0.4657258 -36.56401 -8.320565 0.19015121 ## gear 2.135685 -0.6491935 -50.80262 -6.358871 0.27598790 ## carb -5.363105 1.5201613 79.06875 83.036290 -0.07840726 ## wt qsec vs am gear ## mpg -5.1166847 4.50914919 2.01713710 1.80393145 2.1356855 ## cyl 1.3673710 -1.88685484 -0.72983871 -0.46572581 -0.6491935 ## disp 107.6842040 -96.05168145 -44.37762097 -36.56401210 -50.8026210 ## hp 44.1926613 -86.77008065 -24.98790323 -8.32056452 -6.3588710 ## drat -0.3727207 0.08714073 0.11864919 0.19015121 0.2759879 ## wt 0.9573790 -0.30548161 -0.27366129 -0.33810484 -0.4210806 ## qsec -0.3054816 3.19316613 0.67056452 -0.20495968 -0.2804032 ## vs -0.2736613 0.67056452 0.25403226 0.04233871 0.0766129 ## am -0.3381048 -0.20495968 0.04233871 0.24899194 0.2923387 ## gear -0.4210806 -0.28040323 0.07661290 0.29233871 0.5443548 ## carb 0.6757903 -1.89411290 -0.46370968 0.04637097 0.3266129 ## carb ## mpg -5.36310484 ## cyl 1.52016129 ## disp 79.06875000 ## hp 83.03629032 ## drat -0.07840726 ## wt 0.67579032 ## qsec -1.89411290 ## vs -0.46370968 ## am 0.04637097 ## gear 0.32661290 ## carb 2.60887097 1 2 3 4 # correlations with significance levels library ( Hmisc ) rcorr ( as.matrix ( mtcars ), type = \"pearson\" ) # type can be pearson or spearman 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 ## mpg cyl disp hp drat wt qsec vs am gear carb ## mpg 1.00 -0.85 -0.85 -0.78 0.68 -0.87 0.42 0.66 0.60 0.48 -0.55 ## cyl -0.85 1.00 0.90 0.83 -0.70 0.78 -0.59 -0.81 -0.52 -0.49 0.53 ## disp -0.85 0.90 1.00 0.79 -0.71 0.89 -0.43 -0.71 -0.59 -0.56 0.39 ## hp -0.78 0.83 0.79 1.00 -0.45 0.66 -0.71 -0.72 -0.24 -0.13 0.75 ## drat 0.68 -0.70 -0.71 -0.45 1.00 -0.71 0.09 0.44 0.71 0.70 -0.09 ## wt -0.87 0.78 0.89 0.66 -0.71 1.00 -0.17 -0.55 -0.69 -0.58 0.43 ## qsec 0.42 -0.59 -0.43 -0.71 0.09 -0.17 1.00 0.74 -0.23 -0.21 -0.66 ## vs 0.66 -0.81 -0.71 -0.72 0.44 -0.55 0.74 1.00 0.17 0.21 -0.57 ## am 0.60 -0.52 -0.59 -0.24 0.71 -0.69 -0.23 0.17 1.00 0.79 0.06 ## gear 0.48 -0.49 -0.56 -0.13 0.70 -0.58 -0.21 0.21 0.79 1.00 0.27 ## carb -0.55 0.53 0.39 0.75 -0.09 0.43 -0.66 -0.57 0.06 0.27 1.00 ## ## n= 32 ## ## ## P ## mpg cyl disp hp drat wt qsec vs am gear ## mpg 0.0000 0.0000 0.0000 0.0000 0.0000 0.0171 0.0000 0.0003 0.0054 ## cyl 0.0000 0.0000 0.0000 0.0000 0.0000 0.0004 0.0000 0.0022 0.0042 ## disp 0.0000 0.0000 0.0000 0.0000 0.0000 0.0131 0.0000 0.0004 0.0010 ## hp 0.0000 0.0000 0.0000 0.0100 0.0000 0.0000 0.0000 0.1798 0.4930 ## drat 0.0000 0.0000 0.0000 0.0100 0.0000 0.6196 0.0117 0.0000 0.0000 ## wt 0.0000 0.0000 0.0000 0.0000 0.0000 0.3389 0.0010 0.0000 0.0005 ## qsec 0.0171 0.0004 0.0131 0.0000 0.6196 0.3389 0.0000 0.2057 0.2425 ## vs 0.0000 0.0000 0.0000 0.0000 0.0117 0.0010 0.0000 0.3570 0.2579 ## am 0.0003 0.0022 0.0004 0.1798 0.0000 0.0000 0.2057 0.3570 0.0000 ## gear 0.0054 0.0042 0.0010 0.4930 0.0000 0.0005 0.2425 0.2579 0.0000 ## carb 0.0011 0.0019 0.0253 0.0000 0.6212 0.0146 0.0000 0.0007 0.7545 0.1290 ## carb ## mpg 0.0011 ## cyl 0.0019 ## disp 0.0253 ## hp 0.0000 ## drat 0.6212 ## wt 0.0146 ## qsec 0.0000 ## vs 0.0007 ## am 0.7545 ## gear 0.1290 ## carb 1 2 3 # dataset x <- mtcars[1 : 3 ] y <- mtcars[4 : 6 ] 1 2 # correlation between two vectors cor ( x , y ) 1 2 3 4 ## hp drat wt ## mpg -0.7761684 0.6811719 -0.8676594 ## cyl 0.8324475 -0.6999381 0.7824958 ## disp 0.7909486 -0.7102139 0.8879799 Polychoric correlation The correlation between two theorised normally distributed continuous latent variables, from two observed ordinal variables. 1 2 3 # dataset # 2-way contingency table of counts colors 1 2 3 4 5 ## fair.hair red.hair medium.hair dark.hair black.hair ## blue eyes 326 38 241 110 3 ## light eyes 688 116 584 188 4 ## medium eyes 343 84 909 412 26 ## dark eyes 98 48 403 681 85 1 2 3 4 # polychoric correlation library ( polycor ) polychor ( colors ) 1 ## [1] 0.4743984 1 2 3 4 5 6 # heterogeneous correlations in one matrix # pearson (numeric-numeric), # polyserial (numeric-ordinal), # and polychoric (ordinal-ordinal) # a data frame with ordered factors and numeric variables hetcor ( colors ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 ## ## Two-Step Estimates ## ## Correlations/Type of Correlation: ## fair.hair red.hair medium.hair dark.hair black.hair ## fair.hair 1 Pearson Pearson Pearson Pearson ## red.hair 0.8333 1 Pearson Pearson Pearson ## medium.hair 0.2597 0.6467 1 Pearson Pearson ## dark.hair -0.7091 -0.2251 0.1911 1 Pearson ## black.hair -0.781 -0.3875 -0.06329 0.9674 1 ## ## Standard Errors: ## fair.hair red.hair medium.hair dark.hair ## fair.hair ## red.hair 0.03628 ## medium.hair 0.3607 0.1383 ## dark.hair 0.09977 0.3733 0.3841 ## black.hair 0.06019 0.3004 0.4092 0.001491 ## ## n = 4 ## ## P-values for Tests of Bivariate Normality: ## fair.hair red.hair medium.hair dark.hair ## fair.hair ## red.hair 0.8306 ## medium.hair 0.6939 0.8609 ## dark.hair 0.7465 0.6335 0.7161 ## black.hair 0.6582 0.5927 0.6711 0.9873","title":"Correlations"},{"location":"a_hands-on_introduction_to_statistics_with_r_notes/#t-tests","text":"1 2 3 4 5 6 7 8 9 10 11 # independent 2-group t-test t.test ( y ~ x ) # where y is numeric and x is a binary factor # independent 2-group t-test t.test ( y1 , y2 ) # where y1 and y2 are numeric # paired t-test t.test ( y1 , y2 , paired = TRUE ) # where y1 & y2 are numeric # one sample t-test t.test ( y , mu = 3 ) # Ho: mu=3 1 2 3 # dataset # 2-way contingency table colors 1 2 3 4 5 ## fair.hair red.hair medium.hair dark.hair black.hair ## blue eyes 326 38 241 110 3 ## light eyes 688 116 584 188 4 ## medium eyes 343 84 909 412 26 ## dark eyes 98 48 403 681 85 1 mean ( as.numeric ( colors[1 , ] )) # row 1 average 1 ## [1] 143.6 1 2 # independent 2-group t-test t.test ( as.numeric ( colors[1 , ] ), as.numeric ( colors[2 , ] )) # where y1 and y2 are numeric 1 2 3 4 5 6 7 8 9 10 11 ## ## Welch Two Sample t-test ## ## data: as.numeric(colors[1, ]) and as.numeric(colors[2, ]) ## t = -1.164, df = 5.5777, p-value = 0.2918 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -541.5725 196.7725 ## sample estimates: ## mean of x mean of y ## 143.6 316.0 1 2 # one sample t-test t.test ( as.numeric ( colors[1 , ] ), mu = 0 ) # Ho: mu=0 1 2 3 4 5 6 7 8 9 10 11 ## ## One Sample t-test ## ## data: as.numeric(colors[1, ]) ## t = 2.348, df = 4, p-value = 0.07868 ## alternative hypothesis: true mean is not equal to 0 ## 95 percent confidence interval: ## -26.2009 313.4009 ## sample estimates: ## mean of x ## 143.6 1 t.test ( as.numeric ( colors[1 , ] ), mu = 0 , alternative = \"greater\" ) # Ho: mu=<0 1 2 3 4 5 6 7 8 9 10 11 ## ## One Sample t-test ## ## data: as.numeric(colors[1, ]) ## t = 2.348, df = 4, p-value = 0.03934 ## alternative hypothesis: true mean is greater than 0 ## 95 percent confidence interval: ## 13.22123 Inf ## sample estimates: ## mean of x ## 143.6 alternative=\"less\" or alternative=\"greater\" option to specify a one tailed test. var.equal = TRUE option to specify equal variances and a pooled variance estimate. For multivariate tests and ANOVA, see sections 8 and 9.","title":"t-tests"},{"location":"a_hands-on_introduction_to_statistics_with_r_notes/#nonparametric-statistics","text":"Nonnormal distributions.","title":"Nonparametric statistics"},{"location":"a_hands-on_introduction_to_statistics_with_r_notes/#bivariate-tests","text":"1 2 3 4 5 6 7 8 # independent 2-group Mann-Whitney U test wilcox.test ( y ~ A ) # where y is numeric and A is A binary factor # independent 2-group Mann-Whitney U test wilcox.test ( y , x ) # where y and x are numeric # dependent 2-group Wilcoxon signed rank test wilcox.test ( y1 , y2 , paired = TRUE ) # where y1 and y2 are numeric 1 2 # 2-way contingency table colors 1 2 3 4 5 ## fair.hair red.hair medium.hair dark.hair black.hair ## blue eyes 326 38 241 110 3 ## light eyes 688 116 584 188 4 ## medium eyes 343 84 909 412 26 ## dark eyes 98 48 403 681 85 1 2 # independent 2-group Mann-Whitney U Test wilcox.test ( as.numeric ( colors[1 , ] ), as.numeric ( colors[2 , ] )) # where y and x are numeric 1 2 3 4 5 6 ## ## Wilcoxon rank sum test ## ## data: as.numeric(colors[1, ]) and as.numeric(colors[2, ]) ## W = 8, p-value = 0.4206 ## alternative hypothesis: true location shift is not equal to 0 alternative=\"less\" or alternative=\"greater\" option to specify a one tailed test.","title":"Bivariate tests"},{"location":"a_hands-on_introduction_to_statistics_with_r_notes/#anova","text":"1 2 3 4 5 6 7 8 # Kruskal Wallis test one-Way ANOVA by ranks kruskal.test ( y ~ A ) # where y1 is numeric and A is a factor # randomized block design - Friedman test friedman.test ( y ~ A | B ) # where y are the data values, A is a grouping factor and B is a blocking factor # Kruskal Wallis test one-way ANOVA by ranks kruskal.test ( y , x ) # where y and x are numeric 1 2 3 # dataset # 2-way contingency table colors 1 2 3 4 5 ## fair.hair red.hair medium.hair dark.hair black.hair ## blue eyes 326 38 241 110 3 ## light eyes 688 116 584 188 4 ## medium eyes 343 84 909 412 26 ## dark eyes 98 48 403 681 85 1 2 # Kruskal Wallis test one-way ANOVA by ranks kruskal.test ( as.numeric ( colors[1 , ] ), as.numeric ( colors[2 , ] )) # where y and x are numeric 1 2 3 4 5 ## ## Kruskal-Wallis rank sum test ## ## data: as.numeric(colors[1, ]) and as.numeric(colors[2, ]) ## Kruskal-Wallis chi-squared = 4, df = 4, p-value = 0.406","title":"ANOVA"},{"location":"a_hands-on_introduction_to_statistics_with_r_notes/#multiple-regressions","text":"","title":"Multiple Regressions"},{"location":"a_hands-on_introduction_to_statistics_with_r_notes/#fitting-the-model","text":"1 2 3 4 5 6 7 8 9 10 11 12 # multiple linear regression fit <- lm ( y ~ x1 + x2 + x3 , data = mydata ) summary ( fit ) # show results # useful functions coefficients ( fit ) # model coefficients confint ( fit , level = 0.95 ) # CIs for model parameters fitted ( fit ) # predicted values residuals ( fit ) # residuals anova ( fit ) # ANOVA table vcov ( fit ) # covariance matrix for model parameters influence ( fit ) # regression diagnostics 1 2 # dataset str ( longley ) 1 2 3 4 5 6 7 8 ## 'data.frame': 16 obs. of 7 variables: ## $ GNP.deflator: num 83 88.5 88.2 89.5 96.2 ... ## $ GNP : num 234 259 258 285 329 ... ## $ Unemployed : num 236 232 368 335 210 ... ## $ Armed.Forces: num 159 146 162 165 310 ... ## $ Population : num 108 109 110 111 112 ... ## $ Year : int 1947 1948 1949 1950 1951 1952 1953 1954 1955 1956 ... ## $ Employed : num 60.3 61.1 60.2 61.2 63.2 ... 1 2 3 # multiple linear regression example fit <- lm ( Armed.Forces ~ GNP + Population , data = longley ) summary ( fit ) # show results 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 ## ## Call: ## lm(formula = Armed.Forces ~ GNP + Population, data = longley) ## ## Residuals: ## Min 1Q Median 3Q Max ## -70.349 -33.569 5.076 16.409 104.037 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 4123.922 1276.579 3.230 0.00657 ** ## GNP 3.365 0.986 3.413 0.00463 ** ## Population -44.011 14.089 -3.124 0.00807 ** ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## Residual standard error: 50.56 on 13 degrees of freedom ## Multiple R-squared: 0.5426, Adjusted R-squared: 0.4723 ## F-statistic: 7.712 on 2 and 13 DF, p-value: 0.006191 1 2 # other useful functions coefficients ( fit ) # model coefficients 1 2 ## (Intercept) GNP Population ## 4123.922484 3.365215 -44.010955 1 confint ( fit , level = 0.95 ) # CIs for model parameters 1 2 3 4 ## 2.5 % 97.5 % ## (Intercept) 1366.042123 6881.802846 ## GNP 1.235097 5.495333 ## Population -74.447969 -13.573942 1 fitted ( fit ) # predicted values 1 2 3 4 ## 1947 1948 1949 1950 1951 1952 1953 1954 ## 176.4245 215.9487 161.1151 199.5681 298.4663 306.5279 288.1248 230.9633 ## 1955 1956 1957 1958 1959 1960 1961 1962 ## 295.1332 308.9566 313.0359 252.7794 318.8698 297.7176 240.7975 266.2711 1 residuals ( fit ) # residuals 1 2 3 4 5 6 ## 1947 1948 1949 1950 1951 1952 ## -17.4245114 -70.3487085 0.4848669 -34.5681071 11.4336564 52.8721086 ## 1953 1954 1955 1956 1957 1958 ## 66.5752438 104.0367029 9.6668098 -23.2566323 -33.2359499 10.9205505 ## 1959 1960 1961 1962 ## -63.6698197 -46.3175746 16.4025069 16.4288577 1 anova ( fit ) # ANOVA table 1 2 3 4 5 6 7 8 9 ## Analysis of Variance Table ## ## Response: Armed.Forces ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## GNP 1 14479 14478.7 5.6649 0.033301 * ## Population 1 24941 24940.8 9.7583 0.008068 ** ## Residuals 13 33226 2555.9 ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 1 vcov ( fit ) # covariance matrix for model parameters 1 2 3 4 ## (Intercept) GNP Population ## (Intercept) 1629652.882 1239.7478355 -17970.27388 ## GNP 1239.748 0.9721911 -13.76775 ## Population -17970.274 -13.7677545 198.49444 1 influence ( fit ) # regression diagnostics 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 ## $hat ## 1947 1948 1949 1950 1951 1952 ## 0.27412252 0.17438942 0.31563199 0.16765687 0.21219668 0.21127212 ## 1953 1954 1955 1956 1957 1958 ## 0.11339116 0.08602104 0.10270245 0.12845683 0.13251347 0.11070418 ## 1959 1960 1961 1962 ## 0.15598638 0.15164367 0.32488437 0.33842686 ## ## $coefficients ## (Intercept) GNP Population ## 1947 128.042531 0.131479474 -1.53731106 ## 1948 29.040635 0.121992502 -0.69544934 ## 1949 -6.396740 -0.005738655 0.07379997 ## 1950 177.778426 0.175668519 -2.11609661 ## 1951 133.333008 0.093997366 -1.43810938 ## 1952 638.680267 0.462229807 -6.92955762 ## 1953 422.107967 0.305133565 -4.56222462 ## 1954 -385.998493 -0.325674565 4.42308458 ## 1955 54.458118 0.042127997 -0.59713302 ## 1956 -163.371695 -0.131240587 1.81041089 ## 1957 -212.039316 -0.179084306 2.37664757 ## 1958 -51.395691 -0.033854337 0.55600613 ## 1959 -329.488802 -0.311550864 3.79446941 ## 1960 3.116881 -0.049904757 0.10916689 ## 1961 -242.199361 -0.158976865 2.60043035 ## 1962 -194.416431 -0.113799395 2.04462752 ## ## $sigma ## 1947 1948 1949 1950 1951 1952 1953 1954 ## 52.28757 47.63741 52.61955 51.47046 52.48826 49.73420 48.50003 42.21357 ## 1955 1956 1957 1958 1959 1960 1961 1962 ## 52.53729 52.12610 51.60167 52.51353 48.66817 50.57779 52.30331 52.29577 ## ## $wt.res ## 1947 1948 1949 1950 1951 1952 ## -17.4245114 -70.3487085 0.4848669 -34.5681071 11.4336564 52.8721086 ## 1953 1954 1955 1956 1957 1958 ## 66.5752438 104.0367029 9.6668098 -23.2566323 -33.2359499 10.9205505 ## 1959 1960 1961 1962 ## -63.6698197 -46.3175746 16.4025069 16.4288577","title":"Fitting the Model"},{"location":"a_hands-on_introduction_to_statistics_with_r_notes/#diagnostic-plots","text":"1 2 3 # diagnostic plots layout ( matrix ( c ( 1 , 2 , 3 , 4 ), 2 , 2 )) # optional 4 graphs/page plot ( fit ) 1 layout ( matrix ( c ( 1 , 1 , 1 , 1 ), 2 , 2 ))","title":"Diagnostic plots"},{"location":"a_hands-on_introduction_to_statistics_with_r_notes/#comparing-two-models-with-anova","text":"1 2 3 4 5 # compare models fit1 <- lm ( y ~ x1 + x2 + x3 + x4 , data = mydata ) fit2 <- lm ( y ~ x1 + x2 ) anova ( fit1 , fit2 ) 1 2 3 4 5 # compare models fit1 <- lm ( Armed.Forces ~ GNP + Population , data = longley ) fit2 <- lm ( Armed.Forces ~ GNP , data = longley ) anova ( fit1 , fit2 ) 1 2 3 4 5 6 7 8 9 ## Analysis of Variance Table ## ## Model 1: Armed.Forces ~ GNP + Population ## Model 2: Armed.Forces ~ GNP ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 13 33226 ## 2 14 58167 -1 -24941 9.7583 0.008068 ** ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1","title":"Comparing two models with ANOVA"},{"location":"a_hands-on_introduction_to_statistics_with_r_notes/#cross-validation","text":"1 2 3 4 5 # dataset library ( DAAG ) data ( houseprices ) head ( houseprices , 3 ) 1 2 3 4 ## area bedrooms sale.price ## 9 694 4 192 ## 10 905 4 215 ## 11 802 4 215 1 2 3 4 5 # k-fold cross-validation library ( DAAG ) # case 1, # 3 fold cross-validation CVlm ( houseprices , form.lm = formula ( sale.price ~ area ), m = 3 , dots = FALSE , seed = 29 , plotit = c ( \"Observed\" , \"Residual\" ), main = \"Small symbols show cross-validation predicted values\" , legend.pos = \"topleft\" , printit = TRUE ) 1 2 3 4 5 6 7 8 ## Analysis of Variance Table ## ## Response: sale.price ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## area 1 18566 18566 8 0.014 * ## Residuals 13 30179 2321 ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 ## ## fold 1 ## Observations in test set: 5 ## 11 20 21 22 23 ## area 802 696 771.0 1006.0 1191 ## cvpred 204 188 199.3 234.7 262 ## sale.price 215 255 260.0 293.0 375 ## CV residual 11 67 60.7 58.3 113 ## ## Sum of squares = 24351 Mean square = 4870 n = 5 ## ## fold 2 ## Observations in test set: 5 ## 10 13 14 17 18 ## area 905 716 963.0 1018.00 887.00 ## cvpred 255 224 264.4 273.38 252.06 ## sale.price 215 113 185.0 276.00 260.00 ## CV residual -40 -112 -79.4 2.62 7.94 ## ## Sum of squares = 20416 Mean square = 4083 n = 5 ## ## fold 3 ## Observations in test set: 5 ## 9 12 15 16 19 ## area 694.0 1366 821.00 714.0 790.00 ## cvpred 183.2 388 221.94 189.3 212.49 ## sale.price 192.0 274 212.00 220.0 221.50 ## CV residual 8.8 -114 -9.94 30.7 9.01 ## ## Sum of squares = 14241 Mean square = 2848 n = 5 ## ## Overall (Sum over all 5 folds) ## ms ## 3934 1 2 # dataset head ( longley , 3 ) 1 2 3 4 ## GNP.deflator GNP Unemployed Armed.Forces Population Year Employed ## 1947 83.0 234 236 159 108 1947 60.3 ## 1948 88.5 259 232 146 109 1948 61.1 ## 1949 88.2 258 368 162 110 1949 60.2 1 2 # case 2, # 3 fold cross-validation CVlm ( longley , form.lm = formula ( Armed.Forces ~ GNP + Population ), m = 3 , dots = FALSE , seed = 29 , plotit = c ( \"Observed\" , \"Residual\" ), main = \"Small symbols show cross-validation predicted values\" , legend.pos = \"topleft\" , printit = TRUE ) 1 2 3 4 5 6 7 8 9 ## Analysis of Variance Table ## ## Response: Armed.Forces ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## GNP 1 14479 14479 5.66 0.0333 * ## Population 1 24941 24941 9.76 0.0081 ** ## Residuals 13 33226 2556 ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 ## ## fold 1 ## Observations in test set: 5 ## 1953 1954 1957 1960 1962 ## Predicted 288.1 231 313.0 297.7 266.3 ## cvpred 281.4 219 310.6 295.7 263.1 ## Armed.Forces 354.7 335 279.8 251.4 282.7 ## CV residual 73.3 116 -30.8 -44.3 19.6 ## ## Sum of squares = 22004 Mean square = 4401 n = 5 ## ## fold 2 ## Observations in test set: 6 ## 1948 1949 1955 1958 1959 1961 ## Predicted 215.9 161.12 295.13 252.78 318.9 240.8 ## cvpred 231.9 171.41 306.33 255.07 324.5 234.9 ## Armed.Forces 145.6 161.60 304.80 263.70 255.2 257.2 ## CV residual -86.3 -9.81 -1.53 8.63 -69.3 22.3 ## ## Sum of squares = 12917 Mean square = 2153 n = 6 ## ## fold 3 ## Observations in test set: 5 ## 1947 1950 1951 1952 1956 ## Predicted 176.4 199.6 298.5 306.5 308.96 ## cvpred 192.8 211.9 282.3 288.9 295.17 ## Armed.Forces 159.0 165.0 309.9 359.4 285.70 ## CV residual -33.8 -46.9 27.6 70.5 -9.47 ## ## Sum of squares = 9161 Mean square = 1832 n = 5 ## ## Overall (Sum over all 5 folds) ## ms ## 2755","title":"Cross validation"},{"location":"a_hands-on_introduction_to_statistics_with_r_notes/#variable-selection-heuristic-methods","text":"1 2 # dataset head ( longley , 3 ) 1 2 3 4 ## GNP.deflator GNP Unemployed Armed.Forces Population Year Employed ## 1947 83.0 234 236 159 108 1947 60.3 ## 1948 88.5 259 232 146 109 1948 61.1 ## 1949 88.2 258 368 162 110 1949 60.2 1 2 3 4 5 # Stepwise Regression library ( MASS ) fit <- lm ( Armed.Forces ~ GNP + Population + Employed + Unemployed , data = longley ) step <- stepAIC ( fit , direction = \"both\" ) 1 2 3 4 5 6 7 8 9 ## Start: AIC=125 ## Armed.Forces ~ GNP + Population + Employed + Unemployed ## ## Df Sum of Sq RSS AIC ## &lt;none&gt; 21655 125 ## - Unemployed 1 4508 26163 126 ## - Population 1 5522 27177 127 ## - Employed 1 10208 31863 130 ## - GNP 1 15323 36978 132 1 step $ anova # display results 1 2 3 4 5 6 7 8 9 10 11 12 ## Stepwise Model Path ## Analysis of Deviance Table ## ## Initial Model: ## Armed.Forces ~ GNP + Population + Employed + Unemployed ## ## Final Model: ## Armed.Forces ~ GNP + Population + Employed + Unemployed ## ## ## Step Df Deviance Resid. Df Resid. Dev AIC ## 1 11 21655 125 The goal is to reduce the AIC. Adding variable does not improve the model. Let\u2019s opt for the most valuable variable. 1 2 fit <- lm ( Armed.Forces ~ Unemployed , data = longley ) step <- stepAIC ( fit , direction = \"both\" ) 1 2 3 4 5 6 7 8 9 10 11 12 13 ## Start: AIC=138 ## Armed.Forces ~ Unemployed ## ## Df Sum of Sq RSS AIC ## - Unemployed 1 2287 72646 137 ## &lt;none&gt; 70359 138 ## ## Step: AIC=137 ## Armed.Forces ~ 1 ## ## Df Sum of Sq RSS AIC ## &lt;none&gt; 72646 137 ## + Unemployed 1 2287 70359 138 1 step $ anova # display results 1 2 3 4 5 6 7 8 9 10 11 12 13 ## Stepwise Model Path ## Analysis of Deviance Table ## ## Initial Model: ## Armed.Forces ~ Unemployed ## ## Final Model: ## Armed.Forces ~ 1 ## ## ## Step Df Deviance Resid. Df Resid. Dev AIC ## 1 14 70359 138 ## 2 - Unemployed 1 2287 15 72646 137","title":"Variable selection -- Heuristic methods"},{"location":"a_hands-on_introduction_to_statistics_with_r_notes/#variable-selection-graphical-methods","text":"1 2 # model fit <- lm ( Armed.Forces ~ GNP + Population + Employed + Unemployed , data = longley ) 1 2 3 4 5 6 7 # all Subsets Regression library ( leaps ) leaps <- regsubsets ( Armed.Forces ~ GNP + Population + Employed + Unemployed , data = longley , nbest = 10 ) # view results summary ( leaps ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 ## Subset selection object ## Call: regsubsets.formula(Armed.Forces ~ GNP + Population + Employed + ## Unemployed, data = longley, nbest = 10) ## 4 Variables (and intercept) ## Forced in Forced out ## GNP FALSE FALSE ## Population FALSE FALSE ## Employed FALSE FALSE ## Unemployed FALSE FALSE ## 10 subsets of each size up to 4 ## Selection Algorithm: exhaustive ## GNP Population Employed Unemployed ## 1 ( 1 ) \" \" \" \" \"*\" \" \" ## 1 ( 2 ) \"*\" \" \" \" \" \" \" ## 1 ( 3 ) \" \" \"*\" \" \" \" \" ## 1 ( 4 ) \" \" \" \" \" \" \"*\" ## 2 ( 1 ) \"*\" \"*\" \" \" \" \" ## 2 ( 2 ) \"*\" \" \" \" \" \"*\" ## 2 ( 3 ) \" \" \"*\" \" \" \"*\" ## 2 ( 4 ) \" \" \" \" \"*\" \"*\" ## 2 ( 5 ) \" \" \"*\" \"*\" \" \" ## 2 ( 6 ) \"*\" \" \" \"*\" \" \" ## 3 ( 1 ) \"*\" \"*\" \"*\" \" \" ## 3 ( 2 ) \"*\" \" \" \"*\" \"*\" ## 3 ( 3 ) \"*\" \"*\" \" \" \"*\" ## 3 ( 4 ) \" \" \"*\" \"*\" \"*\" ## 4 ( 1 ) \"*\" \"*\" \"*\" \"*\" 1 2 3 # plot a table of models showing variables in each model # models are ordered by the selection statistic plot ( leaps , scale = \"r2\" ) 1 2 3 4 # plot statistic by subset size library ( car ) subsets ( leaps , statistic = \"rsq\" , legend = FALSE ) # available criteria are rsq, rss, adjr2, cp, bic 1 2 3 4 5 ## Abbreviation ## GNP G ## Population P ## Employed E ## Unemployed U 1 subsets ( leaps , statistic = \"bic\" , legend = FALSE ) # available criteria are rsq, rss, adjr2, cp, bic 1 2 3 4 5 ## Abbreviation ## GNP G ## Population P ## Employed E ## Unemployed U","title":"Variable selection -- Graphical methods"},{"location":"a_hands-on_introduction_to_statistics_with_r_notes/#variable-selection-relative-importance","text":"Model: fit <- lm ( Armed.Forces ~ GNP + Population + Employed + Unemployed , data = longley ) . Warning: eval=FALSE . 1 2 3 4 # calculate the relative importance of each predictor library ( relaimpo ) calc.relimp ( fit , type = c ( \"lmg\" , \"last\" , \"first\" , \"pratt\" ), rela = TRUE ) Results. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 Response variable: Armed.Forces Total response variance: 4843 Analysis based on 16 observations 4 Regressors: GNP Population Employed Unemployed Proportion of variance explained by model: 70.2% Metrics are normalized to sum to 100% (rela=TRUE). Relative importance metrics: lmg last first pratt GNP 0.328 0.431 0.348 4.566 Population 0.241 0.155 0.232 -1.934 Employed 0.217 0.287 0.365 -1.780 Unemployed 0.215 0.127 0.055 0.148 Average coefficients for different model sizes: 1X 2Xs 3Xs 4Xs GNP 0.313 1.301 3.641 5.026 Population 3.646 -14.815 -24.637 -37.260 Employed 9.062 17.646 -34.249 -54.144 Unemployed -0.132 -0.511 -0.584 -0.436 Bootstrapping Model: fit <- lm ( Armed.Forces ~ GNP + Population + Employed + Unemployed , data = longley ) . Warning: eval=FALSE . 1 2 3 # bootstrap measures of relative importance (1000 samples) boot <- boot.relimp ( fit , b = 1000 , type = c ( \"lmg\" , \"last\" , \"first\" , \"pratt\" ), rank = TRUE , diff = TRUE , rela = TRUE ) booteval.relimp ( boot ) # print result Results. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 Response variable: Armed.Forces Total response variance: 4843 Analysis based on 16 observations 4 Regressors: GNP Population Employed Unemployed Proportion of variance explained by model: 70.2% Metrics are normalized to sum to 100% (rela=TRUE). Relative importance metrics: lmg last first pratt GNP 0.328 0.431 0.348 4.566 Population 0.241 0.155 0.232 -1.934 Employed 0.217 0.287 0.365 -1.780 Unemployed 0.215 0.127 0.055 0.148 Average coefficients for different model sizes: 1X 2Xs 3Xs 4Xs GNP 0.313 1.301 3.641 5.026 Population 3.646 -14.815 -24.637 -37.260 Employed 9.062 17.646 -34.249 -54.144 Unemployed -0.132 -0.511 -0.584 -0.436 Confidence interval information ( 1000 bootstrap replicates, bty= perc ): Relative Contributions with confidence intervals: Lower Upper percentage 0.95 0.95 0.95 GNP.lmg 0.3280 ABC_ 0.1488 0.4030 Population.lmg 0.2410 _BCD 0.1572 0.3060 Employed.lmg 0.2170 ABCD 0.1107 0.3240 Unemployed.lmg 0.2150 ABCD 0.0586 0.5550 GNP.last 0.4310 ABCD 0.0101 0.5660 Population.last 0.1550 _BCD 0.0003 0.3850 Employed.last 0.2870 ABCD 0.0476 0.6280 Unemployed.last 0.1270 ABCD 0.0009 0.7810 GNP.first 0.3480 ABCD 0.0151 0.3960 Population.first 0.2320 _BCD 0.0070 0.3060 Employed.first 0.3650 ABCD 0.0159 0.4020 Unemployed.first 0.0550 ABCD 0.0004 0.9280 GNP.pratt 4.5660 ABCD -1.0160 10.5380 Population.pratt -1.9340 ABCD -6.0800 2.2050 Employed.pratt -1.7800 _BCD -5.0290 0.8510 Unemployed.pratt 0.1480 ABC_ -0.3250 1.0910 Letters indicate the ranks covered by bootstrap CIs. (Rank bootstrap confidence intervals always obtained by percentile method) CAUTION: Bootstrap confidence intervals can be somewhat liberal. Differences between Relative Contributions: Lower Upper difference 0.95 0.95 0.95 GNP-Population.lmg 0.0871 -0.0188 0.1521 GNP-Employed.lmg 0.1106 -0.0479 0.1949 GNP-Unemployed.lmg 0.1130 -0.4021 0.3281 Population-Employed.lmg 0.0236 -0.1091 0.1124 Population-Unemployed.lmg 0.0259 -0.3904 0.2417 Employed-Unemployed.lmg 0.0023 -0.4315 0.2295 GNP-Population.last 0.2756 -0.1062 0.4502 GNP-Employed.last 0.1438 -0.5564 0.4011 GNP-Unemployed.last 0.3041 -0.7408 0.5502 Population-Employed.last -0.1318 -0.6191 0.2653 Population-Unemployed.last 0.0285 -0.7382 0.3622 Employed-Unemployed.last 0.1603 -0.6784 0.4852 GNP-Population.first 0.1161 -0.0590 0.1736 GNP-Employed.first -0.0172 -0.0893 0.0934 GNP-Unemployed.first 0.2930 -0.9047 0.3766 Population-Employed.first -0.1333 -0.2134 0.0688 Population-Unemployed.first 0.1769 -0.8967 0.2969 Employed-Unemployed.first 0.3102 -0.8937 0.3823 GNP-Population.pratt 6.4995 -2.2069 15.7978 GNP-Employed.pratt 6.3461 -1.3879 15.0093 GNP-Unemployed.pratt 4.4180 -1.9134 10.6456 Population-Employed.pratt -0.1534 -4.1860 5.9691 Population-Unemployed.pratt -2.0815 -6.1439 2.2348 Employed-Unemployed.pratt -1.9281 -5.2057 0.1725 * indicates that CI for difference does not include 0. CAUTION: Bootstrap confidence intervals can be somewhat liberal. Warning: eval=FALSE . 1 plot ( booteval.relimp ( boot , sort = TRUE )) # to plot the results The .png file.","title":"Variable selection -- Relative importance"},{"location":"a_hands-on_introduction_to_statistics_with_r_notes/#going-further","text":"The nls package provides functions for nonlinear regression. Perform robust regression with the rlm function in the MASS package. The robust package provides a comprehensive library of robust methods, including regression. The robustbase package also provides basic robust statistics including model selection methods.","title":"Going further"},{"location":"a_hands-on_introduction_to_statistics_with_r_notes/#regression-diagnostics","text":"1 2 3 # assume that we are fitting a multiple linear regression fit <- lm ( mpg ~ disp + hp + wt + drat , data = mtcars ) fit 1 2 3 4 5 6 7 ## ## Call: ## lm(formula = mpg ~ disp + hp + wt + drat, data = mtcars) ## ## Coefficients: ## (Intercept) disp hp wt drat ## 29.14874 0.00382 -0.03478 -3.47967 1.76805","title":"Regression diagnostics"},{"location":"a_hands-on_introduction_to_statistics_with_r_notes/#outliers","text":"1 2 3 4 library ( car ) # assessing outliers outlierTest ( fit ) # Bonferonni p-value for most extreme obs 1 2 3 4 5 ## ## No Studentized residuals with Bonferonni p &lt; 0.05 ## Largest |rstudent|: ## rstudent unadjusted p-value Bonferonni p ## Toyota Corolla 2.52 0.0184 0.588 1 qqPlot ( fit , main = \"QQ Plot\" ) #qq plot for studentized resid 1 leveragePlots ( fit ) # leverage plots","title":"Outliers"},{"location":"a_hands-on_introduction_to_statistics_with_r_notes/#influential-observations","text":"Model: fit <- lm ( mpg ~ disp + hp + wt + drat , data = mtcars ) . 1 2 3 4 5 # influential observations # added variable plots library ( car ) avPlots ( fit ) 1 2 3 4 5 6 par ( mfrow = c ( 1 , 1 )) # Cook's D plot # identify D values > 4/(n-k-1) cutoff <- 4 / (( nrow ( mtcars ) - length ( fit $ coefficients ) - 2 )) plot ( fit , which = 4 , cook.levels = cutoff ) Warning: eval=FALSE ; interactive function. 1 2 # influence plot influencePlot ( fit , id.method = \"identify\" , main = \"Influence Plot\" , sub = \"Circle size is proportial to Cook's Distance\" )","title":"Influential observations"},{"location":"a_hands-on_introduction_to_statistics_with_r_notes/#nonnormality","text":"Model: fit <- lm ( mpg ~ disp + hp + wt + drat , data = mtcars ) . 1 2 3 4 5 # normality of residuals # qq plot for studentized resid library ( car ) qqPlot ( fit , main = \"QQ Plot\" ) 1 2 3 4 5 6 7 8 # distribution of studentized residuals library ( MASS ) sresid <- studres ( fit ) hist ( sresid , freq = FALSE , main = \"Distribution of Studentized Residuals\" ) xfit <- seq ( min ( sresid ), max ( sresid ), length = 40 ) yfit <- dnorm ( xfit ) lines ( xfit , yfit )","title":"Nonnormality"},{"location":"a_hands-on_introduction_to_statistics_with_r_notes/#heteroscedasticity","text":"Nonconstant error variance. Model: fit <- lm ( mpg ~ disp + hp + wt + drat , data = mtcars ) . 1 2 3 # evaluate homoscedasticity # non-constant error variance test ncvTest ( fit ) 1 2 3 ## Non-constant Variance Score Test ## Variance formula: ~ fitted.values ## Chisquare = 1.43 Df = 1 p = 0.232 1 2 # plot studentized residuals vs. fitted values spreadLevelPlot ( fit ) 1 2 ## ## Suggested power transformation: 0.662","title":"Heteroscedasticity"},{"location":"a_hands-on_introduction_to_statistics_with_r_notes/#multicollinearity","text":"Model: fit <- lm ( mpg ~ disp + hp + wt + drat , data = mtcars ) . 1 2 # evaluatecCollinearity vif ( fit ) 1 2 ## disp hp wt drat ## 8.21 2.89 5.10 2.28 1 sqrt ( vif ( fit )) > 2 # benchmark = 1.96, rounded to 2 1 2 ## disp hp wt drat ## TRUE FALSE TRUE FALSE","title":"Multicollinearity"},{"location":"a_hands-on_introduction_to_statistics_with_r_notes/#nonlinearity","text":"Model: fit <- lm ( mpg ~ disp + hp + wt + drat , data = mtcars ) . 1 2 3 # evaluate nonlinearity # component + residual plot crPlots ( fit ) 1 2 # Ceres plots ceresPlots ( fit )","title":"Nonlinearity"},{"location":"a_hands-on_introduction_to_statistics_with_r_notes/#autocorrelation","text":"Serial correlation or non-independence of errors. Model: fit <- lm ( mpg ~ disp + hp + wt + drat , data = mtcars ) . 1 2 # test for autocorrelated errors durbinWatsonTest ( fit ) 1 2 3 ## lag Autocorrelation D-W Statistic p-value ## 1 0.101 1.74 0.29 ## Alternative hypothesis: rho != 0","title":"Autocorrelation"},{"location":"a_hands-on_introduction_to_statistics_with_r_notes/#global-diagnostic","text":"Model: fit <- lm ( mpg ~ disp + hp + wt + drat , data = mtcars ) . 1 2 3 4 5 # global test of model assumptions library ( gvlma ) gvmodel <- gvlma ( fit ) summary ( gvmodel ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 ## ## Call: ## lm(formula = mpg ~ disp + hp + wt + drat, data = mtcars) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.508 -1.905 -0.506 0.982 5.688 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 29.14874 6.29359 4.63 8.2e-05 *** ## disp 0.00382 0.01080 0.35 0.7268 ## hp -0.03478 0.01160 -3.00 0.0058 ** ## wt -3.47967 1.07837 -3.23 0.0033 ** ## drat 1.76805 1.31978 1.34 0.1915 ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## Residual standard error: 2.6 on 27 degrees of freedom ## Multiple R-squared: 0.838, Adjusted R-squared: 0.814 ## F-statistic: 34.8 on 4 and 27 DF, p-value: 2.7e-10 ## ## ## ASSESSMENT OF THE LINEAR MODEL ASSUMPTIONS ## USING THE GLOBAL TEST ON 4 DEGREES-OF-FREEDOM: ## Level of Significance = 0.05 ## ## Call: ## gvlma(x = fit) ## ## Value p-value Decision ## Global Stat 13.9382 0.00750 Assumptions NOT satisfied! ## Skewness 4.3131 0.03782 Assumptions NOT satisfied! ## Kurtosis 0.0138 0.90654 Assumptions acceptable. ## Link Function 8.7166 0.00315 Assumptions NOT satisfied! ## Heteroscedasticity 0.8947 0.34421 Assumptions acceptable.","title":"Global diagnostic"},{"location":"a_hands-on_introduction_to_statistics_with_r_notes/#anova_1","text":"Analysis of variance (ANOVA) is an alternative to regressions among other applications. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 # lower case letters are numeric variables # upper case letters are factors # one-way ANOVA (completely randomized design) fit <- aov ( y ~ A , data = mydataframe ) # randomized block design (B is the blocking factor) fit <- aov ( y ~ A + B , data = mydataframe ) # two-way factorial design fit <- aov ( y ~ A + B + A : B , data = mydataframe ) fit <- aov ( y ~ A * B , data = mydataframe ) # same thing # analysis of covariance fit <- aov ( y ~ A + x , data = mydataframe ) 1 2 # dataset head ( CO2 , 3 ) 1 2 3 4 5 ## Grouped Data: uptake ~ conc | Plant ## Plant Type Treatment conc uptake ## 1 Qn1 Quebec nonchilled 95 16.0 ## 2 Qn1 Quebec nonchilled 175 30.4 ## 3 Qn1 Quebec nonchilled 250 34.8 1 2 3 # one-way ANOVA (completely randomized design) fit <- aov ( uptake ~ Plant , data = CO2 ) fit 1 2 3 4 5 6 7 8 9 10 ## Call: ## aov(formula = uptake ~ Plant, data = CO2) ## ## Terms: ## Plant Residuals ## Sum of Squares 4862 4845 ## Deg. of Freedom 11 72 ## ## Residual standard error: 8.2 ## Estimated effects are balanced 1 summary ( fit ) 1 2 3 4 5 ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Plant 11 4862 442 6.57 1.8e-07 *** ## Residuals 72 4845 67 ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 1 2 3 # randomized block design (B is the blocking factor) fit <- aov ( uptake ~ Plant + Type , data = CO2 ) fit 1 2 3 4 5 6 7 8 9 10 11 ## Call: ## aov(formula = uptake ~ Plant + Type, data = CO2) ## ## Terms: ## Plant Residuals ## Sum of Squares 4862 4845 ## Deg. of Freedom 11 72 ## ## Residual standard error: 8.2 ## 1 out of 13 effects not estimable ## Estimated effects are balanced 1 summary ( fit ) 1 2 3 4 5 ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Plant 11 4862 442 6.57 1.8e-07 *** ## Residuals 72 4845 67 ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 1 2 3 # two-way factorial design fit <- aov ( uptake ~ Plant + Type + Plant : Type , data = CO2 ) fit 1 2 3 4 5 6 7 8 9 10 11 ## Call: ## aov(formula = uptake ~ Plant + Type + Plant:Type, data = CO2) ## ## Terms: ## Plant Residuals ## Sum of Squares 4862 4845 ## Deg. of Freedom 11 72 ## ## Residual standard error: 8.2 ## 12 out of 24 effects not estimable ## Estimated effects are balanced 1 summary ( fit ) 1 2 3 4 5 ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Plant 11 4862 442 6.57 1.8e-07 *** ## Residuals 72 4845 67 ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 1 2 fit <- aov ( uptake ~ Plant * Type , data = CO2 ) # same thing fit 1 2 3 4 5 6 7 8 9 10 11 ## Call: ## aov(formula = uptake ~ Plant * Type, data = CO2) ## ## Terms: ## Plant Residuals ## Sum of Squares 4862 4845 ## Deg. of Freedom 11 72 ## ## Residual standard error: 8.2 ## 12 out of 24 effects not estimable ## Estimated effects are balanced 1 summary ( fit ) 1 2 3 4 5 ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Plant 11 4862 442 6.57 1.8e-07 *** ## Residuals 72 4845 67 ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 1 2 3 # Analysis of Covariance fit <- aov ( uptake ~ uptake + conc , data = CO2 ) fit 1 2 3 4 5 6 7 8 9 10 ## Call: ## aov(formula = uptake ~ uptake + conc, data = CO2) ## ## Terms: ## conc Residuals ## Sum of Squares 2285 7422 ## Deg. of Freedom 1 82 ## ## Residual standard error: 9.51 ## Estimated effects may be unbalanced 1 summary ( fit ) 1 2 3 4 5 ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## conc 1 2285 2285 25.2 2.9e-06 *** ## Residuals 82 7422 91 ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1","title":"ANOVA"},{"location":"a_hands-on_introduction_to_statistics_with_r_notes/#evaluate-model-effects","text":"Type I sequential SS: A+B and B+A will produce different results! Use the drop1 to produce the familiar Type III results; compare each term with the full model. 1 2 3 # display Type I ANOVA table fit1 <- aov ( uptake ~ Plant + Type , data = CO2 ) summary ( fit1 ) 1 2 3 4 5 ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Plant 11 4862 442 6.57 1.8e-07 *** ## Residuals 72 4845 67 ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 1 2 3 # display Type I ANOVA table fit2 <- aov ( uptake ~ Type + Plant , data = CO2 ) summary ( fit2 ) 1 2 3 4 5 6 ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Type 1 3366 3366 50.02 8.1e-10 *** ## Plant 10 1497 150 2.22 0.026 * ## Residuals 72 4845 67 ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 1 2 # display type III SS and F tests drop1 ( fit1 , ~ ., test = \"F\" ) 1 2 3 4 5 6 7 8 9 10 ## Single term deletions ## ## Model: ## uptake ~ Plant + Type ## Df Sum of Sq RSS AIC F value Pr(&gt;F) ## &lt;none&gt; 4845 365 ## Plant 10 1497 6341 367 2.22 0.026 * ## Type 0 0 4845 365 ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 1 drop1 ( fit2 , ~ ., test = \"F\" ) 1 2 3 4 5 6 7 8 9 10 ## Single term deletions ## ## Model: ## uptake ~ Type + Plant ## Df Sum of Sq RSS AIC F value Pr(&gt;F) ## &lt;none&gt; 4845 365 ## Type 0 0 4845 365 ## Plant 10 1497 6341 367 2.22 0.026 * ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1","title":"Evaluate model effects"},{"location":"a_hands-on_introduction_to_statistics_with_r_notes/#compare-nested-models-directly","text":"1 2 3 4 fit1 <- aov ( uptake ~ Plant + Type , data = CO2 ) fit2 <- aov ( uptake ~ Plant , data = CO2 ) anova ( fit1 , fit2 ) 1 2 3 4 5 6 7 ## Analysis of Variance Table ## ## Model 1: uptake ~ Plant + Type ## Model 2: uptake ~ Plant ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 72 4845 ## 2 72 4845 0 0 1 2 3 4 fit2 <- aov ( uptake ~ Plant + Type , data = CO2 ) fit1 <- aov ( uptake ~ Plant , data = CO2 ) anova ( fit1 , fit2 ) 1 2 3 4 5 6 7 ## Analysis of Variance Table ## ## Model 1: uptake ~ Plant ## Model 2: uptake ~ Plant + Type ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 72 4845 ## 2 72 4845 0 0","title":"Compare nested models directly"},{"location":"a_hands-on_introduction_to_statistics_with_r_notes/#multiple-comparisons","text":"Tukey HSD tests for post hoc comparisons on each factor in the model. Factors as an option. Type I SS. 1 2 3 4 5 # model fit <- aov ( uptake ~ Plant + Type , data = CO2 ) # Tukey honestly significant differences TukeyHSD ( fit ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 ## Tukey multiple comparisons of means ## 95% family-wise confidence level ## ## Fit: aov(formula = uptake ~ Plant + Type, data = CO2) ## ## $Plant ## diff lwr upr p adj ## Qn2-Qn1 1.929 -12.88 16.739 1.000 ## Qn3-Qn1 4.386 -10.42 19.196 0.997 ## Qc1-Qn1 -3.257 -18.07 11.553 1.000 ## Qc3-Qn1 -0.643 -15.45 14.168 1.000 ## Qc2-Qn1 -0.529 -15.34 14.282 1.000 ## Mn3-Qn1 -9.114 -23.92 5.696 0.639 ## Mn2-Qn1 -5.886 -20.70 8.925 0.970 ## Mn1-Qn1 -6.829 -21.64 7.982 0.918 ## Mc2-Qn1 -21.086 -35.90 -6.275 0.000 ## Mc3-Qn1 -15.929 -30.74 -1.118 0.024 ## Mc1-Qn1 -15.229 -30.04 -0.418 0.038 ## Qn3-Qn2 2.457 -12.35 17.268 1.000 ## Qc1-Qn2 -5.186 -20.00 9.625 0.989 ## Qc3-Qn2 -2.571 -17.38 12.239 1.000 ## Qc2-Qn2 -2.457 -17.27 12.353 1.000 ## Mn3-Qn2 -11.043 -25.85 3.768 0.346 ## Mn2-Qn2 -7.814 -22.62 6.996 0.822 ## Mn1-Qn2 -8.757 -23.57 6.053 0.694 ## Mc2-Qn2 -23.014 -37.82 -8.204 0.000 ## Mc3-Qn2 -17.857 -32.67 -3.047 0.006 ## Mc1-Qn2 -17.157 -31.97 -2.347 0.010 ## Qc1-Qn3 -7.643 -22.45 7.168 0.842 ## Qc3-Qn3 -5.029 -19.84 9.782 0.991 ## Qc2-Qn3 -4.914 -19.72 9.896 0.993 ## Mn3-Qn3 -13.500 -28.31 1.311 0.108 ## Mn2-Qn3 -10.271 -25.08 4.539 0.457 ## Mn1-Qn3 -11.214 -26.02 3.596 0.323 ## Mc2-Qn3 -25.471 -40.28 -10.661 0.000 ## Mc3-Qn3 -20.314 -35.12 -5.504 0.001 ## Mc1-Qn3 -19.614 -34.42 -4.804 0.002 ## Qc3-Qc1 2.614 -12.20 17.425 1.000 ## Qc2-Qc1 2.729 -12.08 17.539 1.000 ## Mn3-Qc1 -5.857 -20.67 8.953 0.971 ## Mn2-Qc1 -2.629 -17.44 12.182 1.000 ## Mn1-Qc1 -3.571 -18.38 11.239 1.000 ## Mc2-Qc1 -17.829 -32.64 -3.018 0.006 ## Mc3-Qc1 -12.671 -27.48 2.139 0.167 ## Mc1-Qc1 -11.971 -26.78 2.839 0.233 ## Qc2-Qc3 0.114 -14.70 14.925 1.000 ## Mn3-Qc3 -8.471 -23.28 6.339 0.735 ## Mn2-Qc3 -5.243 -20.05 9.568 0.988 ## Mn1-Qc3 -6.186 -21.00 8.625 0.958 ## Mc2-Qc3 -20.443 -35.25 -5.632 0.001 ## Mc3-Qc3 -15.286 -30.10 -0.475 0.037 ## Mc1-Qc3 -14.586 -29.40 0.225 0.057 ## Mn3-Qc2 -8.586 -23.40 6.225 0.719 ## Mn2-Qc2 -5.357 -20.17 9.453 0.985 ## Mn1-Qc2 -6.300 -21.11 8.511 0.952 ## Mc2-Qc2 -20.557 -35.37 -5.747 0.001 ## Mc3-Qc2 -15.400 -30.21 -0.589 0.034 ## Mc1-Qc2 -14.700 -29.51 0.111 0.054 ## Mn2-Mn3 3.229 -11.58 18.039 1.000 ## Mn1-Mn3 2.286 -12.52 17.096 1.000 ## Mc2-Mn3 -11.971 -26.78 2.839 0.233 ## Mc3-Mn3 -6.814 -21.62 7.996 0.919 ## Mc1-Mn3 -6.114 -20.92 8.696 0.961 ## Mn1-Mn2 -0.943 -15.75 13.868 1.000 ## Mc2-Mn2 -15.200 -30.01 -0.389 0.039 ## Mc3-Mn2 -10.043 -24.85 4.768 0.493 ## Mc1-Mn2 -9.343 -24.15 5.468 0.603 ## Mc2-Mn1 -14.257 -29.07 0.553 0.070 ## Mc3-Mn1 -9.100 -23.91 5.711 0.641 ## Mc1-Mn1 -8.400 -23.21 6.411 0.746 ## Mc3-Mc2 5.157 -9.65 19.968 0.989 ## Mc1-Mc2 5.857 -8.95 20.668 0.971 ## Mc1-Mc3 0.700 -14.11 15.511 1.000","title":"Multiple comparisons"},{"location":"a_hands-on_introduction_to_statistics_with_r_notes/#visualizing-results","text":"1 2 3 4 5 6 7 8 9 10 # two-way interaction plot attach ( mtcars ) gear <- factor ( gear ) cyl <- factor ( cyl ) # two-way interactions library ( car ) interaction.plot ( cyl , gear , mpg , type = \"b\" , col = c ( 1 : 3 ), leg.bty = \"o\" , leg.bg = \"beige\" , lwd = 2 , pch = c ( 18 , 24 , 22 ), xlab = \"Number of Cylinders\" , ylab = \"Mean Miles Per Gallon\" , main = \"Interaction Plot\" ) 1 2 3 4 # mean plots for single factors, and includes confidence intervals library ( gplots ) plotmeans ( mpg ~ cyl , xlab = \"Number of Cylinders\" , ylab = \"Miles Per Gallon\" , main = \"Mean Plot\\nwith 95% CI\" ) 1 detach ( mtcars )","title":"Visualizing results"},{"location":"a_hands-on_introduction_to_statistics_with_r_notes/#manova","text":"Multivariate analysis of variance (MANOVA) With more than one dependent variable Y. We can run several ANOVA over different Y, or one MANOVA with one Y built with several variables. 1 head ( longley , 3 ) 1 2 3 4 ## GNP.deflator GNP Unemployed Armed.Forces Population Year Employed ## 1947 83.0 234 236 159 108 1947 60.3 ## 1948 88.5 259 232 146 109 1948 61.1 ## 1949 88.2 258 368 162 110 1949 60.2 1 2 3 4 5 6 7 8 attach ( longley ) # 2x2 factorial MANOVA with 3 dependent variables, Y Y <- cbind ( Unemployed , Armed.Forces , Employed ) # Y fit <- manova ( Y ~ Population * GNP ) # Y ~ X # display type I SS summary ( fit , test = \"Pillai\" ) 1 2 3 4 5 6 7 ## Df Pillai approx F num Df den Df Pr(&gt;F) ## Population 1 0.997 1074 3 10 7.7e-13 *** ## GNP 1 0.888 26 3 10 4.6e-05 *** ## Population:GNP 1 0.870 22 3 10 9.4e-05 *** ## Residuals 12 ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 1 2 3 4 # test = \"Wilks\", \"Hotelling-Lawley\", and \"Roy\" # display univariate statistics summary.aov ( fit ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 ## Response Unemployed : ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Population 1 61739 61739 45.92 2e-05 *** ## GNP 1 42841 42841 31.87 0.00011 *** ## Population:GNP 1 10270 10270 7.64 0.01715 * ## Residuals 12 16133 1344 ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## Response Armed.Forces : ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Population 1 9647 9647 4.25 0.0617 . ## GNP 1 29772 29772 13.10 0.0035 ** ## Population:GNP 1 5958 5958 2.62 0.1314 ## Residuals 12 27268 2272 ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## Response Employed : ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Population 1 170.6 170.6 546.45 2.2e-11 *** ## GNP 1 10.5 10.5 33.60 8.5e-05 *** ## Population:GNP 1 0.1 0.1 0.41 0.54 ## Residuals 12 3.7 0.3 ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 1 2 3 4 5 # display type III SS fit1 <- manova ( Y ~ Population * GNP ) fit2 <- manova ( Y ~ GNP * Population ) # type III GNP effect summary ( fit1 ) 1 2 3 4 5 6 7 ## Df Pillai approx F num Df den Df Pr(&gt;F) ## Population 1 0.997 1074 3 10 7.7e-13 *** ## GNP 1 0.888 26 3 10 4.6e-05 *** ## Population:GNP 1 0.870 22 3 10 9.4e-05 *** ## Residuals 12 ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 1 2 # type III Population effect summary ( fit2 ) 1 2 3 4 5 6 7 ## Df Pillai approx F num Df den Df Pr(&gt;F) ## GNP 1 0.997 1088 3 10 7.2e-13 *** ## Population 1 0.786 12 3 10 0.0011 ** ## GNP:Population 1 0.870 22 3 10 9.4e-05 *** ## Residuals 12 ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 Multiple comparisons TukeyHSD and plot do work with a MANOVA fit. Run each dependent variable separately to obtain them\u2026 or proceed with ANOVA on each dependent variable.","title":"MANOVA"},{"location":"a_hands-on_introduction_to_statistics_with_r_notes/#going-further_1","text":"Package lme4 has excellent facilities for fitting linear and generalized linear mixed-effects models.","title":"Going further"},{"location":"a_hands-on_introduction_to_statistics_with_r_notes/#manova-assumptions","text":"","title":"(M)ANOVA Assumptions"},{"location":"a_hands-on_introduction_to_statistics_with_r_notes/#outliers_1","text":"Outliers can severely affect normality and homogeneity of variance. 1 2 # dataset head ( mtcars , 3 ) 1 2 3 4 ## mpg cyl disp hp drat wt qsec vs am gear carb ## Mazda RX4 21.0 6 160 110 3.90 2.62 16.5 0 1 4 4 ## Mazda RX4 Wag 21.0 6 160 110 3.90 2.88 17.0 0 1 4 4 ## Datsun 710 22.8 4 108 93 3.85 2.32 18.6 1 1 4 1 1 2 3 4 5 6 # detect outliers # ordered squared robust Mahalanobis distances of the observations against the empirical distribution function of the MD2i library ( mvoutlier ) outliers <- aq.plot ( mtcars [c ( \"mpg\" , \"disp\" , \"hp\" , \"drat\" , \"wt\" , \"qsec\" ) ] ) 1 2 ## Projection to the first and second robust principal components. ## Proportion of total variation (explained variance): 0.974 1 outliers # show list of outliers 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 ## $outliers ## Mazda RX4 Mazda RX4 Wag Datsun 710 ## FALSE FALSE FALSE ## Hornet 4 Drive Hornet Sportabout Valiant ## FALSE FALSE FALSE ## Duster 360 Merc 240D Merc 230 ## TRUE TRUE TRUE ## Merc 280 Merc 280C Merc 450SE ## FALSE FALSE FALSE ## Merc 450SL Merc 450SLC Cadillac Fleetwood ## FALSE FALSE FALSE ## Lincoln Continental Chrysler Imperial Fiat 128 ## FALSE FALSE TRUE ## Honda Civic Toyota Corolla Toyota Corona ## FALSE FALSE FALSE ## Dodge Challenger AMC Javelin Camaro Z28 ## FALSE FALSE TRUE ## Pontiac Firebird Fiat X1-9 Porsche 914-2 ## FALSE FALSE FALSE ## Lotus Europa Ford Pantera L Ferrari Dino ## FALSE TRUE FALSE ## Maserati Bora Volvo 142E ## TRUE FALSE 1 par ( mfrow = c ( 1 , 1 ))","title":"Outliers"},{"location":"a_hands-on_introduction_to_statistics_with_r_notes/#univariate-normality","text":"1 2 3 # Q-Q plot qqnorm ( mtcars $ mpg ) qqline ( mtcars $ mpg ) 1 2 # Shapiro-Wilk test of normality shapiro.test ( mtcars $ mpg ) 1 2 3 4 5 ## ## Shapiro-Wilk normality test ## ## data: mtcars$mpg ## W = 0.9, p-value = 0.1 1 2 3 4 library ( nortest ) # Anderson-Darling test for normality ad.test ( mtcars $ mpg ) 1 2 3 4 5 ## ## Anderson-Darling normality test ## ## data: mtcars$mpg ## A = 0.6, p-value = 0.1 1 2 # Cramer-von Mises test for normality cvm.test ( mtcars $ mpg ) 1 2 3 4 5 ## ## Cramer-von Mises normality test ## ## data: mtcars$mpg ## W = 0.09, p-value = 0.2 1 2 # Lilliefors (Kolmogorov-Smirnov) test for normality lillie.test ( mtcars $ mpg ) 1 2 3 4 5 ## ## Lilliefors (Kolmogorov-Smirnov) normality test ## ## data: mtcars$mpg ## D = 0.1, p-value = 0.2 1 2 # Pearson chi-square test for normality pearson.test ( mtcars $ mpg ) 1 2 3 4 5 ## ## Pearson chi-square normality test ## ## data: mtcars$mpg ## P = 8, p-value = 0.2 1 2 # Shapiro-Francia test for normality sf.test ( mtcars $ mpg ) 1 2 3 4 5 ## ## Shapiro-Francia normality test ## ## data: mtcars$mpg ## W = 1, p-value = 0.1","title":"Univariate normality"},{"location":"a_hands-on_introduction_to_statistics_with_r_notes/#multivariate-normality","text":"1 head ( EuStockMarkets , 3 ) 1 2 3 4 ## DAX SMI CAC FTSE ## [1,] 1629 1678 1773 2444 ## [2,] 1614 1688 1750 2460 ## [3,] 1607 1679 1718 2448 1 2 3 4 library ( mvnormtest ) # Shapiro-Wilk test for multivariate normality of numeric matrix mshapiro.test ( t ( EuStockMarkets )) 1 2 3 4 5 ## ## Shapiro-Wilk normality test ## ## data: Z ## W = 0.9, p-value &lt;2e-16 With a p x 1 multivariate normal random vector x vector, the squared Mahalanobis distance between x and ?? is going to be chi-square distributed with p degrees of freedom. 1 2 3 4 5 6 7 8 9 # graphical assessment of multivariate normality x <- as.matrix ( EuStockMarkets ) # n x p numeric matrix center <- colMeans ( x ) # centroid n <- nrow ( x ); p <- ncol ( x ) cov <- cov ( x ) d <- mahalanobis ( x , center , cov ) # distances qqplot ( qchisq ( ppoints ( n ), df = p ), d , main = \"QQ Plot Assessing Multivariate Normality\" , ylab = \"Mahalanobis D2\" ) abline ( a = 0 , b = 1 )","title":"Multivariate normality"},{"location":"a_hands-on_introduction_to_statistics_with_r_notes/#heteroscedasticity_1","text":"Nonconstant error variance or non-homogeneity of variances. 1 2 # dataset head ( mtcars , 3 ) 1 2 3 4 ## mpg cyl disp hp drat wt qsec vs am gear carb ## Mazda RX4 21.0 6 160 110 3.90 2.62 16.5 0 1 4 4 ## Mazda RX4 Wag 21.0 6 160 110 3.90 2.88 17.0 0 1 4 4 ## Datsun 710 22.8 4 108 93 3.85 2.32 18.6 1 1 4 1 1 2 3 # y is a numeric variable and G is the grouping variable # Bartlett parametric test of homogeneity of variances bartlett.test ( mpg ~ cyl , data = mtcars ) 1 2 3 4 5 ## ## Bartlett test of homogeneity of variances ## ## data: mpg by cyl ## Bartlett's K-squared = 8, df = 2, p-value = 0.02 1 2 # Figner-Killeen non-parametric test of homogeneity of variances fligner.test ( mpg ~ cyl , data = mtcars ) 1 2 3 4 5 ## ## Fligner-Killeen test of homogeneity of variances ## ## data: mpg by cyl ## Fligner-Killeen:med chi-squared = 7, df = 2, p-value = 0.03 1 2 3 4 5 library ( HH ) # y is numeric and G is a grouping factor # G must be of type factor hov ( mpg ~ factor ( cyl ), data = mtcars ) 1 2 3 4 5 6 ## ## hov: Brown-Forsyth ## ## data: mpg ## F = 6, df:factor(cyl) = 2, df:Residuals = 30, p-value = 0.009 ## alternative hypothesis: variances are not identical 1 2 3 # homogeneity of variance plot # graphic test of homogeneity of variances based on Brown-Forsyth hovPlot ( mpg ~ factor ( cyl ), data = mtcars ) Non-homogeneity of covariance matrices 1 2 # dataset head ( iris , 3 ) 1 2 3 4 ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3.0 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa 1 library ( biotools ) 1 2 ## --- ## biotools version 3.0 1 2 3 # Box's M test # very sensitive to violations of normality, leading to rejection in most typical cases boxM ( iris[ , -5 ] , iris[ , 5 ] ) 1 2 3 4 5 ## ## Box's M-test for Homogeneity of Covariance Matrices ## ## data: iris[, -5] ## Chi-Sq (approx.) = 100, df = 20, p-value &lt;2e-16","title":"Heteroscedasticity"},{"location":"a_hands-on_introduction_to_statistics_with_r_notes/#resampling-statistics","text":"","title":"Resampling Statistics"},{"location":"a_hands-on_introduction_to_statistics_with_r_notes/#independent-k-sample-location-tests","text":"1 2 # dataset head ( mtcars , 3 ) 1 2 3 4 ## mpg cyl disp hp drat wt qsec vs am gear carb ## Mazda RX4 21.0 6 160 110 3.90 2.62 16.5 0 1 4 4 ## Mazda RX4 Wag 21.0 6 160 110 3.90 2.88 17.0 0 1 4 4 ## Datsun 710 22.8 4 108 93 3.85 2.32 18.6 1 1 4 1 1 2 3 4 5 6 # exact Wilcoxon Mann Whitney rank sum test # re-randomization or permutation based statistical tests # where y is numeric and A is a binary factor library ( coin ) wilcox_test ( mpg ~ factor ( am ), data = mtcars , distribution = \"exact\" ) 1 2 3 4 5 6 ## ## Exact Wilcoxon-Mann-Whitney Test ## ## data: mpg by factor(am) (0, 1) ## Z = -3, p-value = 0.001 ## alternative hypothesis: true mu is not equal to 0 1 # lower case letters represent numerical variables and upper case letters represent categorical factors Monte-Carlo simulations are available for all tests. Exact tests are available for 2 group procedures. These tests do not assume random sampling from well-defined populations. They can be a reasonable alternative to classical procedures when test assumptions can not be met. 1 2 3 4 5 6 # one-way permutation test based on 9999 Monte-Carlo # resamplings # y is numeric and A is a categorical factor library ( coin ) oneway_test ( mpg ~ factor ( am ), data = mtcars , distribution = approximate ( B = 9999 )) 1 2 3 4 5 6 ## ## Approximative Two-Sample Fisher-Pitman Permutation Test ## ## data: mpg by factor(am) (0, 1) ## Z = -3, p-value = 5e-04 ## alternative hypothesis: true mu is not equal to 0","title":"Independent k-sample location tests"},{"location":"a_hands-on_introduction_to_statistics_with_r_notes/#symmetry-of-a-response-for-repeated-measurements","text":"1 2 3 4 5 # exact Wilcoxon signed rank test # where y1 and y2 are repeated measures library ( coin ) wilcoxsign_test ( mpg ~ factor ( am ), data = mtcars , distribution = \"exact\" ) 1 2 3 4 5 6 7 ## ## Exact Wilcoxon-Pratt Signed-Rank Test ## ## data: y by x (pos, neg) ## stratified by block ## Z = 5, p-value = 5e-10 ## alternative hypothesis: true mu is not equal to 0 1 2 3 4 # Freidman Test based on 9999 Monte-Carlo resamplings. # y is numeric, A is a grouping factor, and B is a # blocking factor friedman_test ( y ~ A | B , data = mydata , distribution = approximate ( B = 9999 ))","title":"Symmetry of a response for repeated measurements"},{"location":"a_hands-on_introduction_to_statistics_with_r_notes/#independence-of-two-numeric-variables","text":"1 2 3 4 5 # Spearman Test of independence based on 9999 Monte-Carlo # resamplings. x and y are numeric variables library ( coin ) spearman_test ( mpg ~ am , data = mtcars , distribution = approximate ( B = 9999 )) 1 2 3 4 5 6 ## ## Approximative Spearman Correlation Test ## ## data: mpg by am ## Z = 3, p-value = 0.001 ## alternative hypothesis: true rho is not equal to 0","title":"Independence of two numeric variables"},{"location":"a_hands-on_introduction_to_statistics_with_r_notes/#independence-in-contingency-tables","text":"1 2 # dataset head ( CO2 ) 1 2 3 4 5 6 7 8 ## Grouped Data: uptake ~ conc | Plant ## Plant Type Treatment conc uptake ## 1 Qn1 Quebec nonchilled 95 16.0 ## 2 Qn1 Quebec nonchilled 175 30.4 ## 3 Qn1 Quebec nonchilled 250 34.8 ## 4 Qn1 Quebec nonchilled 350 37.2 ## 5 Qn1 Quebec nonchilled 500 35.3 ## 6 Qn1 Quebec nonchilled 675 39.2 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 cases <- c ( 4 , 2 , 3 , 1 , 59 ) n <- sum ( cases ) cochran <- data.frame ( diphtheria = factor ( unlist ( rep ( list ( c ( 1 , 1 , 1 , 1 ), c ( 1 , 1 , 0 , 1 ), c ( 0 , 1 , 1 , 1 ), c ( 0 , 1 , 0 , 1 ), c ( 0 , 0 , 0 , 0 )), cases )) ), media = factor ( rep ( LETTERS [1 : 4 ] , n )), case = factor ( rep ( seq_len ( n ), each = 4 )) ) head ( cochran ) 1 2 3 4 5 6 7 ## diphtheria media case ## 1 1 A 1 ## 2 1 B 1 ## 3 1 C 1 ## 4 1 D 1 ## 5 1 A 2 ## 6 1 B 2 1 2 3 4 5 # independence in 2-way contingency table based on # 9999 Monte-Carlo resamplings. A and B are factors library ( coin ) chisq_test ( Plant ~ Type , data = CO2 , distribution = approximate ( B = 9999 )) 1 2 3 4 5 6 ## ## Approximative Linear-by-Linear Association Test ## ## data: Plant (ordered) by Type (Quebec, Mississippi) ## Z = -8, p-value &lt;2e-16 ## alternative hypothesis: two.sided 1 2 3 4 # Cochran-Mantel-Haenzsel Test of 3-way Contingency Table # based on 9999 Monte-Carlo resamplings. A, B, are factors # and C is a stratefying factor mh_test ( diphtheria ~ media | case , data = cochran , distribution = approximate ( B = 9999 )) 1 2 3 4 5 6 7 ## ## Approximative Marginal Homogeneity Test ## ## data: diphtheria by ## media (A, B, C, D) ## stratified by case ## chi-squared = 8, p-value = 0.05 1 2 3 4 5 6 # linear by linear association test based on 9999 # Monte-Carlo resamplings # A and B are ordered factors library ( coin ) lbl_test ( Plant ~ Type , data = CO2 , distribution = approximate ( B = 9999 )) 1 2 3 4 5 6 ## ## Approximative Linear-by-Linear Association Test ## ## data: Plant (ordered) by Type (Quebec, Mississippi) ## Z = -8, p-value &lt;2e-16 ## alternative hypothesis: two.sided Many other univariate and multivariate tests are possible using the functions in the coin package.","title":"Independence in contingency tables"},{"location":"basics_cs/","text":"Basics \u00b6 Base R . PDF. Control Structures . PDF. Data Import . PDF. Tidy Data . PDF only. Advanced \u00b6 Advanced R . PDF. Edition & Reporting \u00b6 RMarkdown \u00b6 R Markdown Reference Guide . PDF only. syntax, chunk options, pandoc options, slide formats, LaTeX options R Markdown . PDF. RStudio \u00b6 RStudio . PDF.","title":"Basics Cheat Sheets"},{"location":"basics_cs/#basics","text":"Base R . PDF. Control Structures . PDF. Data Import . PDF. Tidy Data . PDF only.","title":"Basics"},{"location":"basics_cs/#advanced","text":"Advanced R . PDF.","title":"Advanced"},{"location":"basics_cs/#edition-reporting","text":"","title":"Edition &amp; Reporting"},{"location":"basics_cs/#rmarkdown","text":"R Markdown Reference Guide . PDF only. syntax, chunk options, pandoc options, slide formats, LaTeX options R Markdown . PDF.","title":"RMarkdown"},{"location":"basics_cs/#rstudio","text":"RStudio . PDF.","title":"RStudio"},{"location":"big_data_analysis_with_revolution_r_enterprise/","text":"Foreword Output options: the \u2018tango\u2019 syntax and the \u2018readable\u2019 theme. Snippets only. Documentation \u00b6 Microsoft R Server: previously called Revolution R Enterprise for Hadoop, Linux and Teradata and included new Microsoft enterprise support and purchasing options. Microsoft R Server was further made available to students through the Microsoft DreamSpark programme. Microsoft R Server Developer Edition: a gratis version for developers that with a feature set akin to the commercial edition. Microsoft Data Science Virtual Machine: an analytics tool developed by the Revolution Analytics division premiered in January 2015. Microsoft R Open: a rebranded version of Revolution R Open. Introduction \u00b6 Importing data with rxImport function 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 # Declare the file paths for the csv and xdf files # find the path or directory where the file is, load the path variable myAirlineCsv <- file.path ( rxGetOption ( 'sampleDataDir' ), '2007_subset.csv' ) # fin the data in this directory and load the data variable myAirlineXdf <- '2007_subset.xdf' # Use rxImport to import the data into xdf format # rxImport(inData = myAirlineCsv, outFile = myAirlineXdf, overwrite = TRUE) # or # function within a function for more stats system.time ( rxImport ( inData = myAirlineCsv , outFile = myAirlineXdf , overwrite = TRUE )) list.files () Functions for summarizing data 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 # Get basic information about your data rxGetInfo ( data = myAirlineXdf , getVarInfo = TRUE , numRows = 10 ) # Summarize the variables corresponding to actual elapsed time, time in the air, departure delay, flight Distance rxSummary ( formula = ~ ActualElapsedTime + AirTime + DepDelay + Distance , data = myAirlineXdf ) # Histogram of departure delays rxHistogram ( formula = ~ DepDelay , data = myAirlineXdf ) # Use parameters similar to a regular histogram to zero in on the interesting area rxHistogram ( formula = ~ DepDelay , data = myAirlineXdf , xAxisMinMax = c ( -100 , 400 ), numBreaks = 500 , xNumTicks = 10 ) Creating new variables using rxDataStep 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 # Calculate an additional variable: airspeed (distance traveled / time in the air) rxDataStep ( inData = myAirlineXdf , outFile = myAirlineXdf , varsToKeep = c ( 'Distance' , 'AirTime' ), transforms = list ( airSpeed = Distance / Airtime ), append = 'cols' , overwrite = TRUE ) # Get Variable Information for airspeed rxGetInfo ( data = myAirlineXdf , getVarInfo = TRUE , varsToKeep = 'airSpeed' ) # Summary for the airspeed variable rxSummary ( ~ airSpeed , data = myAirlineXdf ) # Construct a histogtam for airspeed # We can use the xAxisMinMax argument to limit the X-axis rxHistogram ( ~ airSpeed , data = myAirlineXdf ) rxHistogram ( ~ airSpeed , data = myAirlineXdf , xNumTicks = 10 , numBreaks = 1500 , xAxisMinMax = c ( 0 , 12 )) Transforming variables using rxDataStep 1 2 3 4 5 6 7 8 9 10 # Conversion to miles per hour rxDataStep ( inData = myAirlineXdf , outFile = myAirlineXdf , varsToKeep = c ( 'airSpeed' ), transforms = list ( airSpeed = airSpeed * 60 ), overwrite = TRUE ) # Histogram for airspeed after conversion rxHistogram ( ~ airSpeed , data = myAirlineXdf ) Correlations 1 2 3 4 # Correlation for departure delay, arrival delay, and air speed rxCor ( formula = ~ DepDelay + ArrDelay + airSpeed , data = myAirlineXdf , rowSelection = ( airSpeed > 50 ) & ( airSpeed < 800 )) Linear regression 1 2 3 4 5 6 # Regression for airSpeed based on departure delay myLMobj <- rxLinMod ( formula = airSpeed ~ DepDelay , data = myAirlineXdf , rowSelection = ( airSpeed > 50 ) & ( airSpeed < 800 )) summary ( myLMobj ) Data Exploration \u00b6 RevoScaleR options 1 2 3 4 5 6 7 8 9 10 11 # Extract the names of the possible options names ( rxOptions ()) # Extract the sample data directory rxGetOption ( 'sampleDataDir' ) # View the current value of the reportProgress option rxGetOption ( 'reportProgress' ) # Set the value of the reportProgress option to 0 rxOptions ( reportProgress = 0 ) Import and explore Dow Jones data 1 2 3 4 5 # Set up the variable that has the address of the relevant data file djiXdf <- file.path ( rxGetOption ( 'sampleDataDir' ), 'DJIAdaily.xdf' ) # Get information about that dataset rxGetInfo ( djiXdf , getVarInfo = TRUE ) Extracting meta data about a variable using rxGetVarInfo 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 # Get variable information for the dataset djiVarInfo <- rxGetVarInfo ( djiXdf ) names ( djiVarInfo ) # Extract information about the closing cost variable ( closeVarInfo <- djiVarInfo[[ 'Close' ]] ) # Get the class of the closeVarInfo object class ( closeVarInfo ) # Examine the structure of the closeVarInfo object str ( closeVarInfo ) # Extract the global maximum of the closing cost variable closeMax <- closeVarInfo[[ 'high' ]] Summarizing variables with rxSummary 1 2 3 4 5 6 7 8 9 10 11 12 # Basic summary statistics rxSummary ( ~ DayOfWeek + Close + Volume , data = djiXdf ) # Frequency weighted rxSummary ( ~ DayOfWeek + Close , data = djiXdf , fweights = 'Volume' ) # Basic frequency count rxCrossTabs ( ~ DayOfWeek , data = djiXdf ) Exploring a distribution with rxHistogram 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 # Numeric Variables rxHistogram ( ~ Close , data = djiXdf ) # Categorical Variable rxHistogram ( ~ DayOfWeek , data = djiXdf ) # Different panels for different days of the week rxHistogram ( ~ Close | DayOfWeek , data = djiXdf ) # Numeric Variables with a frequency weighting rxHistogram ( ~ Close , data = djiXdf , fweights = 'Volume' ) Plotting bivariate relationships with rxLinePlot 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 # Simple bivariate line plot rxLinePlot ( Close ~ DaysSince1928 , data = djiXdf ) # Using different panels for different days of the week rxLinePlot ( Close ~ DaysSince1928 | DayOfWeek , data = djiXdf ) # Using different groups rxLinePlot ( Close ~ DaysSince1928 , groups = DayOfWeek , data = djiXdf ) # Simple bivariate line plot, after taking the log() of the ordinate (y) variable rxLinePlot ( log ( Close ) ~ DaysSince1928 , data = djiXdf ) Summarzing variables with rxCrossTabs 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 # Compute the the summed volume for each day of the week rxCrossTabs ( formula = Volume ~ DayOfWeek , data = djiXdf ) # Compute the the summed volume for each day of the week for each month rxCrossTabs ( formula = Volume ~ F ( Month ) : DayOfWeek , data = djiXdf ) # Compute the the average volume for each day of the week for each month rxCrossTabs ( formula = Volume ~ F ( Month ) : DayOfWeek , data = djiXdf , means = TRUE ) # Compute the the average closing price for each day of the week for each month, using volume as frequency weights rxCrossTabs ( formula = Close ~ F ( Month ) : DayOfWeek , data = djiXdf , means = TRUE , fweights = 'Volume' ) Summarzing variables with rxCube 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 # Compute the the summed volume for each day of the week rxCrossTabs ( Volume ~ DayOfWeek , data = djiXdf ) rxCube ( Volume ~ DayOfWeek , data = djiXdf , means = FALSE ) # Compute the the summed volume for each day of the week for each month rxCrossTabs ( Volume ~ F ( Month ) : DayOfWeek , data = djiXdf ) rxCube ( Volume ~ F ( Month ) : DayOfWeek , data = djiXdf , means = FALSE ) # Compute the the average volume for each day of the week for each month rxCube ( Volume ~ F ( Month ) : DayOfWeek , data = djiXdf ) # Compute the the average closing price for each day of the week for each month, using volume as frequency weights rxCube ( Close ~ DayOfWeek , data = djiXdf , fweights = 'Volume' ) Data Manipulation \u00b6 Using rxDataStep to transform data 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 # Get information on mortData rxGetInfo ( mortData ) ## Set up my personal copy of the data myMortData <- 'myMD.xdf' # Create the transform rxDataStep ( inData = mortData , outFile = myMortData , transforms = list ( highDebtRow = ccDebt > 8000 ), overwrite = TRUE ) #rxDataStep(inData = mortData, outFile = myMortData, transforms = list(highDebtRow = ccDebt > 8000)) # Get the variable information rxGetVarInfo ( myMortData ) # Get the proportion of values that are 1 rxSummary ( ~ highDebtRow , data = myMortData ) # Compute multiple transforms! rxDataStep ( inData = myMortData , outFile = myMortData , transforms = list ( newHouse = houseAge < 10 , ccsXhd = creditScore * highDebtRow ), append = 'cols' , overwrite = TRUE ) More complex transforms using transformFuncs 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 # Compute the summary statistics ( csSummary <- rxSummary ( ~ creditScore , data = mortData )) # Extract the mean and std. deviation meanCS <- csSummary $ sDataFrame $ Mean[1] sdCS <- csSummary $ sDataFrame $ StdDev[1] # Create a function to compute the scaled variable scaleCS <- function ( mylist ){ mylist[[ 'scaledCreditScore' ]] <- ( mylist[[ 'creditScore' ]] - myCenter ) / myScale return ( mylist ) } # Run it with rxDataStep (A above in B below) myMortData <- 'myMD.xdf' rxDataStep ( inData = mortData , outFile = myMortData , transformFunc = scaleCS , transformObjects = list ( myCenter = meanCS , myScale = sdCS )) # Check the new variable rxGetVarInfo ( myMortData ) rxSummary ( ~ scaledCreditScore , data = myMortData ) Data Analysis \u00b6 Preparing data for analysis: import 1 2 3 4 5 6 7 8 9 10 11 12 13 # Declare the file paths for the csv and xdf files myAirlineCsv <- file.path ( rxGetOption ( 'sampleDataDir' ), 'AirlineDemoSmall.csv' ) myAirlineXdf <- 'ADS.xdf' # Use rxImport to import the data into xdf format rxImport ( inData = myAirlineCsv , outFile = myAirlineXdf , overwrite = TRUE , colInfo = list ( DayOfWeek = list ( type = 'factor' , levels = c ( 'Monday' , 'Tuesday' , 'Wednesday' , 'Thursday' , 'Friday' , 'Saturday' , 'Sunday' )))) Preparing data Ffor analysis: exploration 1 2 3 4 5 6 7 # Summarize arrival delay for each day of the week rxSummary ( formula = ArrDelay ~ DayOfWeek , data = myAirlineXdf ) # Vizualize the arrival delay histogram rxHistogram ( formula = ~ ArrDelay , data = myAirlineXdf ) Construct a linear model 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 # predict arrival delay by day of the week myLM1 <- rxLinMod ( ArrDelay ~ DayOfWeek , data = myAirlineXdf ) # summarize the model smmary ( myLM1 ) # Use the transforms argument to create a factor variable associated with departure time 'on the fly,' # predict Arrival Delay by the interaction between Day of the week and that new factor variable myLM2 <- rxLinMod ( ArrDelay ~ DayOfWeek , data = myAirlineXdf , transforms = list ( catDepTime = cut ( CRSDepTime , breaks = seq ( from = 5 , to = 23 , by = 2 ))), cube = TRUE ) # summarize the model summary ( myLM2 ) Generating predictions and residuals 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 # Summarize model first summary ( myLM2 ) # Path to new dataset storing predictions myNewADS <- 'myNEWADS.xdf' # Generate predictions rxPredict ( modelObject = myLM2 , data = myAirlineXdf , outData = myNewADS , writeModelVars = TRUE ) # Get information on the new dataset rxGetInfo ( myNewADS , getVarInfo = TRUE ) # Generate residuals. rxPredict ( modelObject = myLM2 , data = myAirlineXdf , outData = myNewADS , writeModelVars = TRUE , computeResiduals = TRUE , overwrite = TRUE ) # Get information on the new dataset rxGetInfo ( myNewADS , getVarInfo = TRUE ) Logistic regression 1 2 3 4 5 6 7 8 9 10 # look at the meta data ls () rxGetInfo ( data = mortData , getVarInfo = TRUE ) # Construct the logit model logitModel <- rxLogit ( formula = default ~ houseAge + F ( year ) + ccDebt + creditScore + yearsEmploy , data = mortData ) # Summarize the result contained in logitModel summary ( logitModel Individual mortgage information 1 2 3 4 5 6 7 8 9 10 11 12 13 14 # Summarize the model summary ( logitModel ) # view the first few rows head ( newData ) # Make predictions dataWithPredictions <- rxPredict ( modelObject = logitModel , data = newData , outData = newData , type = 'response' ) # view the predictions dataWithPredictions Computing k-means with rxKmeans 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 # Examine the mortData dataset rxGetInfo ( mortData , getVarInfo = TRUE ) # Set up a path to a new xdf file myNewMortData = 'myMDwithKMeans.xdf' # Run k-means KMout <- rxKmeans ( formula = ~ ccDebt + creditScore + houseAge , data = mortData , outFile = myNewMortData , rowSelection = year == 2000 , numClusters = 4 , writeModelVars = TRUE ) print ( KMout ) # Examine the variables in the new dataset: rxGetInfo ( myNewMortData , getVarInfo = TRUE ) # Summarize the cluster variable: rxSummary ( ~ F ( .rxCluster ), data = myNewMortData ) # Read into memory 10% of the data: mydf <- rxXdfToDataFrame ( myNewMortData , rowSelection = randSamp == 1 , varsToDrop = 'year' , transforms = list ( randSamp = sample ( 10 , size = .rxNumRows , replace = TRUE ))) ## Visualize the clusters plot ( mydf[ -1 ] , col = mydf $ .rxCluster ) Create some decision trees 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 # regression tree regTreeOut <- rxDTree ( default ~ creditScore + ccDebt + yearsEmploy + houseAge , rowSelection = year == 2000 , data = mortData , maxdepth = 5 ) # print out the object print ( regTreeOut ) # plot a dendrogram, and add node labels plot ( rxAddInheritance ( regTreeOut )) text ( rxAddInheritance ( regTreeOut )) # Another visualization #library(RevoTreeView) #createTreeView(regTreeOut) # predict values myNewData = 'myNewMortData.xdf' rxPredict ( regTreeOut , data = mortData , outData = myNewData , writeModelVars = TRUE , predVarNames = 'default_RegPred' ) # visualize ROC curve rxRocCurve ( actualVarName = 'default' , predVarNames = 'default_RegPred' , data = myNewData )","title":"Big Data Analysis with Revolution R Enterprise"},{"location":"big_data_analysis_with_revolution_r_enterprise/#documentation","text":"Microsoft R Server: previously called Revolution R Enterprise for Hadoop, Linux and Teradata and included new Microsoft enterprise support and purchasing options. Microsoft R Server was further made available to students through the Microsoft DreamSpark programme. Microsoft R Server Developer Edition: a gratis version for developers that with a feature set akin to the commercial edition. Microsoft Data Science Virtual Machine: an analytics tool developed by the Revolution Analytics division premiered in January 2015. Microsoft R Open: a rebranded version of Revolution R Open.","title":"Documentation"},{"location":"big_data_analysis_with_revolution_r_enterprise/#introduction","text":"Importing data with rxImport function 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 # Declare the file paths for the csv and xdf files # find the path or directory where the file is, load the path variable myAirlineCsv <- file.path ( rxGetOption ( 'sampleDataDir' ), '2007_subset.csv' ) # fin the data in this directory and load the data variable myAirlineXdf <- '2007_subset.xdf' # Use rxImport to import the data into xdf format # rxImport(inData = myAirlineCsv, outFile = myAirlineXdf, overwrite = TRUE) # or # function within a function for more stats system.time ( rxImport ( inData = myAirlineCsv , outFile = myAirlineXdf , overwrite = TRUE )) list.files () Functions for summarizing data 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 # Get basic information about your data rxGetInfo ( data = myAirlineXdf , getVarInfo = TRUE , numRows = 10 ) # Summarize the variables corresponding to actual elapsed time, time in the air, departure delay, flight Distance rxSummary ( formula = ~ ActualElapsedTime + AirTime + DepDelay + Distance , data = myAirlineXdf ) # Histogram of departure delays rxHistogram ( formula = ~ DepDelay , data = myAirlineXdf ) # Use parameters similar to a regular histogram to zero in on the interesting area rxHistogram ( formula = ~ DepDelay , data = myAirlineXdf , xAxisMinMax = c ( -100 , 400 ), numBreaks = 500 , xNumTicks = 10 ) Creating new variables using rxDataStep 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 # Calculate an additional variable: airspeed (distance traveled / time in the air) rxDataStep ( inData = myAirlineXdf , outFile = myAirlineXdf , varsToKeep = c ( 'Distance' , 'AirTime' ), transforms = list ( airSpeed = Distance / Airtime ), append = 'cols' , overwrite = TRUE ) # Get Variable Information for airspeed rxGetInfo ( data = myAirlineXdf , getVarInfo = TRUE , varsToKeep = 'airSpeed' ) # Summary for the airspeed variable rxSummary ( ~ airSpeed , data = myAirlineXdf ) # Construct a histogtam for airspeed # We can use the xAxisMinMax argument to limit the X-axis rxHistogram ( ~ airSpeed , data = myAirlineXdf ) rxHistogram ( ~ airSpeed , data = myAirlineXdf , xNumTicks = 10 , numBreaks = 1500 , xAxisMinMax = c ( 0 , 12 )) Transforming variables using rxDataStep 1 2 3 4 5 6 7 8 9 10 # Conversion to miles per hour rxDataStep ( inData = myAirlineXdf , outFile = myAirlineXdf , varsToKeep = c ( 'airSpeed' ), transforms = list ( airSpeed = airSpeed * 60 ), overwrite = TRUE ) # Histogram for airspeed after conversion rxHistogram ( ~ airSpeed , data = myAirlineXdf ) Correlations 1 2 3 4 # Correlation for departure delay, arrival delay, and air speed rxCor ( formula = ~ DepDelay + ArrDelay + airSpeed , data = myAirlineXdf , rowSelection = ( airSpeed > 50 ) & ( airSpeed < 800 )) Linear regression 1 2 3 4 5 6 # Regression for airSpeed based on departure delay myLMobj <- rxLinMod ( formula = airSpeed ~ DepDelay , data = myAirlineXdf , rowSelection = ( airSpeed > 50 ) & ( airSpeed < 800 )) summary ( myLMobj )","title":"Introduction"},{"location":"big_data_analysis_with_revolution_r_enterprise/#data-exploration","text":"RevoScaleR options 1 2 3 4 5 6 7 8 9 10 11 # Extract the names of the possible options names ( rxOptions ()) # Extract the sample data directory rxGetOption ( 'sampleDataDir' ) # View the current value of the reportProgress option rxGetOption ( 'reportProgress' ) # Set the value of the reportProgress option to 0 rxOptions ( reportProgress = 0 ) Import and explore Dow Jones data 1 2 3 4 5 # Set up the variable that has the address of the relevant data file djiXdf <- file.path ( rxGetOption ( 'sampleDataDir' ), 'DJIAdaily.xdf' ) # Get information about that dataset rxGetInfo ( djiXdf , getVarInfo = TRUE ) Extracting meta data about a variable using rxGetVarInfo 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 # Get variable information for the dataset djiVarInfo <- rxGetVarInfo ( djiXdf ) names ( djiVarInfo ) # Extract information about the closing cost variable ( closeVarInfo <- djiVarInfo[[ 'Close' ]] ) # Get the class of the closeVarInfo object class ( closeVarInfo ) # Examine the structure of the closeVarInfo object str ( closeVarInfo ) # Extract the global maximum of the closing cost variable closeMax <- closeVarInfo[[ 'high' ]] Summarizing variables with rxSummary 1 2 3 4 5 6 7 8 9 10 11 12 # Basic summary statistics rxSummary ( ~ DayOfWeek + Close + Volume , data = djiXdf ) # Frequency weighted rxSummary ( ~ DayOfWeek + Close , data = djiXdf , fweights = 'Volume' ) # Basic frequency count rxCrossTabs ( ~ DayOfWeek , data = djiXdf ) Exploring a distribution with rxHistogram 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 # Numeric Variables rxHistogram ( ~ Close , data = djiXdf ) # Categorical Variable rxHistogram ( ~ DayOfWeek , data = djiXdf ) # Different panels for different days of the week rxHistogram ( ~ Close | DayOfWeek , data = djiXdf ) # Numeric Variables with a frequency weighting rxHistogram ( ~ Close , data = djiXdf , fweights = 'Volume' ) Plotting bivariate relationships with rxLinePlot 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 # Simple bivariate line plot rxLinePlot ( Close ~ DaysSince1928 , data = djiXdf ) # Using different panels for different days of the week rxLinePlot ( Close ~ DaysSince1928 | DayOfWeek , data = djiXdf ) # Using different groups rxLinePlot ( Close ~ DaysSince1928 , groups = DayOfWeek , data = djiXdf ) # Simple bivariate line plot, after taking the log() of the ordinate (y) variable rxLinePlot ( log ( Close ) ~ DaysSince1928 , data = djiXdf ) Summarzing variables with rxCrossTabs 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 # Compute the the summed volume for each day of the week rxCrossTabs ( formula = Volume ~ DayOfWeek , data = djiXdf ) # Compute the the summed volume for each day of the week for each month rxCrossTabs ( formula = Volume ~ F ( Month ) : DayOfWeek , data = djiXdf ) # Compute the the average volume for each day of the week for each month rxCrossTabs ( formula = Volume ~ F ( Month ) : DayOfWeek , data = djiXdf , means = TRUE ) # Compute the the average closing price for each day of the week for each month, using volume as frequency weights rxCrossTabs ( formula = Close ~ F ( Month ) : DayOfWeek , data = djiXdf , means = TRUE , fweights = 'Volume' ) Summarzing variables with rxCube 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 # Compute the the summed volume for each day of the week rxCrossTabs ( Volume ~ DayOfWeek , data = djiXdf ) rxCube ( Volume ~ DayOfWeek , data = djiXdf , means = FALSE ) # Compute the the summed volume for each day of the week for each month rxCrossTabs ( Volume ~ F ( Month ) : DayOfWeek , data = djiXdf ) rxCube ( Volume ~ F ( Month ) : DayOfWeek , data = djiXdf , means = FALSE ) # Compute the the average volume for each day of the week for each month rxCube ( Volume ~ F ( Month ) : DayOfWeek , data = djiXdf ) # Compute the the average closing price for each day of the week for each month, using volume as frequency weights rxCube ( Close ~ DayOfWeek , data = djiXdf , fweights = 'Volume' )","title":"Data Exploration"},{"location":"big_data_analysis_with_revolution_r_enterprise/#data-manipulation","text":"Using rxDataStep to transform data 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 # Get information on mortData rxGetInfo ( mortData ) ## Set up my personal copy of the data myMortData <- 'myMD.xdf' # Create the transform rxDataStep ( inData = mortData , outFile = myMortData , transforms = list ( highDebtRow = ccDebt > 8000 ), overwrite = TRUE ) #rxDataStep(inData = mortData, outFile = myMortData, transforms = list(highDebtRow = ccDebt > 8000)) # Get the variable information rxGetVarInfo ( myMortData ) # Get the proportion of values that are 1 rxSummary ( ~ highDebtRow , data = myMortData ) # Compute multiple transforms! rxDataStep ( inData = myMortData , outFile = myMortData , transforms = list ( newHouse = houseAge < 10 , ccsXhd = creditScore * highDebtRow ), append = 'cols' , overwrite = TRUE ) More complex transforms using transformFuncs 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 # Compute the summary statistics ( csSummary <- rxSummary ( ~ creditScore , data = mortData )) # Extract the mean and std. deviation meanCS <- csSummary $ sDataFrame $ Mean[1] sdCS <- csSummary $ sDataFrame $ StdDev[1] # Create a function to compute the scaled variable scaleCS <- function ( mylist ){ mylist[[ 'scaledCreditScore' ]] <- ( mylist[[ 'creditScore' ]] - myCenter ) / myScale return ( mylist ) } # Run it with rxDataStep (A above in B below) myMortData <- 'myMD.xdf' rxDataStep ( inData = mortData , outFile = myMortData , transformFunc = scaleCS , transformObjects = list ( myCenter = meanCS , myScale = sdCS )) # Check the new variable rxGetVarInfo ( myMortData ) rxSummary ( ~ scaledCreditScore , data = myMortData )","title":"Data Manipulation"},{"location":"big_data_analysis_with_revolution_r_enterprise/#data-analysis","text":"Preparing data for analysis: import 1 2 3 4 5 6 7 8 9 10 11 12 13 # Declare the file paths for the csv and xdf files myAirlineCsv <- file.path ( rxGetOption ( 'sampleDataDir' ), 'AirlineDemoSmall.csv' ) myAirlineXdf <- 'ADS.xdf' # Use rxImport to import the data into xdf format rxImport ( inData = myAirlineCsv , outFile = myAirlineXdf , overwrite = TRUE , colInfo = list ( DayOfWeek = list ( type = 'factor' , levels = c ( 'Monday' , 'Tuesday' , 'Wednesday' , 'Thursday' , 'Friday' , 'Saturday' , 'Sunday' )))) Preparing data Ffor analysis: exploration 1 2 3 4 5 6 7 # Summarize arrival delay for each day of the week rxSummary ( formula = ArrDelay ~ DayOfWeek , data = myAirlineXdf ) # Vizualize the arrival delay histogram rxHistogram ( formula = ~ ArrDelay , data = myAirlineXdf ) Construct a linear model 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 # predict arrival delay by day of the week myLM1 <- rxLinMod ( ArrDelay ~ DayOfWeek , data = myAirlineXdf ) # summarize the model smmary ( myLM1 ) # Use the transforms argument to create a factor variable associated with departure time 'on the fly,' # predict Arrival Delay by the interaction between Day of the week and that new factor variable myLM2 <- rxLinMod ( ArrDelay ~ DayOfWeek , data = myAirlineXdf , transforms = list ( catDepTime = cut ( CRSDepTime , breaks = seq ( from = 5 , to = 23 , by = 2 ))), cube = TRUE ) # summarize the model summary ( myLM2 ) Generating predictions and residuals 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 # Summarize model first summary ( myLM2 ) # Path to new dataset storing predictions myNewADS <- 'myNEWADS.xdf' # Generate predictions rxPredict ( modelObject = myLM2 , data = myAirlineXdf , outData = myNewADS , writeModelVars = TRUE ) # Get information on the new dataset rxGetInfo ( myNewADS , getVarInfo = TRUE ) # Generate residuals. rxPredict ( modelObject = myLM2 , data = myAirlineXdf , outData = myNewADS , writeModelVars = TRUE , computeResiduals = TRUE , overwrite = TRUE ) # Get information on the new dataset rxGetInfo ( myNewADS , getVarInfo = TRUE ) Logistic regression 1 2 3 4 5 6 7 8 9 10 # look at the meta data ls () rxGetInfo ( data = mortData , getVarInfo = TRUE ) # Construct the logit model logitModel <- rxLogit ( formula = default ~ houseAge + F ( year ) + ccDebt + creditScore + yearsEmploy , data = mortData ) # Summarize the result contained in logitModel summary ( logitModel Individual mortgage information 1 2 3 4 5 6 7 8 9 10 11 12 13 14 # Summarize the model summary ( logitModel ) # view the first few rows head ( newData ) # Make predictions dataWithPredictions <- rxPredict ( modelObject = logitModel , data = newData , outData = newData , type = 'response' ) # view the predictions dataWithPredictions Computing k-means with rxKmeans 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 # Examine the mortData dataset rxGetInfo ( mortData , getVarInfo = TRUE ) # Set up a path to a new xdf file myNewMortData = 'myMDwithKMeans.xdf' # Run k-means KMout <- rxKmeans ( formula = ~ ccDebt + creditScore + houseAge , data = mortData , outFile = myNewMortData , rowSelection = year == 2000 , numClusters = 4 , writeModelVars = TRUE ) print ( KMout ) # Examine the variables in the new dataset: rxGetInfo ( myNewMortData , getVarInfo = TRUE ) # Summarize the cluster variable: rxSummary ( ~ F ( .rxCluster ), data = myNewMortData ) # Read into memory 10% of the data: mydf <- rxXdfToDataFrame ( myNewMortData , rowSelection = randSamp == 1 , varsToDrop = 'year' , transforms = list ( randSamp = sample ( 10 , size = .rxNumRows , replace = TRUE ))) ## Visualize the clusters plot ( mydf[ -1 ] , col = mydf $ .rxCluster ) Create some decision trees 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 # regression tree regTreeOut <- rxDTree ( default ~ creditScore + ccDebt + yearsEmploy + houseAge , rowSelection = year == 2000 , data = mortData , maxdepth = 5 ) # print out the object print ( regTreeOut ) # plot a dendrogram, and add node labels plot ( rxAddInheritance ( regTreeOut )) text ( rxAddInheritance ( regTreeOut )) # Another visualization #library(RevoTreeView) #createTreeView(regTreeOut) # predict values myNewData = 'myNewMortData.xdf' rxPredict ( regTreeOut , data = mortData , outData = myNewData , writeModelVars = TRUE , predVarNames = 'default_RegPred' ) # visualize ROC curve rxRocCurve ( actualVarName = 'default' , predVarNames = 'default_RegPred' , data = myNewData )","title":"Data Analysis"},{"location":"data_analysis_in_r_the_data_table_way/","text":"Foreword Output options: the \u2018tango\u2019 syntax and the \u2018readable\u2019 theme. Snippets and results. Documentation \u00b6 data.table Extension of data.frame . Fast aggregation of large data (e.g. 100GB in RAM), fast ordered joins, fast add/modify/delete of columns by group using no copies at all, list columns, a fast friendly file reader and parallel file writer. Offers a natural and flexible syntax, for faster development. dplyr A fast, consistent tool for working with data frame like objects, both in memory and out of memory. Pipelines. tidyr An evolution of \u2018reshape2\u2019. It\u2019s designed specifically for data tidying (not general reshaping or aggregating) and works well with dplyr data pipelines. package 'narrower' 'wider' tidyr gather spread reshape2 melt cast spreadsheets unpivot pivot databases fold unfold data.table novice \u00b6 Find out more with ?data.table . Create and subset a data.table \u00b6 1 2 3 4 5 6 7 8 9 10 11 # The data.table package library ( data.table ) # Create my_first_data_table my_first_data_table <- data.table ( x = c ( 'a' , 'b' , 'c' , 'd' , 'e' ), y = c ( 1 , 2 , 3 , 4 , 5 )) # Create a data.table using recycling DT <- data.table ( a = 1 : 2 , b = c ( 'A' , 'B' , 'C' , 'D' )) # Print the third row to the console DT[3 , ] 1 2 ## a b ## 1: 1 C 1 2 # Print the second and third row to the console, but do not commas DT[2 : 3 ] 1 2 3 ## a b ## 1: 2 B ## 2: 1 C Getting to know a data.table \u00b6 Like head , tail . 1 2 # Print the penultimate row of DT using .N DT[.N - 1 ] 1 2 ## a b ## 1: 1 C 1 2 # Print the column names of DT, and number of rows and number of columns colnames ( DT ) 1 ## [1] \"a\" \"b\" 1 dim ( DT ) 1 ## [1] 4 2 1 2 # Select row 2 twice and row 3, returning a data.table with three rows where row 2 is a duplicate of row 1. DT [c ( 2 , 2 , 3 ) ] 1 2 3 4 ## a b ## 1: 2 B ## 2: 2 B ## 3: 1 C DT is a data.table/data.frame, but DT[ , B] is a vector; DT[ , .(B)] is a subsetted data.table. Subsetting data tables \u00b6 DT[i, j, by] means take DT , subset rows using i , then calculate j grouped by by . You can wrap j with .() . 1 2 3 4 5 6 7 A <- c ( 1 , 2 , 3 , 4 , 5 ) B <- c ( 'a' , 'b' , 'c' , 'd' , 'e' ) C <- c ( 6 , 7 , 8 , 9 , 10 ) DT <- data.table ( A , B , C ) # Subset rows 1 and 3, and columns B and C DT [c ( 1 , 3 ) , .(B , C ) ] 1 2 3 ## B C ## 1: a 6 ## 2: c 8 1 2 3 4 5 6 # Assign to ans the correct value ans <- data.table ( DT[ , .(B , val = A * C ) ] ) # Fill in the blanks such that ans2 equals target #target <- data.table(B = c('a', 'b', 'c', 'd', 'e', 'a', 'b', 'c', 'd', 'e'), val = as.integer(c(6:10, 1:5))) ans2 <- data.table ( DT[ , .(B , val = as.integer ( c ( 6 : 10 , 1 : 5 ))) ] ) The by basics \u00b6 1 2 3 4 5 6 7 # iris and iris3 are already available in the workspace # Convert iris to a data.table: DT DT <- as.data.table ( iris ) # For each Species, print the mean Sepal.Length DT[ , .(mean ( Sepal.Length )), by = .(Species ) ] 1 2 3 4 ## Species V1 ## 1: setosa 5.006 ## 2: versicolor 5.936 ## 3: virginica 6.588 1 2 # Print mean Sepal.Length, grouping by first letter of Species DT[ , .(mean ( Sepal.Length )), by = .(substr ( Species , 1 , 1 )) ] 1 2 3 ## substr V1 ## 1: s 5.006 ## 2: v 6.262 Using .N and by \u00b6 .N , number, in row or column. 1 2 3 4 5 # data.table version of iris: DT DT <- as.data.table ( iris ) # Group the specimens by Sepal area (to the nearest 10 cm2) and count how many occur in each group. DT[ , .N , by = 10 * round ( Sepal.Length * Sepal.Width / 10 ) ] 1 2 3 4 ## round N ## 1: 20 117 ## 2: 10 29 ## 3: 30 4 1 2 # Now name the output columns `Area` and `Count` DT[ , .(Count = .N ), by = .(Area = 10 * round ( Sepal.Length * Sepal.Width / 10 )) ] 1 2 3 4 ## Area Count ## 1: 20 117 ## 2: 10 29 ## 3: 30 4 Return multiple numbers in j \u00b6 1 2 3 4 5 6 7 8 9 # Create the data.table DT set.seed ( 1L ) DT <- data.table ( A = rep ( letters [2 : 1 ] , each = 4L ), B = rep ( 1 : 4 , each = 2L ), C = sample ( 8 )) # Create the new data.table, DT2 DT2 <- DT[ , .(C = cumsum ( C )), by = .(A , B ) ] # Select from DT2 the last two values from C while you group by A DT2[ , .(C = tail ( C , 2 )), by = A] 1 2 3 4 5 ## A C ## 1: b 4 ## 2: b 9 ## 3: a 2 ## 4: a 8 data.table yeoman \u00b6 Chaining, the basics \u00b6 1 2 3 4 # Build DT set.seed ( 1L ) DT <- data.table ( A = rep ( letters [2 : 1 ] , each = 4L ), B = rep ( 1 : 4 , each = 2L ), C = sample ( 8 )) DT 1 2 3 4 5 6 7 8 9 ## A B C ## 1: b 1 3 ## 2: b 1 8 ## 3: b 2 4 ## 4: b 2 5 ## 5: a 3 1 ## 6: a 3 7 ## 7: a 4 2 ## 8: a 4 6 1 2 3 # Use chaining # Cumsum of C while grouping by A and B, and then select last two values of C while grouping by A DT[ , .(C = cumsum ( C )), by = .(A , B ) ][ , .(C = tail ( C , 2 )), by = .(A ) ] 1 2 3 4 5 ## A C ## 1: b 4 ## 2: b 9 ## 3: a 2 ## 4: a 8 Chaining your iris dataset 1 2 3 4 DT <- data.table ( iris ) # Perform chained operations on DT DT[ , .(Sepal.Length = median ( Sepal.Length ), Sepal.Width = median ( Sepal.Width ), Petal.Length = median ( Petal.Length ), Petal.Width = median ( Petal.Width )), by = .(Species ) ] [order ( Species , decreasing = TRUE ) ] 1 2 3 4 ## Species Sepal.Length Sepal.Width Petal.Length Petal.Width ## 1: virginica 6.5 3.0 5.55 2.0 ## 2: versicolor 5.9 2.8 4.35 1.3 ## 3: setosa 5.0 3.4 1.50 0.2 1 DT 1 2 3 4 5 6 7 8 9 10 11 12 ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1: 5.1 3.5 1.4 0.2 setosa ## 2: 4.9 3.0 1.4 0.2 setosa ## 3: 4.7 3.2 1.3 0.2 setosa ## 4: 4.6 3.1 1.5 0.2 setosa ## 5: 5.0 3.6 1.4 0.2 setosa ## --- ## 146: 6.7 3.0 5.2 2.3 virginica ## 147: 6.3 2.5 5.0 1.9 virginica ## 148: 6.5 3.0 5.2 2.0 virginica ## 149: 6.2 3.4 5.4 2.3 virginica ## 150: 5.9 3.0 5.1 1.8 virginica Programming time vs readability \u00b6 1 2 3 4 5 6 7 x <- c ( 2 , 1 , 2 , 1 , 2 , 2 , 1 ) y <- c ( 1 , 3 , 5 , 7 , 9 , 11 , 13 ) z <- c ( 2 , 4 , 6 , 8 , 10 , 12 , 14 ) DT <- data.table ( x , y , z ) # Mean of columns DT[ , lapply ( .SD , mean ), by = .(x ) ] 1 2 3 ## x y z ## 1: 2 6.500000 7.500000 ## 2: 1 7.666667 8.666667 1 2 # Median of columns DT[ , lapply ( .SD , median ), by = .(x ) ] 1 2 3 ## x y z ## 1: 2 7 8 ## 2: 1 7 8 Introducing .SDcols \u00b6 .SDcols specifies the columns of DT that are included in .SD . 1 2 3 4 5 6 7 8 9 10 grp <- c ( 6 , 6 , 8 , 8 , 8 ) Q1 <- c ( 4 , 3 , 3 , 5 , 3 ) Q2 <- c ( 1 , 4 , 1 , 4 , 4 ) Q3 <- c ( 3 , 1 , 5 , 5 , 2 ) H1 <- c ( 1 , 2 , 3 , 2 , 4 ) H2 <- c ( 1 , 4 , 3 , 4 , 3 ) DT <- data.table ( grp , Q1 , Q2 , Q3 , H1 , H2 ) # Calculate the sum of the Q columns DT[ , lapply ( .SD , sum ), .SDcols = 2 : 4 ] 1 2 ## Q1 Q2 Q3 ## 1: 18 14 16 1 2 # Calculate the sum of columns H1 and H2 DT[ , lapply ( .SD , sum ), .SDcols = 5 : 6 ] 1 2 ## H1 H2 ## 1: 12 15 1 2 # Select all but the first row of groups 1 and 2, returning only the grp column and the Q columns. DT[ , .SD[ -1 ] , .SDcols = 2 : 4 , by = .(grp ) ] 1 2 3 4 ## grp Q1 Q2 Q3 ## 1: 6 3 4 1 ## 2: 8 5 4 5 ## 3: 8 3 4 2 Mixing it together: lapply , .SD , SDcols and .N \u00b6 1 2 3 4 5 6 7 8 x <- c ( 2 , 1 , 2 , 1 , 2 , 2 , 1 ) y <- c ( 1 , 3 , 5 , 7 , 9 , 11 , 13 ) z <- c ( 2 , 4 , 6 , 8 , 10 , 12 , 14 ) DT <- data.table ( x , y , z ) # Sum of all columns and the number of rows # For the first part, you need to combine the returned list from lapply, .SD and .SDcols and the integer vector of .N. You have to this because the result of the two together has to be a list again, with all values put together. DT 1 2 3 4 5 6 7 8 ## x y z ## 1: 2 1 2 ## 2: 1 3 4 ## 3: 2 5 6 ## 4: 1 7 8 ## 5: 2 9 10 ## 6: 2 11 12 ## 7: 1 13 14 1 DT[ , c ( lapply ( .SD , sum ), .N ), .SDcols = 1 : 3 , by = x] 1 2 3 ## x x y z N ## 1: 2 8 26 30 4 ## 2: 1 3 23 26 3 1 2 # Cumulative sum of column x and y while grouping by x and z > 8 DT[ , lapply ( .SD , cumsum ), .SDcols = 1 : 2 , by = .(by1 = x , by2 = z > 8 ) ] 1 2 3 4 5 6 7 8 ## by1 by2 x y ## 1: 2 FALSE 2 1 ## 2: 2 FALSE 4 6 ## 3: 1 FALSE 1 3 ## 4: 1 FALSE 2 10 ## 5: 2 TRUE 2 9 ## 6: 2 TRUE 4 20 ## 7: 1 TRUE 1 13 1 2 # Chaining DT[ , lapply ( .SD , cumsum ), .SDcols = 1 : 2 , by = .(by1 = x , by2 = z > 8 ) ][ , lapply ( .SD , max ), .SDcols = 3 : 4 , by = by1] 1 2 3 ## by1 x y ## 1: 2 4 20 ## 2: 1 2 13 Adding, updating, and removing columns \u00b6 := is defined for use in j only. 1 2 3 # The data.table DT DT <- data.table ( A = letters [c ( 1 , 1 , 1 , 2 , 2 ) ] , B = 1 : 5 ) DT 1 2 3 4 5 6 ## A B ## 1: a 1 ## 2: a 2 ## 3: a 3 ## 4: b 4 ## 5: b 5 1 2 # Add column by reference: Total DT[ , ( 'Total' ) := sum ( B ), by = .(A ) ] 1 2 3 4 5 6 ## A B Total ## 1: a 1 6 ## 2: a 2 6 ## 3: a 3 6 ## 4: b 4 9 ## 5: b 5 9 1 2 # Add 1 to column B DT [c ( 2 , 4 ), ( 'B' ) := as.integer ( 1 + B ) ] 1 2 3 4 5 6 ## A B Total ## 1: a 1 6 ## 2: a 3 6 ## 3: a 3 6 ## 4: b 5 9 ## 5: b 5 9 1 2 # Add a new column Total2 DT[2 : 4 , ':=' ( Total2 = sum ( B )), by = .(A ) ] 1 2 3 4 5 6 ## A B Total Total2 ## 1: a 1 6 NA ## 2: a 3 6 6 ## 3: a 3 6 6 ## 4: b 5 9 5 ## 5: b 5 9 NA 1 2 # Remove the Total column DT[ , Total := NULL ] 1 2 3 4 5 6 ## A B Total2 ## 1: a 1 NA ## 2: a 3 6 ## 3: a 3 6 ## 4: b 5 5 ## 5: b 5 NA 1 2 # Select the third column using `[[` DT[[3]] 1 ## [1] NA 6 6 5 NA The functional form \u00b6 1 2 3 4 5 # A data.table DT DT <- data.table ( A = c ( 1 , 1 , 1 , 2 , 2 ), B = 1 : 5 ) # Update B, add C and D DT[ , `:=` ( B = B + 1 , C = A + B , D = 2 ) ] 1 2 3 4 5 6 ## A B C D ## 1: 1 2 2 2 ## 2: 1 3 3 2 ## 3: 1 4 4 2 ## 4: 2 5 6 2 ## 5: 2 6 7 2 1 2 3 # Delete my_cols my_cols <- c ( 'B' , 'C' ) DT[ , ( my_cols ) := NULL ] 1 2 3 4 5 6 ## A D ## 1: 1 2 ## 2: 1 2 ## 3: 1 2 ## 4: 2 2 ## 5: 2 2 1 2 # Delete column 2 by number DT[ , 2 := NULL ] 1 2 3 4 5 6 ## A ## 1: 1 ## 2: 1 ## 3: 1 ## 4: 2 ## 5: 2 Ready, set , go! \u00b6 The set function is used to repeatedly update a data.table by reference. You can think of the set function as a loopable. 1 2 3 4 5 6 7 8 9 10 11 A <- c ( 2 , 2 , 3 , 5 , 2 , 5 , 5 , 4 , 4 , 1 ) B <- c ( 2 , 1 , 4 , 2 , 4 , 3 , 4 , 5 , 2 , 4 ) C <- c ( 5 , 2 , 4 , 1 , 2 , 2 , 1 , 2 , 5 , 2 ) D <- c ( 3 , 3 , 3 , 1 , 5 , 4 , 4 , 1 , 4 , 3 ) DT <- data.table ( A , B , C , D ) # Set the seed set.seed ( 1 ) # Check the DT DT 1 2 3 4 5 6 7 8 9 10 11 ## A B C D ## 1: 2 2 5 3 ## 2: 2 1 2 3 ## 3: 3 4 4 3 ## 4: 5 2 1 1 ## 5: 2 4 2 5 ## 6: 5 3 2 4 ## 7: 5 4 1 4 ## 8: 4 5 2 1 ## 9: 4 2 5 4 ## 10: 1 4 2 3 1 2 3 4 5 6 7 8 # For loop with set for ( l in 2 : 4 ) set ( DT , sample ( 10 , 3 ), l , NA ) # Change the column names to lowercase setnames ( DT , c ( 'A' , 'B' , 'C' , 'D' ), c ( 'a' , 'b' , 'c' , 'd' )) # Print the resulting DT to the console DT 1 2 3 4 5 6 7 8 9 10 11 ## a b c d ## 1: 2 2 5 3 ## 2: 2 1 NA 3 ## 3: 3 NA 4 3 ## 4: 5 NA 1 1 ## 5: 2 NA 2 5 ## 6: 5 3 2 NA ## 7: 5 4 1 4 ## 8: 4 5 NA 1 ## 9: 4 2 5 NA ## 10: 1 4 NA NA The set family 1 2 3 # Define DT DT <- data.table ( a = letters [c ( 1 , 1 , 1 , 2 , 2 ) ] , b = 1 ) DT 1 2 3 4 5 6 ## a b ## 1: a 1 ## 2: a 1 ## 3: a 1 ## 4: b 1 ## 5: b 1 1 2 3 # Add a postfix '_2' to all column names setnames ( DT , c ( 1 : 2 ), paste0 ( c ( 'a' , 'b' ), '_2' )) DT 1 2 3 4 5 6 ## a_2 b_2 ## 1: a 1 ## 2: a 1 ## 3: a 1 ## 4: b 1 ## 5: b 1 1 2 3 # Change column name 'a_2' to 'A2' setnames ( DT , 'a_2' , 'A2' ) DT 1 2 3 4 5 6 ## A2 b_2 ## 1: a 1 ## 2: a 1 ## 3: a 1 ## 4: b 1 ## 5: b 1 1 2 # Reverse the order of the columns setcolorder ( DT , c ( 'b_2' , 'A2' )) data.table expert \u00b6 Selecting rows the data.table way \u00b6 1 2 3 # Convert iris to a data.table iris <- data.table ( 'Sepal.Length' = iris $ Sepal.Length , 'Sepal.Width' = iris $ Sepal.Width , 'Petal.Length' = iris $ Petal.Length , 'Petal.Width' = iris $ Petal.Width , 'Species' = iris $ Species ) iris 1 2 3 4 5 6 7 8 9 10 11 12 ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1: 5.1 3.5 1.4 0.2 setosa ## 2: 4.9 3.0 1.4 0.2 setosa ## 3: 4.7 3.2 1.3 0.2 setosa ## 4: 4.6 3.1 1.5 0.2 setosa ## 5: 5.0 3.6 1.4 0.2 setosa ## --- ## 146: 6.7 3.0 5.2 2.3 virginica ## 147: 6.3 2.5 5.0 1.9 virginica ## 148: 6.5 3.0 5.2 2.0 virginica ## 149: 6.2 3.4 5.4 2.3 virginica ## 150: 5.9 3.0 5.1 1.8 virginica 1 2 # Species is 'virginica' head ( iris[Species == 'virginica' ] , 20 ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1: 6.3 3.3 6.0 2.5 virginica ## 2: 5.8 2.7 5.1 1.9 virginica ## 3: 7.1 3.0 5.9 2.1 virginica ## 4: 6.3 2.9 5.6 1.8 virginica ## 5: 6.5 3.0 5.8 2.2 virginica ## 6: 7.6 3.0 6.6 2.1 virginica ## 7: 4.9 2.5 4.5 1.7 virginica ## 8: 7.3 2.9 6.3 1.8 virginica ## 9: 6.7 2.5 5.8 1.8 virginica ## 10: 7.2 3.6 6.1 2.5 virginica ## 11: 6.5 3.2 5.1 2.0 virginica ## 12: 6.4 2.7 5.3 1.9 virginica ## 13: 6.8 3.0 5.5 2.1 virginica ## 14: 5.7 2.5 5.0 2.0 virginica ## 15: 5.8 2.8 5.1 2.4 virginica ## 16: 6.4 3.2 5.3 2.3 virginica ## 17: 6.5 3.0 5.5 1.8 virginica ## 18: 7.7 3.8 6.7 2.2 virginica ## 19: 7.7 2.6 6.9 2.3 virginica ## 20: 6.0 2.2 5.0 1.5 virginica 1 2 # Species is either 'virginica' or 'versicolor' head ( iris[Species %in% c ( 'virginica' , 'versicolor' ) ] , 20 ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1: 7.0 3.2 4.7 1.4 versicolor ## 2: 6.4 3.2 4.5 1.5 versicolor ## 3: 6.9 3.1 4.9 1.5 versicolor ## 4: 5.5 2.3 4.0 1.3 versicolor ## 5: 6.5 2.8 4.6 1.5 versicolor ## 6: 5.7 2.8 4.5 1.3 versicolor ## 7: 6.3 3.3 4.7 1.6 versicolor ## 8: 4.9 2.4 3.3 1.0 versicolor ## 9: 6.6 2.9 4.6 1.3 versicolor ## 10: 5.2 2.7 3.9 1.4 versicolor ## 11: 5.0 2.0 3.5 1.0 versicolor ## 12: 5.9 3.0 4.2 1.5 versicolor ## 13: 6.0 2.2 4.0 1.0 versicolor ## 14: 6.1 2.9 4.7 1.4 versicolor ## 15: 5.6 2.9 3.6 1.3 versicolor ## 16: 6.7 3.1 4.4 1.4 versicolor ## 17: 5.6 3.0 4.5 1.5 versicolor ## 18: 5.8 2.7 4.1 1.0 versicolor ## 19: 6.2 2.2 4.5 1.5 versicolor ## 20: 5.6 2.5 3.9 1.1 versicolor Removing columns and adapting your column names \u00b6 Refer to a regex cheat sheet for metacharacter. 1 2 3 # iris as a data.table iris <- as.data.table ( iris ) iris 1 2 3 4 5 6 7 8 9 10 11 12 ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1: 5.1 3.5 1.4 0.2 setosa ## 2: 4.9 3.0 1.4 0.2 setosa ## 3: 4.7 3.2 1.3 0.2 setosa ## 4: 4.6 3.1 1.5 0.2 setosa ## 5: 5.0 3.6 1.4 0.2 setosa ## --- ## 146: 6.7 3.0 5.2 2.3 virginica ## 147: 6.3 2.5 5.0 1.9 virginica ## 148: 6.5 3.0 5.2 2.0 virginica ## 149: 6.2 3.4 5.4 2.3 virginica ## 150: 5.9 3.0 5.1 1.8 virginica 1 2 3 4 5 6 7 # Remove the 'Sepal.' prefix #gsub('([ab])', '\\\\1_\\\\1_', 'abc and ABC') = pattern, replacement, x setnames ( iris , c ( 'Sepal.Length' , 'Sepal.Width' ), c ( 'Length' , 'Width' )) #gsub('^Sepal\\\\.','', iris) # Remove the two columns starting with 'Petal' iris[ , c ( 'Petal.Length' , 'Petal.Width' ) := NULL ] 1 2 3 4 5 6 7 8 9 10 11 12 ## Length Width Species ## 1: 5.1 3.5 setosa ## 2: 4.9 3.0 setosa ## 3: 4.7 3.2 setosa ## 4: 4.6 3.1 setosa ## 5: 5.0 3.6 setosa ## --- ## 146: 6.7 3.0 virginica ## 147: 6.3 2.5 virginica ## 148: 6.5 3.0 virginica ## 149: 6.2 3.4 virginica ## 150: 5.9 3.0 virginica Understanding automatic indexing \u00b6 1 2 3 # Cleaned up iris data.table iris2 <- data.frame ( Length = iris $ Sepal.Length , Width = iris $ Sepal.Width , Species = iris $ Species ) iris2 <- as.data.table ( iris2 ) 1 2 # Area is greater than 20 square centimeters iris2[ Width * Length > 20 ] , 20 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 Length Width Species is_large 1: 5.4 3.9 setosa FALSE 2: 5.8 4.0 setosa FALSE 3: 5.7 4.4 setosa TRUE 4: 5.4 3.9 setosa FALSE 5: 5.7 3.8 setosa FALSE 6: 5.2 4.1 setosa FALSE 7: 5.5 4.2 setosa FALSE 8: 7.0 3.2 versicolor FALSE 9: 6.4 3.2 versicolor FALSE 10: 6.9 3.1 versicolor FALSE 11: 6.3 3.3 versicolor FALSE 12: 6.7 3.1 versicolor FALSE 13: 6.7 3.0 versicolor FALSE 14: 6.0 3.4 versicolor FALSE 15: 6.7 3.1 versicolor FALSE 16: 6.3 3.3 virginica FALSE 17: 7.1 3.0 virginica FALSE 18: 7.6 3.0 virginica FALSE 19: 7.3 2.9 virginica FALSE 20: 7.2 3.6 virginica TRUE ... 1 2 # Add new boolean column iris2[ , is_large := Width * Length > 25 ] 1 2 # Now large observations with is_large iris2[is_large == TRUE ] 1 2 3 4 5 Length Width Species is_large 1: 5.7 4.4 setosa TRUE 2: 7.2 3.6 virginica TRUE 3: 7.7 3.8 virginica TRUE 4: 7.9 3.8 virginica TRUE 1 iris2 [ ( is_large ) ] # Also OK 1 2 3 4 5 Length Width Species is_large 1: 5.7 4.4 setosa TRUE 2: 7.2 3.6 virginica TRUE 3: 7.7 3.8 virginica TRUE 4: 7.9 3.8 virginica TRUE Selecting groups or parts of groups \u00b6 1 2 3 4 5 6 7 8 # The 'keyed' data.table DT DT <- data.table ( A = letters [c ( 2 , 1 , 2 , 3 , 1 , 2 , 3 ) ] , B = c ( 5 , 4 , 1 , 9 , 8 , 8 , 6 ), C = 6 : 12 ) setkey ( DT , A , B ) # Select the 'b' group DT[ 'b' ] 1 2 3 4 ## A B C ## 1: b 1 8 ## 2: b 5 6 ## 3: b 8 11 1 2 # 'b' and 'c' groups DT [c ( 'b' , 'c' ) ] 1 2 3 4 5 6 ## A B C ## 1: b 1 8 ## 2: b 5 6 ## 3: b 8 11 ## 4: c 6 12 ## 5: c 9 9 1 2 # The first row of the 'b' and 'c' groups DT [c ( 'b' , 'c' ), mult = 'first' ] 1 2 3 ## A B C ## 1: b 1 8 ## 2: c 6 12 1 2 # First and last row of the 'b' and 'c' groups DT [c ( 'b' , 'c' ), .SD [c ( 1 , .N ) ] , by = .EACHI] 1 2 3 4 5 ## A B C ## 1: b 1 8 ## 2: b 8 11 ## 3: c 6 12 ## 4: c 9 9 1 2 # Copy and extend code for instruction 4: add printout DT [c ( 'b' , 'c' ), { print ( .SD ); .SD [c ( 1 , .N ) ] }, by = .EACHI] 1 2 3 4 5 6 7 8 9 10 11 12 13 ## B C ## 1: 1 8 ## 2: 5 6 ## 3: 8 11 ## B C ## 1: 6 12 ## 2: 9 9 ## A B C ## 1: b 1 8 ## 2: b 8 11 ## 3: c 6 12 ## 4: c 9 9 Rolling joins \u00b6 Rolling joins \u2013 part one 1 2 3 4 5 6 7 # Keyed data.table DT DT <- data.table ( A = letters [c ( 2 , 1 , 2 , 3 , 1 , 2 , 3 ) ] , B = c ( 5 , 4 , 1 , 9 , 8 , 8 , 6 ), C = 6 : 12 , key = 'A,B' ) # Get the key of DT key ( DT ) 1 ## [1] \"A\" \"B\" 1 2 3 # Row where A == 'b' & B == 6 setkey ( DT , A , B ) DT [. ( 'b' , 6 ) ] 1 2 ## A B C ## 1: b 6 NA 1 2 # Return the prevailing row DT [. ( 'b' , 6 ), roll = TRUE ] 1 2 ## A B C ## 1: b 6 6 1 2 # Return the nearest row DT [. ( 'b' , 6 ), roll =+ Inf ] 1 2 ## A B C ## 1: b 6 6 Rolling joins \u2013 part two 1 2 3 4 5 6 7 # Keyed data.table DT DT <- data.table ( A = letters [c ( 2 , 1 , 2 , 3 , 1 , 2 , 3 ) ] , B = c ( 5 , 4 , 1 , 9 , 8 , 8 , 6 ), C = 6 : 12 , key = 'A,B' ) # Look at the sequence (-2):10 for the 'b' group DT [. ( 'b' , ( -2 ) : 10 ) ] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 ## A B C ## 1: b -2 NA ## 2: b -1 NA ## 3: b 0 NA ## 4: b 1 8 ## 5: b 2 NA ## 6: b 3 NA ## 7: b 4 NA ## 8: b 5 6 ## 9: b 6 NA ## 10: b 7 NA ## 11: b 8 11 ## 12: b 9 NA ## 13: b 10 NA 1 2 # Add code: carry the prevailing values forwards DT [. ( 'b' , ( -2 ) : 10 ), roll = TRUE ] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 ## A B C ## 1: b -2 NA ## 2: b -1 NA ## 3: b 0 NA ## 4: b 1 8 ## 5: b 2 8 ## 6: b 3 8 ## 7: b 4 8 ## 8: b 5 6 ## 9: b 6 6 ## 10: b 7 6 ## 11: b 8 11 ## 12: b 9 11 ## 13: b 10 11 1 2 # Add code: carry the first observation backwards DT [. ( 'b' , ( -2 ) : 10 ), roll = TRUE , rollends = TRUE ] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 ## A B C ## 1: b -2 8 ## 2: b -1 8 ## 3: b 0 8 ## 4: b 1 8 ## 5: b 2 8 ## 6: b 3 8 ## 7: b 4 8 ## 8: b 5 6 ## 9: b 6 6 ## 10: b 7 6 ## 11: b 8 11 ## 12: b 9 11 ## 13: b 10 11","title":"Data Analysis in R, the data.table Way"},{"location":"data_analysis_in_r_the_data_table_way/#documentation","text":"data.table Extension of data.frame . Fast aggregation of large data (e.g. 100GB in RAM), fast ordered joins, fast add/modify/delete of columns by group using no copies at all, list columns, a fast friendly file reader and parallel file writer. Offers a natural and flexible syntax, for faster development. dplyr A fast, consistent tool for working with data frame like objects, both in memory and out of memory. Pipelines. tidyr An evolution of \u2018reshape2\u2019. It\u2019s designed specifically for data tidying (not general reshaping or aggregating) and works well with dplyr data pipelines. package 'narrower' 'wider' tidyr gather spread reshape2 melt cast spreadsheets unpivot pivot databases fold unfold","title":"Documentation"},{"location":"data_analysis_in_r_the_data_table_way/#datatable-novice","text":"Find out more with ?data.table .","title":"data.table novice"},{"location":"data_analysis_in_r_the_data_table_way/#create-and-subset-a-datatable","text":"1 2 3 4 5 6 7 8 9 10 11 # The data.table package library ( data.table ) # Create my_first_data_table my_first_data_table <- data.table ( x = c ( 'a' , 'b' , 'c' , 'd' , 'e' ), y = c ( 1 , 2 , 3 , 4 , 5 )) # Create a data.table using recycling DT <- data.table ( a = 1 : 2 , b = c ( 'A' , 'B' , 'C' , 'D' )) # Print the third row to the console DT[3 , ] 1 2 ## a b ## 1: 1 C 1 2 # Print the second and third row to the console, but do not commas DT[2 : 3 ] 1 2 3 ## a b ## 1: 2 B ## 2: 1 C","title":"Create and subset a data.table"},{"location":"data_analysis_in_r_the_data_table_way/#getting-to-know-a-datatable","text":"Like head , tail . 1 2 # Print the penultimate row of DT using .N DT[.N - 1 ] 1 2 ## a b ## 1: 1 C 1 2 # Print the column names of DT, and number of rows and number of columns colnames ( DT ) 1 ## [1] \"a\" \"b\" 1 dim ( DT ) 1 ## [1] 4 2 1 2 # Select row 2 twice and row 3, returning a data.table with three rows where row 2 is a duplicate of row 1. DT [c ( 2 , 2 , 3 ) ] 1 2 3 4 ## a b ## 1: 2 B ## 2: 2 B ## 3: 1 C DT is a data.table/data.frame, but DT[ , B] is a vector; DT[ , .(B)] is a subsetted data.table.","title":"Getting to know a data.table"},{"location":"data_analysis_in_r_the_data_table_way/#subsetting-data-tables","text":"DT[i, j, by] means take DT , subset rows using i , then calculate j grouped by by . You can wrap j with .() . 1 2 3 4 5 6 7 A <- c ( 1 , 2 , 3 , 4 , 5 ) B <- c ( 'a' , 'b' , 'c' , 'd' , 'e' ) C <- c ( 6 , 7 , 8 , 9 , 10 ) DT <- data.table ( A , B , C ) # Subset rows 1 and 3, and columns B and C DT [c ( 1 , 3 ) , .(B , C ) ] 1 2 3 ## B C ## 1: a 6 ## 2: c 8 1 2 3 4 5 6 # Assign to ans the correct value ans <- data.table ( DT[ , .(B , val = A * C ) ] ) # Fill in the blanks such that ans2 equals target #target <- data.table(B = c('a', 'b', 'c', 'd', 'e', 'a', 'b', 'c', 'd', 'e'), val = as.integer(c(6:10, 1:5))) ans2 <- data.table ( DT[ , .(B , val = as.integer ( c ( 6 : 10 , 1 : 5 ))) ] )","title":"Subsetting data tables"},{"location":"data_analysis_in_r_the_data_table_way/#the-by-basics","text":"1 2 3 4 5 6 7 # iris and iris3 are already available in the workspace # Convert iris to a data.table: DT DT <- as.data.table ( iris ) # For each Species, print the mean Sepal.Length DT[ , .(mean ( Sepal.Length )), by = .(Species ) ] 1 2 3 4 ## Species V1 ## 1: setosa 5.006 ## 2: versicolor 5.936 ## 3: virginica 6.588 1 2 # Print mean Sepal.Length, grouping by first letter of Species DT[ , .(mean ( Sepal.Length )), by = .(substr ( Species , 1 , 1 )) ] 1 2 3 ## substr V1 ## 1: s 5.006 ## 2: v 6.262","title":"The by basics"},{"location":"data_analysis_in_r_the_data_table_way/#using-n-and-by","text":".N , number, in row or column. 1 2 3 4 5 # data.table version of iris: DT DT <- as.data.table ( iris ) # Group the specimens by Sepal area (to the nearest 10 cm2) and count how many occur in each group. DT[ , .N , by = 10 * round ( Sepal.Length * Sepal.Width / 10 ) ] 1 2 3 4 ## round N ## 1: 20 117 ## 2: 10 29 ## 3: 30 4 1 2 # Now name the output columns `Area` and `Count` DT[ , .(Count = .N ), by = .(Area = 10 * round ( Sepal.Length * Sepal.Width / 10 )) ] 1 2 3 4 ## Area Count ## 1: 20 117 ## 2: 10 29 ## 3: 30 4","title":"Using .N and by"},{"location":"data_analysis_in_r_the_data_table_way/#return-multiple-numbers-in-j","text":"1 2 3 4 5 6 7 8 9 # Create the data.table DT set.seed ( 1L ) DT <- data.table ( A = rep ( letters [2 : 1 ] , each = 4L ), B = rep ( 1 : 4 , each = 2L ), C = sample ( 8 )) # Create the new data.table, DT2 DT2 <- DT[ , .(C = cumsum ( C )), by = .(A , B ) ] # Select from DT2 the last two values from C while you group by A DT2[ , .(C = tail ( C , 2 )), by = A] 1 2 3 4 5 ## A C ## 1: b 4 ## 2: b 9 ## 3: a 2 ## 4: a 8","title":"Return multiple numbers in j"},{"location":"data_analysis_in_r_the_data_table_way/#datatable-yeoman","text":"","title":"data.table yeoman"},{"location":"data_analysis_in_r_the_data_table_way/#chaining-the-basics","text":"1 2 3 4 # Build DT set.seed ( 1L ) DT <- data.table ( A = rep ( letters [2 : 1 ] , each = 4L ), B = rep ( 1 : 4 , each = 2L ), C = sample ( 8 )) DT 1 2 3 4 5 6 7 8 9 ## A B C ## 1: b 1 3 ## 2: b 1 8 ## 3: b 2 4 ## 4: b 2 5 ## 5: a 3 1 ## 6: a 3 7 ## 7: a 4 2 ## 8: a 4 6 1 2 3 # Use chaining # Cumsum of C while grouping by A and B, and then select last two values of C while grouping by A DT[ , .(C = cumsum ( C )), by = .(A , B ) ][ , .(C = tail ( C , 2 )), by = .(A ) ] 1 2 3 4 5 ## A C ## 1: b 4 ## 2: b 9 ## 3: a 2 ## 4: a 8 Chaining your iris dataset 1 2 3 4 DT <- data.table ( iris ) # Perform chained operations on DT DT[ , .(Sepal.Length = median ( Sepal.Length ), Sepal.Width = median ( Sepal.Width ), Petal.Length = median ( Petal.Length ), Petal.Width = median ( Petal.Width )), by = .(Species ) ] [order ( Species , decreasing = TRUE ) ] 1 2 3 4 ## Species Sepal.Length Sepal.Width Petal.Length Petal.Width ## 1: virginica 6.5 3.0 5.55 2.0 ## 2: versicolor 5.9 2.8 4.35 1.3 ## 3: setosa 5.0 3.4 1.50 0.2 1 DT 1 2 3 4 5 6 7 8 9 10 11 12 ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1: 5.1 3.5 1.4 0.2 setosa ## 2: 4.9 3.0 1.4 0.2 setosa ## 3: 4.7 3.2 1.3 0.2 setosa ## 4: 4.6 3.1 1.5 0.2 setosa ## 5: 5.0 3.6 1.4 0.2 setosa ## --- ## 146: 6.7 3.0 5.2 2.3 virginica ## 147: 6.3 2.5 5.0 1.9 virginica ## 148: 6.5 3.0 5.2 2.0 virginica ## 149: 6.2 3.4 5.4 2.3 virginica ## 150: 5.9 3.0 5.1 1.8 virginica","title":"Chaining, the basics"},{"location":"data_analysis_in_r_the_data_table_way/#programming-time-vs-readability","text":"1 2 3 4 5 6 7 x <- c ( 2 , 1 , 2 , 1 , 2 , 2 , 1 ) y <- c ( 1 , 3 , 5 , 7 , 9 , 11 , 13 ) z <- c ( 2 , 4 , 6 , 8 , 10 , 12 , 14 ) DT <- data.table ( x , y , z ) # Mean of columns DT[ , lapply ( .SD , mean ), by = .(x ) ] 1 2 3 ## x y z ## 1: 2 6.500000 7.500000 ## 2: 1 7.666667 8.666667 1 2 # Median of columns DT[ , lapply ( .SD , median ), by = .(x ) ] 1 2 3 ## x y z ## 1: 2 7 8 ## 2: 1 7 8","title":"Programming time vs readability"},{"location":"data_analysis_in_r_the_data_table_way/#introducing-sdcols","text":".SDcols specifies the columns of DT that are included in .SD . 1 2 3 4 5 6 7 8 9 10 grp <- c ( 6 , 6 , 8 , 8 , 8 ) Q1 <- c ( 4 , 3 , 3 , 5 , 3 ) Q2 <- c ( 1 , 4 , 1 , 4 , 4 ) Q3 <- c ( 3 , 1 , 5 , 5 , 2 ) H1 <- c ( 1 , 2 , 3 , 2 , 4 ) H2 <- c ( 1 , 4 , 3 , 4 , 3 ) DT <- data.table ( grp , Q1 , Q2 , Q3 , H1 , H2 ) # Calculate the sum of the Q columns DT[ , lapply ( .SD , sum ), .SDcols = 2 : 4 ] 1 2 ## Q1 Q2 Q3 ## 1: 18 14 16 1 2 # Calculate the sum of columns H1 and H2 DT[ , lapply ( .SD , sum ), .SDcols = 5 : 6 ] 1 2 ## H1 H2 ## 1: 12 15 1 2 # Select all but the first row of groups 1 and 2, returning only the grp column and the Q columns. DT[ , .SD[ -1 ] , .SDcols = 2 : 4 , by = .(grp ) ] 1 2 3 4 ## grp Q1 Q2 Q3 ## 1: 6 3 4 1 ## 2: 8 5 4 5 ## 3: 8 3 4 2","title":"Introducing .SDcols"},{"location":"data_analysis_in_r_the_data_table_way/#mixing-it-together-lapply-sd-sdcols-and-n","text":"1 2 3 4 5 6 7 8 x <- c ( 2 , 1 , 2 , 1 , 2 , 2 , 1 ) y <- c ( 1 , 3 , 5 , 7 , 9 , 11 , 13 ) z <- c ( 2 , 4 , 6 , 8 , 10 , 12 , 14 ) DT <- data.table ( x , y , z ) # Sum of all columns and the number of rows # For the first part, you need to combine the returned list from lapply, .SD and .SDcols and the integer vector of .N. You have to this because the result of the two together has to be a list again, with all values put together. DT 1 2 3 4 5 6 7 8 ## x y z ## 1: 2 1 2 ## 2: 1 3 4 ## 3: 2 5 6 ## 4: 1 7 8 ## 5: 2 9 10 ## 6: 2 11 12 ## 7: 1 13 14 1 DT[ , c ( lapply ( .SD , sum ), .N ), .SDcols = 1 : 3 , by = x] 1 2 3 ## x x y z N ## 1: 2 8 26 30 4 ## 2: 1 3 23 26 3 1 2 # Cumulative sum of column x and y while grouping by x and z > 8 DT[ , lapply ( .SD , cumsum ), .SDcols = 1 : 2 , by = .(by1 = x , by2 = z > 8 ) ] 1 2 3 4 5 6 7 8 ## by1 by2 x y ## 1: 2 FALSE 2 1 ## 2: 2 FALSE 4 6 ## 3: 1 FALSE 1 3 ## 4: 1 FALSE 2 10 ## 5: 2 TRUE 2 9 ## 6: 2 TRUE 4 20 ## 7: 1 TRUE 1 13 1 2 # Chaining DT[ , lapply ( .SD , cumsum ), .SDcols = 1 : 2 , by = .(by1 = x , by2 = z > 8 ) ][ , lapply ( .SD , max ), .SDcols = 3 : 4 , by = by1] 1 2 3 ## by1 x y ## 1: 2 4 20 ## 2: 1 2 13","title":"Mixing it together: lapply, .SD, SDcols and .N"},{"location":"data_analysis_in_r_the_data_table_way/#adding-updating-and-removing-columns","text":":= is defined for use in j only. 1 2 3 # The data.table DT DT <- data.table ( A = letters [c ( 1 , 1 , 1 , 2 , 2 ) ] , B = 1 : 5 ) DT 1 2 3 4 5 6 ## A B ## 1: a 1 ## 2: a 2 ## 3: a 3 ## 4: b 4 ## 5: b 5 1 2 # Add column by reference: Total DT[ , ( 'Total' ) := sum ( B ), by = .(A ) ] 1 2 3 4 5 6 ## A B Total ## 1: a 1 6 ## 2: a 2 6 ## 3: a 3 6 ## 4: b 4 9 ## 5: b 5 9 1 2 # Add 1 to column B DT [c ( 2 , 4 ), ( 'B' ) := as.integer ( 1 + B ) ] 1 2 3 4 5 6 ## A B Total ## 1: a 1 6 ## 2: a 3 6 ## 3: a 3 6 ## 4: b 5 9 ## 5: b 5 9 1 2 # Add a new column Total2 DT[2 : 4 , ':=' ( Total2 = sum ( B )), by = .(A ) ] 1 2 3 4 5 6 ## A B Total Total2 ## 1: a 1 6 NA ## 2: a 3 6 6 ## 3: a 3 6 6 ## 4: b 5 9 5 ## 5: b 5 9 NA 1 2 # Remove the Total column DT[ , Total := NULL ] 1 2 3 4 5 6 ## A B Total2 ## 1: a 1 NA ## 2: a 3 6 ## 3: a 3 6 ## 4: b 5 5 ## 5: b 5 NA 1 2 # Select the third column using `[[` DT[[3]] 1 ## [1] NA 6 6 5 NA","title":"Adding, updating, and removing columns"},{"location":"data_analysis_in_r_the_data_table_way/#the-functional-form","text":"1 2 3 4 5 # A data.table DT DT <- data.table ( A = c ( 1 , 1 , 1 , 2 , 2 ), B = 1 : 5 ) # Update B, add C and D DT[ , `:=` ( B = B + 1 , C = A + B , D = 2 ) ] 1 2 3 4 5 6 ## A B C D ## 1: 1 2 2 2 ## 2: 1 3 3 2 ## 3: 1 4 4 2 ## 4: 2 5 6 2 ## 5: 2 6 7 2 1 2 3 # Delete my_cols my_cols <- c ( 'B' , 'C' ) DT[ , ( my_cols ) := NULL ] 1 2 3 4 5 6 ## A D ## 1: 1 2 ## 2: 1 2 ## 3: 1 2 ## 4: 2 2 ## 5: 2 2 1 2 # Delete column 2 by number DT[ , 2 := NULL ] 1 2 3 4 5 6 ## A ## 1: 1 ## 2: 1 ## 3: 1 ## 4: 2 ## 5: 2","title":"The functional form"},{"location":"data_analysis_in_r_the_data_table_way/#ready-set-go","text":"The set function is used to repeatedly update a data.table by reference. You can think of the set function as a loopable. 1 2 3 4 5 6 7 8 9 10 11 A <- c ( 2 , 2 , 3 , 5 , 2 , 5 , 5 , 4 , 4 , 1 ) B <- c ( 2 , 1 , 4 , 2 , 4 , 3 , 4 , 5 , 2 , 4 ) C <- c ( 5 , 2 , 4 , 1 , 2 , 2 , 1 , 2 , 5 , 2 ) D <- c ( 3 , 3 , 3 , 1 , 5 , 4 , 4 , 1 , 4 , 3 ) DT <- data.table ( A , B , C , D ) # Set the seed set.seed ( 1 ) # Check the DT DT 1 2 3 4 5 6 7 8 9 10 11 ## A B C D ## 1: 2 2 5 3 ## 2: 2 1 2 3 ## 3: 3 4 4 3 ## 4: 5 2 1 1 ## 5: 2 4 2 5 ## 6: 5 3 2 4 ## 7: 5 4 1 4 ## 8: 4 5 2 1 ## 9: 4 2 5 4 ## 10: 1 4 2 3 1 2 3 4 5 6 7 8 # For loop with set for ( l in 2 : 4 ) set ( DT , sample ( 10 , 3 ), l , NA ) # Change the column names to lowercase setnames ( DT , c ( 'A' , 'B' , 'C' , 'D' ), c ( 'a' , 'b' , 'c' , 'd' )) # Print the resulting DT to the console DT 1 2 3 4 5 6 7 8 9 10 11 ## a b c d ## 1: 2 2 5 3 ## 2: 2 1 NA 3 ## 3: 3 NA 4 3 ## 4: 5 NA 1 1 ## 5: 2 NA 2 5 ## 6: 5 3 2 NA ## 7: 5 4 1 4 ## 8: 4 5 NA 1 ## 9: 4 2 5 NA ## 10: 1 4 NA NA The set family 1 2 3 # Define DT DT <- data.table ( a = letters [c ( 1 , 1 , 1 , 2 , 2 ) ] , b = 1 ) DT 1 2 3 4 5 6 ## a b ## 1: a 1 ## 2: a 1 ## 3: a 1 ## 4: b 1 ## 5: b 1 1 2 3 # Add a postfix '_2' to all column names setnames ( DT , c ( 1 : 2 ), paste0 ( c ( 'a' , 'b' ), '_2' )) DT 1 2 3 4 5 6 ## a_2 b_2 ## 1: a 1 ## 2: a 1 ## 3: a 1 ## 4: b 1 ## 5: b 1 1 2 3 # Change column name 'a_2' to 'A2' setnames ( DT , 'a_2' , 'A2' ) DT 1 2 3 4 5 6 ## A2 b_2 ## 1: a 1 ## 2: a 1 ## 3: a 1 ## 4: b 1 ## 5: b 1 1 2 # Reverse the order of the columns setcolorder ( DT , c ( 'b_2' , 'A2' ))","title":"Ready, set, go!"},{"location":"data_analysis_in_r_the_data_table_way/#datatable-expert","text":"","title":"data.table expert"},{"location":"data_analysis_in_r_the_data_table_way/#selecting-rows-the-datatable-way","text":"1 2 3 # Convert iris to a data.table iris <- data.table ( 'Sepal.Length' = iris $ Sepal.Length , 'Sepal.Width' = iris $ Sepal.Width , 'Petal.Length' = iris $ Petal.Length , 'Petal.Width' = iris $ Petal.Width , 'Species' = iris $ Species ) iris 1 2 3 4 5 6 7 8 9 10 11 12 ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1: 5.1 3.5 1.4 0.2 setosa ## 2: 4.9 3.0 1.4 0.2 setosa ## 3: 4.7 3.2 1.3 0.2 setosa ## 4: 4.6 3.1 1.5 0.2 setosa ## 5: 5.0 3.6 1.4 0.2 setosa ## --- ## 146: 6.7 3.0 5.2 2.3 virginica ## 147: 6.3 2.5 5.0 1.9 virginica ## 148: 6.5 3.0 5.2 2.0 virginica ## 149: 6.2 3.4 5.4 2.3 virginica ## 150: 5.9 3.0 5.1 1.8 virginica 1 2 # Species is 'virginica' head ( iris[Species == 'virginica' ] , 20 ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1: 6.3 3.3 6.0 2.5 virginica ## 2: 5.8 2.7 5.1 1.9 virginica ## 3: 7.1 3.0 5.9 2.1 virginica ## 4: 6.3 2.9 5.6 1.8 virginica ## 5: 6.5 3.0 5.8 2.2 virginica ## 6: 7.6 3.0 6.6 2.1 virginica ## 7: 4.9 2.5 4.5 1.7 virginica ## 8: 7.3 2.9 6.3 1.8 virginica ## 9: 6.7 2.5 5.8 1.8 virginica ## 10: 7.2 3.6 6.1 2.5 virginica ## 11: 6.5 3.2 5.1 2.0 virginica ## 12: 6.4 2.7 5.3 1.9 virginica ## 13: 6.8 3.0 5.5 2.1 virginica ## 14: 5.7 2.5 5.0 2.0 virginica ## 15: 5.8 2.8 5.1 2.4 virginica ## 16: 6.4 3.2 5.3 2.3 virginica ## 17: 6.5 3.0 5.5 1.8 virginica ## 18: 7.7 3.8 6.7 2.2 virginica ## 19: 7.7 2.6 6.9 2.3 virginica ## 20: 6.0 2.2 5.0 1.5 virginica 1 2 # Species is either 'virginica' or 'versicolor' head ( iris[Species %in% c ( 'virginica' , 'versicolor' ) ] , 20 ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1: 7.0 3.2 4.7 1.4 versicolor ## 2: 6.4 3.2 4.5 1.5 versicolor ## 3: 6.9 3.1 4.9 1.5 versicolor ## 4: 5.5 2.3 4.0 1.3 versicolor ## 5: 6.5 2.8 4.6 1.5 versicolor ## 6: 5.7 2.8 4.5 1.3 versicolor ## 7: 6.3 3.3 4.7 1.6 versicolor ## 8: 4.9 2.4 3.3 1.0 versicolor ## 9: 6.6 2.9 4.6 1.3 versicolor ## 10: 5.2 2.7 3.9 1.4 versicolor ## 11: 5.0 2.0 3.5 1.0 versicolor ## 12: 5.9 3.0 4.2 1.5 versicolor ## 13: 6.0 2.2 4.0 1.0 versicolor ## 14: 6.1 2.9 4.7 1.4 versicolor ## 15: 5.6 2.9 3.6 1.3 versicolor ## 16: 6.7 3.1 4.4 1.4 versicolor ## 17: 5.6 3.0 4.5 1.5 versicolor ## 18: 5.8 2.7 4.1 1.0 versicolor ## 19: 6.2 2.2 4.5 1.5 versicolor ## 20: 5.6 2.5 3.9 1.1 versicolor","title":"Selecting rows the data.table way"},{"location":"data_analysis_in_r_the_data_table_way/#removing-columns-and-adapting-your-column-names","text":"Refer to a regex cheat sheet for metacharacter. 1 2 3 # iris as a data.table iris <- as.data.table ( iris ) iris 1 2 3 4 5 6 7 8 9 10 11 12 ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1: 5.1 3.5 1.4 0.2 setosa ## 2: 4.9 3.0 1.4 0.2 setosa ## 3: 4.7 3.2 1.3 0.2 setosa ## 4: 4.6 3.1 1.5 0.2 setosa ## 5: 5.0 3.6 1.4 0.2 setosa ## --- ## 146: 6.7 3.0 5.2 2.3 virginica ## 147: 6.3 2.5 5.0 1.9 virginica ## 148: 6.5 3.0 5.2 2.0 virginica ## 149: 6.2 3.4 5.4 2.3 virginica ## 150: 5.9 3.0 5.1 1.8 virginica 1 2 3 4 5 6 7 # Remove the 'Sepal.' prefix #gsub('([ab])', '\\\\1_\\\\1_', 'abc and ABC') = pattern, replacement, x setnames ( iris , c ( 'Sepal.Length' , 'Sepal.Width' ), c ( 'Length' , 'Width' )) #gsub('^Sepal\\\\.','', iris) # Remove the two columns starting with 'Petal' iris[ , c ( 'Petal.Length' , 'Petal.Width' ) := NULL ] 1 2 3 4 5 6 7 8 9 10 11 12 ## Length Width Species ## 1: 5.1 3.5 setosa ## 2: 4.9 3.0 setosa ## 3: 4.7 3.2 setosa ## 4: 4.6 3.1 setosa ## 5: 5.0 3.6 setosa ## --- ## 146: 6.7 3.0 virginica ## 147: 6.3 2.5 virginica ## 148: 6.5 3.0 virginica ## 149: 6.2 3.4 virginica ## 150: 5.9 3.0 virginica","title":"Removing columns and adapting your column names"},{"location":"data_analysis_in_r_the_data_table_way/#understanding-automatic-indexing","text":"1 2 3 # Cleaned up iris data.table iris2 <- data.frame ( Length = iris $ Sepal.Length , Width = iris $ Sepal.Width , Species = iris $ Species ) iris2 <- as.data.table ( iris2 ) 1 2 # Area is greater than 20 square centimeters iris2[ Width * Length > 20 ] , 20 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 Length Width Species is_large 1: 5.4 3.9 setosa FALSE 2: 5.8 4.0 setosa FALSE 3: 5.7 4.4 setosa TRUE 4: 5.4 3.9 setosa FALSE 5: 5.7 3.8 setosa FALSE 6: 5.2 4.1 setosa FALSE 7: 5.5 4.2 setosa FALSE 8: 7.0 3.2 versicolor FALSE 9: 6.4 3.2 versicolor FALSE 10: 6.9 3.1 versicolor FALSE 11: 6.3 3.3 versicolor FALSE 12: 6.7 3.1 versicolor FALSE 13: 6.7 3.0 versicolor FALSE 14: 6.0 3.4 versicolor FALSE 15: 6.7 3.1 versicolor FALSE 16: 6.3 3.3 virginica FALSE 17: 7.1 3.0 virginica FALSE 18: 7.6 3.0 virginica FALSE 19: 7.3 2.9 virginica FALSE 20: 7.2 3.6 virginica TRUE ... 1 2 # Add new boolean column iris2[ , is_large := Width * Length > 25 ] 1 2 # Now large observations with is_large iris2[is_large == TRUE ] 1 2 3 4 5 Length Width Species is_large 1: 5.7 4.4 setosa TRUE 2: 7.2 3.6 virginica TRUE 3: 7.7 3.8 virginica TRUE 4: 7.9 3.8 virginica TRUE 1 iris2 [ ( is_large ) ] # Also OK 1 2 3 4 5 Length Width Species is_large 1: 5.7 4.4 setosa TRUE 2: 7.2 3.6 virginica TRUE 3: 7.7 3.8 virginica TRUE 4: 7.9 3.8 virginica TRUE","title":"Understanding automatic indexing"},{"location":"data_analysis_in_r_the_data_table_way/#selecting-groups-or-parts-of-groups","text":"1 2 3 4 5 6 7 8 # The 'keyed' data.table DT DT <- data.table ( A = letters [c ( 2 , 1 , 2 , 3 , 1 , 2 , 3 ) ] , B = c ( 5 , 4 , 1 , 9 , 8 , 8 , 6 ), C = 6 : 12 ) setkey ( DT , A , B ) # Select the 'b' group DT[ 'b' ] 1 2 3 4 ## A B C ## 1: b 1 8 ## 2: b 5 6 ## 3: b 8 11 1 2 # 'b' and 'c' groups DT [c ( 'b' , 'c' ) ] 1 2 3 4 5 6 ## A B C ## 1: b 1 8 ## 2: b 5 6 ## 3: b 8 11 ## 4: c 6 12 ## 5: c 9 9 1 2 # The first row of the 'b' and 'c' groups DT [c ( 'b' , 'c' ), mult = 'first' ] 1 2 3 ## A B C ## 1: b 1 8 ## 2: c 6 12 1 2 # First and last row of the 'b' and 'c' groups DT [c ( 'b' , 'c' ), .SD [c ( 1 , .N ) ] , by = .EACHI] 1 2 3 4 5 ## A B C ## 1: b 1 8 ## 2: b 8 11 ## 3: c 6 12 ## 4: c 9 9 1 2 # Copy and extend code for instruction 4: add printout DT [c ( 'b' , 'c' ), { print ( .SD ); .SD [c ( 1 , .N ) ] }, by = .EACHI] 1 2 3 4 5 6 7 8 9 10 11 12 13 ## B C ## 1: 1 8 ## 2: 5 6 ## 3: 8 11 ## B C ## 1: 6 12 ## 2: 9 9 ## A B C ## 1: b 1 8 ## 2: b 8 11 ## 3: c 6 12 ## 4: c 9 9","title":"Selecting groups or parts of groups"},{"location":"data_analysis_in_r_the_data_table_way/#rolling-joins","text":"Rolling joins \u2013 part one 1 2 3 4 5 6 7 # Keyed data.table DT DT <- data.table ( A = letters [c ( 2 , 1 , 2 , 3 , 1 , 2 , 3 ) ] , B = c ( 5 , 4 , 1 , 9 , 8 , 8 , 6 ), C = 6 : 12 , key = 'A,B' ) # Get the key of DT key ( DT ) 1 ## [1] \"A\" \"B\" 1 2 3 # Row where A == 'b' & B == 6 setkey ( DT , A , B ) DT [. ( 'b' , 6 ) ] 1 2 ## A B C ## 1: b 6 NA 1 2 # Return the prevailing row DT [. ( 'b' , 6 ), roll = TRUE ] 1 2 ## A B C ## 1: b 6 6 1 2 # Return the nearest row DT [. ( 'b' , 6 ), roll =+ Inf ] 1 2 ## A B C ## 1: b 6 6 Rolling joins \u2013 part two 1 2 3 4 5 6 7 # Keyed data.table DT DT <- data.table ( A = letters [c ( 2 , 1 , 2 , 3 , 1 , 2 , 3 ) ] , B = c ( 5 , 4 , 1 , 9 , 8 , 8 , 6 ), C = 6 : 12 , key = 'A,B' ) # Look at the sequence (-2):10 for the 'b' group DT [. ( 'b' , ( -2 ) : 10 ) ] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 ## A B C ## 1: b -2 NA ## 2: b -1 NA ## 3: b 0 NA ## 4: b 1 8 ## 5: b 2 NA ## 6: b 3 NA ## 7: b 4 NA ## 8: b 5 6 ## 9: b 6 NA ## 10: b 7 NA ## 11: b 8 11 ## 12: b 9 NA ## 13: b 10 NA 1 2 # Add code: carry the prevailing values forwards DT [. ( 'b' , ( -2 ) : 10 ), roll = TRUE ] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 ## A B C ## 1: b -2 NA ## 2: b -1 NA ## 3: b 0 NA ## 4: b 1 8 ## 5: b 2 8 ## 6: b 3 8 ## 7: b 4 8 ## 8: b 5 6 ## 9: b 6 6 ## 10: b 7 6 ## 11: b 8 11 ## 12: b 9 11 ## 13: b 10 11 1 2 # Add code: carry the first observation backwards DT [. ( 'b' , ( -2 ) : 10 ), roll = TRUE , rollends = TRUE ] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 ## A B C ## 1: b -2 8 ## 2: b -1 8 ## 3: b 0 8 ## 4: b 1 8 ## 5: b 2 8 ## 6: b 3 8 ## 7: b 4 8 ## 8: b 5 6 ## 9: b 6 6 ## 10: b 7 6 ## 11: b 8 11 ## 12: b 9 11 ## 13: b 10 11","title":"Rolling joins"},{"location":"data_manipulation_in_r_with_dplyr/","text":"Foreword Output options: the \u2018tango\u2019 syntax and the \u2018readable\u2019 theme. Snippets and results. Documentation \u00b6 data.table extension of data.frame . Fast aggregation of large data (e.g. 100GB in RAM), fast ordered joins, fast add/modify/delete of columns by group using no copies at all, list columns, a fast friendly file reader and parallel file writer. Offers a natural and flexible syntax, for faster development. dplyr A fast, consistent tool for working with data frame like objects, both in memory and out of memory. Pipelines. tidyr An evolution of \u2018reshape2\u2019. It\u2019s designed specifically for data tidying (not general reshaping or aggregating) and works well with dplyr data pipelines. package 'narrower' 'wider' tidyr gather spread reshape2 melt cast spreadsheets unpivot pivot databases fold unfold Introduction to dplyr \u00b6 Load the dplyr and hflights package \u00b6 1 2 3 4 5 6 7 8 9 10 # Load the dplyr package library ( dplyr ) library ( dtplyr ) # Load the hflights package # A data only package containing commercial domestic flights that departed Houston (IAH and HOU) in 2011 library ( hflights ) # Call both head() and summary() on hflights head ( hflights ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 ## Year Month DayofMonth DayOfWeek DepTime ArrTime UniqueCarrier ## 5424 2011 1 1 6 1400 1500 AA ## 5425 2011 1 2 7 1401 1501 AA ## 5426 2011 1 3 1 1352 1502 AA ## 5427 2011 1 4 2 1403 1513 AA ## 5428 2011 1 5 3 1405 1507 AA ## 5429 2011 1 6 4 1359 1503 AA ## FlightNum TailNum ActualElapsedTime AirTime ArrDelay DepDelay Origin ## 5424 428 N576AA 60 40 -10 0 IAH ## 5425 428 N557AA 60 45 -9 1 IAH ## 5426 428 N541AA 70 48 -8 -8 IAH ## 5427 428 N403AA 70 39 3 3 IAH ## 5428 428 N492AA 62 44 -3 5 IAH ## 5429 428 N262AA 64 45 -7 -1 IAH ## Dest Distance TaxiIn TaxiOut Cancelled CancellationCode Diverted ## 5424 DFW 224 7 13 0 0 ## 5425 DFW 224 6 9 0 0 ## 5426 DFW 224 5 17 0 0 ## 5427 DFW 224 9 22 0 0 ## 5428 DFW 224 9 9 0 0 ## 5429 DFW 224 6 13 0 0 1 summary ( hflights ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 ## Year Month DayofMonth DayOfWeek ## Min. :2011 Min. : 1.000 Min. : 1.00 Min. :1.000 ## 1st Qu.:2011 1st Qu.: 4.000 1st Qu.: 8.00 1st Qu.:2.000 ## Median :2011 Median : 7.000 Median :16.00 Median :4.000 ## Mean :2011 Mean : 6.514 Mean :15.74 Mean :3.948 ## 3rd Qu.:2011 3rd Qu.: 9.000 3rd Qu.:23.00 3rd Qu.:6.000 ## Max. :2011 Max. :12.000 Max. :31.00 Max. :7.000 ## ## DepTime ArrTime UniqueCarrier FlightNum ## Min. : 1 Min. : 1 Length:227496 Min. : 1 ## 1st Qu.:1021 1st Qu.:1215 Class :character 1st Qu.: 855 ## Median :1416 Median :1617 Mode :character Median :1696 ## Mean :1396 Mean :1578 Mean :1962 ## 3rd Qu.:1801 3rd Qu.:1953 3rd Qu.:2755 ## Max. :2400 Max. :2400 Max. :7290 ## NA's :2905 NA's :3066 ## TailNum ActualElapsedTime AirTime ArrDelay ## Length:227496 Min. : 34.0 Min. : 11.0 Min. :-70.000 ## Class :character 1st Qu.: 77.0 1st Qu.: 58.0 1st Qu.: -8.000 ## Mode :character Median :128.0 Median :107.0 Median : 0.000 ## Mean :129.3 Mean :108.1 Mean : 7.094 ## 3rd Qu.:165.0 3rd Qu.:141.0 3rd Qu.: 11.000 ## Max. :575.0 Max. :549.0 Max. :978.000 ## NA's :3622 NA's :3622 NA's :3622 ## DepDelay Origin Dest Distance ## Min. :-33.000 Length:227496 Length:227496 Min. : 79.0 ## 1st Qu.: -3.000 Class :character Class :character 1st Qu.: 376.0 ## Median : 0.000 Mode :character Mode :character Median : 809.0 ## Mean : 9.445 Mean : 787.8 ## 3rd Qu.: 9.000 3rd Qu.:1042.0 ## Max. :981.000 Max. :3904.0 ## NA's :2905 ## TaxiIn TaxiOut Cancelled CancellationCode ## Min. : 1.000 Min. : 1.00 Min. :0.00000 Length:227496 ## 1st Qu.: 4.000 1st Qu.: 10.00 1st Qu.:0.00000 Class :character ## Median : 5.000 Median : 14.00 Median :0.00000 Mode :character ## Mean : 6.099 Mean : 15.09 Mean :0.01307 ## 3rd Qu.: 7.000 3rd Qu.: 18.00 3rd Qu.:0.00000 ## Max. :165.000 Max. :163.00 Max. :1.00000 ## NA's :3066 NA's :2947 ## Diverted ## Min. :0.000000 ## 1st Qu.:0.000000 ## Median :0.000000 ## Mean :0.002853 ## 3rd Qu.:0.000000 ## Max. :1.000000 ## Convert data.frame to table \u00b6 1 2 3 4 5 # Convert the hflights data.frame into a hflights tbl hflights <- tbl_df ( hflights ) # Display the hflights tbl hflights 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 ## # A tibble : 227 , 496 \u00d7 21 ## Year Month DayofMonth DayOfWeek DepTime ArrTime UniqueCarrier ## * & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; chr & gt ; ## 1 2011 1 1 6 1400 1500 AA ## 2 2011 1 2 7 1401 1501 AA ## 3 2011 1 3 1 1352 1502 AA ## 4 2011 1 4 2 1403 1513 AA ## 5 2011 1 5 3 1405 1507 AA ## 6 2011 1 6 4 1359 1503 AA ## 7 2011 1 7 5 1359 1509 AA ## 8 2011 1 8 6 1355 1454 AA ## 9 2011 1 9 7 1443 1554 AA ## 10 2011 1 10 1 1443 1553 AA ## # ... with 227 , 486 more rows , and 14 more variables : FlightNum & lt ; int & gt ;, ## # TailNum & lt ; chr & gt ;, ActualElapsedTime & lt ; int & gt ;, AirTime & lt ; int & gt ;, ArrDelay & lt ; int & gt ;, ## # DepDelay & lt ; int & gt ;, Origin & lt ; chr & gt ;, Dest & lt ; chr & gt ;, Distance & lt ; int & gt ;, ## # TaxiIn & lt ; int & gt ;, TaxiOut & lt ; int & gt ;, Cancelled & lt ; int & gt ;, CancellationCode & lt ; chr & gt ;, ## # Diverted & lt ; int & gt ; 1 2 # Create the object carriers, containing only the UniqueCarrier variable of hflights carriers <- hflights $ UniqueCarrier Changing labels of hflights \u00b6 part 1 of 2 1 2 3 4 5 6 7 8 9 10 11 # add lut <- c ( 'AA' = 'American' , 'AS' = 'Alaska' , 'B6' = 'JetBlue' , 'CO' = 'Continental' , 'DL' = 'Delta' , 'OO' = 'SkyWest' , 'UA' = 'United' , 'US' = 'US_Airways' , 'WN' = 'Southwest' , 'EV' = 'Atlantic_Southeast' , 'F9' = 'Frontier' , 'FL' = 'AirTran' , 'MQ' = 'American_Eagle' , 'XE' = 'ExpressJet' , 'YV' = 'Mesa' ) # Use lut to translate the UniqueCarrier column of hflights hflights $ UniqueCarrier <- lut[hflights $ UniqueCarrier] # Inspect the resulting raw values of your variables glimpse ( hflights ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 ## Observations: 227,496 ## Variables: 21 ## $ Year &lt;int&gt; 2011, 2011, 2011, 2011, 2011, 2011, 2011, 20... ## $ Month &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,... ## $ DayofMonth &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 1... ## $ DayOfWeek &lt;int&gt; 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6,... ## $ DepTime &lt;int&gt; 1400, 1401, 1352, 1403, 1405, 1359, 1359, 13... ## $ ArrTime &lt;int&gt; 1500, 1501, 1502, 1513, 1507, 1503, 1509, 14... ## $ UniqueCarrier &lt;chr&gt; \"American\", \"American\", \"American\", \"America... ## $ FlightNum &lt;int&gt; 428, 428, 428, 428, 428, 428, 428, 428, 428,... ## $ TailNum &lt;chr&gt; \"N576AA\", \"N557AA\", \"N541AA\", \"N403AA\", \"N49... ## $ ActualElapsedTime &lt;int&gt; 60, 60, 70, 70, 62, 64, 70, 59, 71, 70, 70, ... ## $ AirTime &lt;int&gt; 40, 45, 48, 39, 44, 45, 43, 40, 41, 45, 42, ... ## $ ArrDelay &lt;int&gt; -10, -9, -8, 3, -3, -7, -1, -16, 44, 43, 29,... ## $ DepDelay &lt;int&gt; 0, 1, -8, 3, 5, -1, -1, -5, 43, 43, 29, 19, ... ## $ Origin &lt;chr&gt; \"IAH\", \"IAH\", \"IAH\", \"IAH\", \"IAH\", \"IAH\", \"I... ## $ Dest &lt;chr&gt; \"DFW\", \"DFW\", \"DFW\", \"DFW\", \"DFW\", \"DFW\", \"D... ## $ Distance &lt;int&gt; 224, 224, 224, 224, 224, 224, 224, 224, 224,... ## $ TaxiIn &lt;int&gt; 7, 6, 5, 9, 9, 6, 12, 7, 8, 6, 8, 4, 6, 5, 6... ## $ TaxiOut &lt;int&gt; 13, 9, 17, 22, 9, 13, 15, 12, 22, 19, 20, 11... ## $ Cancelled &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,... ## $ CancellationCode &lt;chr&gt; \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", ... ## $ Diverted &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,... part 2 of 2 1 2 3 4 5 6 7 8 # Build the lookup table: lut lut <- c ( \"A\" = \"carrier\" , \"B\" = \"weather\" , \"C\" = \"FFA\" , \"D\" = \"security\" , \"E\" = \"not cancelled\" ) # Add the Code column hflights $ Code <- lut[hflights $ CancellationCode] # Glimpse at hflights glimpse ( hflights ) Result. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 Observations: 227,496 Variables: 22 $ Year <int> 2011, 2011, 2011, 2011, 2011, 2011,... $ Month <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,... $ DayofMonth <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, ... $ DayOfWeek <int> 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3,... $ DepTime <int> 1400, 1401, 1352, 1403, 1405, 1359,... $ ArrTime <int> 1500, 1501, 1502, 1513, 1507, 1503,... $ UniqueCarrier <chr> \"American\", \"American\", \"American\",... $ FlightNum <int> 428, 428, 428, 428, 428, 428, 428, ... $ TailNum <chr> \"N576AA\", \"N557AA\", \"N541AA\", \"N403... $ ActualElapsedTime <int> 60, 60, 70, 70, 62, 64, 70, 59, 71,... $ AirTime <int> 40, 45, 48, 39, 44, 45, 43, 40, 41,... $ ArrDelay <int> -10, -9, -8, 3, -3, -7, -1, -16, 44... $ DepDelay <int> 0, 1, -8, 3, 5, -1, -1, -5, 43, 43,... $ Origin <chr> \"IAH\", \"IAH\", \"IAH\", \"IAH\", \"IAH\", ... $ Dest <chr> \"DFW\", \"DFW\", \"DFW\", \"DFW\", \"DFW\", ... $ Distance <int> 224, 224, 224, 224, 224, 224, 224, ... $ TaxiIn <int> 7, 6, 5, 9, 9, 6, 12, 7, 8, 6, 8, 4... $ TaxiOut <int> 13, 9, 17, 22, 9, 13, 15, 12, 22, 1... $ Cancelled <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,... $ CancellationCode <chr> \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\",... $ Diverted <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,... $ Code <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA,... select and mutate \u00b6 The five verbs and their meaning \u00b6 select ; which returns a subset of the columns. filter ; that is able to return a subset of the rows. arrange ; that reorders the rows according to single or multiple variables. mutate ; used to add columns from existing data. summarise ; which reduces each group to a single row by calculating aggregate measures. The select verb \u00b6 1 2 # Print out a tbl with the four columns of hflights related to delay select ( hflights , ActualElapsedTime , AirTime , ArrDelay , DepDelay ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 ## # A tibble: 227,496 \u00d7 4 ## ActualElapsedTime AirTime ArrDelay DepDelay ## * &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 60 40 -10 0 ## 2 60 45 -9 1 ## 3 70 48 -8 -8 ## 4 70 39 3 3 ## 5 62 44 -3 5 ## 6 64 45 -7 -1 ## 7 70 43 -1 -1 ## 8 59 40 -16 -5 ## 9 71 41 44 43 ## 10 70 45 43 43 ## # ... with 227,486 more rows 1 2 # Print out hflights, nothing has changed! hflights 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 ## # A tibble : 227 , 496 \u00d7 21 ## Year Month DayofMonth DayOfWeek DepTime ArrTime UniqueCarrier ## * & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; chr & gt ; ## 1 2011 1 1 6 1400 1500 American ## 2 2011 1 2 7 1401 1501 American ## 3 2011 1 3 1 1352 1502 American ## 4 2011 1 4 2 1403 1513 American ## 5 2011 1 5 3 1405 1507 American ## 6 2011 1 6 4 1359 1503 American ## 7 2011 1 7 5 1359 1509 American ## 8 2011 1 8 6 1355 1454 American ## 9 2011 1 9 7 1443 1554 American ## 10 2011 1 10 1 1443 1553 American ## # ... with 227 , 486 more rows , and 14 more variables : FlightNum & lt ; int & gt ;, ## # TailNum & lt ; chr & gt ;, ActualElapsedTime & lt ; int & gt ;, AirTime & lt ; int & gt ;, ArrDelay & lt ; int & gt ;, ## # DepDelay & lt ; int & gt ;, Origin & lt ; chr & gt ;, Dest & lt ; chr & gt ;, Distance & lt ; int & gt ;, ## # TaxiIn & lt ; int & gt ;, TaxiOut & lt ; int & gt ;, Cancelled & lt ; int & gt ;, CancellationCode & lt ; chr & gt ;, ## # Diverted & lt ; int & gt ; 1 2 # Print out the columns Origin up to Cancelled of hflights select ( hflights , 14 : 19 ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 ## # A tibble: 227,496 \u00d7 6 ## Origin Dest Distance TaxiIn TaxiOut Cancelled ## * &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 IAH DFW 224 7 13 0 ## 2 IAH DFW 224 6 9 0 ## 3 IAH DFW 224 5 17 0 ## 4 IAH DFW 224 9 22 0 ## 5 IAH DFW 224 9 9 0 ## 6 IAH DFW 224 6 13 0 ## 7 IAH DFW 224 12 15 0 ## 8 IAH DFW 224 7 12 0 ## 9 IAH DFW 224 8 22 0 ## 10 IAH DFW 224 6 19 0 ## # ... with 227,486 more rows 1 2 # Answer to last question: be concise! select ( hflights , 1 : 4 , 12 : 21 ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 ## # A tibble : 227 , 496 \u00d7 14 ## Year Month DayofMonth DayOfWeek ArrDelay DepDelay Origin Dest ## * & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; chr & gt ; & lt ; chr & gt ; ## 1 2011 1 1 6 -10 0 IAH DFW ## 2 2011 1 2 7 -9 1 IAH DFW ## 3 2011 1 3 1 -8 -8 IAH DFW ## 4 2011 1 4 2 3 3 IAH DFW ## 5 2011 1 5 3 -3 5 IAH DFW ## 6 2011 1 6 4 -7 -1 IAH DFW ## 7 2011 1 7 5 -1 -1 IAH DFW ## 8 2011 1 8 6 -16 -5 IAH DFW ## 9 2011 1 9 7 44 43 IAH DFW ## 10 2011 1 10 1 43 43 IAH DFW ## # ... with 227 , 486 more rows , and 6 more variables : Distance & lt ; int & gt ;, ## # TaxiIn & lt ; int & gt ;, TaxiOut & lt ; int & gt ;, Cancelled & lt ; int & gt ;, CancellationCode & lt ; chr & gt ;, ## # Diverted & lt ; int & gt ; Helper functions for variable selection \u00b6 select : starts_with(\"X\") ; every name that starts with \"X\" , ends_with(\"X\") ; every name that ends with \"X\" , contains(\"X\") ; every name that contains \"X\" , matches(\"X\") ; every name that matches \"X\" , where \"X\" can be a regular expression, num_range(\"x\", 1:5) ; the variables named x01 , x02 , x03 , x04 and x05 , one_of(x) ; every name that appears in x , which should be a character vector. 1 2 # Print out a tbl containing just ArrDelay and DepDelay select ( hflights , ArrDelay , DepDelay ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 ## # A tibble: 227,496 \u00d7 2 ## ArrDelay DepDelay ## * &lt;int&gt; &lt;int&gt; ## 1 -10 0 ## 2 -9 1 ## 3 -8 -8 ## 4 3 3 ## 5 -3 5 ## 6 -7 -1 ## 7 -1 -1 ## 8 -16 -5 ## 9 44 43 ## 10 43 43 ## # ... with 227,486 more rows 1 2 # Print out a tbl as described in the second instruction, using both helper functions and variable names select ( hflights , UniqueCarrier , ends_with ( 'Num' ), starts_with ( 'Cancel' )) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 ## # A tibble: 227,496 \u00d7 5 ## UniqueCarrier FlightNum TailNum Cancelled CancellationCode ## * &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; ## 1 American 428 N576AA 0 ## 2 American 428 N557AA 0 ## 3 American 428 N541AA 0 ## 4 American 428 N403AA 0 ## 5 American 428 N492AA 0 ## 6 American 428 N262AA 0 ## 7 American 428 N493AA 0 ## 8 American 428 N477AA 0 ## 9 American 428 N476AA 0 ## 10 American 428 N504AA 0 ## # ... with 227,486 more rows 1 2 # Print out a tbl as described in the third instruction, using only helper functions. select ( hflights , ends_with ( 'Time' ), ends_with ( 'Delay' )) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 ## # A tibble: 227,496 \u00d7 6 ## DepTime ArrTime ActualElapsedTime AirTime ArrDelay DepDelay ## * &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 1400 1500 60 40 -10 0 ## 2 1401 1501 60 45 -9 1 ## 3 1352 1502 70 48 -8 -8 ## 4 1403 1513 70 39 3 3 ## 5 1405 1507 62 44 -3 5 ## 6 1359 1503 64 45 -7 -1 ## 7 1359 1509 70 43 -1 -1 ## 8 1355 1454 59 40 -16 -5 ## 9 1443 1554 71 41 44 43 ## 10 1443 1553 70 45 43 43 ## # ... with 227,486 more rows Comparison to basic R \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 # add ex1r <- hflights [c ( 'TaxiIn' , 'TaxiOut' , 'Distance' ) ] ex1d <- select ( hflights , starts_with ( 'Taxi' ), Distance ) ex2r <- hflights [c ( 'Year' , 'Month' , 'DayOfWeek' , 'DepTime' , 'ArrTime' ) ] ex2d <- select ( hflights , Year , Month , DayOfWeek , DepTime , ArrTime ) ex3r <- hflights [c ( 'TailNum' , 'TaxiIn' , 'TaxiOut' ) ] ex3d <- select ( hflights , TailNum , starts_with ( 'Taxi' )) mutate is creating \u00b6 1 2 3 # Add the new variable ActualGroundTime to a copy of hflights and save the result as g1 g1 <- mutate ( hflights , ActualGroundTime = ActualElapsedTime - AirTime ) glimpse ( hflights ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 ## Observations: 227,496 ## Variables: 21 ## $ Year &lt;int&gt; 2011, 2011, 2011, 2011, 2011, 2011, 2011, 20... ## $ Month &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,... ## $ DayofMonth &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 1... ## $ DayOfWeek &lt;int&gt; 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6,... ## $ DepTime &lt;int&gt; 1400, 1401, 1352, 1403, 1405, 1359, 1359, 13... ## $ ArrTime &lt;int&gt; 1500, 1501, 1502, 1513, 1507, 1503, 1509, 14... ## $ UniqueCarrier &lt;chr&gt; \"American\", \"American\", \"American\", \"America... ## $ FlightNum &lt;int&gt; 428, 428, 428, 428, 428, 428, 428, 428, 428,... ## $ TailNum &lt;chr&gt; \"N576AA\", \"N557AA\", \"N541AA\", \"N403AA\", \"N49... ## $ ActualElapsedTime &lt;int&gt; 60, 60, 70, 70, 62, 64, 70, 59, 71, 70, 70, ... ## $ AirTime &lt;int&gt; 40, 45, 48, 39, 44, 45, 43, 40, 41, 45, 42, ... ## $ ArrDelay &lt;int&gt; -10, -9, -8, 3, -3, -7, -1, -16, 44, 43, 29,... ## $ DepDelay &lt;int&gt; 0, 1, -8, 3, 5, -1, -1, -5, 43, 43, 29, 19, ... ## $ Origin &lt;chr&gt; \"IAH\", \"IAH\", \"IAH\", \"IAH\", \"IAH\", \"IAH\", \"I... ## $ Dest &lt;chr&gt; \"DFW\", \"DFW\", \"DFW\", \"DFW\", \"DFW\", \"DFW\", \"D... ## $ Distance &lt;int&gt; 224, 224, 224, 224, 224, 224, 224, 224, 224,... ## $ TaxiIn &lt;int&gt; 7, 6, 5, 9, 9, 6, 12, 7, 8, 6, 8, 4, 6, 5, 6... ## $ TaxiOut &lt;int&gt; 13, 9, 17, 22, 9, 13, 15, 12, 22, 19, 20, 11... ## $ Cancelled &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,... ## $ CancellationCode &lt;chr&gt; \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", ... ## $ Diverted &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,... 1 glimpse ( g1 ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 ## Observations: 227,496 ## Variables: 22 ## $ Year &lt;int&gt; 2011, 2011, 2011, 2011, 2011, 2011, 2011, 20... ## $ Month &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,... ## $ DayofMonth &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 1... ## $ DayOfWeek &lt;int&gt; 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6,... ## $ DepTime &lt;int&gt; 1400, 1401, 1352, 1403, 1405, 1359, 1359, 13... ## $ ArrTime &lt;int&gt; 1500, 1501, 1502, 1513, 1507, 1503, 1509, 14... ## $ UniqueCarrier &lt;chr&gt; \"American\", \"American\", \"American\", \"America... ## $ FlightNum &lt;int&gt; 428, 428, 428, 428, 428, 428, 428, 428, 428,... ## $ TailNum &lt;chr&gt; \"N576AA\", \"N557AA\", \"N541AA\", \"N403AA\", \"N49... ## $ ActualElapsedTime &lt;int&gt; 60, 60, 70, 70, 62, 64, 70, 59, 71, 70, 70, ... ## $ AirTime &lt;int&gt; 40, 45, 48, 39, 44, 45, 43, 40, 41, 45, 42, ... ## $ ArrDelay &lt;int&gt; -10, -9, -8, 3, -3, -7, -1, -16, 44, 43, 29,... ## $ DepDelay &lt;int&gt; 0, 1, -8, 3, 5, -1, -1, -5, 43, 43, 29, 19, ... ## $ Origin &lt;chr&gt; \"IAH\", \"IAH\", \"IAH\", \"IAH\", \"IAH\", \"IAH\", \"I... ## $ Dest &lt;chr&gt; \"DFW\", \"DFW\", \"DFW\", \"DFW\", \"DFW\", \"DFW\", \"D... ## $ Distance &lt;int&gt; 224, 224, 224, 224, 224, 224, 224, 224, 224,... ## $ TaxiIn &lt;int&gt; 7, 6, 5, 9, 9, 6, 12, 7, 8, 6, 8, 4, 6, 5, 6... ## $ TaxiOut &lt;int&gt; 13, 9, 17, 22, 9, 13, 15, 12, 22, 19, 20, 11... ## $ Cancelled &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,... ## $ CancellationCode &lt;chr&gt; \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", ... ## $ Diverted &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,... ## $ ActualGroundTime &lt;int&gt; 20, 15, 22, 31, 18, 19, 27, 19, 30, 25, 28, ... 1 2 3 4 # Add the new variable GroundTime to a g1; save the result as g2 g2 <- mutate ( g1 , GroundTime = TaxiIn + TaxiOut ) head ( g1 $ ActualGroundTime == g2 $ GroundTime , 20 ) 1 2 ## [1] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE ## [15] TRUE TRUE TRUE TRUE TRUE TRUE 1 2 3 # Add the new variable AverageSpeed to g2; save the result as g3 g3 <- mutate ( g2 , AverageSpeed = Distance / AirTime * 60 ) g3 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 ## # A tibble : 227 , 496 \u00d7 24 ## Year Month DayofMonth DayOfWeek DepTime ArrTime UniqueCarrier ## & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; chr & gt ; ## 1 2011 1 1 6 1400 1500 American ## 2 2011 1 2 7 1401 1501 American ## 3 2011 1 3 1 1352 1502 American ## 4 2011 1 4 2 1403 1513 American ## 5 2011 1 5 3 1405 1507 American ## 6 2011 1 6 4 1359 1503 American ## 7 2011 1 7 5 1359 1509 American ## 8 2011 1 8 6 1355 1454 American ## 9 2011 1 9 7 1443 1554 American ## 10 2011 1 10 1 1443 1553 American ## # ... with 227 , 486 more rows , and 17 more variables : FlightNum & lt ; int & gt ;, ## # TailNum & lt ; chr & gt ;, ActualElapsedTime & lt ; int & gt ;, AirTime & lt ; int & gt ;, ArrDelay & lt ; int & gt ;, ## # DepDelay & lt ; int & gt ;, Origin & lt ; chr & gt ;, Dest & lt ; chr & gt ;, Distance & lt ; int & gt ;, ## # TaxiIn & lt ; int & gt ;, TaxiOut & lt ; int & gt ;, Cancelled & lt ; int & gt ;, CancellationCode & lt ; chr & gt ;, ## # Diverted & lt ; int & gt ;, ActualGroundTime & lt ; int & gt ;, GroundTime & lt ; int & gt ;, ## # AverageSpeed & lt ; dbl & gt ; Add multiple variables using mutate \u00b6 1 2 3 4 5 6 7 8 9 # Add a second variable loss_percent to the dataset: m1 m1 <- mutate ( hflights , loss = ArrDelay - DepDelay , loss_percent = ( ArrDelay - DepDelay ) / DepDelay * 100 ) # Copy and adapt the previous command to reduce redendancy: m2 m2 <- mutate ( hflights , loss = ArrDelay - DepDelay , loss_percent = loss / DepDelay * 100 ) # Add the three variables as described in the third instruction: m3 m3 <- mutate ( hflights , TotalTaxi = TaxiIn + TaxiOut , ActualGroundTime = ActualElapsedTime - AirTime , Diff = TotalTaxi - ActualGroundTime ) glimpse ( m3 ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 ## Observations: 227,496 ## Variables: 24 ## $ Year &lt;int&gt; 2011, 2011, 2011, 2011, 2011, 2011, 2011, 20... ## $ Month &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,... ## $ DayofMonth &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 1... ## $ DayOfWeek &lt;int&gt; 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6,... ## $ DepTime &lt;int&gt; 1400, 1401, 1352, 1403, 1405, 1359, 1359, 13... ## $ ArrTime &lt;int&gt; 1500, 1501, 1502, 1513, 1507, 1503, 1509, 14... ## $ UniqueCarrier &lt;chr&gt; \"American\", \"American\", \"American\", \"America... ## $ FlightNum &lt;int&gt; 428, 428, 428, 428, 428, 428, 428, 428, 428,... ## $ TailNum &lt;chr&gt; \"N576AA\", \"N557AA\", \"N541AA\", \"N403AA\", \"N49... ## $ ActualElapsedTime &lt;int&gt; 60, 60, 70, 70, 62, 64, 70, 59, 71, 70, 70, ... ## $ AirTime &lt;int&gt; 40, 45, 48, 39, 44, 45, 43, 40, 41, 45, 42, ... ## $ ArrDelay &lt;int&gt; -10, -9, -8, 3, -3, -7, -1, -16, 44, 43, 29,... ## $ DepDelay &lt;int&gt; 0, 1, -8, 3, 5, -1, -1, -5, 43, 43, 29, 19, ... ## $ Origin &lt;chr&gt; \"IAH\", \"IAH\", \"IAH\", \"IAH\", \"IAH\", \"IAH\", \"I... ## $ Dest &lt;chr&gt; \"DFW\", \"DFW\", \"DFW\", \"DFW\", \"DFW\", \"DFW\", \"D... ## $ Distance &lt;int&gt; 224, 224, 224, 224, 224, 224, 224, 224, 224,... ## $ TaxiIn &lt;int&gt; 7, 6, 5, 9, 9, 6, 12, 7, 8, 6, 8, 4, 6, 5, 6... ## $ TaxiOut &lt;int&gt; 13, 9, 17, 22, 9, 13, 15, 12, 22, 19, 20, 11... ## $ Cancelled &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,... ## $ CancellationCode &lt;chr&gt; \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", ... ## $ Diverted &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,... ## $ TotalTaxi &lt;int&gt; 20, 15, 22, 31, 18, 19, 27, 19, 30, 25, 28, ... ## $ ActualGroundTime &lt;int&gt; 20, 15, 22, 31, 18, 19, 27, 19, 30, 25, 28, ... ## $ Diff &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,... filter and arrange \u00b6 Logical operators \u00b6 filter : x < y ; TRUE if x is less than y . x <= y ; TRUE if x is less than or equal to y . x == y ; TRUE if x equals y . x != y ; TRUE if x does not equal y . x >= y ; TRUE if x is greater than or equal to y . x > y ; TRUE if x is greater than y . x %in% c(a, b, c) ; TRUE if x is in the vector c(a, b, c) . 1 2 # All flights that traveled 3000 miles or more filter ( hflights , Distance >= 3000 ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 ## # A tibble : 527 \u00d7 21 ## Year Month DayofMonth DayOfWeek DepTime ArrTime UniqueCarrier ## & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; chr & gt ; ## 1 2011 1 31 1 924 1413 Continental ## 2 2011 1 30 7 925 1410 Continental ## 3 2011 1 29 6 1045 1445 Continental ## 4 2011 1 28 5 1516 1916 Continental ## 5 2011 1 27 4 950 1344 Continental ## 6 2011 1 26 3 944 1350 Continental ## 7 2011 1 25 2 924 1337 Continental ## 8 2011 1 24 1 1144 1605 Continental ## 9 2011 1 23 7 926 1335 Continental ## 10 2011 1 22 6 942 1340 Continental ## # ... with 517 more rows , and 14 more variables : FlightNum & lt ; int & gt ;, ## # TailNum & lt ; chr & gt ;, ActualElapsedTime & lt ; int & gt ;, AirTime & lt ; int & gt ;, ArrDelay & lt ; int & gt ;, ## # DepDelay & lt ; int & gt ;, Origin & lt ; chr & gt ;, Dest & lt ; chr & gt ;, Distance & lt ; int & gt ;, ## # TaxiIn & lt ; int & gt ;, TaxiOut & lt ; int & gt ;, Cancelled & lt ; int & gt ;, CancellationCode & lt ; chr & gt ;, ## # Diverted & lt ; int & gt ; 1 2 # All flights flown by one of JetBlue, Southwest, or Delta filter ( hflights , UniqueCarrier %in% c ( 'JetBlue' , 'Southwest' , 'Delta' )) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 ## # A tibble : 48 , 679 \u00d7 21 ## Year Month DayofMonth DayOfWeek DepTime ArrTime UniqueCarrier ## & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; chr & gt ; ## 1 2011 1 1 6 654 1124 JetBlue ## 2 2011 1 1 6 1639 2110 JetBlue ## 3 2011 1 2 7 703 1113 JetBlue ## 4 2011 1 2 7 1604 2040 JetBlue ## 5 2011 1 3 1 659 1100 JetBlue ## 6 2011 1 3 1 1801 2200 JetBlue ## 7 2011 1 4 2 654 1103 JetBlue ## 8 2011 1 4 2 1608 2034 JetBlue ## 9 2011 1 5 3 700 1103 JetBlue ## 10 2011 1 5 3 1544 1954 JetBlue ## # ... with 48 , 669 more rows , and 14 more variables : FlightNum & lt ; int & gt ;, ## # TailNum & lt ; chr & gt ;, ActualElapsedTime & lt ; int & gt ;, AirTime & lt ; int & gt ;, ArrDelay & lt ; int & gt ;, ## # DepDelay & lt ; int & gt ;, Origin & lt ; chr & gt ;, Dest & lt ; chr & gt ;, Distance & lt ; int & gt ;, ## # TaxiIn & lt ; int & gt ;, TaxiOut & lt ; int & gt ;, Cancelled & lt ; int & gt ;, CancellationCode & lt ; chr & gt ;, ## # Diverted & lt ; int & gt ; 1 2 # All flights where taxiing took longer than flying filter ( hflights , ( TaxiIn + TaxiOut ) > AirTime ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 ## # A tibble : 1 , 389 \u00d7 21 ## Year Month DayofMonth DayOfWeek DepTime ArrTime UniqueCarrier ## & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; chr & gt ; ## 1 2011 1 24 1 731 904 American ## 2 2011 1 30 7 1959 2132 American ## 3 2011 1 24 1 1621 1749 American ## 4 2011 1 10 1 941 1113 American ## 5 2011 1 31 1 1301 1356 Continental ## 6 2011 1 31 1 2113 2215 Continental ## 7 2011 1 31 1 1434 1539 Continental ## 8 2011 1 31 1 900 1006 Continental ## 9 2011 1 30 7 1304 1408 Continental ## 10 2011 1 30 7 2004 2128 Continental ## # ... with 1 , 379 more rows , and 14 more variables : FlightNum & lt ; int & gt ;, ## # TailNum & lt ; chr & gt ;, ActualElapsedTime & lt ; int & gt ;, AirTime & lt ; int & gt ;, ArrDelay & lt ; int & gt ;, ## # DepDelay & lt ; int & gt ;, Origin & lt ; chr & gt ;, Dest & lt ; chr & gt ;, Distance & lt ; int & gt ;, ## # TaxiIn & lt ; int & gt ;, TaxiOut & lt ; int & gt ;, Cancelled & lt ; int & gt ;, CancellationCode & lt ; chr & gt ;, ## # Diverted & lt ; int & gt ; Combining tests using boolean operators \u00b6 1 2 # All flights that departed before 5am or arrived after 10pm filter ( hflights , DepTime < 500 | ArrTime > 2200 ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 ## # A tibble : 27 , 799 \u00d7 21 ## Year Month DayofMonth DayOfWeek DepTime ArrTime UniqueCarrier ## & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; chr & gt ; ## 1 2011 1 4 2 2100 2207 American ## 2 2011 1 14 5 2119 2229 American ## 3 2011 1 10 1 1934 2235 American ## 4 2011 1 26 3 1905 2211 American ## 5 2011 1 30 7 1856 2209 American ## 6 2011 1 9 7 1938 2228 Alaska ## 7 2011 1 31 1 1919 2231 Continental ## 8 2011 1 31 1 2116 2344 Continental ## 9 2011 1 31 1 1850 2211 Continental ## 10 2011 1 31 1 2102 2216 Continental ## # ... with 27 , 789 more rows , and 14 more variables : FlightNum & lt ; int & gt ;, ## # TailNum & lt ; chr & gt ;, ActualElapsedTime & lt ; int & gt ;, AirTime & lt ; int & gt ;, ArrDelay & lt ; int & gt ;, ## # DepDelay & lt ; int & gt ;, Origin & lt ; chr & gt ;, Dest & lt ; chr & gt ;, Distance & lt ; int & gt ;, ## # TaxiIn & lt ; int & gt ;, TaxiOut & lt ; int & gt ;, Cancelled & lt ; int & gt ;, CancellationCode & lt ; chr & gt ;, ## # Diverted & lt ; int & gt ; 1 2 # All flights that departed late but arrived ahead of schedule filter ( hflights , DepDelay > 0 & ArrDelay < 0 ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 ## # A tibble : 27 , 712 \u00d7 21 ## Year Month DayofMonth DayOfWeek DepTime ArrTime UniqueCarrier ## & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; chr & gt ; ## 1 2011 1 2 7 1401 1501 American ## 2 2011 1 5 3 1405 1507 American ## 3 2011 1 18 2 1408 1508 American ## 4 2011 1 18 2 721 827 American ## 5 2011 1 12 3 2015 2113 American ## 6 2011 1 13 4 2020 2116 American ## 7 2011 1 26 3 2009 2103 American ## 8 2011 1 1 6 1631 1736 American ## 9 2011 1 10 1 1639 1740 American ## 10 2011 1 12 3 1631 1739 American ## # ... with 27 , 702 more rows , and 14 more variables : FlightNum & lt ; int & gt ;, ## # TailNum & lt ; chr & gt ;, ActualElapsedTime & lt ; int & gt ;, AirTime & lt ; int & gt ;, ArrDelay & lt ; int & gt ;, ## # DepDelay & lt ; int & gt ;, Origin & lt ; chr & gt ;, Dest & lt ; chr & gt ;, Distance & lt ; int & gt ;, ## # TaxiIn & lt ; int & gt ;, TaxiOut & lt ; int & gt ;, Cancelled & lt ; int & gt ;, CancellationCode & lt ; chr & gt ;, ## # Diverted & lt ; int & gt ; 1 2 # All cancelled weekend flights filter ( hflights , DayOfWeek %in% c ( 6 , 7 ) & Cancelled == 1 ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 ## # A tibble : 585 \u00d7 21 ## Year Month DayofMonth DayOfWeek DepTime ArrTime UniqueCarrier ## & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; chr & gt ; ## 1 2011 1 9 7 NA NA American ## 2 2011 1 29 6 NA NA Continental ## 3 2011 1 9 7 NA NA Continental ## 4 2011 1 9 7 NA NA Delta ## 5 2011 1 9 7 NA NA SkyWest ## 6 2011 1 2 7 NA NA Southwest ## 7 2011 1 29 6 NA NA Delta ## 8 2011 1 9 7 NA NA Atlantic_Southeast ## 9 2011 1 1 6 NA NA AirTran ## 10 2011 1 9 7 NA NA AirTran ## # ... with 575 more rows , and 14 more variables : FlightNum & lt ; int & gt ;, ## # TailNum & lt ; chr & gt ;, ActualElapsedTime & lt ; int & gt ;, AirTime & lt ; int & gt ;, ArrDelay & lt ; int & gt ;, ## # DepDelay & lt ; int & gt ;, Origin & lt ; chr & gt ;, Dest & lt ; chr & gt ;, Distance & lt ; int & gt ;, ## # TaxiIn & lt ; int & gt ;, TaxiOut & lt ; int & gt ;, Cancelled & lt ; int & gt ;, CancellationCode & lt ; chr & gt ;, ## # Diverted & lt ; int & gt ; 1 2 # All flights that were cancelled after being delayed filter ( hflights , DepDelay > 0 & Cancelled == 1 ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 ## # A tibble : 40 \u00d7 21 ## Year Month DayofMonth DayOfWeek DepTime ArrTime UniqueCarrier ## & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; chr & gt ; ## 1 2011 1 26 3 1926 NA Continental ## 2 2011 1 11 2 1100 NA US_Airways ## 3 2011 1 19 3 1811 NA ExpressJet ## 4 2011 1 7 5 2028 NA ExpressJet ## 5 2011 2 4 5 1638 NA American ## 6 2011 2 8 2 1057 NA Continental ## 7 2011 2 2 3 802 NA ExpressJet ## 8 2011 2 9 3 904 NA ExpressJet ## 9 2011 2 1 2 1508 NA SkyWest ## 10 2011 3 31 4 1016 NA Continental ## # ... with 30 more rows , and 14 more variables : FlightNum & lt ; int & gt ;, ## # TailNum & lt ; chr & gt ;, ActualElapsedTime & lt ; int & gt ;, AirTime & lt ; int & gt ;, ArrDelay & lt ; int & gt ;, ## # DepDelay & lt ; int & gt ;, Origin & lt ; chr & gt ;, Dest & lt ; chr & gt ;, Distance & lt ; int & gt ;, ## # TaxiIn & lt ; int & gt ;, TaxiOut & lt ; int & gt ;, Cancelled & lt ; int & gt ;, CancellationCode & lt ; chr & gt ;, ## # Diverted & lt ; int & gt ; Blend together \u00b6 1 2 3 4 5 6 7 8 # Select the flights that had JFK as their destination: c1 c1 <- filter ( hflights , Dest == 'JFK' ) # Combine the Year, Month and DayofMonth variables to create a Date column: c2 c2 <- mutate ( c1 , Date = paste ( Year , Month , DayofMonth , sep = '-' )) # Print out a selection of columns of c2 select ( c2 , Date , DepTime , ArrTime , TailNum ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 ## # A tibble: 695 \u00d7 4 ## Date DepTime ArrTime TailNum ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; ## 1 2011-1-1 654 1124 N324JB ## 2 2011-1-1 1639 2110 N324JB ## 3 2011-1-2 703 1113 N324JB ## 4 2011-1-2 1604 2040 N324JB ## 5 2011-1-3 659 1100 N229JB ## 6 2011-1-3 1801 2200 N206JB ## 7 2011-1-4 654 1103 N267JB ## 8 2011-1-4 1608 2034 N267JB ## 9 2011-1-5 700 1103 N708JB ## 10 2011-1-5 1544 1954 N644JB ## # ... with 685 more rows Arranging your data \u00b6 1 2 3 4 5 # Definition of dtc dtc <- filter ( hflights , Cancelled == 1 , ! is.na ( DepDelay )) # Arrange dtc by departure delays arrange ( dtc , DepDelay ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 ## # A tibble : 68 \u00d7 21 ## Year Month DayofMonth DayOfWeek DepTime ArrTime UniqueCarrier ## & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; chr & gt ; ## 1 2011 7 23 6 605 NA Frontier ## 2 2011 1 17 1 916 NA ExpressJet ## 3 2011 12 1 4 541 NA US_Airways ## 4 2011 10 12 3 2022 NA American_Eagle ## 5 2011 7 29 5 1424 NA Continental ## 6 2011 9 29 4 1639 NA SkyWest ## 7 2011 2 9 3 555 NA American_Eagle ## 8 2011 5 9 1 715 NA SkyWest ## 9 2011 1 20 4 1413 NA United ## 10 2011 1 17 1 831 NA Southwest ## # ... with 58 more rows , and 14 more variables : FlightNum & lt ; int & gt ;, ## # TailNum & lt ; chr & gt ;, ActualElapsedTime & lt ; int & gt ;, AirTime & lt ; int & gt ;, ArrDelay & lt ; int & gt ;, ## # DepDelay & lt ; int & gt ;, Origin & lt ; chr & gt ;, Dest & lt ; chr & gt ;, Distance & lt ; int & gt ;, ## # TaxiIn & lt ; int & gt ;, TaxiOut & lt ; int & gt ;, Cancelled & lt ; int & gt ;, CancellationCode & lt ; chr & gt ;, ## # Diverted & lt ; int & gt ; 1 2 # Arrange dtc so that cancellation reasons are grouped arrange ( dtc , CancellationCode ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 ## # A tibble : 68 \u00d7 21 ## Year Month DayofMonth DayOfWeek DepTime ArrTime UniqueCarrier ## & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; chr & gt ; ## 1 2011 1 20 4 1413 NA United ## 2 2011 1 7 5 2028 NA ExpressJet ## 3 2011 2 4 5 1638 NA American ## 4 2011 2 8 2 1057 NA Continental ## 5 2011 2 1 2 1508 NA SkyWest ## 6 2011 2 21 1 2257 NA SkyWest ## 7 2011 2 9 3 555 NA American_Eagle ## 8 2011 3 18 5 727 NA United ## 9 2011 4 4 1 1632 NA Delta ## 10 2011 4 8 5 1608 NA Southwest ## # ... with 58 more rows , and 14 more variables : FlightNum & lt ; int & gt ;, ## # TailNum & lt ; chr & gt ;, ActualElapsedTime & lt ; int & gt ;, AirTime & lt ; int & gt ;, ArrDelay & lt ; int & gt ;, ## # DepDelay & lt ; int & gt ;, Origin & lt ; chr & gt ;, Dest & lt ; chr & gt ;, Distance & lt ; int & gt ;, ## # TaxiIn & lt ; int & gt ;, TaxiOut & lt ; int & gt ;, Cancelled & lt ; int & gt ;, CancellationCode & lt ; chr & gt ;, ## # Diverted & lt ; int & gt ; 1 2 # Arrange dtc according to carrier and departure delays arrange ( dtc , UniqueCarrier , DepDelay ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 ## # A tibble : 68 \u00d7 21 ## Year Month DayofMonth DayOfWeek DepTime ArrTime UniqueCarrier ## & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; chr & gt ; ## 1 2011 6 11 6 1649 NA AirTran ## 2 2011 8 18 4 1808 NA American ## 3 2011 2 4 5 1638 NA American ## 4 2011 10 12 3 2022 NA American_Eagle ## 5 2011 2 9 3 555 NA American_Eagle ## 6 2011 7 17 7 1917 NA American_Eagle ## 7 2011 4 30 6 612 NA Atlantic_Southeast ## 8 2011 4 10 7 1147 NA Atlantic_Southeast ## 9 2011 5 23 1 657 NA Atlantic_Southeast ## 10 2011 9 29 4 723 NA Atlantic_Southeast ## # ... with 58 more rows , and 14 more variables : FlightNum & lt ; int & gt ;, ## # TailNum & lt ; chr & gt ;, ActualElapsedTime & lt ; int & gt ;, AirTime & lt ; int & gt ;, ArrDelay & lt ; int & gt ;, ## # DepDelay & lt ; int & gt ;, Origin & lt ; chr & gt ;, Dest & lt ; chr & gt ;, Distance & lt ; int & gt ;, ## # TaxiIn & lt ; int & gt ;, TaxiOut & lt ; int & gt ;, Cancelled & lt ; int & gt ;, CancellationCode & lt ; chr & gt ;, ## # Diverted & lt ; int & gt ; Reverse the order of arranging \u00b6 1 2 # Arrange according to carrier and decreasing departure delays arrange ( hflights , UniqueCarrier , desc ( DepDelay )) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 ## # A tibble : 227 , 496 \u00d7 21 ## Year Month DayofMonth DayOfWeek DepTime ArrTime UniqueCarrier ## & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; chr & gt ; ## 1 2011 2 19 6 1902 2143 AirTran ## 2 2011 3 14 1 2024 2309 AirTran ## 3 2011 2 16 3 2349 227 AirTran ## 4 2011 11 13 7 2312 213 AirTran ## 5 2011 5 26 4 2353 305 AirTran ## 6 2011 5 26 4 1922 2229 AirTran ## 7 2011 4 28 4 1045 1328 AirTran ## 8 2011 6 5 7 2207 52 AirTran ## 9 2011 5 7 6 1009 1256 AirTran ## 10 2011 7 25 1 2107 14 AirTran ## # ... with 227 , 486 more rows , and 14 more variables : FlightNum & lt ; int & gt ;, ## # TailNum & lt ; chr & gt ;, ActualElapsedTime & lt ; int & gt ;, AirTime & lt ; int & gt ;, ArrDelay & lt ; int & gt ;, ## # DepDelay & lt ; int & gt ;, Origin & lt ; chr & gt ;, Dest & lt ; chr & gt ;, Distance & lt ; int & gt ;, ## # TaxiIn & lt ; int & gt ;, TaxiOut & lt ; int & gt ;, Cancelled & lt ; int & gt ;, CancellationCode & lt ; chr & gt ;, ## # Diverted & lt ; int & gt ; 1 2 # Arrange flights by total delay (normal order). arrange ( hflights , ( ArrDelay + DepDelay )) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 ## # A tibble : 227 , 496 \u00d7 21 ## Year Month DayofMonth DayOfWeek DepTime ArrTime UniqueCarrier ## & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; chr & gt ; ## 1 2011 7 3 7 1914 2039 ExpressJet ## 2 2011 8 31 3 934 1039 SkyWest ## 3 2011 8 21 7 935 1039 SkyWest ## 4 2011 8 28 7 2059 2206 SkyWest ## 5 2011 8 29 1 935 1041 SkyWest ## 6 2011 12 25 7 741 926 SkyWest ## 7 2011 1 30 7 620 812 SkyWest ## 8 2011 8 3 3 1741 1810 ExpressJet ## 9 2011 8 4 4 930 1041 SkyWest ## 10 2011 8 18 4 939 1043 SkyWest ## # ... with 227 , 486 more rows , and 14 more variables : FlightNum & lt ; int & gt ;, ## # TailNum & lt ; chr & gt ;, ActualElapsedTime & lt ; int & gt ;, AirTime & lt ; int & gt ;, ArrDelay & lt ; int & gt ;, ## # DepDelay & lt ; int & gt ;, Origin & lt ; chr & gt ;, Dest & lt ; chr & gt ;, Distance & lt ; int & gt ;, ## # TaxiIn & lt ; int & gt ;, TaxiOut & lt ; int & gt ;, Cancelled & lt ; int & gt ;, CancellationCode & lt ; chr & gt ;, ## # Diverted & lt ; int & gt ; 1 2 # Keep flights leaving to DFW before 8am and arrange according to decreasing AirTime arrange ( filter ( hflights , Dest == 'DFW' & DepTime < 800 ), desc ( AirTime )) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 ## # A tibble : 799 \u00d7 21 ## Year Month DayofMonth DayOfWeek DepTime ArrTime UniqueCarrier ## & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; chr & gt ; ## 1 2011 11 22 2 635 825 American ## 2 2011 8 25 4 602 758 American_Eagle ## 3 2011 10 12 3 559 738 American_Eagle ## 4 2011 5 2 1 716 854 American ## 5 2011 4 4 1 741 949 American ## 6 2011 4 4 1 627 742 American_Eagle ## 7 2011 6 21 2 726 848 ExpressJet ## 8 2011 9 1 4 715 844 American ## 9 2011 3 14 1 729 917 Continental ## 10 2011 12 5 1 724 847 Continental ## # ... with 789 more rows , and 14 more variables : FlightNum & lt ; int & gt ;, ## # TailNum & lt ; chr & gt ;, ActualElapsedTime & lt ; int & gt ;, AirTime & lt ; int & gt ;, ArrDelay & lt ; int & gt ;, ## # DepDelay & lt ; int & gt ;, Origin & lt ; chr & gt ;, Dest & lt ; chr & gt ;, Distance & lt ; int & gt ;, ## # TaxiIn & lt ; int & gt ;, TaxiOut & lt ; int & gt ;, Cancelled & lt ; int & gt ;, CancellationCode & lt ; chr & gt ;, ## # Diverted & lt ; int & gt ; summarise and the Pipe Operator \u00b6 The syntax of summarise \u00b6 1 2 # Print out a summary with variables min_dist and max_dist summarise ( hflights , min_dist = min ( Distance ), max_dist = max ( Distance )) 1 2 3 4 ## # A tibble: 1 \u00d7 2 ## min_dist max_dist ## &lt;int&gt; &lt;int&gt; ## 1 79 3904 1 2 # Print out a summary with variable max_div summarise ( filter ( hflights , Diverted == 1 ), max_div = max ( Distance )) 1 2 3 4 ## # A tibble: 1 \u00d7 1 ## max_div ## &lt;int&gt; ## 1 3904 Aggregate functions \u00b6 summarise : min(x) ; minimum value of vector x. max(x) ; maximum value of vector x. mean(x) ; mean value of vector x. median(x) ; median value of vector x. quantile(x, p) ; pth quantile of vector x. sd(x) ; standard deviation of vector x. var(x) ; variance of vector x. IQR(x) ; Inter Quartile Range (IQR) of vector x. diff(range(x)) ; total range of vector x. 1 2 3 4 5 # Remove rows that have NA ArrDelay: temp1 temp1 <- filter ( hflights , ! is.na ( ArrDelay )) # Generate summary about ArrDelay column of temp1 summarise ( temp1 , earliest = min ( ArrDelay ), average = mean ( ArrDelay ), latest = max ( ArrDelay ), sd = sd ( ArrDelay )) 1 2 3 4 ## # A tibble: 1 \u00d7 4 ## earliest average latest sd ## &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 -70 7.094334 978 30.70852 1 2 3 4 5 # Keep rows that have no NA TaxiIn and no NA TaxiOut: temp2 temp2 <- filter ( hflights , ! is.na ( TaxiIn ), ! is.na ( TaxiOut )) # Print the maximum taxiing difference of temp2 with summarise() summarise ( temp2 , max_taxi_diff = max ( abs ( TaxiIn - TaxiOut ))) 1 2 3 4 ## # A tibble: 1 \u00d7 1 ## max_taxi_diff ## &lt;int&gt; ## 1 160 dplyr aggregate functions \u00b6 first(x) ; the first element of vector x . last(x) ; the last element of vector x . nth(x, n) ; The n th element of vector x . n() ; The number of rows in the data.frame or group of observations that summarise() describes. n_distinct(x) ; The number of unique values in vector x . 1 2 # Generate summarizing statistics for hflights summarise ( hflights , n_obs = n (), n_carrier = n_distinct ( UniqueCarrier ), n_dest = n_distinct ( Dest ), dest100 = nth ( Dest , 100 )) 1 2 3 4 ## # A tibble: 1 \u00d7 4 ## n_obs n_carrier n_dest dest100 ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; ## 1 227496 15 116 DFW 1 2 3 4 5 # Filter hflights to keep all American Airline flights: aa aa <- filter ( hflights , UniqueCarrier == 'American' ) # Generate summarizing statistics for aa summarise ( aa , n_flights = n (), n_canc = sum ( Cancelled ), p_canc = n_canc / n_flights * 100 , avg_delay = mean ( ArrDelay , na.rm = TRUE )) 1 2 3 4 ## # A tibble: 1 \u00d7 4 ## n_flights n_canc p_canc avg_delay ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 3244 60 1.849568 0.8917558 Overview of syntax 1 2 3 4 5 # Write the 'piped' version of the English sentences hflights %>% mutate ( diff = TaxiOut - TaxiIn ) %>% filter ( ! is.na ( diff )) %>% summarise ( avg = mean ( diff )) 1 2 3 4 ## # A tibble: 1 \u00d7 1 ## avg ## &lt;dbl&gt; ## 1 8.992064 Drive or fly? Part 1 of 2 1 2 3 4 5 6 7 8 9 10 11 12 # Part 1, concerning the selection and creation of columns d <- hflights %>% select ( Dest , UniqueCarrier , Distance , ActualElapsedTime ) %>% mutate ( RealTime = ActualElapsedTime + 100 , mph = Distance / RealTime * 60 ) # Part 2, concerning flights that had an actual average speed of < 70 mph. d %>% filter ( ! is.na ( mph ), mph < 70 ) %>% summarise ( n_less = n (), n_dest = n_distinct ( Dest ), min_dist = min ( Distance ), max_dist = max ( Distance )) 1 2 3 4 ## # A tibble: 1 \u00d7 4 ## n_less n_dest min_dist max_dist ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 6726 13 79 305 Drive or fly? Part 2 of 2 1 2 3 4 5 # Solve the exercise using a combination of dplyr verbs and %>% hflights %>% #summarise(all_flights = n()) %>% filter ((( Distance / ( ActualElapsedTime + 100 ) * 60 ) < 105 ) | Cancelled == 1 | Diverted == 1 ) %>% summarise ( n_non = n (), p_non = n_non / 22751 * 100 , n_dest = n_distinct ( Dest ), min_dist = min ( Distance ), max_dist = max ( Distance )) 1 2 3 4 ## # A tibble: 1 \u00d7 5 ## n_non p_non n_dest min_dist max_dist ## &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 42400 186.3654 113 79 3904 Advanced piping exercise 1 2 3 4 # Count the number of overnight flights hflights %>% filter ( ArrTime < DepTime & ! is.na ( DepTime ) & ! is.na ( ArrTime )) %>% summarise ( n = n ()) 1 2 3 4 ## # A tibble: 1 \u00d7 1 ## n ## &lt;int&gt; ## 1 2718 group_by and working with data \u00b6 Unite and conquer using group_by \u00b6 1 2 3 4 5 6 7 8 # Make an ordered per-carrier summary of hflights hflights %>% group_by ( UniqueCarrier ) %>% summarise ( n_flights = n (), n_canc = sum ( Cancelled == 1 ), p_canc = mean ( Cancelled == 1 ) * 100 , avg_delay = mean ( ArrDelay , na.rm = TRUE )) %>% arrange ( avg_delay , p_canc ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 ## # A tibble: 15 \u00d7 5 ## UniqueCarrier n_flights n_canc p_canc avg_delay ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 US_Airways 4082 46 1.1268986 -0.6307692 ## 2 American 3244 60 1.8495684 0.8917558 ## 3 AirTran 2139 21 0.9817672 1.8536239 ## 4 Alaska 365 0 0.0000000 3.1923077 ## 5 Mesa 79 1 1.2658228 4.0128205 ## 6 Delta 2641 42 1.5903067 6.0841374 ## 7 Continental 70032 475 0.6782614 6.0986983 ## 8 American_Eagle 4648 135 2.9044750 7.1529751 ## 9 Atlantic_Southeast 2204 76 3.4482759 7.2569543 ## 10 Southwest 45343 703 1.5504047 7.5871430 ## 11 Frontier 838 6 0.7159905 7.6682692 ## 12 ExpressJet 73053 1132 1.5495599 8.1865242 ## 13 SkyWest 16061 224 1.3946828 8.6934922 ## 14 JetBlue 695 18 2.5899281 9.8588410 ## 15 United 2072 34 1.6409266 10.4628628 1 2 3 4 5 # Make an ordered per-day summary of hflights hflights %>% group_by ( DayOfWeek ) %>% summarise ( avg_taxi = mean ( TaxiIn + TaxiOut , na.rm = TRUE )) %>% arrange ( desc ( avg_taxi )) 1 2 3 4 5 6 7 8 9 10 ## # A tibble: 7 \u00d7 2 ## DayOfWeek avg_taxi ## &lt;int&gt; &lt;dbl&gt; ## 1 1 21.77027 ## 2 2 21.43505 ## 3 4 21.26076 ## 4 3 21.19055 ## 5 5 21.15805 ## 6 7 20.93726 ## 7 6 20.43061 Combine group_by with mutate \u00b6 1 2 3 4 5 6 7 # Solution to first instruction hflights %>% filter ( ! is.na ( ArrDelay )) %>% group_by ( UniqueCarrier ) %>% summarise ( p_delay = sum ( ArrDelay > 0 ) / n ()) %>% mutate ( rank = rank ( p_delay )) %>% arrange ( rank ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 ## # A tibble: 15 \u00d7 3 ## UniqueCarrier p_delay rank ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 American 0.3030208 1 ## 2 AirTran 0.3112269 2 ## 3 US_Airways 0.3267990 3 ## 4 Atlantic_Southeast 0.3677511 4 ## 5 American_Eagle 0.3696714 5 ## 6 Delta 0.3871092 6 ## 7 JetBlue 0.3952452 7 ## 8 Alaska 0.4368132 8 ## 9 Southwest 0.4644557 9 ## 10 Mesa 0.4743590 10 ## 11 Continental 0.4907385 11 ## 12 ExpressJet 0.4943420 12 ## 13 United 0.4963109 13 ## 14 SkyWest 0.5350105 14 ## 15 Frontier 0.5564904 15 1 2 3 4 5 6 7 # Solution to second instruction hflights %>% filter ( ! is.na ( ArrDelay ), ArrDelay > 0 ) %>% group_by ( UniqueCarrier ) %>% summarise ( avg = mean ( ArrDelay )) %>% mutate ( rank = rank ( avg )) %>% arrange ( rank ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 ## # A tibble: 15 \u00d7 3 ## UniqueCarrier avg rank ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Mesa 18.67568 1 ## 2 Frontier 18.68683 2 ## 3 US_Airways 20.70235 3 ## 4 Continental 22.13374 4 ## 5 Alaska 22.91195 5 ## 6 SkyWest 24.14663 6 ## 7 ExpressJet 24.19337 7 ## 8 Southwest 25.27750 8 ## 9 AirTran 27.85693 9 ## 10 American 28.49740 10 ## 11 Delta 32.12463 11 ## 12 United 32.48067 12 ## 13 American_Eagle 38.75135 13 ## 14 Atlantic_Southeast 40.24231 14 ## 15 JetBlue 45.47744 15 Advanced group_by \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 # Which plane (by tail number) flew out of Houston the most times? How many times? adv1 adv1 <- hflights %>% group_by ( TailNum ) %>% summarise ( n = n ()) %>% filter ( n == max ( n )) # How many airplanes only flew to one destination from Houston? adv2 adv2 <- hflights %>% group_by ( TailNum ) %>% summarise ( ndest = n_distinct ( Dest )) %>% filter ( ndest == 1 ) %>% summarise ( nplanes = n ()) # Find the most visited destination for each carrier: adv3 adv3 <- hflights %>% group_by ( UniqueCarrier , Dest ) %>% summarise ( n = n ()) %>% mutate ( rank = rank ( desc ( n ))) %>% filter ( rank == 1 ) # Find the carrier that travels to each destination the most: adv4 adv4 <- hflights %>% group_by ( Dest , UniqueCarrier ) %>% summarise ( n = n ()) %>% mutate ( rank = rank ( desc ( n ))) %>% filter ( rank == 1 ) dplyr deals with different types \u00b6 1 2 3 # Use summarise to calculate n_carrier s2 <- hflights %>% summarise ( n_carrier = n_distinct ( UniqueCarrier )) dplyr and mySQL databases \u00b6 Code only. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 # set up a src that connects to the mysql database (src_mysql is provided by dplyr) my_db <- src_mysql ( dbname = 'dplyr' , host = 'dplyr.csrrinzqubik.us-east-1.rds.amazonaws.com' , port = 3306 , user = 'dplyr' , password = 'dplyr' ) # and reference a table within that src: nycflights is now available as an R object that references to the remote nycflights table nycflights <- tbl ( my_db , 'dplyr' ) # glimpse at nycflights glimpse ( nycflights ) # Calculate the grouped summaries detailed in the instructions nycflights %>% group_by ( carrier ) %>% summarise ( n_flights = n (), avg_delay = mean ( arr_delay )) %>% arrange ( avg_delay ) Adding tidyr Functions \u00b6 complete . drop_na . expand . extract . extract_numeric . complete . fill . full_seq . gather . nest . replace_na . separate . separate_rows . separate_rows_ . smiths . spread . table1 . unite . unnest . who . Joining Data in R with dplyr \u00b6 In development 1 2 3 4 5 6 7 8 9 ## 1, Mutating Joins ## 2, Filtering Joins and Set Operations ## 3, Assembling Data ## 4, Advanced Joining ## 5, Case Study","title":"Data Manipulation in R with dplyr"},{"location":"data_manipulation_in_r_with_dplyr/#documentation","text":"data.table extension of data.frame . Fast aggregation of large data (e.g. 100GB in RAM), fast ordered joins, fast add/modify/delete of columns by group using no copies at all, list columns, a fast friendly file reader and parallel file writer. Offers a natural and flexible syntax, for faster development. dplyr A fast, consistent tool for working with data frame like objects, both in memory and out of memory. Pipelines. tidyr An evolution of \u2018reshape2\u2019. It\u2019s designed specifically for data tidying (not general reshaping or aggregating) and works well with dplyr data pipelines. package 'narrower' 'wider' tidyr gather spread reshape2 melt cast spreadsheets unpivot pivot databases fold unfold","title":"Documentation"},{"location":"data_manipulation_in_r_with_dplyr/#introduction-to-dplyr","text":"","title":"Introduction to dplyr"},{"location":"data_manipulation_in_r_with_dplyr/#load-the-dplyr-and-hflights-package","text":"1 2 3 4 5 6 7 8 9 10 # Load the dplyr package library ( dplyr ) library ( dtplyr ) # Load the hflights package # A data only package containing commercial domestic flights that departed Houston (IAH and HOU) in 2011 library ( hflights ) # Call both head() and summary() on hflights head ( hflights ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 ## Year Month DayofMonth DayOfWeek DepTime ArrTime UniqueCarrier ## 5424 2011 1 1 6 1400 1500 AA ## 5425 2011 1 2 7 1401 1501 AA ## 5426 2011 1 3 1 1352 1502 AA ## 5427 2011 1 4 2 1403 1513 AA ## 5428 2011 1 5 3 1405 1507 AA ## 5429 2011 1 6 4 1359 1503 AA ## FlightNum TailNum ActualElapsedTime AirTime ArrDelay DepDelay Origin ## 5424 428 N576AA 60 40 -10 0 IAH ## 5425 428 N557AA 60 45 -9 1 IAH ## 5426 428 N541AA 70 48 -8 -8 IAH ## 5427 428 N403AA 70 39 3 3 IAH ## 5428 428 N492AA 62 44 -3 5 IAH ## 5429 428 N262AA 64 45 -7 -1 IAH ## Dest Distance TaxiIn TaxiOut Cancelled CancellationCode Diverted ## 5424 DFW 224 7 13 0 0 ## 5425 DFW 224 6 9 0 0 ## 5426 DFW 224 5 17 0 0 ## 5427 DFW 224 9 22 0 0 ## 5428 DFW 224 9 9 0 0 ## 5429 DFW 224 6 13 0 0 1 summary ( hflights ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 ## Year Month DayofMonth DayOfWeek ## Min. :2011 Min. : 1.000 Min. : 1.00 Min. :1.000 ## 1st Qu.:2011 1st Qu.: 4.000 1st Qu.: 8.00 1st Qu.:2.000 ## Median :2011 Median : 7.000 Median :16.00 Median :4.000 ## Mean :2011 Mean : 6.514 Mean :15.74 Mean :3.948 ## 3rd Qu.:2011 3rd Qu.: 9.000 3rd Qu.:23.00 3rd Qu.:6.000 ## Max. :2011 Max. :12.000 Max. :31.00 Max. :7.000 ## ## DepTime ArrTime UniqueCarrier FlightNum ## Min. : 1 Min. : 1 Length:227496 Min. : 1 ## 1st Qu.:1021 1st Qu.:1215 Class :character 1st Qu.: 855 ## Median :1416 Median :1617 Mode :character Median :1696 ## Mean :1396 Mean :1578 Mean :1962 ## 3rd Qu.:1801 3rd Qu.:1953 3rd Qu.:2755 ## Max. :2400 Max. :2400 Max. :7290 ## NA's :2905 NA's :3066 ## TailNum ActualElapsedTime AirTime ArrDelay ## Length:227496 Min. : 34.0 Min. : 11.0 Min. :-70.000 ## Class :character 1st Qu.: 77.0 1st Qu.: 58.0 1st Qu.: -8.000 ## Mode :character Median :128.0 Median :107.0 Median : 0.000 ## Mean :129.3 Mean :108.1 Mean : 7.094 ## 3rd Qu.:165.0 3rd Qu.:141.0 3rd Qu.: 11.000 ## Max. :575.0 Max. :549.0 Max. :978.000 ## NA's :3622 NA's :3622 NA's :3622 ## DepDelay Origin Dest Distance ## Min. :-33.000 Length:227496 Length:227496 Min. : 79.0 ## 1st Qu.: -3.000 Class :character Class :character 1st Qu.: 376.0 ## Median : 0.000 Mode :character Mode :character Median : 809.0 ## Mean : 9.445 Mean : 787.8 ## 3rd Qu.: 9.000 3rd Qu.:1042.0 ## Max. :981.000 Max. :3904.0 ## NA's :2905 ## TaxiIn TaxiOut Cancelled CancellationCode ## Min. : 1.000 Min. : 1.00 Min. :0.00000 Length:227496 ## 1st Qu.: 4.000 1st Qu.: 10.00 1st Qu.:0.00000 Class :character ## Median : 5.000 Median : 14.00 Median :0.00000 Mode :character ## Mean : 6.099 Mean : 15.09 Mean :0.01307 ## 3rd Qu.: 7.000 3rd Qu.: 18.00 3rd Qu.:0.00000 ## Max. :165.000 Max. :163.00 Max. :1.00000 ## NA's :3066 NA's :2947 ## Diverted ## Min. :0.000000 ## 1st Qu.:0.000000 ## Median :0.000000 ## Mean :0.002853 ## 3rd Qu.:0.000000 ## Max. :1.000000 ##","title":"Load the dplyr and hflights package"},{"location":"data_manipulation_in_r_with_dplyr/#convert-dataframe-to-table","text":"1 2 3 4 5 # Convert the hflights data.frame into a hflights tbl hflights <- tbl_df ( hflights ) # Display the hflights tbl hflights 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 ## # A tibble : 227 , 496 \u00d7 21 ## Year Month DayofMonth DayOfWeek DepTime ArrTime UniqueCarrier ## * & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; chr & gt ; ## 1 2011 1 1 6 1400 1500 AA ## 2 2011 1 2 7 1401 1501 AA ## 3 2011 1 3 1 1352 1502 AA ## 4 2011 1 4 2 1403 1513 AA ## 5 2011 1 5 3 1405 1507 AA ## 6 2011 1 6 4 1359 1503 AA ## 7 2011 1 7 5 1359 1509 AA ## 8 2011 1 8 6 1355 1454 AA ## 9 2011 1 9 7 1443 1554 AA ## 10 2011 1 10 1 1443 1553 AA ## # ... with 227 , 486 more rows , and 14 more variables : FlightNum & lt ; int & gt ;, ## # TailNum & lt ; chr & gt ;, ActualElapsedTime & lt ; int & gt ;, AirTime & lt ; int & gt ;, ArrDelay & lt ; int & gt ;, ## # DepDelay & lt ; int & gt ;, Origin & lt ; chr & gt ;, Dest & lt ; chr & gt ;, Distance & lt ; int & gt ;, ## # TaxiIn & lt ; int & gt ;, TaxiOut & lt ; int & gt ;, Cancelled & lt ; int & gt ;, CancellationCode & lt ; chr & gt ;, ## # Diverted & lt ; int & gt ; 1 2 # Create the object carriers, containing only the UniqueCarrier variable of hflights carriers <- hflights $ UniqueCarrier","title":"Convert data.frame to table"},{"location":"data_manipulation_in_r_with_dplyr/#changing-labels-of-hflights","text":"part 1 of 2 1 2 3 4 5 6 7 8 9 10 11 # add lut <- c ( 'AA' = 'American' , 'AS' = 'Alaska' , 'B6' = 'JetBlue' , 'CO' = 'Continental' , 'DL' = 'Delta' , 'OO' = 'SkyWest' , 'UA' = 'United' , 'US' = 'US_Airways' , 'WN' = 'Southwest' , 'EV' = 'Atlantic_Southeast' , 'F9' = 'Frontier' , 'FL' = 'AirTran' , 'MQ' = 'American_Eagle' , 'XE' = 'ExpressJet' , 'YV' = 'Mesa' ) # Use lut to translate the UniqueCarrier column of hflights hflights $ UniqueCarrier <- lut[hflights $ UniqueCarrier] # Inspect the resulting raw values of your variables glimpse ( hflights ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 ## Observations: 227,496 ## Variables: 21 ## $ Year &lt;int&gt; 2011, 2011, 2011, 2011, 2011, 2011, 2011, 20... ## $ Month &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,... ## $ DayofMonth &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 1... ## $ DayOfWeek &lt;int&gt; 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6,... ## $ DepTime &lt;int&gt; 1400, 1401, 1352, 1403, 1405, 1359, 1359, 13... ## $ ArrTime &lt;int&gt; 1500, 1501, 1502, 1513, 1507, 1503, 1509, 14... ## $ UniqueCarrier &lt;chr&gt; \"American\", \"American\", \"American\", \"America... ## $ FlightNum &lt;int&gt; 428, 428, 428, 428, 428, 428, 428, 428, 428,... ## $ TailNum &lt;chr&gt; \"N576AA\", \"N557AA\", \"N541AA\", \"N403AA\", \"N49... ## $ ActualElapsedTime &lt;int&gt; 60, 60, 70, 70, 62, 64, 70, 59, 71, 70, 70, ... ## $ AirTime &lt;int&gt; 40, 45, 48, 39, 44, 45, 43, 40, 41, 45, 42, ... ## $ ArrDelay &lt;int&gt; -10, -9, -8, 3, -3, -7, -1, -16, 44, 43, 29,... ## $ DepDelay &lt;int&gt; 0, 1, -8, 3, 5, -1, -1, -5, 43, 43, 29, 19, ... ## $ Origin &lt;chr&gt; \"IAH\", \"IAH\", \"IAH\", \"IAH\", \"IAH\", \"IAH\", \"I... ## $ Dest &lt;chr&gt; \"DFW\", \"DFW\", \"DFW\", \"DFW\", \"DFW\", \"DFW\", \"D... ## $ Distance &lt;int&gt; 224, 224, 224, 224, 224, 224, 224, 224, 224,... ## $ TaxiIn &lt;int&gt; 7, 6, 5, 9, 9, 6, 12, 7, 8, 6, 8, 4, 6, 5, 6... ## $ TaxiOut &lt;int&gt; 13, 9, 17, 22, 9, 13, 15, 12, 22, 19, 20, 11... ## $ Cancelled &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,... ## $ CancellationCode &lt;chr&gt; \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", ... ## $ Diverted &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,... part 2 of 2 1 2 3 4 5 6 7 8 # Build the lookup table: lut lut <- c ( \"A\" = \"carrier\" , \"B\" = \"weather\" , \"C\" = \"FFA\" , \"D\" = \"security\" , \"E\" = \"not cancelled\" ) # Add the Code column hflights $ Code <- lut[hflights $ CancellationCode] # Glimpse at hflights glimpse ( hflights ) Result. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 Observations: 227,496 Variables: 22 $ Year <int> 2011, 2011, 2011, 2011, 2011, 2011,... $ Month <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,... $ DayofMonth <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, ... $ DayOfWeek <int> 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3,... $ DepTime <int> 1400, 1401, 1352, 1403, 1405, 1359,... $ ArrTime <int> 1500, 1501, 1502, 1513, 1507, 1503,... $ UniqueCarrier <chr> \"American\", \"American\", \"American\",... $ FlightNum <int> 428, 428, 428, 428, 428, 428, 428, ... $ TailNum <chr> \"N576AA\", \"N557AA\", \"N541AA\", \"N403... $ ActualElapsedTime <int> 60, 60, 70, 70, 62, 64, 70, 59, 71,... $ AirTime <int> 40, 45, 48, 39, 44, 45, 43, 40, 41,... $ ArrDelay <int> -10, -9, -8, 3, -3, -7, -1, -16, 44... $ DepDelay <int> 0, 1, -8, 3, 5, -1, -1, -5, 43, 43,... $ Origin <chr> \"IAH\", \"IAH\", \"IAH\", \"IAH\", \"IAH\", ... $ Dest <chr> \"DFW\", \"DFW\", \"DFW\", \"DFW\", \"DFW\", ... $ Distance <int> 224, 224, 224, 224, 224, 224, 224, ... $ TaxiIn <int> 7, 6, 5, 9, 9, 6, 12, 7, 8, 6, 8, 4... $ TaxiOut <int> 13, 9, 17, 22, 9, 13, 15, 12, 22, 1... $ Cancelled <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,... $ CancellationCode <chr> \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\",... $ Diverted <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,... $ Code <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA,...","title":"Changing labels of hflights"},{"location":"data_manipulation_in_r_with_dplyr/#select-and-mutate","text":"","title":"select and mutate"},{"location":"data_manipulation_in_r_with_dplyr/#the-five-verbs-and-their-meaning","text":"select ; which returns a subset of the columns. filter ; that is able to return a subset of the rows. arrange ; that reorders the rows according to single or multiple variables. mutate ; used to add columns from existing data. summarise ; which reduces each group to a single row by calculating aggregate measures.","title":"The five verbs and their meaning"},{"location":"data_manipulation_in_r_with_dplyr/#the-select-verb","text":"1 2 # Print out a tbl with the four columns of hflights related to delay select ( hflights , ActualElapsedTime , AirTime , ArrDelay , DepDelay ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 ## # A tibble: 227,496 \u00d7 4 ## ActualElapsedTime AirTime ArrDelay DepDelay ## * &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 60 40 -10 0 ## 2 60 45 -9 1 ## 3 70 48 -8 -8 ## 4 70 39 3 3 ## 5 62 44 -3 5 ## 6 64 45 -7 -1 ## 7 70 43 -1 -1 ## 8 59 40 -16 -5 ## 9 71 41 44 43 ## 10 70 45 43 43 ## # ... with 227,486 more rows 1 2 # Print out hflights, nothing has changed! hflights 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 ## # A tibble : 227 , 496 \u00d7 21 ## Year Month DayofMonth DayOfWeek DepTime ArrTime UniqueCarrier ## * & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; chr & gt ; ## 1 2011 1 1 6 1400 1500 American ## 2 2011 1 2 7 1401 1501 American ## 3 2011 1 3 1 1352 1502 American ## 4 2011 1 4 2 1403 1513 American ## 5 2011 1 5 3 1405 1507 American ## 6 2011 1 6 4 1359 1503 American ## 7 2011 1 7 5 1359 1509 American ## 8 2011 1 8 6 1355 1454 American ## 9 2011 1 9 7 1443 1554 American ## 10 2011 1 10 1 1443 1553 American ## # ... with 227 , 486 more rows , and 14 more variables : FlightNum & lt ; int & gt ;, ## # TailNum & lt ; chr & gt ;, ActualElapsedTime & lt ; int & gt ;, AirTime & lt ; int & gt ;, ArrDelay & lt ; int & gt ;, ## # DepDelay & lt ; int & gt ;, Origin & lt ; chr & gt ;, Dest & lt ; chr & gt ;, Distance & lt ; int & gt ;, ## # TaxiIn & lt ; int & gt ;, TaxiOut & lt ; int & gt ;, Cancelled & lt ; int & gt ;, CancellationCode & lt ; chr & gt ;, ## # Diverted & lt ; int & gt ; 1 2 # Print out the columns Origin up to Cancelled of hflights select ( hflights , 14 : 19 ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 ## # A tibble: 227,496 \u00d7 6 ## Origin Dest Distance TaxiIn TaxiOut Cancelled ## * &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 IAH DFW 224 7 13 0 ## 2 IAH DFW 224 6 9 0 ## 3 IAH DFW 224 5 17 0 ## 4 IAH DFW 224 9 22 0 ## 5 IAH DFW 224 9 9 0 ## 6 IAH DFW 224 6 13 0 ## 7 IAH DFW 224 12 15 0 ## 8 IAH DFW 224 7 12 0 ## 9 IAH DFW 224 8 22 0 ## 10 IAH DFW 224 6 19 0 ## # ... with 227,486 more rows 1 2 # Answer to last question: be concise! select ( hflights , 1 : 4 , 12 : 21 ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 ## # A tibble : 227 , 496 \u00d7 14 ## Year Month DayofMonth DayOfWeek ArrDelay DepDelay Origin Dest ## * & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; chr & gt ; & lt ; chr & gt ; ## 1 2011 1 1 6 -10 0 IAH DFW ## 2 2011 1 2 7 -9 1 IAH DFW ## 3 2011 1 3 1 -8 -8 IAH DFW ## 4 2011 1 4 2 3 3 IAH DFW ## 5 2011 1 5 3 -3 5 IAH DFW ## 6 2011 1 6 4 -7 -1 IAH DFW ## 7 2011 1 7 5 -1 -1 IAH DFW ## 8 2011 1 8 6 -16 -5 IAH DFW ## 9 2011 1 9 7 44 43 IAH DFW ## 10 2011 1 10 1 43 43 IAH DFW ## # ... with 227 , 486 more rows , and 6 more variables : Distance & lt ; int & gt ;, ## # TaxiIn & lt ; int & gt ;, TaxiOut & lt ; int & gt ;, Cancelled & lt ; int & gt ;, CancellationCode & lt ; chr & gt ;, ## # Diverted & lt ; int & gt ;","title":"The select verb"},{"location":"data_manipulation_in_r_with_dplyr/#helper-functions-for-variable-selection","text":"select : starts_with(\"X\") ; every name that starts with \"X\" , ends_with(\"X\") ; every name that ends with \"X\" , contains(\"X\") ; every name that contains \"X\" , matches(\"X\") ; every name that matches \"X\" , where \"X\" can be a regular expression, num_range(\"x\", 1:5) ; the variables named x01 , x02 , x03 , x04 and x05 , one_of(x) ; every name that appears in x , which should be a character vector. 1 2 # Print out a tbl containing just ArrDelay and DepDelay select ( hflights , ArrDelay , DepDelay ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 ## # A tibble: 227,496 \u00d7 2 ## ArrDelay DepDelay ## * &lt;int&gt; &lt;int&gt; ## 1 -10 0 ## 2 -9 1 ## 3 -8 -8 ## 4 3 3 ## 5 -3 5 ## 6 -7 -1 ## 7 -1 -1 ## 8 -16 -5 ## 9 44 43 ## 10 43 43 ## # ... with 227,486 more rows 1 2 # Print out a tbl as described in the second instruction, using both helper functions and variable names select ( hflights , UniqueCarrier , ends_with ( 'Num' ), starts_with ( 'Cancel' )) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 ## # A tibble: 227,496 \u00d7 5 ## UniqueCarrier FlightNum TailNum Cancelled CancellationCode ## * &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; ## 1 American 428 N576AA 0 ## 2 American 428 N557AA 0 ## 3 American 428 N541AA 0 ## 4 American 428 N403AA 0 ## 5 American 428 N492AA 0 ## 6 American 428 N262AA 0 ## 7 American 428 N493AA 0 ## 8 American 428 N477AA 0 ## 9 American 428 N476AA 0 ## 10 American 428 N504AA 0 ## # ... with 227,486 more rows 1 2 # Print out a tbl as described in the third instruction, using only helper functions. select ( hflights , ends_with ( 'Time' ), ends_with ( 'Delay' )) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 ## # A tibble: 227,496 \u00d7 6 ## DepTime ArrTime ActualElapsedTime AirTime ArrDelay DepDelay ## * &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 1400 1500 60 40 -10 0 ## 2 1401 1501 60 45 -9 1 ## 3 1352 1502 70 48 -8 -8 ## 4 1403 1513 70 39 3 3 ## 5 1405 1507 62 44 -3 5 ## 6 1359 1503 64 45 -7 -1 ## 7 1359 1509 70 43 -1 -1 ## 8 1355 1454 59 40 -16 -5 ## 9 1443 1554 71 41 44 43 ## 10 1443 1553 70 45 43 43 ## # ... with 227,486 more rows","title":"Helper functions for variable selection"},{"location":"data_manipulation_in_r_with_dplyr/#comparison-to-basic-r","text":"1 2 3 4 5 6 7 8 9 10 11 12 # add ex1r <- hflights [c ( 'TaxiIn' , 'TaxiOut' , 'Distance' ) ] ex1d <- select ( hflights , starts_with ( 'Taxi' ), Distance ) ex2r <- hflights [c ( 'Year' , 'Month' , 'DayOfWeek' , 'DepTime' , 'ArrTime' ) ] ex2d <- select ( hflights , Year , Month , DayOfWeek , DepTime , ArrTime ) ex3r <- hflights [c ( 'TailNum' , 'TaxiIn' , 'TaxiOut' ) ] ex3d <- select ( hflights , TailNum , starts_with ( 'Taxi' ))","title":"Comparison to basic R"},{"location":"data_manipulation_in_r_with_dplyr/#mutate-is-creating","text":"1 2 3 # Add the new variable ActualGroundTime to a copy of hflights and save the result as g1 g1 <- mutate ( hflights , ActualGroundTime = ActualElapsedTime - AirTime ) glimpse ( hflights ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 ## Observations: 227,496 ## Variables: 21 ## $ Year &lt;int&gt; 2011, 2011, 2011, 2011, 2011, 2011, 2011, 20... ## $ Month &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,... ## $ DayofMonth &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 1... ## $ DayOfWeek &lt;int&gt; 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6,... ## $ DepTime &lt;int&gt; 1400, 1401, 1352, 1403, 1405, 1359, 1359, 13... ## $ ArrTime &lt;int&gt; 1500, 1501, 1502, 1513, 1507, 1503, 1509, 14... ## $ UniqueCarrier &lt;chr&gt; \"American\", \"American\", \"American\", \"America... ## $ FlightNum &lt;int&gt; 428, 428, 428, 428, 428, 428, 428, 428, 428,... ## $ TailNum &lt;chr&gt; \"N576AA\", \"N557AA\", \"N541AA\", \"N403AA\", \"N49... ## $ ActualElapsedTime &lt;int&gt; 60, 60, 70, 70, 62, 64, 70, 59, 71, 70, 70, ... ## $ AirTime &lt;int&gt; 40, 45, 48, 39, 44, 45, 43, 40, 41, 45, 42, ... ## $ ArrDelay &lt;int&gt; -10, -9, -8, 3, -3, -7, -1, -16, 44, 43, 29,... ## $ DepDelay &lt;int&gt; 0, 1, -8, 3, 5, -1, -1, -5, 43, 43, 29, 19, ... ## $ Origin &lt;chr&gt; \"IAH\", \"IAH\", \"IAH\", \"IAH\", \"IAH\", \"IAH\", \"I... ## $ Dest &lt;chr&gt; \"DFW\", \"DFW\", \"DFW\", \"DFW\", \"DFW\", \"DFW\", \"D... ## $ Distance &lt;int&gt; 224, 224, 224, 224, 224, 224, 224, 224, 224,... ## $ TaxiIn &lt;int&gt; 7, 6, 5, 9, 9, 6, 12, 7, 8, 6, 8, 4, 6, 5, 6... ## $ TaxiOut &lt;int&gt; 13, 9, 17, 22, 9, 13, 15, 12, 22, 19, 20, 11... ## $ Cancelled &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,... ## $ CancellationCode &lt;chr&gt; \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", ... ## $ Diverted &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,... 1 glimpse ( g1 ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 ## Observations: 227,496 ## Variables: 22 ## $ Year &lt;int&gt; 2011, 2011, 2011, 2011, 2011, 2011, 2011, 20... ## $ Month &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,... ## $ DayofMonth &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 1... ## $ DayOfWeek &lt;int&gt; 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6,... ## $ DepTime &lt;int&gt; 1400, 1401, 1352, 1403, 1405, 1359, 1359, 13... ## $ ArrTime &lt;int&gt; 1500, 1501, 1502, 1513, 1507, 1503, 1509, 14... ## $ UniqueCarrier &lt;chr&gt; \"American\", \"American\", \"American\", \"America... ## $ FlightNum &lt;int&gt; 428, 428, 428, 428, 428, 428, 428, 428, 428,... ## $ TailNum &lt;chr&gt; \"N576AA\", \"N557AA\", \"N541AA\", \"N403AA\", \"N49... ## $ ActualElapsedTime &lt;int&gt; 60, 60, 70, 70, 62, 64, 70, 59, 71, 70, 70, ... ## $ AirTime &lt;int&gt; 40, 45, 48, 39, 44, 45, 43, 40, 41, 45, 42, ... ## $ ArrDelay &lt;int&gt; -10, -9, -8, 3, -3, -7, -1, -16, 44, 43, 29,... ## $ DepDelay &lt;int&gt; 0, 1, -8, 3, 5, -1, -1, -5, 43, 43, 29, 19, ... ## $ Origin &lt;chr&gt; \"IAH\", \"IAH\", \"IAH\", \"IAH\", \"IAH\", \"IAH\", \"I... ## $ Dest &lt;chr&gt; \"DFW\", \"DFW\", \"DFW\", \"DFW\", \"DFW\", \"DFW\", \"D... ## $ Distance &lt;int&gt; 224, 224, 224, 224, 224, 224, 224, 224, 224,... ## $ TaxiIn &lt;int&gt; 7, 6, 5, 9, 9, 6, 12, 7, 8, 6, 8, 4, 6, 5, 6... ## $ TaxiOut &lt;int&gt; 13, 9, 17, 22, 9, 13, 15, 12, 22, 19, 20, 11... ## $ Cancelled &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,... ## $ CancellationCode &lt;chr&gt; \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", ... ## $ Diverted &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,... ## $ ActualGroundTime &lt;int&gt; 20, 15, 22, 31, 18, 19, 27, 19, 30, 25, 28, ... 1 2 3 4 # Add the new variable GroundTime to a g1; save the result as g2 g2 <- mutate ( g1 , GroundTime = TaxiIn + TaxiOut ) head ( g1 $ ActualGroundTime == g2 $ GroundTime , 20 ) 1 2 ## [1] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE ## [15] TRUE TRUE TRUE TRUE TRUE TRUE 1 2 3 # Add the new variable AverageSpeed to g2; save the result as g3 g3 <- mutate ( g2 , AverageSpeed = Distance / AirTime * 60 ) g3 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 ## # A tibble : 227 , 496 \u00d7 24 ## Year Month DayofMonth DayOfWeek DepTime ArrTime UniqueCarrier ## & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; chr & gt ; ## 1 2011 1 1 6 1400 1500 American ## 2 2011 1 2 7 1401 1501 American ## 3 2011 1 3 1 1352 1502 American ## 4 2011 1 4 2 1403 1513 American ## 5 2011 1 5 3 1405 1507 American ## 6 2011 1 6 4 1359 1503 American ## 7 2011 1 7 5 1359 1509 American ## 8 2011 1 8 6 1355 1454 American ## 9 2011 1 9 7 1443 1554 American ## 10 2011 1 10 1 1443 1553 American ## # ... with 227 , 486 more rows , and 17 more variables : FlightNum & lt ; int & gt ;, ## # TailNum & lt ; chr & gt ;, ActualElapsedTime & lt ; int & gt ;, AirTime & lt ; int & gt ;, ArrDelay & lt ; int & gt ;, ## # DepDelay & lt ; int & gt ;, Origin & lt ; chr & gt ;, Dest & lt ; chr & gt ;, Distance & lt ; int & gt ;, ## # TaxiIn & lt ; int & gt ;, TaxiOut & lt ; int & gt ;, Cancelled & lt ; int & gt ;, CancellationCode & lt ; chr & gt ;, ## # Diverted & lt ; int & gt ;, ActualGroundTime & lt ; int & gt ;, GroundTime & lt ; int & gt ;, ## # AverageSpeed & lt ; dbl & gt ;","title":"mutate is creating"},{"location":"data_manipulation_in_r_with_dplyr/#add-multiple-variables-using-mutate","text":"1 2 3 4 5 6 7 8 9 # Add a second variable loss_percent to the dataset: m1 m1 <- mutate ( hflights , loss = ArrDelay - DepDelay , loss_percent = ( ArrDelay - DepDelay ) / DepDelay * 100 ) # Copy and adapt the previous command to reduce redendancy: m2 m2 <- mutate ( hflights , loss = ArrDelay - DepDelay , loss_percent = loss / DepDelay * 100 ) # Add the three variables as described in the third instruction: m3 m3 <- mutate ( hflights , TotalTaxi = TaxiIn + TaxiOut , ActualGroundTime = ActualElapsedTime - AirTime , Diff = TotalTaxi - ActualGroundTime ) glimpse ( m3 ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 ## Observations: 227,496 ## Variables: 24 ## $ Year &lt;int&gt; 2011, 2011, 2011, 2011, 2011, 2011, 2011, 20... ## $ Month &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,... ## $ DayofMonth &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 1... ## $ DayOfWeek &lt;int&gt; 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6,... ## $ DepTime &lt;int&gt; 1400, 1401, 1352, 1403, 1405, 1359, 1359, 13... ## $ ArrTime &lt;int&gt; 1500, 1501, 1502, 1513, 1507, 1503, 1509, 14... ## $ UniqueCarrier &lt;chr&gt; \"American\", \"American\", \"American\", \"America... ## $ FlightNum &lt;int&gt; 428, 428, 428, 428, 428, 428, 428, 428, 428,... ## $ TailNum &lt;chr&gt; \"N576AA\", \"N557AA\", \"N541AA\", \"N403AA\", \"N49... ## $ ActualElapsedTime &lt;int&gt; 60, 60, 70, 70, 62, 64, 70, 59, 71, 70, 70, ... ## $ AirTime &lt;int&gt; 40, 45, 48, 39, 44, 45, 43, 40, 41, 45, 42, ... ## $ ArrDelay &lt;int&gt; -10, -9, -8, 3, -3, -7, -1, -16, 44, 43, 29,... ## $ DepDelay &lt;int&gt; 0, 1, -8, 3, 5, -1, -1, -5, 43, 43, 29, 19, ... ## $ Origin &lt;chr&gt; \"IAH\", \"IAH\", \"IAH\", \"IAH\", \"IAH\", \"IAH\", \"I... ## $ Dest &lt;chr&gt; \"DFW\", \"DFW\", \"DFW\", \"DFW\", \"DFW\", \"DFW\", \"D... ## $ Distance &lt;int&gt; 224, 224, 224, 224, 224, 224, 224, 224, 224,... ## $ TaxiIn &lt;int&gt; 7, 6, 5, 9, 9, 6, 12, 7, 8, 6, 8, 4, 6, 5, 6... ## $ TaxiOut &lt;int&gt; 13, 9, 17, 22, 9, 13, 15, 12, 22, 19, 20, 11... ## $ Cancelled &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,... ## $ CancellationCode &lt;chr&gt; \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", ... ## $ Diverted &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,... ## $ TotalTaxi &lt;int&gt; 20, 15, 22, 31, 18, 19, 27, 19, 30, 25, 28, ... ## $ ActualGroundTime &lt;int&gt; 20, 15, 22, 31, 18, 19, 27, 19, 30, 25, 28, ... ## $ Diff &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...","title":"Add multiple variables using mutate"},{"location":"data_manipulation_in_r_with_dplyr/#filter-and-arrange","text":"","title":"filter and arrange"},{"location":"data_manipulation_in_r_with_dplyr/#logical-operators","text":"filter : x < y ; TRUE if x is less than y . x <= y ; TRUE if x is less than or equal to y . x == y ; TRUE if x equals y . x != y ; TRUE if x does not equal y . x >= y ; TRUE if x is greater than or equal to y . x > y ; TRUE if x is greater than y . x %in% c(a, b, c) ; TRUE if x is in the vector c(a, b, c) . 1 2 # All flights that traveled 3000 miles or more filter ( hflights , Distance >= 3000 ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 ## # A tibble : 527 \u00d7 21 ## Year Month DayofMonth DayOfWeek DepTime ArrTime UniqueCarrier ## & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; chr & gt ; ## 1 2011 1 31 1 924 1413 Continental ## 2 2011 1 30 7 925 1410 Continental ## 3 2011 1 29 6 1045 1445 Continental ## 4 2011 1 28 5 1516 1916 Continental ## 5 2011 1 27 4 950 1344 Continental ## 6 2011 1 26 3 944 1350 Continental ## 7 2011 1 25 2 924 1337 Continental ## 8 2011 1 24 1 1144 1605 Continental ## 9 2011 1 23 7 926 1335 Continental ## 10 2011 1 22 6 942 1340 Continental ## # ... with 517 more rows , and 14 more variables : FlightNum & lt ; int & gt ;, ## # TailNum & lt ; chr & gt ;, ActualElapsedTime & lt ; int & gt ;, AirTime & lt ; int & gt ;, ArrDelay & lt ; int & gt ;, ## # DepDelay & lt ; int & gt ;, Origin & lt ; chr & gt ;, Dest & lt ; chr & gt ;, Distance & lt ; int & gt ;, ## # TaxiIn & lt ; int & gt ;, TaxiOut & lt ; int & gt ;, Cancelled & lt ; int & gt ;, CancellationCode & lt ; chr & gt ;, ## # Diverted & lt ; int & gt ; 1 2 # All flights flown by one of JetBlue, Southwest, or Delta filter ( hflights , UniqueCarrier %in% c ( 'JetBlue' , 'Southwest' , 'Delta' )) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 ## # A tibble : 48 , 679 \u00d7 21 ## Year Month DayofMonth DayOfWeek DepTime ArrTime UniqueCarrier ## & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; chr & gt ; ## 1 2011 1 1 6 654 1124 JetBlue ## 2 2011 1 1 6 1639 2110 JetBlue ## 3 2011 1 2 7 703 1113 JetBlue ## 4 2011 1 2 7 1604 2040 JetBlue ## 5 2011 1 3 1 659 1100 JetBlue ## 6 2011 1 3 1 1801 2200 JetBlue ## 7 2011 1 4 2 654 1103 JetBlue ## 8 2011 1 4 2 1608 2034 JetBlue ## 9 2011 1 5 3 700 1103 JetBlue ## 10 2011 1 5 3 1544 1954 JetBlue ## # ... with 48 , 669 more rows , and 14 more variables : FlightNum & lt ; int & gt ;, ## # TailNum & lt ; chr & gt ;, ActualElapsedTime & lt ; int & gt ;, AirTime & lt ; int & gt ;, ArrDelay & lt ; int & gt ;, ## # DepDelay & lt ; int & gt ;, Origin & lt ; chr & gt ;, Dest & lt ; chr & gt ;, Distance & lt ; int & gt ;, ## # TaxiIn & lt ; int & gt ;, TaxiOut & lt ; int & gt ;, Cancelled & lt ; int & gt ;, CancellationCode & lt ; chr & gt ;, ## # Diverted & lt ; int & gt ; 1 2 # All flights where taxiing took longer than flying filter ( hflights , ( TaxiIn + TaxiOut ) > AirTime ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 ## # A tibble : 1 , 389 \u00d7 21 ## Year Month DayofMonth DayOfWeek DepTime ArrTime UniqueCarrier ## & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; chr & gt ; ## 1 2011 1 24 1 731 904 American ## 2 2011 1 30 7 1959 2132 American ## 3 2011 1 24 1 1621 1749 American ## 4 2011 1 10 1 941 1113 American ## 5 2011 1 31 1 1301 1356 Continental ## 6 2011 1 31 1 2113 2215 Continental ## 7 2011 1 31 1 1434 1539 Continental ## 8 2011 1 31 1 900 1006 Continental ## 9 2011 1 30 7 1304 1408 Continental ## 10 2011 1 30 7 2004 2128 Continental ## # ... with 1 , 379 more rows , and 14 more variables : FlightNum & lt ; int & gt ;, ## # TailNum & lt ; chr & gt ;, ActualElapsedTime & lt ; int & gt ;, AirTime & lt ; int & gt ;, ArrDelay & lt ; int & gt ;, ## # DepDelay & lt ; int & gt ;, Origin & lt ; chr & gt ;, Dest & lt ; chr & gt ;, Distance & lt ; int & gt ;, ## # TaxiIn & lt ; int & gt ;, TaxiOut & lt ; int & gt ;, Cancelled & lt ; int & gt ;, CancellationCode & lt ; chr & gt ;, ## # Diverted & lt ; int & gt ;","title":"Logical operators"},{"location":"data_manipulation_in_r_with_dplyr/#combining-tests-using-boolean-operators","text":"1 2 # All flights that departed before 5am or arrived after 10pm filter ( hflights , DepTime < 500 | ArrTime > 2200 ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 ## # A tibble : 27 , 799 \u00d7 21 ## Year Month DayofMonth DayOfWeek DepTime ArrTime UniqueCarrier ## & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; chr & gt ; ## 1 2011 1 4 2 2100 2207 American ## 2 2011 1 14 5 2119 2229 American ## 3 2011 1 10 1 1934 2235 American ## 4 2011 1 26 3 1905 2211 American ## 5 2011 1 30 7 1856 2209 American ## 6 2011 1 9 7 1938 2228 Alaska ## 7 2011 1 31 1 1919 2231 Continental ## 8 2011 1 31 1 2116 2344 Continental ## 9 2011 1 31 1 1850 2211 Continental ## 10 2011 1 31 1 2102 2216 Continental ## # ... with 27 , 789 more rows , and 14 more variables : FlightNum & lt ; int & gt ;, ## # TailNum & lt ; chr & gt ;, ActualElapsedTime & lt ; int & gt ;, AirTime & lt ; int & gt ;, ArrDelay & lt ; int & gt ;, ## # DepDelay & lt ; int & gt ;, Origin & lt ; chr & gt ;, Dest & lt ; chr & gt ;, Distance & lt ; int & gt ;, ## # TaxiIn & lt ; int & gt ;, TaxiOut & lt ; int & gt ;, Cancelled & lt ; int & gt ;, CancellationCode & lt ; chr & gt ;, ## # Diverted & lt ; int & gt ; 1 2 # All flights that departed late but arrived ahead of schedule filter ( hflights , DepDelay > 0 & ArrDelay < 0 ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 ## # A tibble : 27 , 712 \u00d7 21 ## Year Month DayofMonth DayOfWeek DepTime ArrTime UniqueCarrier ## & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; chr & gt ; ## 1 2011 1 2 7 1401 1501 American ## 2 2011 1 5 3 1405 1507 American ## 3 2011 1 18 2 1408 1508 American ## 4 2011 1 18 2 721 827 American ## 5 2011 1 12 3 2015 2113 American ## 6 2011 1 13 4 2020 2116 American ## 7 2011 1 26 3 2009 2103 American ## 8 2011 1 1 6 1631 1736 American ## 9 2011 1 10 1 1639 1740 American ## 10 2011 1 12 3 1631 1739 American ## # ... with 27 , 702 more rows , and 14 more variables : FlightNum & lt ; int & gt ;, ## # TailNum & lt ; chr & gt ;, ActualElapsedTime & lt ; int & gt ;, AirTime & lt ; int & gt ;, ArrDelay & lt ; int & gt ;, ## # DepDelay & lt ; int & gt ;, Origin & lt ; chr & gt ;, Dest & lt ; chr & gt ;, Distance & lt ; int & gt ;, ## # TaxiIn & lt ; int & gt ;, TaxiOut & lt ; int & gt ;, Cancelled & lt ; int & gt ;, CancellationCode & lt ; chr & gt ;, ## # Diverted & lt ; int & gt ; 1 2 # All cancelled weekend flights filter ( hflights , DayOfWeek %in% c ( 6 , 7 ) & Cancelled == 1 ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 ## # A tibble : 585 \u00d7 21 ## Year Month DayofMonth DayOfWeek DepTime ArrTime UniqueCarrier ## & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; chr & gt ; ## 1 2011 1 9 7 NA NA American ## 2 2011 1 29 6 NA NA Continental ## 3 2011 1 9 7 NA NA Continental ## 4 2011 1 9 7 NA NA Delta ## 5 2011 1 9 7 NA NA SkyWest ## 6 2011 1 2 7 NA NA Southwest ## 7 2011 1 29 6 NA NA Delta ## 8 2011 1 9 7 NA NA Atlantic_Southeast ## 9 2011 1 1 6 NA NA AirTran ## 10 2011 1 9 7 NA NA AirTran ## # ... with 575 more rows , and 14 more variables : FlightNum & lt ; int & gt ;, ## # TailNum & lt ; chr & gt ;, ActualElapsedTime & lt ; int & gt ;, AirTime & lt ; int & gt ;, ArrDelay & lt ; int & gt ;, ## # DepDelay & lt ; int & gt ;, Origin & lt ; chr & gt ;, Dest & lt ; chr & gt ;, Distance & lt ; int & gt ;, ## # TaxiIn & lt ; int & gt ;, TaxiOut & lt ; int & gt ;, Cancelled & lt ; int & gt ;, CancellationCode & lt ; chr & gt ;, ## # Diverted & lt ; int & gt ; 1 2 # All flights that were cancelled after being delayed filter ( hflights , DepDelay > 0 & Cancelled == 1 ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 ## # A tibble : 40 \u00d7 21 ## Year Month DayofMonth DayOfWeek DepTime ArrTime UniqueCarrier ## & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; chr & gt ; ## 1 2011 1 26 3 1926 NA Continental ## 2 2011 1 11 2 1100 NA US_Airways ## 3 2011 1 19 3 1811 NA ExpressJet ## 4 2011 1 7 5 2028 NA ExpressJet ## 5 2011 2 4 5 1638 NA American ## 6 2011 2 8 2 1057 NA Continental ## 7 2011 2 2 3 802 NA ExpressJet ## 8 2011 2 9 3 904 NA ExpressJet ## 9 2011 2 1 2 1508 NA SkyWest ## 10 2011 3 31 4 1016 NA Continental ## # ... with 30 more rows , and 14 more variables : FlightNum & lt ; int & gt ;, ## # TailNum & lt ; chr & gt ;, ActualElapsedTime & lt ; int & gt ;, AirTime & lt ; int & gt ;, ArrDelay & lt ; int & gt ;, ## # DepDelay & lt ; int & gt ;, Origin & lt ; chr & gt ;, Dest & lt ; chr & gt ;, Distance & lt ; int & gt ;, ## # TaxiIn & lt ; int & gt ;, TaxiOut & lt ; int & gt ;, Cancelled & lt ; int & gt ;, CancellationCode & lt ; chr & gt ;, ## # Diverted & lt ; int & gt ;","title":"Combining tests using boolean operators"},{"location":"data_manipulation_in_r_with_dplyr/#blend-together","text":"1 2 3 4 5 6 7 8 # Select the flights that had JFK as their destination: c1 c1 <- filter ( hflights , Dest == 'JFK' ) # Combine the Year, Month and DayofMonth variables to create a Date column: c2 c2 <- mutate ( c1 , Date = paste ( Year , Month , DayofMonth , sep = '-' )) # Print out a selection of columns of c2 select ( c2 , Date , DepTime , ArrTime , TailNum ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 ## # A tibble: 695 \u00d7 4 ## Date DepTime ArrTime TailNum ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; ## 1 2011-1-1 654 1124 N324JB ## 2 2011-1-1 1639 2110 N324JB ## 3 2011-1-2 703 1113 N324JB ## 4 2011-1-2 1604 2040 N324JB ## 5 2011-1-3 659 1100 N229JB ## 6 2011-1-3 1801 2200 N206JB ## 7 2011-1-4 654 1103 N267JB ## 8 2011-1-4 1608 2034 N267JB ## 9 2011-1-5 700 1103 N708JB ## 10 2011-1-5 1544 1954 N644JB ## # ... with 685 more rows","title":"Blend together"},{"location":"data_manipulation_in_r_with_dplyr/#arranging-your-data","text":"1 2 3 4 5 # Definition of dtc dtc <- filter ( hflights , Cancelled == 1 , ! is.na ( DepDelay )) # Arrange dtc by departure delays arrange ( dtc , DepDelay ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 ## # A tibble : 68 \u00d7 21 ## Year Month DayofMonth DayOfWeek DepTime ArrTime UniqueCarrier ## & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; chr & gt ; ## 1 2011 7 23 6 605 NA Frontier ## 2 2011 1 17 1 916 NA ExpressJet ## 3 2011 12 1 4 541 NA US_Airways ## 4 2011 10 12 3 2022 NA American_Eagle ## 5 2011 7 29 5 1424 NA Continental ## 6 2011 9 29 4 1639 NA SkyWest ## 7 2011 2 9 3 555 NA American_Eagle ## 8 2011 5 9 1 715 NA SkyWest ## 9 2011 1 20 4 1413 NA United ## 10 2011 1 17 1 831 NA Southwest ## # ... with 58 more rows , and 14 more variables : FlightNum & lt ; int & gt ;, ## # TailNum & lt ; chr & gt ;, ActualElapsedTime & lt ; int & gt ;, AirTime & lt ; int & gt ;, ArrDelay & lt ; int & gt ;, ## # DepDelay & lt ; int & gt ;, Origin & lt ; chr & gt ;, Dest & lt ; chr & gt ;, Distance & lt ; int & gt ;, ## # TaxiIn & lt ; int & gt ;, TaxiOut & lt ; int & gt ;, Cancelled & lt ; int & gt ;, CancellationCode & lt ; chr & gt ;, ## # Diverted & lt ; int & gt ; 1 2 # Arrange dtc so that cancellation reasons are grouped arrange ( dtc , CancellationCode ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 ## # A tibble : 68 \u00d7 21 ## Year Month DayofMonth DayOfWeek DepTime ArrTime UniqueCarrier ## & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; chr & gt ; ## 1 2011 1 20 4 1413 NA United ## 2 2011 1 7 5 2028 NA ExpressJet ## 3 2011 2 4 5 1638 NA American ## 4 2011 2 8 2 1057 NA Continental ## 5 2011 2 1 2 1508 NA SkyWest ## 6 2011 2 21 1 2257 NA SkyWest ## 7 2011 2 9 3 555 NA American_Eagle ## 8 2011 3 18 5 727 NA United ## 9 2011 4 4 1 1632 NA Delta ## 10 2011 4 8 5 1608 NA Southwest ## # ... with 58 more rows , and 14 more variables : FlightNum & lt ; int & gt ;, ## # TailNum & lt ; chr & gt ;, ActualElapsedTime & lt ; int & gt ;, AirTime & lt ; int & gt ;, ArrDelay & lt ; int & gt ;, ## # DepDelay & lt ; int & gt ;, Origin & lt ; chr & gt ;, Dest & lt ; chr & gt ;, Distance & lt ; int & gt ;, ## # TaxiIn & lt ; int & gt ;, TaxiOut & lt ; int & gt ;, Cancelled & lt ; int & gt ;, CancellationCode & lt ; chr & gt ;, ## # Diverted & lt ; int & gt ; 1 2 # Arrange dtc according to carrier and departure delays arrange ( dtc , UniqueCarrier , DepDelay ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 ## # A tibble : 68 \u00d7 21 ## Year Month DayofMonth DayOfWeek DepTime ArrTime UniqueCarrier ## & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; chr & gt ; ## 1 2011 6 11 6 1649 NA AirTran ## 2 2011 8 18 4 1808 NA American ## 3 2011 2 4 5 1638 NA American ## 4 2011 10 12 3 2022 NA American_Eagle ## 5 2011 2 9 3 555 NA American_Eagle ## 6 2011 7 17 7 1917 NA American_Eagle ## 7 2011 4 30 6 612 NA Atlantic_Southeast ## 8 2011 4 10 7 1147 NA Atlantic_Southeast ## 9 2011 5 23 1 657 NA Atlantic_Southeast ## 10 2011 9 29 4 723 NA Atlantic_Southeast ## # ... with 58 more rows , and 14 more variables : FlightNum & lt ; int & gt ;, ## # TailNum & lt ; chr & gt ;, ActualElapsedTime & lt ; int & gt ;, AirTime & lt ; int & gt ;, ArrDelay & lt ; int & gt ;, ## # DepDelay & lt ; int & gt ;, Origin & lt ; chr & gt ;, Dest & lt ; chr & gt ;, Distance & lt ; int & gt ;, ## # TaxiIn & lt ; int & gt ;, TaxiOut & lt ; int & gt ;, Cancelled & lt ; int & gt ;, CancellationCode & lt ; chr & gt ;, ## # Diverted & lt ; int & gt ;","title":"Arranging your data"},{"location":"data_manipulation_in_r_with_dplyr/#reverse-the-order-of-arranging","text":"1 2 # Arrange according to carrier and decreasing departure delays arrange ( hflights , UniqueCarrier , desc ( DepDelay )) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 ## # A tibble : 227 , 496 \u00d7 21 ## Year Month DayofMonth DayOfWeek DepTime ArrTime UniqueCarrier ## & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; chr & gt ; ## 1 2011 2 19 6 1902 2143 AirTran ## 2 2011 3 14 1 2024 2309 AirTran ## 3 2011 2 16 3 2349 227 AirTran ## 4 2011 11 13 7 2312 213 AirTran ## 5 2011 5 26 4 2353 305 AirTran ## 6 2011 5 26 4 1922 2229 AirTran ## 7 2011 4 28 4 1045 1328 AirTran ## 8 2011 6 5 7 2207 52 AirTran ## 9 2011 5 7 6 1009 1256 AirTran ## 10 2011 7 25 1 2107 14 AirTran ## # ... with 227 , 486 more rows , and 14 more variables : FlightNum & lt ; int & gt ;, ## # TailNum & lt ; chr & gt ;, ActualElapsedTime & lt ; int & gt ;, AirTime & lt ; int & gt ;, ArrDelay & lt ; int & gt ;, ## # DepDelay & lt ; int & gt ;, Origin & lt ; chr & gt ;, Dest & lt ; chr & gt ;, Distance & lt ; int & gt ;, ## # TaxiIn & lt ; int & gt ;, TaxiOut & lt ; int & gt ;, Cancelled & lt ; int & gt ;, CancellationCode & lt ; chr & gt ;, ## # Diverted & lt ; int & gt ; 1 2 # Arrange flights by total delay (normal order). arrange ( hflights , ( ArrDelay + DepDelay )) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 ## # A tibble : 227 , 496 \u00d7 21 ## Year Month DayofMonth DayOfWeek DepTime ArrTime UniqueCarrier ## & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; chr & gt ; ## 1 2011 7 3 7 1914 2039 ExpressJet ## 2 2011 8 31 3 934 1039 SkyWest ## 3 2011 8 21 7 935 1039 SkyWest ## 4 2011 8 28 7 2059 2206 SkyWest ## 5 2011 8 29 1 935 1041 SkyWest ## 6 2011 12 25 7 741 926 SkyWest ## 7 2011 1 30 7 620 812 SkyWest ## 8 2011 8 3 3 1741 1810 ExpressJet ## 9 2011 8 4 4 930 1041 SkyWest ## 10 2011 8 18 4 939 1043 SkyWest ## # ... with 227 , 486 more rows , and 14 more variables : FlightNum & lt ; int & gt ;, ## # TailNum & lt ; chr & gt ;, ActualElapsedTime & lt ; int & gt ;, AirTime & lt ; int & gt ;, ArrDelay & lt ; int & gt ;, ## # DepDelay & lt ; int & gt ;, Origin & lt ; chr & gt ;, Dest & lt ; chr & gt ;, Distance & lt ; int & gt ;, ## # TaxiIn & lt ; int & gt ;, TaxiOut & lt ; int & gt ;, Cancelled & lt ; int & gt ;, CancellationCode & lt ; chr & gt ;, ## # Diverted & lt ; int & gt ; 1 2 # Keep flights leaving to DFW before 8am and arrange according to decreasing AirTime arrange ( filter ( hflights , Dest == 'DFW' & DepTime < 800 ), desc ( AirTime )) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 ## # A tibble : 799 \u00d7 21 ## Year Month DayofMonth DayOfWeek DepTime ArrTime UniqueCarrier ## & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; chr & gt ; ## 1 2011 11 22 2 635 825 American ## 2 2011 8 25 4 602 758 American_Eagle ## 3 2011 10 12 3 559 738 American_Eagle ## 4 2011 5 2 1 716 854 American ## 5 2011 4 4 1 741 949 American ## 6 2011 4 4 1 627 742 American_Eagle ## 7 2011 6 21 2 726 848 ExpressJet ## 8 2011 9 1 4 715 844 American ## 9 2011 3 14 1 729 917 Continental ## 10 2011 12 5 1 724 847 Continental ## # ... with 789 more rows , and 14 more variables : FlightNum & lt ; int & gt ;, ## # TailNum & lt ; chr & gt ;, ActualElapsedTime & lt ; int & gt ;, AirTime & lt ; int & gt ;, ArrDelay & lt ; int & gt ;, ## # DepDelay & lt ; int & gt ;, Origin & lt ; chr & gt ;, Dest & lt ; chr & gt ;, Distance & lt ; int & gt ;, ## # TaxiIn & lt ; int & gt ;, TaxiOut & lt ; int & gt ;, Cancelled & lt ; int & gt ;, CancellationCode & lt ; chr & gt ;, ## # Diverted & lt ; int & gt ;","title":"Reverse the order of arranging"},{"location":"data_manipulation_in_r_with_dplyr/#summarise-and-the-pipe-operator","text":"","title":"summarise and the Pipe Operator"},{"location":"data_manipulation_in_r_with_dplyr/#the-syntax-of-summarise","text":"1 2 # Print out a summary with variables min_dist and max_dist summarise ( hflights , min_dist = min ( Distance ), max_dist = max ( Distance )) 1 2 3 4 ## # A tibble: 1 \u00d7 2 ## min_dist max_dist ## &lt;int&gt; &lt;int&gt; ## 1 79 3904 1 2 # Print out a summary with variable max_div summarise ( filter ( hflights , Diverted == 1 ), max_div = max ( Distance )) 1 2 3 4 ## # A tibble: 1 \u00d7 1 ## max_div ## &lt;int&gt; ## 1 3904","title":"The syntax of summarise"},{"location":"data_manipulation_in_r_with_dplyr/#aggregate-functions","text":"summarise : min(x) ; minimum value of vector x. max(x) ; maximum value of vector x. mean(x) ; mean value of vector x. median(x) ; median value of vector x. quantile(x, p) ; pth quantile of vector x. sd(x) ; standard deviation of vector x. var(x) ; variance of vector x. IQR(x) ; Inter Quartile Range (IQR) of vector x. diff(range(x)) ; total range of vector x. 1 2 3 4 5 # Remove rows that have NA ArrDelay: temp1 temp1 <- filter ( hflights , ! is.na ( ArrDelay )) # Generate summary about ArrDelay column of temp1 summarise ( temp1 , earliest = min ( ArrDelay ), average = mean ( ArrDelay ), latest = max ( ArrDelay ), sd = sd ( ArrDelay )) 1 2 3 4 ## # A tibble: 1 \u00d7 4 ## earliest average latest sd ## &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 -70 7.094334 978 30.70852 1 2 3 4 5 # Keep rows that have no NA TaxiIn and no NA TaxiOut: temp2 temp2 <- filter ( hflights , ! is.na ( TaxiIn ), ! is.na ( TaxiOut )) # Print the maximum taxiing difference of temp2 with summarise() summarise ( temp2 , max_taxi_diff = max ( abs ( TaxiIn - TaxiOut ))) 1 2 3 4 ## # A tibble: 1 \u00d7 1 ## max_taxi_diff ## &lt;int&gt; ## 1 160","title":"Aggregate functions"},{"location":"data_manipulation_in_r_with_dplyr/#dplyr-aggregate-functions","text":"first(x) ; the first element of vector x . last(x) ; the last element of vector x . nth(x, n) ; The n th element of vector x . n() ; The number of rows in the data.frame or group of observations that summarise() describes. n_distinct(x) ; The number of unique values in vector x . 1 2 # Generate summarizing statistics for hflights summarise ( hflights , n_obs = n (), n_carrier = n_distinct ( UniqueCarrier ), n_dest = n_distinct ( Dest ), dest100 = nth ( Dest , 100 )) 1 2 3 4 ## # A tibble: 1 \u00d7 4 ## n_obs n_carrier n_dest dest100 ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; ## 1 227496 15 116 DFW 1 2 3 4 5 # Filter hflights to keep all American Airline flights: aa aa <- filter ( hflights , UniqueCarrier == 'American' ) # Generate summarizing statistics for aa summarise ( aa , n_flights = n (), n_canc = sum ( Cancelled ), p_canc = n_canc / n_flights * 100 , avg_delay = mean ( ArrDelay , na.rm = TRUE )) 1 2 3 4 ## # A tibble: 1 \u00d7 4 ## n_flights n_canc p_canc avg_delay ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 3244 60 1.849568 0.8917558 Overview of syntax 1 2 3 4 5 # Write the 'piped' version of the English sentences hflights %>% mutate ( diff = TaxiOut - TaxiIn ) %>% filter ( ! is.na ( diff )) %>% summarise ( avg = mean ( diff )) 1 2 3 4 ## # A tibble: 1 \u00d7 1 ## avg ## &lt;dbl&gt; ## 1 8.992064 Drive or fly? Part 1 of 2 1 2 3 4 5 6 7 8 9 10 11 12 # Part 1, concerning the selection and creation of columns d <- hflights %>% select ( Dest , UniqueCarrier , Distance , ActualElapsedTime ) %>% mutate ( RealTime = ActualElapsedTime + 100 , mph = Distance / RealTime * 60 ) # Part 2, concerning flights that had an actual average speed of < 70 mph. d %>% filter ( ! is.na ( mph ), mph < 70 ) %>% summarise ( n_less = n (), n_dest = n_distinct ( Dest ), min_dist = min ( Distance ), max_dist = max ( Distance )) 1 2 3 4 ## # A tibble: 1 \u00d7 4 ## n_less n_dest min_dist max_dist ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 6726 13 79 305 Drive or fly? Part 2 of 2 1 2 3 4 5 # Solve the exercise using a combination of dplyr verbs and %>% hflights %>% #summarise(all_flights = n()) %>% filter ((( Distance / ( ActualElapsedTime + 100 ) * 60 ) < 105 ) | Cancelled == 1 | Diverted == 1 ) %>% summarise ( n_non = n (), p_non = n_non / 22751 * 100 , n_dest = n_distinct ( Dest ), min_dist = min ( Distance ), max_dist = max ( Distance )) 1 2 3 4 ## # A tibble: 1 \u00d7 5 ## n_non p_non n_dest min_dist max_dist ## &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 42400 186.3654 113 79 3904 Advanced piping exercise 1 2 3 4 # Count the number of overnight flights hflights %>% filter ( ArrTime < DepTime & ! is.na ( DepTime ) & ! is.na ( ArrTime )) %>% summarise ( n = n ()) 1 2 3 4 ## # A tibble: 1 \u00d7 1 ## n ## &lt;int&gt; ## 1 2718","title":"dplyr aggregate functions"},{"location":"data_manipulation_in_r_with_dplyr/#group_by-and-working-with-data","text":"","title":"group_by and working with data"},{"location":"data_manipulation_in_r_with_dplyr/#unite-and-conquer-using-group_by","text":"1 2 3 4 5 6 7 8 # Make an ordered per-carrier summary of hflights hflights %>% group_by ( UniqueCarrier ) %>% summarise ( n_flights = n (), n_canc = sum ( Cancelled == 1 ), p_canc = mean ( Cancelled == 1 ) * 100 , avg_delay = mean ( ArrDelay , na.rm = TRUE )) %>% arrange ( avg_delay , p_canc ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 ## # A tibble: 15 \u00d7 5 ## UniqueCarrier n_flights n_canc p_canc avg_delay ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 US_Airways 4082 46 1.1268986 -0.6307692 ## 2 American 3244 60 1.8495684 0.8917558 ## 3 AirTran 2139 21 0.9817672 1.8536239 ## 4 Alaska 365 0 0.0000000 3.1923077 ## 5 Mesa 79 1 1.2658228 4.0128205 ## 6 Delta 2641 42 1.5903067 6.0841374 ## 7 Continental 70032 475 0.6782614 6.0986983 ## 8 American_Eagle 4648 135 2.9044750 7.1529751 ## 9 Atlantic_Southeast 2204 76 3.4482759 7.2569543 ## 10 Southwest 45343 703 1.5504047 7.5871430 ## 11 Frontier 838 6 0.7159905 7.6682692 ## 12 ExpressJet 73053 1132 1.5495599 8.1865242 ## 13 SkyWest 16061 224 1.3946828 8.6934922 ## 14 JetBlue 695 18 2.5899281 9.8588410 ## 15 United 2072 34 1.6409266 10.4628628 1 2 3 4 5 # Make an ordered per-day summary of hflights hflights %>% group_by ( DayOfWeek ) %>% summarise ( avg_taxi = mean ( TaxiIn + TaxiOut , na.rm = TRUE )) %>% arrange ( desc ( avg_taxi )) 1 2 3 4 5 6 7 8 9 10 ## # A tibble: 7 \u00d7 2 ## DayOfWeek avg_taxi ## &lt;int&gt; &lt;dbl&gt; ## 1 1 21.77027 ## 2 2 21.43505 ## 3 4 21.26076 ## 4 3 21.19055 ## 5 5 21.15805 ## 6 7 20.93726 ## 7 6 20.43061","title":"Unite and conquer using group_by"},{"location":"data_manipulation_in_r_with_dplyr/#combine-group_by-with-mutate","text":"1 2 3 4 5 6 7 # Solution to first instruction hflights %>% filter ( ! is.na ( ArrDelay )) %>% group_by ( UniqueCarrier ) %>% summarise ( p_delay = sum ( ArrDelay > 0 ) / n ()) %>% mutate ( rank = rank ( p_delay )) %>% arrange ( rank ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 ## # A tibble: 15 \u00d7 3 ## UniqueCarrier p_delay rank ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 American 0.3030208 1 ## 2 AirTran 0.3112269 2 ## 3 US_Airways 0.3267990 3 ## 4 Atlantic_Southeast 0.3677511 4 ## 5 American_Eagle 0.3696714 5 ## 6 Delta 0.3871092 6 ## 7 JetBlue 0.3952452 7 ## 8 Alaska 0.4368132 8 ## 9 Southwest 0.4644557 9 ## 10 Mesa 0.4743590 10 ## 11 Continental 0.4907385 11 ## 12 ExpressJet 0.4943420 12 ## 13 United 0.4963109 13 ## 14 SkyWest 0.5350105 14 ## 15 Frontier 0.5564904 15 1 2 3 4 5 6 7 # Solution to second instruction hflights %>% filter ( ! is.na ( ArrDelay ), ArrDelay > 0 ) %>% group_by ( UniqueCarrier ) %>% summarise ( avg = mean ( ArrDelay )) %>% mutate ( rank = rank ( avg )) %>% arrange ( rank ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 ## # A tibble: 15 \u00d7 3 ## UniqueCarrier avg rank ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Mesa 18.67568 1 ## 2 Frontier 18.68683 2 ## 3 US_Airways 20.70235 3 ## 4 Continental 22.13374 4 ## 5 Alaska 22.91195 5 ## 6 SkyWest 24.14663 6 ## 7 ExpressJet 24.19337 7 ## 8 Southwest 25.27750 8 ## 9 AirTran 27.85693 9 ## 10 American 28.49740 10 ## 11 Delta 32.12463 11 ## 12 United 32.48067 12 ## 13 American_Eagle 38.75135 13 ## 14 Atlantic_Southeast 40.24231 14 ## 15 JetBlue 45.47744 15","title":"Combine group_by with mutate"},{"location":"data_manipulation_in_r_with_dplyr/#advanced-group_by","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 # Which plane (by tail number) flew out of Houston the most times? How many times? adv1 adv1 <- hflights %>% group_by ( TailNum ) %>% summarise ( n = n ()) %>% filter ( n == max ( n )) # How many airplanes only flew to one destination from Houston? adv2 adv2 <- hflights %>% group_by ( TailNum ) %>% summarise ( ndest = n_distinct ( Dest )) %>% filter ( ndest == 1 ) %>% summarise ( nplanes = n ()) # Find the most visited destination for each carrier: adv3 adv3 <- hflights %>% group_by ( UniqueCarrier , Dest ) %>% summarise ( n = n ()) %>% mutate ( rank = rank ( desc ( n ))) %>% filter ( rank == 1 ) # Find the carrier that travels to each destination the most: adv4 adv4 <- hflights %>% group_by ( Dest , UniqueCarrier ) %>% summarise ( n = n ()) %>% mutate ( rank = rank ( desc ( n ))) %>% filter ( rank == 1 )","title":"Advanced group_by"},{"location":"data_manipulation_in_r_with_dplyr/#dplyr-deals-with-different-types","text":"1 2 3 # Use summarise to calculate n_carrier s2 <- hflights %>% summarise ( n_carrier = n_distinct ( UniqueCarrier ))","title":"dplyr deals with different types"},{"location":"data_manipulation_in_r_with_dplyr/#dplyr-and-mysql-databases","text":"Code only. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 # set up a src that connects to the mysql database (src_mysql is provided by dplyr) my_db <- src_mysql ( dbname = 'dplyr' , host = 'dplyr.csrrinzqubik.us-east-1.rds.amazonaws.com' , port = 3306 , user = 'dplyr' , password = 'dplyr' ) # and reference a table within that src: nycflights is now available as an R object that references to the remote nycflights table nycflights <- tbl ( my_db , 'dplyr' ) # glimpse at nycflights glimpse ( nycflights ) # Calculate the grouped summaries detailed in the instructions nycflights %>% group_by ( carrier ) %>% summarise ( n_flights = n (), avg_delay = mean ( arr_delay )) %>% arrange ( avg_delay )","title":"dplyr and mySQL databases"},{"location":"data_manipulation_in_r_with_dplyr/#adding-tidyr-functions","text":"complete . drop_na . expand . extract . extract_numeric . complete . fill . full_seq . gather . nest . replace_na . separate . separate_rows . separate_rows_ . smiths . spread . table1 . unite . unnest . who .","title":"Adding tidyr Functions"},{"location":"data_manipulation_in_r_with_dplyr/#joining-data-in-r-with-dplyr","text":"In development 1 2 3 4 5 6 7 8 9 ## 1, Mutating Joins ## 2, Filtering Joins and Set Operations ## 3, Assembling Data ## 4, Advanced Joining ## 5, Case Study","title":"Joining Data in R with dplyr"},{"location":"data_wrangling/","text":"Foreword Output options: the \u2018tango\u2019 syntax and the \u2018readable\u2019 theme. Snippets and results. Documentation \u00b6 data.table extension of data.frame . Fast aggregation of large data (e.g. 100GB in RAM), fast ordered joins, fast add/modify/delete of columns by group using no copies at all, list columns, a fast friendly file reader and parallel file writer. Offers a natural and flexible syntax, for faster development. dplyr A fast, consistent tool for working with data frame like objects, both in memory and out of memory. Pipelines. tidyr An evolution of \u2018reshape2\u2019. It\u2019s designed specifically for data tidying (not general reshaping or aggregating) and works well with dplyr data pipelines. package 'narrower' 'wider' tidyr gather spread reshape2 melt cast spreadsheets unpivot pivot databases fold unfold Data Analysis in R, the data.table Way \u00b6 1, data.table novice \u00b6 Find out more with ?data.table . Create and subset a data.table 1 2 3 4 5 6 7 8 9 10 11 # The data.table package library ( data.table ) # Create my_first_data_table my_first_data_table <- data.table ( x = c ( 'a' , 'b' , 'c' , 'd' , 'e' ), y = c ( 1 , 2 , 3 , 4 , 5 )) # Create a data.table using recycling DT <- data.table ( a = 1 : 2 , b = c ( 'A' , 'B' , 'C' , 'D' )) # Print the third row to the console DT[3 , ] 1 2 ## a b ## 1: 1 C 1 2 # Print the second and third row to the console, but do not commas DT[2 : 3 ] 1 2 3 ## a b ## 1: 2 B ## 2: 1 C Getting to know a data.table Like head , tail . 1 2 # Print the penultimate row of DT using .N DT[.N - 1 ] 1 2 ## a b ## 1: 1 C 1 2 # Print the column names of DT, and number of rows and number of columns colnames ( DT ) 1 ## [1] \"a\" \"b\" 1 dim ( DT ) 1 ## [1] 4 2 1 2 # Select row 2 twice and row 3, returning a data.table with three rows where row 2 is a duplicate of row 1. DT [c ( 2 , 2 , 3 ) ] 1 2 3 4 ## a b ## 1: 2 B ## 2: 2 B ## 3: 1 C DT is a data.table/data.frame, but DT[ , B] is a vector; DT[ , .(B)] is a subsetted data.table. Subsetting data tables DT[i, j, by] means take DT , subset rows using i , then calculate j grouped by by . You can wrap j with .() . 1 2 3 4 5 6 7 A <- c ( 1 , 2 , 3 , 4 , 5 ) B <- c ( 'a' , 'b' , 'c' , 'd' , 'e' ) C <- c ( 6 , 7 , 8 , 9 , 10 ) DT <- data.table ( A , B , C ) # Subset rows 1 and 3, and columns B and C DT [c ( 1 , 3 ) , .(B , C ) ] 1 2 3 ## B C ## 1: a 6 ## 2: c 8 1 2 3 4 5 6 # Assign to ans the correct value ans <- data.table ( DT[ , .(B , val = A * C ) ] ) # Fill in the blanks such that ans2 equals target #target <- data.table(B = c('a', 'b', 'c', 'd', 'e', 'a', 'b', 'c', 'd', 'e'), val = as.integer(c(6:10, 1:5))) ans2 <- data.table ( DT[ , .(B , val = as.integer ( c ( 6 : 10 , 1 : 5 ))) ] ) The by basics 1 2 3 4 5 6 7 # iris and iris3 are already available in the workspace # Convert iris to a data.table: DT DT <- as.data.table ( iris ) # For each Species, print the mean Sepal.Length DT[ , .(mean ( Sepal.Length )), by = .(Species ) ] 1 2 3 4 ## Species V1 ## 1: setosa 5.006 ## 2: versicolor 5.936 ## 3: virginica 6.588 1 2 # Print mean Sepal.Length, grouping by first letter of Species DT[ , .(mean ( Sepal.Length )), by = .(substr ( Species , 1 , 1 )) ] 1 2 3 ## substr V1 ## 1: s 5.006 ## 2: v 6.262 Using .N and by .N , number, in row or column. 1 2 3 4 5 # data.table version of iris: DT DT <- as.data.table ( iris ) # Group the specimens by Sepal area (to the nearest 10 cm2) and count how many occur in each group. DT[ , .N , by = 10 * round ( Sepal.Length * Sepal.Width / 10 ) ] 1 2 3 4 ## round N ## 1: 20 117 ## 2: 10 29 ## 3: 30 4 1 2 # Now name the output columns `Area` and `Count` DT[ , .(Count = .N ), by = .(Area = 10 * round ( Sepal.Length * Sepal.Width / 10 )) ] 1 2 3 4 ## Area Count ## 1: 20 117 ## 2: 10 29 ## 3: 30 4 Return multiple numbers in j 1 2 3 4 5 6 7 8 9 # Create the data.table DT set.seed ( 1L ) DT <- data.table ( A = rep ( letters [2 : 1 ] , each = 4L ), B = rep ( 1 : 4 , each = 2L ), C = sample ( 8 )) # Create the new data.table, DT2 DT2 <- DT[ , .(C = cumsum ( C )), by = .(A , B ) ] # Select from DT2 the last two values from C while you group by A DT2[ , .(C = tail ( C , 2 )), by = A] 1 2 3 4 5 ## A C ## 1: b 4 ## 2: b 9 ## 3: a 2 ## 4: a 8 2, data.table yeoman \u00b6 Chaining, the basics 1 2 3 4 # Build DT set.seed ( 1L ) DT <- data.table ( A = rep ( letters [2 : 1 ] , each = 4L ), B = rep ( 1 : 4 , each = 2L ), C = sample ( 8 )) DT 1 2 3 4 5 6 7 8 9 ## A B C ## 1: b 1 3 ## 2: b 1 8 ## 3: b 2 4 ## 4: b 2 5 ## 5: a 3 1 ## 6: a 3 7 ## 7: a 4 2 ## 8: a 4 6 1 2 3 # Use chaining # Cumsum of C while grouping by A and B, and then select last two values of C while grouping by A DT[ , .(C = cumsum ( C )), by = .(A , B ) ][ , .(C = tail ( C , 2 )), by = .(A ) ] 1 2 3 4 5 ## A C ## 1: b 4 ## 2: b 9 ## 3: a 2 ## 4: a 8 Chaining your iris dataset 1 2 3 4 DT <- data.table ( iris ) # Perform chained operations on DT DT[ , .(Sepal.Length = median ( Sepal.Length ), Sepal.Width = median ( Sepal.Width ), Petal.Length = median ( Petal.Length ), Petal.Width = median ( Petal.Width )), by = .(Species ) ] [order ( Species , decreasing = TRUE ) ] 1 2 3 4 ## Species Sepal.Length Sepal.Width Petal.Length Petal.Width ## 1: virginica 6.5 3.0 5.55 2.0 ## 2: versicolor 5.9 2.8 4.35 1.3 ## 3: setosa 5.0 3.4 1.50 0.2 1 DT 1 2 3 4 5 6 7 8 9 10 11 12 ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1: 5.1 3.5 1.4 0.2 setosa ## 2: 4.9 3.0 1.4 0.2 setosa ## 3: 4.7 3.2 1.3 0.2 setosa ## 4: 4.6 3.1 1.5 0.2 setosa ## 5: 5.0 3.6 1.4 0.2 setosa ## --- ## 146: 6.7 3.0 5.2 2.3 virginica ## 147: 6.3 2.5 5.0 1.9 virginica ## 148: 6.5 3.0 5.2 2.0 virginica ## 149: 6.2 3.4 5.4 2.3 virginica ## 150: 5.9 3.0 5.1 1.8 virginica Programming time vs readability 1 2 3 4 5 6 7 x <- c ( 2 , 1 , 2 , 1 , 2 , 2 , 1 ) y <- c ( 1 , 3 , 5 , 7 , 9 , 11 , 13 ) z <- c ( 2 , 4 , 6 , 8 , 10 , 12 , 14 ) DT <- data.table ( x , y , z ) # Mean of columns DT[ , lapply ( .SD , mean ), by = .(x ) ] 1 2 3 ## x y z ## 1: 2 6.500000 7.500000 ## 2: 1 7.666667 8.666667 1 2 # Median of columns DT[ , lapply ( .SD , median ), by = .(x ) ] 1 2 3 ## x y z ## 1: 2 7 8 ## 2: 1 7 8 Introducing .SDcols .SDcols specifies the columns of DT that are included in .SD . 1 2 3 4 5 6 7 8 9 10 grp <- c ( 6 , 6 , 8 , 8 , 8 ) Q1 <- c ( 4 , 3 , 3 , 5 , 3 ) Q2 <- c ( 1 , 4 , 1 , 4 , 4 ) Q3 <- c ( 3 , 1 , 5 , 5 , 2 ) H1 <- c ( 1 , 2 , 3 , 2 , 4 ) H2 <- c ( 1 , 4 , 3 , 4 , 3 ) DT <- data.table ( grp , Q1 , Q2 , Q3 , H1 , H2 ) # Calculate the sum of the Q columns DT[ , lapply ( .SD , sum ), .SDcols = 2 : 4 ] 1 2 ## Q1 Q2 Q3 ## 1: 18 14 16 1 2 # Calculate the sum of columns H1 and H2 DT[ , lapply ( .SD , sum ), .SDcols = 5 : 6 ] 1 2 ## H1 H2 ## 1: 12 15 1 2 # Select all but the first row of groups 1 and 2, returning only the grp column and the Q columns. DT[ , .SD[ -1 ] , .SDcols = 2 : 4 , by = .(grp ) ] 1 2 3 4 ## grp Q1 Q2 Q3 ## 1: 6 3 4 1 ## 2: 8 5 4 5 ## 3: 8 3 4 2 Mixing it together: lapply , .SD , SDcols and .N 1 2 3 4 5 6 7 8 x <- c ( 2 , 1 , 2 , 1 , 2 , 2 , 1 ) y <- c ( 1 , 3 , 5 , 7 , 9 , 11 , 13 ) z <- c ( 2 , 4 , 6 , 8 , 10 , 12 , 14 ) DT <- data.table ( x , y , z ) # Sum of all columns and the number of rows # For the first part, you need to combine the returned list from lapply, .SD and .SDcols and the integer vector of .N. You have to this because the result of the two together has to be a list again, with all values put together. DT 1 2 3 4 5 6 7 8 ## x y z ## 1: 2 1 2 ## 2: 1 3 4 ## 3: 2 5 6 ## 4: 1 7 8 ## 5: 2 9 10 ## 6: 2 11 12 ## 7: 1 13 14 1 DT[ , c ( lapply ( .SD , sum ), .N ), .SDcols = 1 : 3 , by = x] 1 2 3 ## x x y z N ## 1: 2 8 26 30 4 ## 2: 1 3 23 26 3 1 2 # Cumulative sum of column x and y while grouping by x and z > 8 DT[ , lapply ( .SD , cumsum ), .SDcols = 1 : 2 , by = .(by1 = x , by2 = z > 8 ) ] 1 2 3 4 5 6 7 8 ## by1 by2 x y ## 1: 2 FALSE 2 1 ## 2: 2 FALSE 4 6 ## 3: 1 FALSE 1 3 ## 4: 1 FALSE 2 10 ## 5: 2 TRUE 2 9 ## 6: 2 TRUE 4 20 ## 7: 1 TRUE 1 13 1 2 # Chaining DT[ , lapply ( .SD , cumsum ), .SDcols = 1 : 2 , by = .(by1 = x , by2 = z > 8 ) ][ , lapply ( .SD , max ), .SDcols = 3 : 4 , by = by1] 1 2 3 ## by1 x y ## 1: 2 4 20 ## 2: 1 2 13 Adding, updating and removing columns := is defined for use in j only. 1 2 3 # The data.table DT DT <- data.table ( A = letters [c ( 1 , 1 , 1 , 2 , 2 ) ] , B = 1 : 5 ) DT 1 2 3 4 5 6 ## A B ## 1: a 1 ## 2: a 2 ## 3: a 3 ## 4: b 4 ## 5: b 5 1 2 # Add column by reference: Total DT[ , ( 'Total' ) := sum ( B ), by = .(A ) ] 1 2 3 4 5 6 ## A B Total ## 1: a 1 6 ## 2: a 2 6 ## 3: a 3 6 ## 4: b 4 9 ## 5: b 5 9 1 2 # Add 1 to column B DT [c ( 2 , 4 ), ( 'B' ) := as.integer ( 1 + B ) ] 1 2 3 4 5 6 ## A B Total ## 1: a 1 6 ## 2: a 3 6 ## 3: a 3 6 ## 4: b 5 9 ## 5: b 5 9 1 2 # Add a new column Total2 DT[2 : 4 , ':=' ( Total2 = sum ( B )), by = .(A ) ] 1 2 3 4 5 6 ## A B Total Total2 ## 1: a 1 6 NA ## 2: a 3 6 6 ## 3: a 3 6 6 ## 4: b 5 9 5 ## 5: b 5 9 NA 1 2 # Remove the Total column DT[ , Total := NULL ] 1 2 3 4 5 6 ## A B Total2 ## 1: a 1 NA ## 2: a 3 6 ## 3: a 3 6 ## 4: b 5 5 ## 5: b 5 NA 1 2 # Select the third column using `[[` DT[[3]] 1 ## [1] NA 6 6 5 NA The functional form 1 2 3 4 5 # A data.table DT DT <- data.table ( A = c ( 1 , 1 , 1 , 2 , 2 ), B = 1 : 5 ) # Update B, add C and D DT[ , `:=` ( B = B + 1 , C = A + B , D = 2 ) ] 1 2 3 4 5 6 ## A B C D ## 1: 1 2 2 2 ## 2: 1 3 3 2 ## 3: 1 4 4 2 ## 4: 2 5 6 2 ## 5: 2 6 7 2 1 2 3 # Delete my_cols my_cols <- c ( 'B' , 'C' ) DT[ , ( my_cols ) := NULL ] 1 2 3 4 5 6 ## A D ## 1: 1 2 ## 2: 1 2 ## 3: 1 2 ## 4: 2 2 ## 5: 2 2 1 2 # Delete column 2 by number DT[ , 2 := NULL ] 1 2 3 4 5 6 ## A ## 1: 1 ## 2: 1 ## 3: 1 ## 4: 2 ## 5: 2 Ready, set , go! The set function is used to repeatedly update a data.table by reference. You can think of the set function as a loopable. 1 2 3 4 5 6 7 8 9 10 11 A <- c ( 2 , 2 , 3 , 5 , 2 , 5 , 5 , 4 , 4 , 1 ) B <- c ( 2 , 1 , 4 , 2 , 4 , 3 , 4 , 5 , 2 , 4 ) C <- c ( 5 , 2 , 4 , 1 , 2 , 2 , 1 , 2 , 5 , 2 ) D <- c ( 3 , 3 , 3 , 1 , 5 , 4 , 4 , 1 , 4 , 3 ) DT <- data.table ( A , B , C , D ) # Set the seed set.seed ( 1 ) # Check the DT DT 1 2 3 4 5 6 7 8 9 10 11 ## A B C D ## 1: 2 2 5 3 ## 2: 2 1 2 3 ## 3: 3 4 4 3 ## 4: 5 2 1 1 ## 5: 2 4 2 5 ## 6: 5 3 2 4 ## 7: 5 4 1 4 ## 8: 4 5 2 1 ## 9: 4 2 5 4 ## 10: 1 4 2 3 1 2 3 4 5 6 7 8 # For loop with set for ( l in 2 : 4 ) set ( DT , sample ( 10 , 3 ), l , NA ) # Change the column names to lowercase setnames ( DT , c ( 'A' , 'B' , 'C' , 'D' ), c ( 'a' , 'b' , 'c' , 'd' )) # Print the resulting DT to the console DT 1 2 3 4 5 6 7 8 9 10 11 ## a b c d ## 1: 2 2 5 3 ## 2: 2 1 NA 3 ## 3: 3 NA 4 3 ## 4: 5 NA 1 1 ## 5: 2 NA 2 5 ## 6: 5 3 2 NA ## 7: 5 4 1 4 ## 8: 4 5 NA 1 ## 9: 4 2 5 NA ## 10: 1 4 NA NA The set family 1 2 3 # Define DT DT <- data.table ( a = letters [c ( 1 , 1 , 1 , 2 , 2 ) ] , b = 1 ) DT 1 2 3 4 5 6 ## a b ## 1: a 1 ## 2: a 1 ## 3: a 1 ## 4: b 1 ## 5: b 1 1 2 3 # Add a postfix '_2' to all column names setnames ( DT , c ( 1 : 2 ), paste0 ( c ( 'a' , 'b' ), '_2' )) DT 1 2 3 4 5 6 ## a_2 b_2 ## 1: a 1 ## 2: a 1 ## 3: a 1 ## 4: b 1 ## 5: b 1 1 2 3 # Change column name 'a_2' to 'A2' setnames ( DT , 'a_2' , 'A2' ) DT 1 2 3 4 5 6 ## A2 b_2 ## 1: a 1 ## 2: a 1 ## 3: a 1 ## 4: b 1 ## 5: b 1 1 2 # Reverse the order of the columns setcolorder ( DT , c ( 'b_2' , 'A2' )) 3, data.table expert \u00b6 Selecting rows the data.table way 1 2 3 # Convert iris to a data.table iris <- data.table ( 'Sepal.Length' = iris $ Sepal.Length , 'Sepal.Width' = iris $ Sepal.Width , 'Petal.Length' = iris $ Petal.Length , 'Petal.Width' = iris $ Petal.Width , 'Species' = iris $ Species ) iris 1 2 3 4 5 6 7 8 9 10 11 12 ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1: 5.1 3.5 1.4 0.2 setosa ## 2: 4.9 3.0 1.4 0.2 setosa ## 3: 4.7 3.2 1.3 0.2 setosa ## 4: 4.6 3.1 1.5 0.2 setosa ## 5: 5.0 3.6 1.4 0.2 setosa ## --- ## 146: 6.7 3.0 5.2 2.3 virginica ## 147: 6.3 2.5 5.0 1.9 virginica ## 148: 6.5 3.0 5.2 2.0 virginica ## 149: 6.2 3.4 5.4 2.3 virginica ## 150: 5.9 3.0 5.1 1.8 virginica 1 2 # Species is 'virginica' head ( iris[Species == 'virginica' ] , 20 ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1: 6.3 3.3 6.0 2.5 virginica ## 2: 5.8 2.7 5.1 1.9 virginica ## 3: 7.1 3.0 5.9 2.1 virginica ## 4: 6.3 2.9 5.6 1.8 virginica ## 5: 6.5 3.0 5.8 2.2 virginica ## 6: 7.6 3.0 6.6 2.1 virginica ## 7: 4.9 2.5 4.5 1.7 virginica ## 8: 7.3 2.9 6.3 1.8 virginica ## 9: 6.7 2.5 5.8 1.8 virginica ## 10: 7.2 3.6 6.1 2.5 virginica ## 11: 6.5 3.2 5.1 2.0 virginica ## 12: 6.4 2.7 5.3 1.9 virginica ## 13: 6.8 3.0 5.5 2.1 virginica ## 14: 5.7 2.5 5.0 2.0 virginica ## 15: 5.8 2.8 5.1 2.4 virginica ## 16: 6.4 3.2 5.3 2.3 virginica ## 17: 6.5 3.0 5.5 1.8 virginica ## 18: 7.7 3.8 6.7 2.2 virginica ## 19: 7.7 2.6 6.9 2.3 virginica ## 20: 6.0 2.2 5.0 1.5 virginica 1 2 # Species is either 'virginica' or 'versicolor' head ( iris[Species %in% c ( 'virginica' , 'versicolor' ) ] , 20 ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1: 7.0 3.2 4.7 1.4 versicolor ## 2: 6.4 3.2 4.5 1.5 versicolor ## 3: 6.9 3.1 4.9 1.5 versicolor ## 4: 5.5 2.3 4.0 1.3 versicolor ## 5: 6.5 2.8 4.6 1.5 versicolor ## 6: 5.7 2.8 4.5 1.3 versicolor ## 7: 6.3 3.3 4.7 1.6 versicolor ## 8: 4.9 2.4 3.3 1.0 versicolor ## 9: 6.6 2.9 4.6 1.3 versicolor ## 10: 5.2 2.7 3.9 1.4 versicolor ## 11: 5.0 2.0 3.5 1.0 versicolor ## 12: 5.9 3.0 4.2 1.5 versicolor ## 13: 6.0 2.2 4.0 1.0 versicolor ## 14: 6.1 2.9 4.7 1.4 versicolor ## 15: 5.6 2.9 3.6 1.3 versicolor ## 16: 6.7 3.1 4.4 1.4 versicolor ## 17: 5.6 3.0 4.5 1.5 versicolor ## 18: 5.8 2.7 4.1 1.0 versicolor ## 19: 6.2 2.2 4.5 1.5 versicolor ## 20: 5.6 2.5 3.9 1.1 versicolor Removing columns and adapting your column names Refer to a regex cheat sheet for metacharacter. 1 2 3 # iris as a data.table iris <- as.data.table ( iris ) iris 1 2 3 4 5 6 7 8 9 10 11 12 ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1: 5.1 3.5 1.4 0.2 setosa ## 2: 4.9 3.0 1.4 0.2 setosa ## 3: 4.7 3.2 1.3 0.2 setosa ## 4: 4.6 3.1 1.5 0.2 setosa ## 5: 5.0 3.6 1.4 0.2 setosa ## --- ## 146: 6.7 3.0 5.2 2.3 virginica ## 147: 6.3 2.5 5.0 1.9 virginica ## 148: 6.5 3.0 5.2 2.0 virginica ## 149: 6.2 3.4 5.4 2.3 virginica ## 150: 5.9 3.0 5.1 1.8 virginica 1 2 3 4 5 6 7 # Remove the 'Sepal.' prefix #gsub('([ab])', '\\\\1_\\\\1_', 'abc and ABC') = pattern, replacement, x setnames ( iris , c ( 'Sepal.Length' , 'Sepal.Width' ), c ( 'Length' , 'Width' )) #gsub('^Sepal\\\\.','', iris) # Remove the two columns starting with 'Petal' iris[ , c ( 'Petal.Length' , 'Petal.Width' ) := NULL ] 1 2 3 4 5 6 7 8 9 10 11 12 ## Length Width Species ## 1: 5.1 3.5 setosa ## 2: 4.9 3.0 setosa ## 3: 4.7 3.2 setosa ## 4: 4.6 3.1 setosa ## 5: 5.0 3.6 setosa ## --- ## 146: 6.7 3.0 virginica ## 147: 6.3 2.5 virginica ## 148: 6.5 3.0 virginica ## 149: 6.2 3.4 virginica ## 150: 5.9 3.0 virginica Understanding automatic indexing 1 2 3 # Cleaned up iris data.table iris2 <- data.frame ( Length = iris $ Sepal.Length , Width = iris $ Sepal.Width , Species = iris $ Species ) iris2 <- as.data.table ( iris2 ) 1 2 # Area is greater than 20 square centimeters iris2[ Width * Length > 20 ] , 20 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 Length Width Species is_large 1: 5.4 3.9 setosa FALSE 2: 5.8 4.0 setosa FALSE 3: 5.7 4.4 setosa TRUE 4: 5.4 3.9 setosa FALSE 5: 5.7 3.8 setosa FALSE 6: 5.2 4.1 setosa FALSE 7: 5.5 4.2 setosa FALSE 8: 7.0 3.2 versicolor FALSE 9: 6.4 3.2 versicolor FALSE 10: 6.9 3.1 versicolor FALSE 11: 6.3 3.3 versicolor FALSE 12: 6.7 3.1 versicolor FALSE 13: 6.7 3.0 versicolor FALSE 14: 6.0 3.4 versicolor FALSE 15: 6.7 3.1 versicolor FALSE 16: 6.3 3.3 virginica FALSE 17: 7.1 3.0 virginica FALSE 18: 7.6 3.0 virginica FALSE 19: 7.3 2.9 virginica FALSE 20: 7.2 3.6 virginica TRUE ... 1 2 # Add new boolean column iris2[ , is_large := Width * Length > 25 ] 1 2 # Now large observations with is_large iris2[is_large == TRUE ] 1 2 3 4 5 Length Width Species is_large 1: 5.7 4.4 setosa TRUE 2: 7.2 3.6 virginica TRUE 3: 7.7 3.8 virginica TRUE 4: 7.9 3.8 virginica TRUE 1 iris2 [ ( is_large ) ] # Also OK 1 2 3 4 5 Length Width Species is_large 1: 5.7 4.4 setosa TRUE 2: 7.2 3.6 virginica TRUE 3: 7.7 3.8 virginica TRUE 4: 7.9 3.8 virginica TRUE Selecting groups or parts of groups 1 2 3 4 5 6 7 8 # The 'keyed' data.table DT DT <- data.table ( A = letters [c ( 2 , 1 , 2 , 3 , 1 , 2 , 3 ) ] , B = c ( 5 , 4 , 1 , 9 , 8 , 8 , 6 ), C = 6 : 12 ) setkey ( DT , A , B ) # Select the 'b' group DT[ 'b' ] 1 2 3 4 ## A B C ## 1: b 1 8 ## 2: b 5 6 ## 3: b 8 11 1 2 # 'b' and 'c' groups DT [c ( 'b' , 'c' ) ] 1 2 3 4 5 6 ## A B C ## 1: b 1 8 ## 2: b 5 6 ## 3: b 8 11 ## 4: c 6 12 ## 5: c 9 9 1 2 # The first row of the 'b' and 'c' groups DT [c ( 'b' , 'c' ), mult = 'first' ] 1 2 3 ## A B C ## 1: b 1 8 ## 2: c 6 12 1 2 # First and last row of the 'b' and 'c' groups DT [c ( 'b' , 'c' ), .SD [c ( 1 , .N ) ] , by = .EACHI] 1 2 3 4 5 ## A B C ## 1: b 1 8 ## 2: b 8 11 ## 3: c 6 12 ## 4: c 9 9 1 2 # Copy and extend code for instruction 4: add printout DT [c ( 'b' , 'c' ), { print ( .SD ); .SD [c ( 1 , .N ) ] }, by = .EACHI] 1 2 3 4 5 6 7 8 9 10 11 12 13 ## B C ## 1: 1 8 ## 2: 5 6 ## 3: 8 11 ## B C ## 1: 6 12 ## 2: 9 9 ## A B C ## 1: b 1 8 ## 2: b 8 11 ## 3: c 6 12 ## 4: c 9 9 Rolling joins - part one 1 2 3 4 5 6 7 # Keyed data.table DT DT <- data.table ( A = letters [c ( 2 , 1 , 2 , 3 , 1 , 2 , 3 ) ] , B = c ( 5 , 4 , 1 , 9 , 8 , 8 , 6 ), C = 6 : 12 , key = 'A,B' ) # Get the key of DT key ( DT ) 1 ## [1] \"A\" \"B\" 1 2 3 # Row where A == 'b' & B == 6 setkey ( DT , A , B ) DT [. ( 'b' , 6 ) ] 1 2 ## A B C ## 1: b 6 NA 1 2 # Return the prevailing row DT [. ( 'b' , 6 ), roll = TRUE ] 1 2 ## A B C ## 1: b 6 6 1 2 # Return the nearest row DT [. ( 'b' , 6 ), roll =+ Inf ] 1 2 ## A B C ## 1: b 6 6 Rolling joins - part two 1 2 3 4 5 6 7 # Keyed data.table DT DT <- data.table ( A = letters [c ( 2 , 1 , 2 , 3 , 1 , 2 , 3 ) ] , B = c ( 5 , 4 , 1 , 9 , 8 , 8 , 6 ), C = 6 : 12 , key = 'A,B' ) # Look at the sequence (-2):10 for the 'b' group DT [. ( 'b' , ( -2 ) : 10 ) ] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 ## A B C ## 1: b -2 NA ## 2: b -1 NA ## 3: b 0 NA ## 4: b 1 8 ## 5: b 2 NA ## 6: b 3 NA ## 7: b 4 NA ## 8: b 5 6 ## 9: b 6 NA ## 10: b 7 NA ## 11: b 8 11 ## 12: b 9 NA ## 13: b 10 NA 1 2 # Add code: carry the prevailing values forwards DT [. ( 'b' , ( -2 ) : 10 ), roll = TRUE ] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 ## A B C ## 1: b -2 NA ## 2: b -1 NA ## 3: b 0 NA ## 4: b 1 8 ## 5: b 2 8 ## 6: b 3 8 ## 7: b 4 8 ## 8: b 5 6 ## 9: b 6 6 ## 10: b 7 6 ## 11: b 8 11 ## 12: b 9 11 ## 13: b 10 11 1 2 # Add code: carry the first observation backwards DT [. ( 'b' , ( -2 ) : 10 ), roll = TRUE , rollends = TRUE ] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 ## A B C ## 1: b -2 8 ## 2: b -1 8 ## 3: b 0 8 ## 4: b 1 8 ## 5: b 2 8 ## 6: b 3 8 ## 7: b 4 8 ## 8: b 5 6 ## 9: b 6 6 ## 10: b 7 6 ## 11: b 8 11 ## 12: b 9 11 ## 13: b 10 11 Data Manipulation in R with dplyr \u00b6 1, Introduction to dplyr \u00b6 Load the dplyr and hflights package 1 2 3 4 5 6 7 8 9 10 # Load the dplyr package library ( dplyr ) library ( dtplyr ) # Load the hflights package # A data only package containing commercial domestic flights that departed Houston (IAH and HOU) in 2011 library ( hflights ) # Call both head() and summary() on hflights head ( hflights ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 ## Year Month DayofMonth DayOfWeek DepTime ArrTime UniqueCarrier ## 5424 2011 1 1 6 1400 1500 AA ## 5425 2011 1 2 7 1401 1501 AA ## 5426 2011 1 3 1 1352 1502 AA ## 5427 2011 1 4 2 1403 1513 AA ## 5428 2011 1 5 3 1405 1507 AA ## 5429 2011 1 6 4 1359 1503 AA ## FlightNum TailNum ActualElapsedTime AirTime ArrDelay DepDelay Origin ## 5424 428 N576AA 60 40 -10 0 IAH ## 5425 428 N557AA 60 45 -9 1 IAH ## 5426 428 N541AA 70 48 -8 -8 IAH ## 5427 428 N403AA 70 39 3 3 IAH ## 5428 428 N492AA 62 44 -3 5 IAH ## 5429 428 N262AA 64 45 -7 -1 IAH ## Dest Distance TaxiIn TaxiOut Cancelled CancellationCode Diverted ## 5424 DFW 224 7 13 0 0 ## 5425 DFW 224 6 9 0 0 ## 5426 DFW 224 5 17 0 0 ## 5427 DFW 224 9 22 0 0 ## 5428 DFW 224 9 9 0 0 ## 5429 DFW 224 6 13 0 0 1 summary ( hflights ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 ## Year Month DayofMonth DayOfWeek ## Min. :2011 Min. : 1.000 Min. : 1.00 Min. :1.000 ## 1st Qu.:2011 1st Qu.: 4.000 1st Qu.: 8.00 1st Qu.:2.000 ## Median :2011 Median : 7.000 Median :16.00 Median :4.000 ## Mean :2011 Mean : 6.514 Mean :15.74 Mean :3.948 ## 3rd Qu.:2011 3rd Qu.: 9.000 3rd Qu.:23.00 3rd Qu.:6.000 ## Max. :2011 Max. :12.000 Max. :31.00 Max. :7.000 ## ## DepTime ArrTime UniqueCarrier FlightNum ## Min. : 1 Min. : 1 Length:227496 Min. : 1 ## 1st Qu.:1021 1st Qu.:1215 Class :character 1st Qu.: 855 ## Median :1416 Median :1617 Mode :character Median :1696 ## Mean :1396 Mean :1578 Mean :1962 ## 3rd Qu.:1801 3rd Qu.:1953 3rd Qu.:2755 ## Max. :2400 Max. :2400 Max. :7290 ## NA's :2905 NA's :3066 ## TailNum ActualElapsedTime AirTime ArrDelay ## Length:227496 Min. : 34.0 Min. : 11.0 Min. :-70.000 ## Class :character 1st Qu.: 77.0 1st Qu.: 58.0 1st Qu.: -8.000 ## Mode :character Median :128.0 Median :107.0 Median : 0.000 ## Mean :129.3 Mean :108.1 Mean : 7.094 ## 3rd Qu.:165.0 3rd Qu.:141.0 3rd Qu.: 11.000 ## Max. :575.0 Max. :549.0 Max. :978.000 ## NA's :3622 NA's :3622 NA's :3622 ## DepDelay Origin Dest Distance ## Min. :-33.000 Length:227496 Length:227496 Min. : 79.0 ## 1st Qu.: -3.000 Class :character Class :character 1st Qu.: 376.0 ## Median : 0.000 Mode :character Mode :character Median : 809.0 ## Mean : 9.445 Mean : 787.8 ## 3rd Qu.: 9.000 3rd Qu.:1042.0 ## Max. :981.000 Max. :3904.0 ## NA's :2905 ## TaxiIn TaxiOut Cancelled CancellationCode ## Min. : 1.000 Min. : 1.00 Min. :0.00000 Length:227496 ## 1st Qu.: 4.000 1st Qu.: 10.00 1st Qu.:0.00000 Class :character ## Median : 5.000 Median : 14.00 Median :0.00000 Mode :character ## Mean : 6.099 Mean : 15.09 Mean :0.01307 ## 3rd Qu.: 7.000 3rd Qu.: 18.00 3rd Qu.:0.00000 ## Max. :165.000 Max. :163.00 Max. :1.00000 ## NA's :3066 NA's :2947 ## Diverted ## Min. :0.000000 ## 1st Qu.:0.000000 ## Median :0.000000 ## Mean :0.002853 ## 3rd Qu.:0.000000 ## Max. :1.000000 ## Convert data.frame to table 1 2 3 4 5 # Convert the hflights data.frame into a hflights tbl hflights <- tbl_df ( hflights ) # Display the hflights tbl hflights 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 ## # A tibble : 227 , 496 \u00d7 21 ## Year Month DayofMonth DayOfWeek DepTime ArrTime UniqueCarrier ## * & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; chr & gt ; ## 1 2011 1 1 6 1400 1500 AA ## 2 2011 1 2 7 1401 1501 AA ## 3 2011 1 3 1 1352 1502 AA ## 4 2011 1 4 2 1403 1513 AA ## 5 2011 1 5 3 1405 1507 AA ## 6 2011 1 6 4 1359 1503 AA ## 7 2011 1 7 5 1359 1509 AA ## 8 2011 1 8 6 1355 1454 AA ## 9 2011 1 9 7 1443 1554 AA ## 10 2011 1 10 1 1443 1553 AA ## # ... with 227 , 486 more rows , and 14 more variables : FlightNum & lt ; int & gt ;, ## # TailNum & lt ; chr & gt ;, ActualElapsedTime & lt ; int & gt ;, AirTime & lt ; int & gt ;, ArrDelay & lt ; int & gt ;, ## # DepDelay & lt ; int & gt ;, Origin & lt ; chr & gt ;, Dest & lt ; chr & gt ;, Distance & lt ; int & gt ;, ## # TaxiIn & lt ; int & gt ;, TaxiOut & lt ; int & gt ;, Cancelled & lt ; int & gt ;, CancellationCode & lt ; chr & gt ;, ## # Diverted & lt ; int & gt ; 1 2 # Create the object carriers, containing only the UniqueCarrier variable of hflights carriers <- hflights $ UniqueCarrier Changing labels of hflights , part 1 of 2 1 2 3 4 5 6 7 8 9 10 11 # add lut <- c ( 'AA' = 'American' , 'AS' = 'Alaska' , 'B6' = 'JetBlue' , 'CO' = 'Continental' , 'DL' = 'Delta' , 'OO' = 'SkyWest' , 'UA' = 'United' , 'US' = 'US_Airways' , 'WN' = 'Southwest' , 'EV' = 'Atlantic_Southeast' , 'F9' = 'Frontier' , 'FL' = 'AirTran' , 'MQ' = 'American_Eagle' , 'XE' = 'ExpressJet' , 'YV' = 'Mesa' ) # Use lut to translate the UniqueCarrier column of hflights hflights $ UniqueCarrier <- lut[hflights $ UniqueCarrier] # Inspect the resulting raw values of your variables glimpse ( hflights ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 ## Observations: 227,496 ## Variables: 21 ## $ Year &lt;int&gt; 2011, 2011, 2011, 2011, 2011, 2011, 2011, 20... ## $ Month &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,... ## $ DayofMonth &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 1... ## $ DayOfWeek &lt;int&gt; 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6,... ## $ DepTime &lt;int&gt; 1400, 1401, 1352, 1403, 1405, 1359, 1359, 13... ## $ ArrTime &lt;int&gt; 1500, 1501, 1502, 1513, 1507, 1503, 1509, 14... ## $ UniqueCarrier &lt;chr&gt; \"American\", \"American\", \"American\", \"America... ## $ FlightNum &lt;int&gt; 428, 428, 428, 428, 428, 428, 428, 428, 428,... ## $ TailNum &lt;chr&gt; \"N576AA\", \"N557AA\", \"N541AA\", \"N403AA\", \"N49... ## $ ActualElapsedTime &lt;int&gt; 60, 60, 70, 70, 62, 64, 70, 59, 71, 70, 70, ... ## $ AirTime &lt;int&gt; 40, 45, 48, 39, 44, 45, 43, 40, 41, 45, 42, ... ## $ ArrDelay &lt;int&gt; -10, -9, -8, 3, -3, -7, -1, -16, 44, 43, 29,... ## $ DepDelay &lt;int&gt; 0, 1, -8, 3, 5, -1, -1, -5, 43, 43, 29, 19, ... ## $ Origin &lt;chr&gt; \"IAH\", \"IAH\", \"IAH\", \"IAH\", \"IAH\", \"IAH\", \"I... ## $ Dest &lt;chr&gt; \"DFW\", \"DFW\", \"DFW\", \"DFW\", \"DFW\", \"DFW\", \"D... ## $ Distance &lt;int&gt; 224, 224, 224, 224, 224, 224, 224, 224, 224,... ## $ TaxiIn &lt;int&gt; 7, 6, 5, 9, 9, 6, 12, 7, 8, 6, 8, 4, 6, 5, 6... ## $ TaxiOut &lt;int&gt; 13, 9, 17, 22, 9, 13, 15, 12, 22, 19, 20, 11... ## $ Cancelled &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,... ## $ CancellationCode &lt;chr&gt; \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", ... ## $ Diverted &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,... Changing labels of hflights , part 2 of 2 1 2 3 4 5 6 7 8 # Build the lookup table: lut lut <- c ( \"A\" = \"carrier\" , \"B\" = \"weather\" , \"C\" = \"FFA\" , \"D\" = \"security\" , \"E\" = \"not cancelled\" ) # Add the Code column hflights $ Code <- lut[hflights $ CancellationCode] # Glimpse at hflights glimpse ( hflights ) Result. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 Observations: 227,496 Variables: 22 $ Year <int> 2011, 2011, 2011, 2011, 2011, 2011,... $ Month <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,... $ DayofMonth <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, ... $ DayOfWeek <int> 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3,... $ DepTime <int> 1400, 1401, 1352, 1403, 1405, 1359,... $ ArrTime <int> 1500, 1501, 1502, 1513, 1507, 1503,... $ UniqueCarrier <chr> \"American\", \"American\", \"American\",... $ FlightNum <int> 428, 428, 428, 428, 428, 428, 428, ... $ TailNum <chr> \"N576AA\", \"N557AA\", \"N541AA\", \"N403... $ ActualElapsedTime <int> 60, 60, 70, 70, 62, 64, 70, 59, 71,... $ AirTime <int> 40, 45, 48, 39, 44, 45, 43, 40, 41,... $ ArrDelay <int> -10, -9, -8, 3, -3, -7, -1, -16, 44... $ DepDelay <int> 0, 1, -8, 3, 5, -1, -1, -5, 43, 43,... $ Origin <chr> \"IAH\", \"IAH\", \"IAH\", \"IAH\", \"IAH\", ... $ Dest <chr> \"DFW\", \"DFW\", \"DFW\", \"DFW\", \"DFW\", ... $ Distance <int> 224, 224, 224, 224, 224, 224, 224, ... $ TaxiIn <int> 7, 6, 5, 9, 9, 6, 12, 7, 8, 6, 8, 4... $ TaxiOut <int> 13, 9, 17, 22, 9, 13, 15, 12, 22, 1... $ Cancelled <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,... $ CancellationCode <chr> \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\",... $ Diverted <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,... $ Code <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA,... 2, select and mutate \u00b6 The five verbs and their meaning select ; which returns a subset of the columns. filter ; that is able to return a subset of the rows. arrange ; that reorders the rows according to single or multiple variables. mutate ; used to add columns from existing data. summarise ; which reduces each group to a single row by calculating aggregate measures. Choosing is not losing! The select verb 1 2 # Print out a tbl with the four columns of hflights related to delay select ( hflights , ActualElapsedTime , AirTime , ArrDelay , DepDelay ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 ## # A tibble: 227,496 \u00d7 4 ## ActualElapsedTime AirTime ArrDelay DepDelay ## * &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 60 40 -10 0 ## 2 60 45 -9 1 ## 3 70 48 -8 -8 ## 4 70 39 3 3 ## 5 62 44 -3 5 ## 6 64 45 -7 -1 ## 7 70 43 -1 -1 ## 8 59 40 -16 -5 ## 9 71 41 44 43 ## 10 70 45 43 43 ## # ... with 227,486 more rows 1 2 # Print out hflights, nothing has changed! hflights 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 ## # A tibble : 227 , 496 \u00d7 21 ## Year Month DayofMonth DayOfWeek DepTime ArrTime UniqueCarrier ## * & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; chr & gt ; ## 1 2011 1 1 6 1400 1500 American ## 2 2011 1 2 7 1401 1501 American ## 3 2011 1 3 1 1352 1502 American ## 4 2011 1 4 2 1403 1513 American ## 5 2011 1 5 3 1405 1507 American ## 6 2011 1 6 4 1359 1503 American ## 7 2011 1 7 5 1359 1509 American ## 8 2011 1 8 6 1355 1454 American ## 9 2011 1 9 7 1443 1554 American ## 10 2011 1 10 1 1443 1553 American ## # ... with 227 , 486 more rows , and 14 more variables : FlightNum & lt ; int & gt ;, ## # TailNum & lt ; chr & gt ;, ActualElapsedTime & lt ; int & gt ;, AirTime & lt ; int & gt ;, ArrDelay & lt ; int & gt ;, ## # DepDelay & lt ; int & gt ;, Origin & lt ; chr & gt ;, Dest & lt ; chr & gt ;, Distance & lt ; int & gt ;, ## # TaxiIn & lt ; int & gt ;, TaxiOut & lt ; int & gt ;, Cancelled & lt ; int & gt ;, CancellationCode & lt ; chr & gt ;, ## # Diverted & lt ; int & gt ; 1 2 # Print out the columns Origin up to Cancelled of hflights select ( hflights , 14 : 19 ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 ## # A tibble: 227,496 \u00d7 6 ## Origin Dest Distance TaxiIn TaxiOut Cancelled ## * &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 IAH DFW 224 7 13 0 ## 2 IAH DFW 224 6 9 0 ## 3 IAH DFW 224 5 17 0 ## 4 IAH DFW 224 9 22 0 ## 5 IAH DFW 224 9 9 0 ## 6 IAH DFW 224 6 13 0 ## 7 IAH DFW 224 12 15 0 ## 8 IAH DFW 224 7 12 0 ## 9 IAH DFW 224 8 22 0 ## 10 IAH DFW 224 6 19 0 ## # ... with 227,486 more rows 1 2 # Answer to last question: be concise! select ( hflights , 1 : 4 , 12 : 21 ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 ## # A tibble : 227 , 496 \u00d7 14 ## Year Month DayofMonth DayOfWeek ArrDelay DepDelay Origin Dest ## * & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; chr & gt ; & lt ; chr & gt ; ## 1 2011 1 1 6 -10 0 IAH DFW ## 2 2011 1 2 7 -9 1 IAH DFW ## 3 2011 1 3 1 -8 -8 IAH DFW ## 4 2011 1 4 2 3 3 IAH DFW ## 5 2011 1 5 3 -3 5 IAH DFW ## 6 2011 1 6 4 -7 -1 IAH DFW ## 7 2011 1 7 5 -1 -1 IAH DFW ## 8 2011 1 8 6 -16 -5 IAH DFW ## 9 2011 1 9 7 44 43 IAH DFW ## 10 2011 1 10 1 43 43 IAH DFW ## # ... with 227 , 486 more rows , and 6 more variables : Distance & lt ; int & gt ;, ## # TaxiIn & lt ; int & gt ;, TaxiOut & lt ; int & gt ;, Cancelled & lt ; int & gt ;, CancellationCode & lt ; chr & gt ;, ## # Diverted & lt ; int & gt ; Helper functions for variable selection select : starts_with(\"X\") ; every name that starts with \"X\" , ends_with(\"X\") ; every name that ends with \"X\" , contains(\"X\") ; every name that contains \"X\" , matches(\"X\") ; every name that matches \"X\" , where \"X\" can be a regular expression, num_range(\"x\", 1:5) ; the variables named x01 , x02 , x03 , - x04 and x05 , one_of(x) ; every name that appears in x , which should be a character vector. 1 2 # Print out a tbl containing just ArrDelay and DepDelay select ( hflights , ArrDelay , DepDelay ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 ## # A tibble: 227,496 \u00d7 2 ## ArrDelay DepDelay ## * &lt;int&gt; &lt;int&gt; ## 1 -10 0 ## 2 -9 1 ## 3 -8 -8 ## 4 3 3 ## 5 -3 5 ## 6 -7 -1 ## 7 -1 -1 ## 8 -16 -5 ## 9 44 43 ## 10 43 43 ## # ... with 227,486 more rows 1 2 # Print out a tbl as described in the second instruction, using both helper functions and variable names select ( hflights , UniqueCarrier , ends_with ( 'Num' ), starts_with ( 'Cancel' )) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 ## # A tibble: 227,496 \u00d7 5 ## UniqueCarrier FlightNum TailNum Cancelled CancellationCode ## * &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; ## 1 American 428 N576AA 0 ## 2 American 428 N557AA 0 ## 3 American 428 N541AA 0 ## 4 American 428 N403AA 0 ## 5 American 428 N492AA 0 ## 6 American 428 N262AA 0 ## 7 American 428 N493AA 0 ## 8 American 428 N477AA 0 ## 9 American 428 N476AA 0 ## 10 American 428 N504AA 0 ## # ... with 227,486 more rows 1 2 # Print out a tbl as described in the third instruction, using only helper functions. select ( hflights , ends_with ( 'Time' ), ends_with ( 'Delay' )) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 ## # A tibble: 227,496 \u00d7 6 ## DepTime ArrTime ActualElapsedTime AirTime ArrDelay DepDelay ## * &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 1400 1500 60 40 -10 0 ## 2 1401 1501 60 45 -9 1 ## 3 1352 1502 70 48 -8 -8 ## 4 1403 1513 70 39 3 3 ## 5 1405 1507 62 44 -3 5 ## 6 1359 1503 64 45 -7 -1 ## 7 1359 1509 70 43 -1 -1 ## 8 1355 1454 59 40 -16 -5 ## 9 1443 1554 71 41 44 43 ## 10 1443 1553 70 45 43 43 ## # ... with 227,486 more rows Comparison to basic R 1 2 3 4 5 6 7 8 9 10 11 12 # add ex1r <- hflights [c ( 'TaxiIn' , 'TaxiOut' , 'Distance' ) ] ex1d <- select ( hflights , starts_with ( 'Taxi' ), Distance ) ex2r <- hflights [c ( 'Year' , 'Month' , 'DayOfWeek' , 'DepTime' , 'ArrTime' ) ] ex2d <- select ( hflights , Year , Month , DayOfWeek , DepTime , ArrTime ) ex3r <- hflights [c ( 'TailNum' , 'TaxiIn' , 'TaxiOut' ) ] ex3d <- select ( hflights , TailNum , starts_with ( 'Taxi' )) Mutating is creating 1 2 3 # Add the new variable ActualGroundTime to a copy of hflights and save the result as g1 g1 <- mutate ( hflights , ActualGroundTime = ActualElapsedTime - AirTime ) glimpse ( hflights ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 ## Observations: 227,496 ## Variables: 21 ## $ Year &lt;int&gt; 2011, 2011, 2011, 2011, 2011, 2011, 2011, 20... ## $ Month &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,... ## $ DayofMonth &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 1... ## $ DayOfWeek &lt;int&gt; 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6,... ## $ DepTime &lt;int&gt; 1400, 1401, 1352, 1403, 1405, 1359, 1359, 13... ## $ ArrTime &lt;int&gt; 1500, 1501, 1502, 1513, 1507, 1503, 1509, 14... ## $ UniqueCarrier &lt;chr&gt; \"American\", \"American\", \"American\", \"America... ## $ FlightNum &lt;int&gt; 428, 428, 428, 428, 428, 428, 428, 428, 428,... ## $ TailNum &lt;chr&gt; \"N576AA\", \"N557AA\", \"N541AA\", \"N403AA\", \"N49... ## $ ActualElapsedTime &lt;int&gt; 60, 60, 70, 70, 62, 64, 70, 59, 71, 70, 70, ... ## $ AirTime &lt;int&gt; 40, 45, 48, 39, 44, 45, 43, 40, 41, 45, 42, ... ## $ ArrDelay &lt;int&gt; -10, -9, -8, 3, -3, -7, -1, -16, 44, 43, 29,... ## $ DepDelay &lt;int&gt; 0, 1, -8, 3, 5, -1, -1, -5, 43, 43, 29, 19, ... ## $ Origin &lt;chr&gt; \"IAH\", \"IAH\", \"IAH\", \"IAH\", \"IAH\", \"IAH\", \"I... ## $ Dest &lt;chr&gt; \"DFW\", \"DFW\", \"DFW\", \"DFW\", \"DFW\", \"DFW\", \"D... ## $ Distance &lt;int&gt; 224, 224, 224, 224, 224, 224, 224, 224, 224,... ## $ TaxiIn &lt;int&gt; 7, 6, 5, 9, 9, 6, 12, 7, 8, 6, 8, 4, 6, 5, 6... ## $ TaxiOut &lt;int&gt; 13, 9, 17, 22, 9, 13, 15, 12, 22, 19, 20, 11... ## $ Cancelled &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,... ## $ CancellationCode &lt;chr&gt; \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", ... ## $ Diverted &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,... 1 glimpse ( g1 ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 ## Observations: 227,496 ## Variables: 22 ## $ Year &lt;int&gt; 2011, 2011, 2011, 2011, 2011, 2011, 2011, 20... ## $ Month &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,... ## $ DayofMonth &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 1... ## $ DayOfWeek &lt;int&gt; 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6,... ## $ DepTime &lt;int&gt; 1400, 1401, 1352, 1403, 1405, 1359, 1359, 13... ## $ ArrTime &lt;int&gt; 1500, 1501, 1502, 1513, 1507, 1503, 1509, 14... ## $ UniqueCarrier &lt;chr&gt; \"American\", \"American\", \"American\", \"America... ## $ FlightNum &lt;int&gt; 428, 428, 428, 428, 428, 428, 428, 428, 428,... ## $ TailNum &lt;chr&gt; \"N576AA\", \"N557AA\", \"N541AA\", \"N403AA\", \"N49... ## $ ActualElapsedTime &lt;int&gt; 60, 60, 70, 70, 62, 64, 70, 59, 71, 70, 70, ... ## $ AirTime &lt;int&gt; 40, 45, 48, 39, 44, 45, 43, 40, 41, 45, 42, ... ## $ ArrDelay &lt;int&gt; -10, -9, -8, 3, -3, -7, -1, -16, 44, 43, 29,... ## $ DepDelay &lt;int&gt; 0, 1, -8, 3, 5, -1, -1, -5, 43, 43, 29, 19, ... ## $ Origin &lt;chr&gt; \"IAH\", \"IAH\", \"IAH\", \"IAH\", \"IAH\", \"IAH\", \"I... ## $ Dest &lt;chr&gt; \"DFW\", \"DFW\", \"DFW\", \"DFW\", \"DFW\", \"DFW\", \"D... ## $ Distance &lt;int&gt; 224, 224, 224, 224, 224, 224, 224, 224, 224,... ## $ TaxiIn &lt;int&gt; 7, 6, 5, 9, 9, 6, 12, 7, 8, 6, 8, 4, 6, 5, 6... ## $ TaxiOut &lt;int&gt; 13, 9, 17, 22, 9, 13, 15, 12, 22, 19, 20, 11... ## $ Cancelled &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,... ## $ CancellationCode &lt;chr&gt; \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", ... ## $ Diverted &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,... ## $ ActualGroundTime &lt;int&gt; 20, 15, 22, 31, 18, 19, 27, 19, 30, 25, 28, ... 1 2 3 4 # Add the new variable GroundTime to a g1; save the result as g2 g2 <- mutate ( g1 , GroundTime = TaxiIn + TaxiOut ) head ( g1 $ ActualGroundTime == g2 $ GroundTime , 20 ) 1 2 ## [1] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE ## [15] TRUE TRUE TRUE TRUE TRUE TRUE 1 2 3 # Add the new variable AverageSpeed to g2; save the result as g3 g3 <- mutate ( g2 , AverageSpeed = Distance / AirTime * 60 ) g3 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 ## # A tibble : 227 , 496 \u00d7 24 ## Year Month DayofMonth DayOfWeek DepTime ArrTime UniqueCarrier ## & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; chr & gt ; ## 1 2011 1 1 6 1400 1500 American ## 2 2011 1 2 7 1401 1501 American ## 3 2011 1 3 1 1352 1502 American ## 4 2011 1 4 2 1403 1513 American ## 5 2011 1 5 3 1405 1507 American ## 6 2011 1 6 4 1359 1503 American ## 7 2011 1 7 5 1359 1509 American ## 8 2011 1 8 6 1355 1454 American ## 9 2011 1 9 7 1443 1554 American ## 10 2011 1 10 1 1443 1553 American ## # ... with 227 , 486 more rows , and 17 more variables : FlightNum & lt ; int & gt ;, ## # TailNum & lt ; chr & gt ;, ActualElapsedTime & lt ; int & gt ;, AirTime & lt ; int & gt ;, ArrDelay & lt ; int & gt ;, ## # DepDelay & lt ; int & gt ;, Origin & lt ; chr & gt ;, Dest & lt ; chr & gt ;, Distance & lt ; int & gt ;, ## # TaxiIn & lt ; int & gt ;, TaxiOut & lt ; int & gt ;, Cancelled & lt ; int & gt ;, CancellationCode & lt ; chr & gt ;, ## # Diverted & lt ; int & gt ;, ActualGroundTime & lt ; int & gt ;, GroundTime & lt ; int & gt ;, ## # AverageSpeed & lt ; dbl & gt ; Add multiple variables using mutate 1 2 3 4 5 6 7 8 9 # Add a second variable loss_percent to the dataset: m1 m1 <- mutate ( hflights , loss = ArrDelay - DepDelay , loss_percent = ( ArrDelay - DepDelay ) / DepDelay * 100 ) # Copy and adapt the previous command to reduce redendancy: m2 m2 <- mutate ( hflights , loss = ArrDelay - DepDelay , loss_percent = loss / DepDelay * 100 ) # Add the three variables as described in the third instruction: m3 m3 <- mutate ( hflights , TotalTaxi = TaxiIn + TaxiOut , ActualGroundTime = ActualElapsedTime - AirTime , Diff = TotalTaxi - ActualGroundTime ) glimpse ( m3 ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 ## Observations: 227,496 ## Variables: 24 ## $ Year &lt;int&gt; 2011, 2011, 2011, 2011, 2011, 2011, 2011, 20... ## $ Month &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,... ## $ DayofMonth &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 1... ## $ DayOfWeek &lt;int&gt; 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6,... ## $ DepTime &lt;int&gt; 1400, 1401, 1352, 1403, 1405, 1359, 1359, 13... ## $ ArrTime &lt;int&gt; 1500, 1501, 1502, 1513, 1507, 1503, 1509, 14... ## $ UniqueCarrier &lt;chr&gt; \"American\", \"American\", \"American\", \"America... ## $ FlightNum &lt;int&gt; 428, 428, 428, 428, 428, 428, 428, 428, 428,... ## $ TailNum &lt;chr&gt; \"N576AA\", \"N557AA\", \"N541AA\", \"N403AA\", \"N49... ## $ ActualElapsedTime &lt;int&gt; 60, 60, 70, 70, 62, 64, 70, 59, 71, 70, 70, ... ## $ AirTime &lt;int&gt; 40, 45, 48, 39, 44, 45, 43, 40, 41, 45, 42, ... ## $ ArrDelay &lt;int&gt; -10, -9, -8, 3, -3, -7, -1, -16, 44, 43, 29,... ## $ DepDelay &lt;int&gt; 0, 1, -8, 3, 5, -1, -1, -5, 43, 43, 29, 19, ... ## $ Origin &lt;chr&gt; \"IAH\", \"IAH\", \"IAH\", \"IAH\", \"IAH\", \"IAH\", \"I... ## $ Dest &lt;chr&gt; \"DFW\", \"DFW\", \"DFW\", \"DFW\", \"DFW\", \"DFW\", \"D... ## $ Distance &lt;int&gt; 224, 224, 224, 224, 224, 224, 224, 224, 224,... ## $ TaxiIn &lt;int&gt; 7, 6, 5, 9, 9, 6, 12, 7, 8, 6, 8, 4, 6, 5, 6... ## $ TaxiOut &lt;int&gt; 13, 9, 17, 22, 9, 13, 15, 12, 22, 19, 20, 11... ## $ Cancelled &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,... ## $ CancellationCode &lt;chr&gt; \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", ... ## $ Diverted &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,... ## $ TotalTaxi &lt;int&gt; 20, 15, 22, 31, 18, 19, 27, 19, 30, 25, 28, ... ## $ ActualGroundTime &lt;int&gt; 20, 15, 22, 31, 18, 19, 27, 19, 30, 25, 28, ... ## $ Diff &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,... 3, filter and arrange \u00b6 Logical operators filter : x < y ; TRUE if x is less than y . x <= y ; TRUE if x is less than or equal to y . x == y ; TRUE if x equals y . x != y ; TRUE if x does not equal y . x >= y ; TRUE if x is greater than or equal to y . x > y ; TRUE if x is greater than y . x %in% c(a, b, c) ; TRUE if x is in the vector c(a, b, c) . 1 2 # All flights that traveled 3000 miles or more filter ( hflights , Distance >= 3000 ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 ## # A tibble : 527 \u00d7 21 ## Year Month DayofMonth DayOfWeek DepTime ArrTime UniqueCarrier ## & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; chr & gt ; ## 1 2011 1 31 1 924 1413 Continental ## 2 2011 1 30 7 925 1410 Continental ## 3 2011 1 29 6 1045 1445 Continental ## 4 2011 1 28 5 1516 1916 Continental ## 5 2011 1 27 4 950 1344 Continental ## 6 2011 1 26 3 944 1350 Continental ## 7 2011 1 25 2 924 1337 Continental ## 8 2011 1 24 1 1144 1605 Continental ## 9 2011 1 23 7 926 1335 Continental ## 10 2011 1 22 6 942 1340 Continental ## # ... with 517 more rows , and 14 more variables : FlightNum & lt ; int & gt ;, ## # TailNum & lt ; chr & gt ;, ActualElapsedTime & lt ; int & gt ;, AirTime & lt ; int & gt ;, ArrDelay & lt ; int & gt ;, ## # DepDelay & lt ; int & gt ;, Origin & lt ; chr & gt ;, Dest & lt ; chr & gt ;, Distance & lt ; int & gt ;, ## # TaxiIn & lt ; int & gt ;, TaxiOut & lt ; int & gt ;, Cancelled & lt ; int & gt ;, CancellationCode & lt ; chr & gt ;, ## # Diverted & lt ; int & gt ; 1 2 # All flights flown by one of JetBlue, Southwest, or Delta filter ( hflights , UniqueCarrier %in% c ( 'JetBlue' , 'Southwest' , 'Delta' )) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 ## # A tibble : 48 , 679 \u00d7 21 ## Year Month DayofMonth DayOfWeek DepTime ArrTime UniqueCarrier ## & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; chr & gt ; ## 1 2011 1 1 6 654 1124 JetBlue ## 2 2011 1 1 6 1639 2110 JetBlue ## 3 2011 1 2 7 703 1113 JetBlue ## 4 2011 1 2 7 1604 2040 JetBlue ## 5 2011 1 3 1 659 1100 JetBlue ## 6 2011 1 3 1 1801 2200 JetBlue ## 7 2011 1 4 2 654 1103 JetBlue ## 8 2011 1 4 2 1608 2034 JetBlue ## 9 2011 1 5 3 700 1103 JetBlue ## 10 2011 1 5 3 1544 1954 JetBlue ## # ... with 48 , 669 more rows , and 14 more variables : FlightNum & lt ; int & gt ;, ## # TailNum & lt ; chr & gt ;, ActualElapsedTime & lt ; int & gt ;, AirTime & lt ; int & gt ;, ArrDelay & lt ; int & gt ;, ## # DepDelay & lt ; int & gt ;, Origin & lt ; chr & gt ;, Dest & lt ; chr & gt ;, Distance & lt ; int & gt ;, ## # TaxiIn & lt ; int & gt ;, TaxiOut & lt ; int & gt ;, Cancelled & lt ; int & gt ;, CancellationCode & lt ; chr & gt ;, ## # Diverted & lt ; int & gt ; 1 2 # All flights where taxiing took longer than flying filter ( hflights , ( TaxiIn + TaxiOut ) > AirTime ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 ## # A tibble : 1 , 389 \u00d7 21 ## Year Month DayofMonth DayOfWeek DepTime ArrTime UniqueCarrier ## & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; chr & gt ; ## 1 2011 1 24 1 731 904 American ## 2 2011 1 30 7 1959 2132 American ## 3 2011 1 24 1 1621 1749 American ## 4 2011 1 10 1 941 1113 American ## 5 2011 1 31 1 1301 1356 Continental ## 6 2011 1 31 1 2113 2215 Continental ## 7 2011 1 31 1 1434 1539 Continental ## 8 2011 1 31 1 900 1006 Continental ## 9 2011 1 30 7 1304 1408 Continental ## 10 2011 1 30 7 2004 2128 Continental ## # ... with 1 , 379 more rows , and 14 more variables : FlightNum & lt ; int & gt ;, ## # TailNum & lt ; chr & gt ;, ActualElapsedTime & lt ; int & gt ;, AirTime & lt ; int & gt ;, ArrDelay & lt ; int & gt ;, ## # DepDelay & lt ; int & gt ;, Origin & lt ; chr & gt ;, Dest & lt ; chr & gt ;, Distance & lt ; int & gt ;, ## # TaxiIn & lt ; int & gt ;, TaxiOut & lt ; int & gt ;, Cancelled & lt ; int & gt ;, CancellationCode & lt ; chr & gt ;, ## # Diverted & lt ; int & gt ; Combining tests using boolean operators 1 2 # All flights that departed before 5am or arrived after 10pm filter ( hflights , DepTime < 500 | ArrTime > 2200 ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 ## # A tibble : 27 , 799 \u00d7 21 ## Year Month DayofMonth DayOfWeek DepTime ArrTime UniqueCarrier ## & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; chr & gt ; ## 1 2011 1 4 2 2100 2207 American ## 2 2011 1 14 5 2119 2229 American ## 3 2011 1 10 1 1934 2235 American ## 4 2011 1 26 3 1905 2211 American ## 5 2011 1 30 7 1856 2209 American ## 6 2011 1 9 7 1938 2228 Alaska ## 7 2011 1 31 1 1919 2231 Continental ## 8 2011 1 31 1 2116 2344 Continental ## 9 2011 1 31 1 1850 2211 Continental ## 10 2011 1 31 1 2102 2216 Continental ## # ... with 27 , 789 more rows , and 14 more variables : FlightNum & lt ; int & gt ;, ## # TailNum & lt ; chr & gt ;, ActualElapsedTime & lt ; int & gt ;, AirTime & lt ; int & gt ;, ArrDelay & lt ; int & gt ;, ## # DepDelay & lt ; int & gt ;, Origin & lt ; chr & gt ;, Dest & lt ; chr & gt ;, Distance & lt ; int & gt ;, ## # TaxiIn & lt ; int & gt ;, TaxiOut & lt ; int & gt ;, Cancelled & lt ; int & gt ;, CancellationCode & lt ; chr & gt ;, ## # Diverted & lt ; int & gt ; 1 2 # All flights that departed late but arrived ahead of schedule filter ( hflights , DepDelay > 0 & ArrDelay < 0 ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 ## # A tibble : 27 , 712 \u00d7 21 ## Year Month DayofMonth DayOfWeek DepTime ArrTime UniqueCarrier ## & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; chr & gt ; ## 1 2011 1 2 7 1401 1501 American ## 2 2011 1 5 3 1405 1507 American ## 3 2011 1 18 2 1408 1508 American ## 4 2011 1 18 2 721 827 American ## 5 2011 1 12 3 2015 2113 American ## 6 2011 1 13 4 2020 2116 American ## 7 2011 1 26 3 2009 2103 American ## 8 2011 1 1 6 1631 1736 American ## 9 2011 1 10 1 1639 1740 American ## 10 2011 1 12 3 1631 1739 American ## # ... with 27 , 702 more rows , and 14 more variables : FlightNum & lt ; int & gt ;, ## # TailNum & lt ; chr & gt ;, ActualElapsedTime & lt ; int & gt ;, AirTime & lt ; int & gt ;, ArrDelay & lt ; int & gt ;, ## # DepDelay & lt ; int & gt ;, Origin & lt ; chr & gt ;, Dest & lt ; chr & gt ;, Distance & lt ; int & gt ;, ## # TaxiIn & lt ; int & gt ;, TaxiOut & lt ; int & gt ;, Cancelled & lt ; int & gt ;, CancellationCode & lt ; chr & gt ;, ## # Diverted & lt ; int & gt ; 1 2 # All cancelled weekend flights filter ( hflights , DayOfWeek %in% c ( 6 , 7 ) & Cancelled == 1 ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 ## # A tibble : 585 \u00d7 21 ## Year Month DayofMonth DayOfWeek DepTime ArrTime UniqueCarrier ## & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; chr & gt ; ## 1 2011 1 9 7 NA NA American ## 2 2011 1 29 6 NA NA Continental ## 3 2011 1 9 7 NA NA Continental ## 4 2011 1 9 7 NA NA Delta ## 5 2011 1 9 7 NA NA SkyWest ## 6 2011 1 2 7 NA NA Southwest ## 7 2011 1 29 6 NA NA Delta ## 8 2011 1 9 7 NA NA Atlantic_Southeast ## 9 2011 1 1 6 NA NA AirTran ## 10 2011 1 9 7 NA NA AirTran ## # ... with 575 more rows , and 14 more variables : FlightNum & lt ; int & gt ;, ## # TailNum & lt ; chr & gt ;, ActualElapsedTime & lt ; int & gt ;, AirTime & lt ; int & gt ;, ArrDelay & lt ; int & gt ;, ## # DepDelay & lt ; int & gt ;, Origin & lt ; chr & gt ;, Dest & lt ; chr & gt ;, Distance & lt ; int & gt ;, ## # TaxiIn & lt ; int & gt ;, TaxiOut & lt ; int & gt ;, Cancelled & lt ; int & gt ;, CancellationCode & lt ; chr & gt ;, ## # Diverted & lt ; int & gt ; 1 2 # All flights that were cancelled after being delayed filter ( hflights , DepDelay > 0 & Cancelled == 1 ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 ## # A tibble : 40 \u00d7 21 ## Year Month DayofMonth DayOfWeek DepTime ArrTime UniqueCarrier ## & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; chr & gt ; ## 1 2011 1 26 3 1926 NA Continental ## 2 2011 1 11 2 1100 NA US_Airways ## 3 2011 1 19 3 1811 NA ExpressJet ## 4 2011 1 7 5 2028 NA ExpressJet ## 5 2011 2 4 5 1638 NA American ## 6 2011 2 8 2 1057 NA Continental ## 7 2011 2 2 3 802 NA ExpressJet ## 8 2011 2 9 3 904 NA ExpressJet ## 9 2011 2 1 2 1508 NA SkyWest ## 10 2011 3 31 4 1016 NA Continental ## # ... with 30 more rows , and 14 more variables : FlightNum & lt ; int & gt ;, ## # TailNum & lt ; chr & gt ;, ActualElapsedTime & lt ; int & gt ;, AirTime & lt ; int & gt ;, ArrDelay & lt ; int & gt ;, ## # DepDelay & lt ; int & gt ;, Origin & lt ; chr & gt ;, Dest & lt ; chr & gt ;, Distance & lt ; int & gt ;, ## # TaxiIn & lt ; int & gt ;, TaxiOut & lt ; int & gt ;, Cancelled & lt ; int & gt ;, CancellationCode & lt ; chr & gt ;, ## # Diverted & lt ; int & gt ; Blend together what you\u2019ve learned! 1 2 3 4 5 6 7 8 # Select the flights that had JFK as their destination: c1 c1 <- filter ( hflights , Dest == 'JFK' ) # Combine the Year, Month and DayofMonth variables to create a Date column: c2 c2 <- mutate ( c1 , Date = paste ( Year , Month , DayofMonth , sep = '-' )) # Print out a selection of columns of c2 select ( c2 , Date , DepTime , ArrTime , TailNum ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 ## # A tibble: 695 \u00d7 4 ## Date DepTime ArrTime TailNum ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; ## 1 2011-1-1 654 1124 N324JB ## 2 2011-1-1 1639 2110 N324JB ## 3 2011-1-2 703 1113 N324JB ## 4 2011-1-2 1604 2040 N324JB ## 5 2011-1-3 659 1100 N229JB ## 6 2011-1-3 1801 2200 N206JB ## 7 2011-1-4 654 1103 N267JB ## 8 2011-1-4 1608 2034 N267JB ## 9 2011-1-5 700 1103 N708JB ## 10 2011-1-5 1544 1954 N644JB ## # ... with 685 more rows Arranging your data 1 2 3 4 5 # Definition of dtc dtc <- filter ( hflights , Cancelled == 1 , ! is.na ( DepDelay )) # Arrange dtc by departure delays arrange ( dtc , DepDelay ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 ## # A tibble : 68 \u00d7 21 ## Year Month DayofMonth DayOfWeek DepTime ArrTime UniqueCarrier ## & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; chr & gt ; ## 1 2011 7 23 6 605 NA Frontier ## 2 2011 1 17 1 916 NA ExpressJet ## 3 2011 12 1 4 541 NA US_Airways ## 4 2011 10 12 3 2022 NA American_Eagle ## 5 2011 7 29 5 1424 NA Continental ## 6 2011 9 29 4 1639 NA SkyWest ## 7 2011 2 9 3 555 NA American_Eagle ## 8 2011 5 9 1 715 NA SkyWest ## 9 2011 1 20 4 1413 NA United ## 10 2011 1 17 1 831 NA Southwest ## # ... with 58 more rows , and 14 more variables : FlightNum & lt ; int & gt ;, ## # TailNum & lt ; chr & gt ;, ActualElapsedTime & lt ; int & gt ;, AirTime & lt ; int & gt ;, ArrDelay & lt ; int & gt ;, ## # DepDelay & lt ; int & gt ;, Origin & lt ; chr & gt ;, Dest & lt ; chr & gt ;, Distance & lt ; int & gt ;, ## # TaxiIn & lt ; int & gt ;, TaxiOut & lt ; int & gt ;, Cancelled & lt ; int & gt ;, CancellationCode & lt ; chr & gt ;, ## # Diverted & lt ; int & gt ; 1 2 # Arrange dtc so that cancellation reasons are grouped arrange ( dtc , CancellationCode ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 ## # A tibble : 68 \u00d7 21 ## Year Month DayofMonth DayOfWeek DepTime ArrTime UniqueCarrier ## & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; chr & gt ; ## 1 2011 1 20 4 1413 NA United ## 2 2011 1 7 5 2028 NA ExpressJet ## 3 2011 2 4 5 1638 NA American ## 4 2011 2 8 2 1057 NA Continental ## 5 2011 2 1 2 1508 NA SkyWest ## 6 2011 2 21 1 2257 NA SkyWest ## 7 2011 2 9 3 555 NA American_Eagle ## 8 2011 3 18 5 727 NA United ## 9 2011 4 4 1 1632 NA Delta ## 10 2011 4 8 5 1608 NA Southwest ## # ... with 58 more rows , and 14 more variables : FlightNum & lt ; int & gt ;, ## # TailNum & lt ; chr & gt ;, ActualElapsedTime & lt ; int & gt ;, AirTime & lt ; int & gt ;, ArrDelay & lt ; int & gt ;, ## # DepDelay & lt ; int & gt ;, Origin & lt ; chr & gt ;, Dest & lt ; chr & gt ;, Distance & lt ; int & gt ;, ## # TaxiIn & lt ; int & gt ;, TaxiOut & lt ; int & gt ;, Cancelled & lt ; int & gt ;, CancellationCode & lt ; chr & gt ;, ## # Diverted & lt ; int & gt ; 1 2 # Arrange dtc according to carrier and departure delays arrange ( dtc , UniqueCarrier , DepDelay ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 ## # A tibble : 68 \u00d7 21 ## Year Month DayofMonth DayOfWeek DepTime ArrTime UniqueCarrier ## & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; chr & gt ; ## 1 2011 6 11 6 1649 NA AirTran ## 2 2011 8 18 4 1808 NA American ## 3 2011 2 4 5 1638 NA American ## 4 2011 10 12 3 2022 NA American_Eagle ## 5 2011 2 9 3 555 NA American_Eagle ## 6 2011 7 17 7 1917 NA American_Eagle ## 7 2011 4 30 6 612 NA Atlantic_Southeast ## 8 2011 4 10 7 1147 NA Atlantic_Southeast ## 9 2011 5 23 1 657 NA Atlantic_Southeast ## 10 2011 9 29 4 723 NA Atlantic_Southeast ## # ... with 58 more rows , and 14 more variables : FlightNum & lt ; int & gt ;, ## # TailNum & lt ; chr & gt ;, ActualElapsedTime & lt ; int & gt ;, AirTime & lt ; int & gt ;, ArrDelay & lt ; int & gt ;, ## # DepDelay & lt ; int & gt ;, Origin & lt ; chr & gt ;, Dest & lt ; chr & gt ;, Distance & lt ; int & gt ;, ## # TaxiIn & lt ; int & gt ;, TaxiOut & lt ; int & gt ;, Cancelled & lt ; int & gt ;, CancellationCode & lt ; chr & gt ;, ## # Diverted & lt ; int & gt ; Reverse the order of arranging 1 2 # Arrange according to carrier and decreasing departure delays arrange ( hflights , UniqueCarrier , desc ( DepDelay )) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 ## # A tibble : 227 , 496 \u00d7 21 ## Year Month DayofMonth DayOfWeek DepTime ArrTime UniqueCarrier ## & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; chr & gt ; ## 1 2011 2 19 6 1902 2143 AirTran ## 2 2011 3 14 1 2024 2309 AirTran ## 3 2011 2 16 3 2349 227 AirTran ## 4 2011 11 13 7 2312 213 AirTran ## 5 2011 5 26 4 2353 305 AirTran ## 6 2011 5 26 4 1922 2229 AirTran ## 7 2011 4 28 4 1045 1328 AirTran ## 8 2011 6 5 7 2207 52 AirTran ## 9 2011 5 7 6 1009 1256 AirTran ## 10 2011 7 25 1 2107 14 AirTran ## # ... with 227 , 486 more rows , and 14 more variables : FlightNum & lt ; int & gt ;, ## # TailNum & lt ; chr & gt ;, ActualElapsedTime & lt ; int & gt ;, AirTime & lt ; int & gt ;, ArrDelay & lt ; int & gt ;, ## # DepDelay & lt ; int & gt ;, Origin & lt ; chr & gt ;, Dest & lt ; chr & gt ;, Distance & lt ; int & gt ;, ## # TaxiIn & lt ; int & gt ;, TaxiOut & lt ; int & gt ;, Cancelled & lt ; int & gt ;, CancellationCode & lt ; chr & gt ;, ## # Diverted & lt ; int & gt ; 1 2 # Arrange flights by total delay (normal order). arrange ( hflights , ( ArrDelay + DepDelay )) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 ## # A tibble : 227 , 496 \u00d7 21 ## Year Month DayofMonth DayOfWeek DepTime ArrTime UniqueCarrier ## & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; chr & gt ; ## 1 2011 7 3 7 1914 2039 ExpressJet ## 2 2011 8 31 3 934 1039 SkyWest ## 3 2011 8 21 7 935 1039 SkyWest ## 4 2011 8 28 7 2059 2206 SkyWest ## 5 2011 8 29 1 935 1041 SkyWest ## 6 2011 12 25 7 741 926 SkyWest ## 7 2011 1 30 7 620 812 SkyWest ## 8 2011 8 3 3 1741 1810 ExpressJet ## 9 2011 8 4 4 930 1041 SkyWest ## 10 2011 8 18 4 939 1043 SkyWest ## # ... with 227 , 486 more rows , and 14 more variables : FlightNum & lt ; int & gt ;, ## # TailNum & lt ; chr & gt ;, ActualElapsedTime & lt ; int & gt ;, AirTime & lt ; int & gt ;, ArrDelay & lt ; int & gt ;, ## # DepDelay & lt ; int & gt ;, Origin & lt ; chr & gt ;, Dest & lt ; chr & gt ;, Distance & lt ; int & gt ;, ## # TaxiIn & lt ; int & gt ;, TaxiOut & lt ; int & gt ;, Cancelled & lt ; int & gt ;, CancellationCode & lt ; chr & gt ;, ## # Diverted & lt ; int & gt ; 1 2 # Keep flights leaving to DFW before 8am and arrange according to decreasing AirTime arrange ( filter ( hflights , Dest == 'DFW' & DepTime < 800 ), desc ( AirTime )) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 ## # A tibble : 799 \u00d7 21 ## Year Month DayofMonth DayOfWeek DepTime ArrTime UniqueCarrier ## & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; chr & gt ; ## 1 2011 11 22 2 635 825 American ## 2 2011 8 25 4 602 758 American_Eagle ## 3 2011 10 12 3 559 738 American_Eagle ## 4 2011 5 2 1 716 854 American ## 5 2011 4 4 1 741 949 American ## 6 2011 4 4 1 627 742 American_Eagle ## 7 2011 6 21 2 726 848 ExpressJet ## 8 2011 9 1 4 715 844 American ## 9 2011 3 14 1 729 917 Continental ## 10 2011 12 5 1 724 847 Continental ## # ... with 789 more rows , and 14 more variables : FlightNum & lt ; int & gt ;, ## # TailNum & lt ; chr & gt ;, ActualElapsedTime & lt ; int & gt ;, AirTime & lt ; int & gt ;, ArrDelay & lt ; int & gt ;, ## # DepDelay & lt ; int & gt ;, Origin & lt ; chr & gt ;, Dest & lt ; chr & gt ;, Distance & lt ; int & gt ;, ## # TaxiIn & lt ; int & gt ;, TaxiOut & lt ; int & gt ;, Cancelled & lt ; int & gt ;, CancellationCode & lt ; chr & gt ;, ## # Diverted & lt ; int & gt ; 4, summarise and the Pipe Operator \u00b6 The syntax of summarise 1 2 # Print out a summary with variables min_dist and max_dist summarise ( hflights , min_dist = min ( Distance ), max_dist = max ( Distance )) 1 2 3 4 ## # A tibble: 1 \u00d7 2 ## min_dist max_dist ## &lt;int&gt; &lt;int&gt; ## 1 79 3904 1 2 # Print out a summary with variable max_div summarise ( filter ( hflights , Diverted == 1 ), max_div = max ( Distance )) 1 2 3 4 ## # A tibble: 1 \u00d7 1 ## max_div ## &lt;int&gt; ## 1 3904 Aggregate functions summarise : min(x) ; minimum value of vector x. max(x) ; maximum value of vector x. mean(x) ; mean value of vector x. median(x) ; median value of vector x. quantile(x, p) ; pth quantile of vector x. sd(x) ; standard deviation of vector x. var(x) ; variance of vector x. IQR(x) ; Inter Quartile Range (IQR) of vector x. diff(range(x)) ; total range of vector x. 1 2 3 4 5 # Remove rows that have NA ArrDelay: temp1 temp1 <- filter ( hflights , ! is.na ( ArrDelay )) # Generate summary about ArrDelay column of temp1 summarise ( temp1 , earliest = min ( ArrDelay ), average = mean ( ArrDelay ), latest = max ( ArrDelay ), sd = sd ( ArrDelay )) 1 2 3 4 ## # A tibble: 1 \u00d7 4 ## earliest average latest sd ## &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 -70 7.094334 978 30.70852 1 2 3 4 5 # Keep rows that have no NA TaxiIn and no NA TaxiOut: temp2 temp2 <- filter ( hflights , ! is.na ( TaxiIn ), ! is.na ( TaxiOut )) # Print the maximum taxiing difference of temp2 with summarise() summarise ( temp2 , max_taxi_diff = max ( abs ( TaxiIn - TaxiOut ))) 1 2 3 4 ## # A tibble: 1 \u00d7 1 ## max_taxi_diff ## &lt;int&gt; ## 1 160 dplyr aggregate functions first(x) ; the first element of vector x . last(x) ; the last element of vector x . nth(x, n) ; The n th element of vector x . n() ; The number of rows in the data.frame or group of observations that summarise() describes. n_distinct(x) ; The number of unique values in vector x . 1 2 # Generate summarizing statistics for hflights summarise ( hflights , n_obs = n (), n_carrier = n_distinct ( UniqueCarrier ), n_dest = n_distinct ( Dest ), dest100 = nth ( Dest , 100 )) 1 2 3 4 ## # A tibble: 1 \u00d7 4 ## n_obs n_carrier n_dest dest100 ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; ## 1 227496 15 116 DFW 1 2 3 4 5 # Filter hflights to keep all American Airline flights: aa aa <- filter ( hflights , UniqueCarrier == 'American' ) # Generate summarizing statistics for aa summarise ( aa , n_flights = n (), n_canc = sum ( Cancelled ), p_canc = n_canc / n_flights * 100 , avg_delay = mean ( ArrDelay , na.rm = TRUE )) 1 2 3 4 ## # A tibble: 1 \u00d7 4 ## n_flights n_canc p_canc avg_delay ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 3244 60 1.849568 0.8917558 Overview of syntax 1 2 3 4 5 # Write the 'piped' version of the English sentences hflights %>% mutate ( diff = TaxiOut - TaxiIn ) %>% filter ( ! is.na ( diff )) %>% summarise ( avg = mean ( diff )) 1 2 3 4 ## # A tibble: 1 \u00d7 1 ## avg ## &lt;dbl&gt; ## 1 8.992064 Drive or fly? Part 1 of 2 1 2 3 4 5 6 7 8 9 10 11 12 # Part 1, concerning the selection and creation of columns d <- hflights %>% select ( Dest , UniqueCarrier , Distance , ActualElapsedTime ) %>% mutate ( RealTime = ActualElapsedTime + 100 , mph = Distance / RealTime * 60 ) # Part 2, concerning flights that had an actual average speed of < 70 mph. d %>% filter ( ! is.na ( mph ), mph < 70 ) %>% summarise ( n_less = n (), n_dest = n_distinct ( Dest ), min_dist = min ( Distance ), max_dist = max ( Distance )) 1 2 3 4 ## # A tibble: 1 \u00d7 4 ## n_less n_dest min_dist max_dist ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 6726 13 79 305 Drive or fly? Part 2 of 2 1 2 3 4 5 # Solve the exercise using a combination of dplyr verbs and %>% hflights %>% #summarise(all_flights = n()) %>% filter ((( Distance / ( ActualElapsedTime + 100 ) * 60 ) < 105 ) | Cancelled == 1 | Diverted == 1 ) %>% summarise ( n_non = n (), p_non = n_non / 22751 * 100 , n_dest = n_distinct ( Dest ), min_dist = min ( Distance ), max_dist = max ( Distance )) 1 2 3 4 ## # A tibble: 1 \u00d7 5 ## n_non p_non n_dest min_dist max_dist ## &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 42400 186.3654 113 79 3904 Advanced piping exercise 1 2 3 4 # Count the number of overnight flights hflights %>% filter ( ArrTime < DepTime & ! is.na ( DepTime ) & ! is.na ( ArrTime )) %>% summarise ( n = n ()) 1 2 3 4 ## # A tibble: 1 \u00d7 1 ## n ## &lt;int&gt; ## 1 2718 5, group_by and working with data \u00b6 Unite and conquer using group_by 1 2 3 4 5 6 7 8 # Make an ordered per-carrier summary of hflights hflights %>% group_by ( UniqueCarrier ) %>% summarise ( n_flights = n (), n_canc = sum ( Cancelled == 1 ), p_canc = mean ( Cancelled == 1 ) * 100 , avg_delay = mean ( ArrDelay , na.rm = TRUE )) %>% arrange ( avg_delay , p_canc ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 ## # A tibble: 15 \u00d7 5 ## UniqueCarrier n_flights n_canc p_canc avg_delay ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 US_Airways 4082 46 1.1268986 -0.6307692 ## 2 American 3244 60 1.8495684 0.8917558 ## 3 AirTran 2139 21 0.9817672 1.8536239 ## 4 Alaska 365 0 0.0000000 3.1923077 ## 5 Mesa 79 1 1.2658228 4.0128205 ## 6 Delta 2641 42 1.5903067 6.0841374 ## 7 Continental 70032 475 0.6782614 6.0986983 ## 8 American_Eagle 4648 135 2.9044750 7.1529751 ## 9 Atlantic_Southeast 2204 76 3.4482759 7.2569543 ## 10 Southwest 45343 703 1.5504047 7.5871430 ## 11 Frontier 838 6 0.7159905 7.6682692 ## 12 ExpressJet 73053 1132 1.5495599 8.1865242 ## 13 SkyWest 16061 224 1.3946828 8.6934922 ## 14 JetBlue 695 18 2.5899281 9.8588410 ## 15 United 2072 34 1.6409266 10.4628628 1 2 3 4 5 # Make an ordered per-day summary of hflights hflights %>% group_by ( DayOfWeek ) %>% summarise ( avg_taxi = mean ( TaxiIn + TaxiOut , na.rm = TRUE )) %>% arrange ( desc ( avg_taxi )) 1 2 3 4 5 6 7 8 9 10 ## # A tibble: 7 \u00d7 2 ## DayOfWeek avg_taxi ## &lt;int&gt; &lt;dbl&gt; ## 1 1 21.77027 ## 2 2 21.43505 ## 3 4 21.26076 ## 4 3 21.19055 ## 5 5 21.15805 ## 6 7 20.93726 ## 7 6 20.43061 Combine group_by with mutate 1 2 3 4 5 6 7 # Solution to first instruction hflights %>% filter ( ! is.na ( ArrDelay )) %>% group_by ( UniqueCarrier ) %>% summarise ( p_delay = sum ( ArrDelay > 0 ) / n ()) %>% mutate ( rank = rank ( p_delay )) %>% arrange ( rank ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 ## # A tibble: 15 \u00d7 3 ## UniqueCarrier p_delay rank ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 American 0.3030208 1 ## 2 AirTran 0.3112269 2 ## 3 US_Airways 0.3267990 3 ## 4 Atlantic_Southeast 0.3677511 4 ## 5 American_Eagle 0.3696714 5 ## 6 Delta 0.3871092 6 ## 7 JetBlue 0.3952452 7 ## 8 Alaska 0.4368132 8 ## 9 Southwest 0.4644557 9 ## 10 Mesa 0.4743590 10 ## 11 Continental 0.4907385 11 ## 12 ExpressJet 0.4943420 12 ## 13 United 0.4963109 13 ## 14 SkyWest 0.5350105 14 ## 15 Frontier 0.5564904 15 1 2 3 4 5 6 7 # Solution to second instruction hflights %>% filter ( ! is.na ( ArrDelay ), ArrDelay > 0 ) %>% group_by ( UniqueCarrier ) %>% summarise ( avg = mean ( ArrDelay )) %>% mutate ( rank = rank ( avg )) %>% arrange ( rank ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 ## # A tibble: 15 \u00d7 3 ## UniqueCarrier avg rank ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Mesa 18.67568 1 ## 2 Frontier 18.68683 2 ## 3 US_Airways 20.70235 3 ## 4 Continental 22.13374 4 ## 5 Alaska 22.91195 5 ## 6 SkyWest 24.14663 6 ## 7 ExpressJet 24.19337 7 ## 8 Southwest 25.27750 8 ## 9 AirTran 27.85693 9 ## 10 American 28.49740 10 ## 11 Delta 32.12463 11 ## 12 United 32.48067 12 ## 13 American_Eagle 38.75135 13 ## 14 Atlantic_Southeast 40.24231 14 ## 15 JetBlue 45.47744 15 Advanced group_by exercises 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 # Which plane (by tail number) flew out of Houston the most times? How many times? adv1 adv1 <- hflights %>% group_by ( TailNum ) %>% summarise ( n = n ()) %>% filter ( n == max ( n )) # How many airplanes only flew to one destination from Houston? adv2 adv2 <- hflights %>% group_by ( TailNum ) %>% summarise ( ndest = n_distinct ( Dest )) %>% filter ( ndest == 1 ) %>% summarise ( nplanes = n ()) # Find the most visited destination for each carrier: adv3 adv3 <- hflights %>% group_by ( UniqueCarrier , Dest ) %>% summarise ( n = n ()) %>% mutate ( rank = rank ( desc ( n ))) %>% filter ( rank == 1 ) # Find the carrier that travels to each destination the most: adv4 adv4 <- hflights %>% group_by ( Dest , UniqueCarrier ) %>% summarise ( n = n ()) %>% mutate ( rank = rank ( desc ( n ))) %>% filter ( rank == 1 ) dplyr deals with different types 1 2 3 # Use summarise to calculate n_carrier s2 <- hflights %>% summarise ( n_carrier = n_distinct ( UniqueCarrier )) dplyr and mySQL databases Code only. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 # set up a src that connects to the mysql database (src_mysql is provided by dplyr) my_db <- src_mysql ( dbname = 'dplyr' , host = 'dplyr.csrrinzqubik.us-east-1.rds.amazonaws.com' , port = 3306 , user = 'dplyr' , password = 'dplyr' ) # and reference a table within that src: nycflights is now available as an R object that references to the remote nycflights table nycflights <- tbl ( my_db , 'dplyr' ) # glimpse at nycflights glimpse ( nycflights ) # Calculate the grouped summaries detailed in the instructions nycflights %>% group_by ( carrier ) %>% summarise ( n_flights = n (), avg_delay = mean ( arr_delay )) %>% arrange ( avg_delay ) Adding tidyr Functions \u00b6 complete drop_na expand extract extract_numeric complete fill full_seq gather nest replace_na separate separate_rows separate_rows_ smiths spread table1 unite unnest who Extension: \u2018Joining Data in R with dplyr \u2018 \u00b6 1 2 3 4 5 6 7 8 9 ## 1, Mutating Joins ## 2, Filtering Joins and Set Operations ## 3, Assembling Data ## 4, Advanced Joining ## 5, Case Study","title":"Data Wrangling"},{"location":"data_wrangling/#documentation","text":"data.table extension of data.frame . Fast aggregation of large data (e.g. 100GB in RAM), fast ordered joins, fast add/modify/delete of columns by group using no copies at all, list columns, a fast friendly file reader and parallel file writer. Offers a natural and flexible syntax, for faster development. dplyr A fast, consistent tool for working with data frame like objects, both in memory and out of memory. Pipelines. tidyr An evolution of \u2018reshape2\u2019. It\u2019s designed specifically for data tidying (not general reshaping or aggregating) and works well with dplyr data pipelines. package 'narrower' 'wider' tidyr gather spread reshape2 melt cast spreadsheets unpivot pivot databases fold unfold","title":"Documentation"},{"location":"data_wrangling/#data-analysis-in-r-the-datatable-way","text":"","title":"Data Analysis in R, the data.table Way"},{"location":"data_wrangling/#1-datatable-novice","text":"Find out more with ?data.table . Create and subset a data.table 1 2 3 4 5 6 7 8 9 10 11 # The data.table package library ( data.table ) # Create my_first_data_table my_first_data_table <- data.table ( x = c ( 'a' , 'b' , 'c' , 'd' , 'e' ), y = c ( 1 , 2 , 3 , 4 , 5 )) # Create a data.table using recycling DT <- data.table ( a = 1 : 2 , b = c ( 'A' , 'B' , 'C' , 'D' )) # Print the third row to the console DT[3 , ] 1 2 ## a b ## 1: 1 C 1 2 # Print the second and third row to the console, but do not commas DT[2 : 3 ] 1 2 3 ## a b ## 1: 2 B ## 2: 1 C Getting to know a data.table Like head , tail . 1 2 # Print the penultimate row of DT using .N DT[.N - 1 ] 1 2 ## a b ## 1: 1 C 1 2 # Print the column names of DT, and number of rows and number of columns colnames ( DT ) 1 ## [1] \"a\" \"b\" 1 dim ( DT ) 1 ## [1] 4 2 1 2 # Select row 2 twice and row 3, returning a data.table with three rows where row 2 is a duplicate of row 1. DT [c ( 2 , 2 , 3 ) ] 1 2 3 4 ## a b ## 1: 2 B ## 2: 2 B ## 3: 1 C DT is a data.table/data.frame, but DT[ , B] is a vector; DT[ , .(B)] is a subsetted data.table. Subsetting data tables DT[i, j, by] means take DT , subset rows using i , then calculate j grouped by by . You can wrap j with .() . 1 2 3 4 5 6 7 A <- c ( 1 , 2 , 3 , 4 , 5 ) B <- c ( 'a' , 'b' , 'c' , 'd' , 'e' ) C <- c ( 6 , 7 , 8 , 9 , 10 ) DT <- data.table ( A , B , C ) # Subset rows 1 and 3, and columns B and C DT [c ( 1 , 3 ) , .(B , C ) ] 1 2 3 ## B C ## 1: a 6 ## 2: c 8 1 2 3 4 5 6 # Assign to ans the correct value ans <- data.table ( DT[ , .(B , val = A * C ) ] ) # Fill in the blanks such that ans2 equals target #target <- data.table(B = c('a', 'b', 'c', 'd', 'e', 'a', 'b', 'c', 'd', 'e'), val = as.integer(c(6:10, 1:5))) ans2 <- data.table ( DT[ , .(B , val = as.integer ( c ( 6 : 10 , 1 : 5 ))) ] ) The by basics 1 2 3 4 5 6 7 # iris and iris3 are already available in the workspace # Convert iris to a data.table: DT DT <- as.data.table ( iris ) # For each Species, print the mean Sepal.Length DT[ , .(mean ( Sepal.Length )), by = .(Species ) ] 1 2 3 4 ## Species V1 ## 1: setosa 5.006 ## 2: versicolor 5.936 ## 3: virginica 6.588 1 2 # Print mean Sepal.Length, grouping by first letter of Species DT[ , .(mean ( Sepal.Length )), by = .(substr ( Species , 1 , 1 )) ] 1 2 3 ## substr V1 ## 1: s 5.006 ## 2: v 6.262 Using .N and by .N , number, in row or column. 1 2 3 4 5 # data.table version of iris: DT DT <- as.data.table ( iris ) # Group the specimens by Sepal area (to the nearest 10 cm2) and count how many occur in each group. DT[ , .N , by = 10 * round ( Sepal.Length * Sepal.Width / 10 ) ] 1 2 3 4 ## round N ## 1: 20 117 ## 2: 10 29 ## 3: 30 4 1 2 # Now name the output columns `Area` and `Count` DT[ , .(Count = .N ), by = .(Area = 10 * round ( Sepal.Length * Sepal.Width / 10 )) ] 1 2 3 4 ## Area Count ## 1: 20 117 ## 2: 10 29 ## 3: 30 4 Return multiple numbers in j 1 2 3 4 5 6 7 8 9 # Create the data.table DT set.seed ( 1L ) DT <- data.table ( A = rep ( letters [2 : 1 ] , each = 4L ), B = rep ( 1 : 4 , each = 2L ), C = sample ( 8 )) # Create the new data.table, DT2 DT2 <- DT[ , .(C = cumsum ( C )), by = .(A , B ) ] # Select from DT2 the last two values from C while you group by A DT2[ , .(C = tail ( C , 2 )), by = A] 1 2 3 4 5 ## A C ## 1: b 4 ## 2: b 9 ## 3: a 2 ## 4: a 8","title":"1, data.table novice"},{"location":"data_wrangling/#2-datatable-yeoman","text":"Chaining, the basics 1 2 3 4 # Build DT set.seed ( 1L ) DT <- data.table ( A = rep ( letters [2 : 1 ] , each = 4L ), B = rep ( 1 : 4 , each = 2L ), C = sample ( 8 )) DT 1 2 3 4 5 6 7 8 9 ## A B C ## 1: b 1 3 ## 2: b 1 8 ## 3: b 2 4 ## 4: b 2 5 ## 5: a 3 1 ## 6: a 3 7 ## 7: a 4 2 ## 8: a 4 6 1 2 3 # Use chaining # Cumsum of C while grouping by A and B, and then select last two values of C while grouping by A DT[ , .(C = cumsum ( C )), by = .(A , B ) ][ , .(C = tail ( C , 2 )), by = .(A ) ] 1 2 3 4 5 ## A C ## 1: b 4 ## 2: b 9 ## 3: a 2 ## 4: a 8 Chaining your iris dataset 1 2 3 4 DT <- data.table ( iris ) # Perform chained operations on DT DT[ , .(Sepal.Length = median ( Sepal.Length ), Sepal.Width = median ( Sepal.Width ), Petal.Length = median ( Petal.Length ), Petal.Width = median ( Petal.Width )), by = .(Species ) ] [order ( Species , decreasing = TRUE ) ] 1 2 3 4 ## Species Sepal.Length Sepal.Width Petal.Length Petal.Width ## 1: virginica 6.5 3.0 5.55 2.0 ## 2: versicolor 5.9 2.8 4.35 1.3 ## 3: setosa 5.0 3.4 1.50 0.2 1 DT 1 2 3 4 5 6 7 8 9 10 11 12 ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1: 5.1 3.5 1.4 0.2 setosa ## 2: 4.9 3.0 1.4 0.2 setosa ## 3: 4.7 3.2 1.3 0.2 setosa ## 4: 4.6 3.1 1.5 0.2 setosa ## 5: 5.0 3.6 1.4 0.2 setosa ## --- ## 146: 6.7 3.0 5.2 2.3 virginica ## 147: 6.3 2.5 5.0 1.9 virginica ## 148: 6.5 3.0 5.2 2.0 virginica ## 149: 6.2 3.4 5.4 2.3 virginica ## 150: 5.9 3.0 5.1 1.8 virginica Programming time vs readability 1 2 3 4 5 6 7 x <- c ( 2 , 1 , 2 , 1 , 2 , 2 , 1 ) y <- c ( 1 , 3 , 5 , 7 , 9 , 11 , 13 ) z <- c ( 2 , 4 , 6 , 8 , 10 , 12 , 14 ) DT <- data.table ( x , y , z ) # Mean of columns DT[ , lapply ( .SD , mean ), by = .(x ) ] 1 2 3 ## x y z ## 1: 2 6.500000 7.500000 ## 2: 1 7.666667 8.666667 1 2 # Median of columns DT[ , lapply ( .SD , median ), by = .(x ) ] 1 2 3 ## x y z ## 1: 2 7 8 ## 2: 1 7 8 Introducing .SDcols .SDcols specifies the columns of DT that are included in .SD . 1 2 3 4 5 6 7 8 9 10 grp <- c ( 6 , 6 , 8 , 8 , 8 ) Q1 <- c ( 4 , 3 , 3 , 5 , 3 ) Q2 <- c ( 1 , 4 , 1 , 4 , 4 ) Q3 <- c ( 3 , 1 , 5 , 5 , 2 ) H1 <- c ( 1 , 2 , 3 , 2 , 4 ) H2 <- c ( 1 , 4 , 3 , 4 , 3 ) DT <- data.table ( grp , Q1 , Q2 , Q3 , H1 , H2 ) # Calculate the sum of the Q columns DT[ , lapply ( .SD , sum ), .SDcols = 2 : 4 ] 1 2 ## Q1 Q2 Q3 ## 1: 18 14 16 1 2 # Calculate the sum of columns H1 and H2 DT[ , lapply ( .SD , sum ), .SDcols = 5 : 6 ] 1 2 ## H1 H2 ## 1: 12 15 1 2 # Select all but the first row of groups 1 and 2, returning only the grp column and the Q columns. DT[ , .SD[ -1 ] , .SDcols = 2 : 4 , by = .(grp ) ] 1 2 3 4 ## grp Q1 Q2 Q3 ## 1: 6 3 4 1 ## 2: 8 5 4 5 ## 3: 8 3 4 2 Mixing it together: lapply , .SD , SDcols and .N 1 2 3 4 5 6 7 8 x <- c ( 2 , 1 , 2 , 1 , 2 , 2 , 1 ) y <- c ( 1 , 3 , 5 , 7 , 9 , 11 , 13 ) z <- c ( 2 , 4 , 6 , 8 , 10 , 12 , 14 ) DT <- data.table ( x , y , z ) # Sum of all columns and the number of rows # For the first part, you need to combine the returned list from lapply, .SD and .SDcols and the integer vector of .N. You have to this because the result of the two together has to be a list again, with all values put together. DT 1 2 3 4 5 6 7 8 ## x y z ## 1: 2 1 2 ## 2: 1 3 4 ## 3: 2 5 6 ## 4: 1 7 8 ## 5: 2 9 10 ## 6: 2 11 12 ## 7: 1 13 14 1 DT[ , c ( lapply ( .SD , sum ), .N ), .SDcols = 1 : 3 , by = x] 1 2 3 ## x x y z N ## 1: 2 8 26 30 4 ## 2: 1 3 23 26 3 1 2 # Cumulative sum of column x and y while grouping by x and z > 8 DT[ , lapply ( .SD , cumsum ), .SDcols = 1 : 2 , by = .(by1 = x , by2 = z > 8 ) ] 1 2 3 4 5 6 7 8 ## by1 by2 x y ## 1: 2 FALSE 2 1 ## 2: 2 FALSE 4 6 ## 3: 1 FALSE 1 3 ## 4: 1 FALSE 2 10 ## 5: 2 TRUE 2 9 ## 6: 2 TRUE 4 20 ## 7: 1 TRUE 1 13 1 2 # Chaining DT[ , lapply ( .SD , cumsum ), .SDcols = 1 : 2 , by = .(by1 = x , by2 = z > 8 ) ][ , lapply ( .SD , max ), .SDcols = 3 : 4 , by = by1] 1 2 3 ## by1 x y ## 1: 2 4 20 ## 2: 1 2 13 Adding, updating and removing columns := is defined for use in j only. 1 2 3 # The data.table DT DT <- data.table ( A = letters [c ( 1 , 1 , 1 , 2 , 2 ) ] , B = 1 : 5 ) DT 1 2 3 4 5 6 ## A B ## 1: a 1 ## 2: a 2 ## 3: a 3 ## 4: b 4 ## 5: b 5 1 2 # Add column by reference: Total DT[ , ( 'Total' ) := sum ( B ), by = .(A ) ] 1 2 3 4 5 6 ## A B Total ## 1: a 1 6 ## 2: a 2 6 ## 3: a 3 6 ## 4: b 4 9 ## 5: b 5 9 1 2 # Add 1 to column B DT [c ( 2 , 4 ), ( 'B' ) := as.integer ( 1 + B ) ] 1 2 3 4 5 6 ## A B Total ## 1: a 1 6 ## 2: a 3 6 ## 3: a 3 6 ## 4: b 5 9 ## 5: b 5 9 1 2 # Add a new column Total2 DT[2 : 4 , ':=' ( Total2 = sum ( B )), by = .(A ) ] 1 2 3 4 5 6 ## A B Total Total2 ## 1: a 1 6 NA ## 2: a 3 6 6 ## 3: a 3 6 6 ## 4: b 5 9 5 ## 5: b 5 9 NA 1 2 # Remove the Total column DT[ , Total := NULL ] 1 2 3 4 5 6 ## A B Total2 ## 1: a 1 NA ## 2: a 3 6 ## 3: a 3 6 ## 4: b 5 5 ## 5: b 5 NA 1 2 # Select the third column using `[[` DT[[3]] 1 ## [1] NA 6 6 5 NA The functional form 1 2 3 4 5 # A data.table DT DT <- data.table ( A = c ( 1 , 1 , 1 , 2 , 2 ), B = 1 : 5 ) # Update B, add C and D DT[ , `:=` ( B = B + 1 , C = A + B , D = 2 ) ] 1 2 3 4 5 6 ## A B C D ## 1: 1 2 2 2 ## 2: 1 3 3 2 ## 3: 1 4 4 2 ## 4: 2 5 6 2 ## 5: 2 6 7 2 1 2 3 # Delete my_cols my_cols <- c ( 'B' , 'C' ) DT[ , ( my_cols ) := NULL ] 1 2 3 4 5 6 ## A D ## 1: 1 2 ## 2: 1 2 ## 3: 1 2 ## 4: 2 2 ## 5: 2 2 1 2 # Delete column 2 by number DT[ , 2 := NULL ] 1 2 3 4 5 6 ## A ## 1: 1 ## 2: 1 ## 3: 1 ## 4: 2 ## 5: 2 Ready, set , go! The set function is used to repeatedly update a data.table by reference. You can think of the set function as a loopable. 1 2 3 4 5 6 7 8 9 10 11 A <- c ( 2 , 2 , 3 , 5 , 2 , 5 , 5 , 4 , 4 , 1 ) B <- c ( 2 , 1 , 4 , 2 , 4 , 3 , 4 , 5 , 2 , 4 ) C <- c ( 5 , 2 , 4 , 1 , 2 , 2 , 1 , 2 , 5 , 2 ) D <- c ( 3 , 3 , 3 , 1 , 5 , 4 , 4 , 1 , 4 , 3 ) DT <- data.table ( A , B , C , D ) # Set the seed set.seed ( 1 ) # Check the DT DT 1 2 3 4 5 6 7 8 9 10 11 ## A B C D ## 1: 2 2 5 3 ## 2: 2 1 2 3 ## 3: 3 4 4 3 ## 4: 5 2 1 1 ## 5: 2 4 2 5 ## 6: 5 3 2 4 ## 7: 5 4 1 4 ## 8: 4 5 2 1 ## 9: 4 2 5 4 ## 10: 1 4 2 3 1 2 3 4 5 6 7 8 # For loop with set for ( l in 2 : 4 ) set ( DT , sample ( 10 , 3 ), l , NA ) # Change the column names to lowercase setnames ( DT , c ( 'A' , 'B' , 'C' , 'D' ), c ( 'a' , 'b' , 'c' , 'd' )) # Print the resulting DT to the console DT 1 2 3 4 5 6 7 8 9 10 11 ## a b c d ## 1: 2 2 5 3 ## 2: 2 1 NA 3 ## 3: 3 NA 4 3 ## 4: 5 NA 1 1 ## 5: 2 NA 2 5 ## 6: 5 3 2 NA ## 7: 5 4 1 4 ## 8: 4 5 NA 1 ## 9: 4 2 5 NA ## 10: 1 4 NA NA The set family 1 2 3 # Define DT DT <- data.table ( a = letters [c ( 1 , 1 , 1 , 2 , 2 ) ] , b = 1 ) DT 1 2 3 4 5 6 ## a b ## 1: a 1 ## 2: a 1 ## 3: a 1 ## 4: b 1 ## 5: b 1 1 2 3 # Add a postfix '_2' to all column names setnames ( DT , c ( 1 : 2 ), paste0 ( c ( 'a' , 'b' ), '_2' )) DT 1 2 3 4 5 6 ## a_2 b_2 ## 1: a 1 ## 2: a 1 ## 3: a 1 ## 4: b 1 ## 5: b 1 1 2 3 # Change column name 'a_2' to 'A2' setnames ( DT , 'a_2' , 'A2' ) DT 1 2 3 4 5 6 ## A2 b_2 ## 1: a 1 ## 2: a 1 ## 3: a 1 ## 4: b 1 ## 5: b 1 1 2 # Reverse the order of the columns setcolorder ( DT , c ( 'b_2' , 'A2' ))","title":"2, data.table yeoman"},{"location":"data_wrangling/#3-datatable-expert","text":"Selecting rows the data.table way 1 2 3 # Convert iris to a data.table iris <- data.table ( 'Sepal.Length' = iris $ Sepal.Length , 'Sepal.Width' = iris $ Sepal.Width , 'Petal.Length' = iris $ Petal.Length , 'Petal.Width' = iris $ Petal.Width , 'Species' = iris $ Species ) iris 1 2 3 4 5 6 7 8 9 10 11 12 ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1: 5.1 3.5 1.4 0.2 setosa ## 2: 4.9 3.0 1.4 0.2 setosa ## 3: 4.7 3.2 1.3 0.2 setosa ## 4: 4.6 3.1 1.5 0.2 setosa ## 5: 5.0 3.6 1.4 0.2 setosa ## --- ## 146: 6.7 3.0 5.2 2.3 virginica ## 147: 6.3 2.5 5.0 1.9 virginica ## 148: 6.5 3.0 5.2 2.0 virginica ## 149: 6.2 3.4 5.4 2.3 virginica ## 150: 5.9 3.0 5.1 1.8 virginica 1 2 # Species is 'virginica' head ( iris[Species == 'virginica' ] , 20 ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1: 6.3 3.3 6.0 2.5 virginica ## 2: 5.8 2.7 5.1 1.9 virginica ## 3: 7.1 3.0 5.9 2.1 virginica ## 4: 6.3 2.9 5.6 1.8 virginica ## 5: 6.5 3.0 5.8 2.2 virginica ## 6: 7.6 3.0 6.6 2.1 virginica ## 7: 4.9 2.5 4.5 1.7 virginica ## 8: 7.3 2.9 6.3 1.8 virginica ## 9: 6.7 2.5 5.8 1.8 virginica ## 10: 7.2 3.6 6.1 2.5 virginica ## 11: 6.5 3.2 5.1 2.0 virginica ## 12: 6.4 2.7 5.3 1.9 virginica ## 13: 6.8 3.0 5.5 2.1 virginica ## 14: 5.7 2.5 5.0 2.0 virginica ## 15: 5.8 2.8 5.1 2.4 virginica ## 16: 6.4 3.2 5.3 2.3 virginica ## 17: 6.5 3.0 5.5 1.8 virginica ## 18: 7.7 3.8 6.7 2.2 virginica ## 19: 7.7 2.6 6.9 2.3 virginica ## 20: 6.0 2.2 5.0 1.5 virginica 1 2 # Species is either 'virginica' or 'versicolor' head ( iris[Species %in% c ( 'virginica' , 'versicolor' ) ] , 20 ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1: 7.0 3.2 4.7 1.4 versicolor ## 2: 6.4 3.2 4.5 1.5 versicolor ## 3: 6.9 3.1 4.9 1.5 versicolor ## 4: 5.5 2.3 4.0 1.3 versicolor ## 5: 6.5 2.8 4.6 1.5 versicolor ## 6: 5.7 2.8 4.5 1.3 versicolor ## 7: 6.3 3.3 4.7 1.6 versicolor ## 8: 4.9 2.4 3.3 1.0 versicolor ## 9: 6.6 2.9 4.6 1.3 versicolor ## 10: 5.2 2.7 3.9 1.4 versicolor ## 11: 5.0 2.0 3.5 1.0 versicolor ## 12: 5.9 3.0 4.2 1.5 versicolor ## 13: 6.0 2.2 4.0 1.0 versicolor ## 14: 6.1 2.9 4.7 1.4 versicolor ## 15: 5.6 2.9 3.6 1.3 versicolor ## 16: 6.7 3.1 4.4 1.4 versicolor ## 17: 5.6 3.0 4.5 1.5 versicolor ## 18: 5.8 2.7 4.1 1.0 versicolor ## 19: 6.2 2.2 4.5 1.5 versicolor ## 20: 5.6 2.5 3.9 1.1 versicolor Removing columns and adapting your column names Refer to a regex cheat sheet for metacharacter. 1 2 3 # iris as a data.table iris <- as.data.table ( iris ) iris 1 2 3 4 5 6 7 8 9 10 11 12 ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1: 5.1 3.5 1.4 0.2 setosa ## 2: 4.9 3.0 1.4 0.2 setosa ## 3: 4.7 3.2 1.3 0.2 setosa ## 4: 4.6 3.1 1.5 0.2 setosa ## 5: 5.0 3.6 1.4 0.2 setosa ## --- ## 146: 6.7 3.0 5.2 2.3 virginica ## 147: 6.3 2.5 5.0 1.9 virginica ## 148: 6.5 3.0 5.2 2.0 virginica ## 149: 6.2 3.4 5.4 2.3 virginica ## 150: 5.9 3.0 5.1 1.8 virginica 1 2 3 4 5 6 7 # Remove the 'Sepal.' prefix #gsub('([ab])', '\\\\1_\\\\1_', 'abc and ABC') = pattern, replacement, x setnames ( iris , c ( 'Sepal.Length' , 'Sepal.Width' ), c ( 'Length' , 'Width' )) #gsub('^Sepal\\\\.','', iris) # Remove the two columns starting with 'Petal' iris[ , c ( 'Petal.Length' , 'Petal.Width' ) := NULL ] 1 2 3 4 5 6 7 8 9 10 11 12 ## Length Width Species ## 1: 5.1 3.5 setosa ## 2: 4.9 3.0 setosa ## 3: 4.7 3.2 setosa ## 4: 4.6 3.1 setosa ## 5: 5.0 3.6 setosa ## --- ## 146: 6.7 3.0 virginica ## 147: 6.3 2.5 virginica ## 148: 6.5 3.0 virginica ## 149: 6.2 3.4 virginica ## 150: 5.9 3.0 virginica Understanding automatic indexing 1 2 3 # Cleaned up iris data.table iris2 <- data.frame ( Length = iris $ Sepal.Length , Width = iris $ Sepal.Width , Species = iris $ Species ) iris2 <- as.data.table ( iris2 ) 1 2 # Area is greater than 20 square centimeters iris2[ Width * Length > 20 ] , 20 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 Length Width Species is_large 1: 5.4 3.9 setosa FALSE 2: 5.8 4.0 setosa FALSE 3: 5.7 4.4 setosa TRUE 4: 5.4 3.9 setosa FALSE 5: 5.7 3.8 setosa FALSE 6: 5.2 4.1 setosa FALSE 7: 5.5 4.2 setosa FALSE 8: 7.0 3.2 versicolor FALSE 9: 6.4 3.2 versicolor FALSE 10: 6.9 3.1 versicolor FALSE 11: 6.3 3.3 versicolor FALSE 12: 6.7 3.1 versicolor FALSE 13: 6.7 3.0 versicolor FALSE 14: 6.0 3.4 versicolor FALSE 15: 6.7 3.1 versicolor FALSE 16: 6.3 3.3 virginica FALSE 17: 7.1 3.0 virginica FALSE 18: 7.6 3.0 virginica FALSE 19: 7.3 2.9 virginica FALSE 20: 7.2 3.6 virginica TRUE ... 1 2 # Add new boolean column iris2[ , is_large := Width * Length > 25 ] 1 2 # Now large observations with is_large iris2[is_large == TRUE ] 1 2 3 4 5 Length Width Species is_large 1: 5.7 4.4 setosa TRUE 2: 7.2 3.6 virginica TRUE 3: 7.7 3.8 virginica TRUE 4: 7.9 3.8 virginica TRUE 1 iris2 [ ( is_large ) ] # Also OK 1 2 3 4 5 Length Width Species is_large 1: 5.7 4.4 setosa TRUE 2: 7.2 3.6 virginica TRUE 3: 7.7 3.8 virginica TRUE 4: 7.9 3.8 virginica TRUE Selecting groups or parts of groups 1 2 3 4 5 6 7 8 # The 'keyed' data.table DT DT <- data.table ( A = letters [c ( 2 , 1 , 2 , 3 , 1 , 2 , 3 ) ] , B = c ( 5 , 4 , 1 , 9 , 8 , 8 , 6 ), C = 6 : 12 ) setkey ( DT , A , B ) # Select the 'b' group DT[ 'b' ] 1 2 3 4 ## A B C ## 1: b 1 8 ## 2: b 5 6 ## 3: b 8 11 1 2 # 'b' and 'c' groups DT [c ( 'b' , 'c' ) ] 1 2 3 4 5 6 ## A B C ## 1: b 1 8 ## 2: b 5 6 ## 3: b 8 11 ## 4: c 6 12 ## 5: c 9 9 1 2 # The first row of the 'b' and 'c' groups DT [c ( 'b' , 'c' ), mult = 'first' ] 1 2 3 ## A B C ## 1: b 1 8 ## 2: c 6 12 1 2 # First and last row of the 'b' and 'c' groups DT [c ( 'b' , 'c' ), .SD [c ( 1 , .N ) ] , by = .EACHI] 1 2 3 4 5 ## A B C ## 1: b 1 8 ## 2: b 8 11 ## 3: c 6 12 ## 4: c 9 9 1 2 # Copy and extend code for instruction 4: add printout DT [c ( 'b' , 'c' ), { print ( .SD ); .SD [c ( 1 , .N ) ] }, by = .EACHI] 1 2 3 4 5 6 7 8 9 10 11 12 13 ## B C ## 1: 1 8 ## 2: 5 6 ## 3: 8 11 ## B C ## 1: 6 12 ## 2: 9 9 ## A B C ## 1: b 1 8 ## 2: b 8 11 ## 3: c 6 12 ## 4: c 9 9 Rolling joins - part one 1 2 3 4 5 6 7 # Keyed data.table DT DT <- data.table ( A = letters [c ( 2 , 1 , 2 , 3 , 1 , 2 , 3 ) ] , B = c ( 5 , 4 , 1 , 9 , 8 , 8 , 6 ), C = 6 : 12 , key = 'A,B' ) # Get the key of DT key ( DT ) 1 ## [1] \"A\" \"B\" 1 2 3 # Row where A == 'b' & B == 6 setkey ( DT , A , B ) DT [. ( 'b' , 6 ) ] 1 2 ## A B C ## 1: b 6 NA 1 2 # Return the prevailing row DT [. ( 'b' , 6 ), roll = TRUE ] 1 2 ## A B C ## 1: b 6 6 1 2 # Return the nearest row DT [. ( 'b' , 6 ), roll =+ Inf ] 1 2 ## A B C ## 1: b 6 6 Rolling joins - part two 1 2 3 4 5 6 7 # Keyed data.table DT DT <- data.table ( A = letters [c ( 2 , 1 , 2 , 3 , 1 , 2 , 3 ) ] , B = c ( 5 , 4 , 1 , 9 , 8 , 8 , 6 ), C = 6 : 12 , key = 'A,B' ) # Look at the sequence (-2):10 for the 'b' group DT [. ( 'b' , ( -2 ) : 10 ) ] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 ## A B C ## 1: b -2 NA ## 2: b -1 NA ## 3: b 0 NA ## 4: b 1 8 ## 5: b 2 NA ## 6: b 3 NA ## 7: b 4 NA ## 8: b 5 6 ## 9: b 6 NA ## 10: b 7 NA ## 11: b 8 11 ## 12: b 9 NA ## 13: b 10 NA 1 2 # Add code: carry the prevailing values forwards DT [. ( 'b' , ( -2 ) : 10 ), roll = TRUE ] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 ## A B C ## 1: b -2 NA ## 2: b -1 NA ## 3: b 0 NA ## 4: b 1 8 ## 5: b 2 8 ## 6: b 3 8 ## 7: b 4 8 ## 8: b 5 6 ## 9: b 6 6 ## 10: b 7 6 ## 11: b 8 11 ## 12: b 9 11 ## 13: b 10 11 1 2 # Add code: carry the first observation backwards DT [. ( 'b' , ( -2 ) : 10 ), roll = TRUE , rollends = TRUE ] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 ## A B C ## 1: b -2 8 ## 2: b -1 8 ## 3: b 0 8 ## 4: b 1 8 ## 5: b 2 8 ## 6: b 3 8 ## 7: b 4 8 ## 8: b 5 6 ## 9: b 6 6 ## 10: b 7 6 ## 11: b 8 11 ## 12: b 9 11 ## 13: b 10 11","title":"3, data.table expert"},{"location":"data_wrangling/#data-manipulation-in-r-with-dplyr","text":"","title":"Data Manipulation in R with dplyr"},{"location":"data_wrangling/#1-introduction-to-dplyr","text":"Load the dplyr and hflights package 1 2 3 4 5 6 7 8 9 10 # Load the dplyr package library ( dplyr ) library ( dtplyr ) # Load the hflights package # A data only package containing commercial domestic flights that departed Houston (IAH and HOU) in 2011 library ( hflights ) # Call both head() and summary() on hflights head ( hflights ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 ## Year Month DayofMonth DayOfWeek DepTime ArrTime UniqueCarrier ## 5424 2011 1 1 6 1400 1500 AA ## 5425 2011 1 2 7 1401 1501 AA ## 5426 2011 1 3 1 1352 1502 AA ## 5427 2011 1 4 2 1403 1513 AA ## 5428 2011 1 5 3 1405 1507 AA ## 5429 2011 1 6 4 1359 1503 AA ## FlightNum TailNum ActualElapsedTime AirTime ArrDelay DepDelay Origin ## 5424 428 N576AA 60 40 -10 0 IAH ## 5425 428 N557AA 60 45 -9 1 IAH ## 5426 428 N541AA 70 48 -8 -8 IAH ## 5427 428 N403AA 70 39 3 3 IAH ## 5428 428 N492AA 62 44 -3 5 IAH ## 5429 428 N262AA 64 45 -7 -1 IAH ## Dest Distance TaxiIn TaxiOut Cancelled CancellationCode Diverted ## 5424 DFW 224 7 13 0 0 ## 5425 DFW 224 6 9 0 0 ## 5426 DFW 224 5 17 0 0 ## 5427 DFW 224 9 22 0 0 ## 5428 DFW 224 9 9 0 0 ## 5429 DFW 224 6 13 0 0 1 summary ( hflights ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 ## Year Month DayofMonth DayOfWeek ## Min. :2011 Min. : 1.000 Min. : 1.00 Min. :1.000 ## 1st Qu.:2011 1st Qu.: 4.000 1st Qu.: 8.00 1st Qu.:2.000 ## Median :2011 Median : 7.000 Median :16.00 Median :4.000 ## Mean :2011 Mean : 6.514 Mean :15.74 Mean :3.948 ## 3rd Qu.:2011 3rd Qu.: 9.000 3rd Qu.:23.00 3rd Qu.:6.000 ## Max. :2011 Max. :12.000 Max. :31.00 Max. :7.000 ## ## DepTime ArrTime UniqueCarrier FlightNum ## Min. : 1 Min. : 1 Length:227496 Min. : 1 ## 1st Qu.:1021 1st Qu.:1215 Class :character 1st Qu.: 855 ## Median :1416 Median :1617 Mode :character Median :1696 ## Mean :1396 Mean :1578 Mean :1962 ## 3rd Qu.:1801 3rd Qu.:1953 3rd Qu.:2755 ## Max. :2400 Max. :2400 Max. :7290 ## NA's :2905 NA's :3066 ## TailNum ActualElapsedTime AirTime ArrDelay ## Length:227496 Min. : 34.0 Min. : 11.0 Min. :-70.000 ## Class :character 1st Qu.: 77.0 1st Qu.: 58.0 1st Qu.: -8.000 ## Mode :character Median :128.0 Median :107.0 Median : 0.000 ## Mean :129.3 Mean :108.1 Mean : 7.094 ## 3rd Qu.:165.0 3rd Qu.:141.0 3rd Qu.: 11.000 ## Max. :575.0 Max. :549.0 Max. :978.000 ## NA's :3622 NA's :3622 NA's :3622 ## DepDelay Origin Dest Distance ## Min. :-33.000 Length:227496 Length:227496 Min. : 79.0 ## 1st Qu.: -3.000 Class :character Class :character 1st Qu.: 376.0 ## Median : 0.000 Mode :character Mode :character Median : 809.0 ## Mean : 9.445 Mean : 787.8 ## 3rd Qu.: 9.000 3rd Qu.:1042.0 ## Max. :981.000 Max. :3904.0 ## NA's :2905 ## TaxiIn TaxiOut Cancelled CancellationCode ## Min. : 1.000 Min. : 1.00 Min. :0.00000 Length:227496 ## 1st Qu.: 4.000 1st Qu.: 10.00 1st Qu.:0.00000 Class :character ## Median : 5.000 Median : 14.00 Median :0.00000 Mode :character ## Mean : 6.099 Mean : 15.09 Mean :0.01307 ## 3rd Qu.: 7.000 3rd Qu.: 18.00 3rd Qu.:0.00000 ## Max. :165.000 Max. :163.00 Max. :1.00000 ## NA's :3066 NA's :2947 ## Diverted ## Min. :0.000000 ## 1st Qu.:0.000000 ## Median :0.000000 ## Mean :0.002853 ## 3rd Qu.:0.000000 ## Max. :1.000000 ## Convert data.frame to table 1 2 3 4 5 # Convert the hflights data.frame into a hflights tbl hflights <- tbl_df ( hflights ) # Display the hflights tbl hflights 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 ## # A tibble : 227 , 496 \u00d7 21 ## Year Month DayofMonth DayOfWeek DepTime ArrTime UniqueCarrier ## * & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; chr & gt ; ## 1 2011 1 1 6 1400 1500 AA ## 2 2011 1 2 7 1401 1501 AA ## 3 2011 1 3 1 1352 1502 AA ## 4 2011 1 4 2 1403 1513 AA ## 5 2011 1 5 3 1405 1507 AA ## 6 2011 1 6 4 1359 1503 AA ## 7 2011 1 7 5 1359 1509 AA ## 8 2011 1 8 6 1355 1454 AA ## 9 2011 1 9 7 1443 1554 AA ## 10 2011 1 10 1 1443 1553 AA ## # ... with 227 , 486 more rows , and 14 more variables : FlightNum & lt ; int & gt ;, ## # TailNum & lt ; chr & gt ;, ActualElapsedTime & lt ; int & gt ;, AirTime & lt ; int & gt ;, ArrDelay & lt ; int & gt ;, ## # DepDelay & lt ; int & gt ;, Origin & lt ; chr & gt ;, Dest & lt ; chr & gt ;, Distance & lt ; int & gt ;, ## # TaxiIn & lt ; int & gt ;, TaxiOut & lt ; int & gt ;, Cancelled & lt ; int & gt ;, CancellationCode & lt ; chr & gt ;, ## # Diverted & lt ; int & gt ; 1 2 # Create the object carriers, containing only the UniqueCarrier variable of hflights carriers <- hflights $ UniqueCarrier Changing labels of hflights , part 1 of 2 1 2 3 4 5 6 7 8 9 10 11 # add lut <- c ( 'AA' = 'American' , 'AS' = 'Alaska' , 'B6' = 'JetBlue' , 'CO' = 'Continental' , 'DL' = 'Delta' , 'OO' = 'SkyWest' , 'UA' = 'United' , 'US' = 'US_Airways' , 'WN' = 'Southwest' , 'EV' = 'Atlantic_Southeast' , 'F9' = 'Frontier' , 'FL' = 'AirTran' , 'MQ' = 'American_Eagle' , 'XE' = 'ExpressJet' , 'YV' = 'Mesa' ) # Use lut to translate the UniqueCarrier column of hflights hflights $ UniqueCarrier <- lut[hflights $ UniqueCarrier] # Inspect the resulting raw values of your variables glimpse ( hflights ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 ## Observations: 227,496 ## Variables: 21 ## $ Year &lt;int&gt; 2011, 2011, 2011, 2011, 2011, 2011, 2011, 20... ## $ Month &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,... ## $ DayofMonth &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 1... ## $ DayOfWeek &lt;int&gt; 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6,... ## $ DepTime &lt;int&gt; 1400, 1401, 1352, 1403, 1405, 1359, 1359, 13... ## $ ArrTime &lt;int&gt; 1500, 1501, 1502, 1513, 1507, 1503, 1509, 14... ## $ UniqueCarrier &lt;chr&gt; \"American\", \"American\", \"American\", \"America... ## $ FlightNum &lt;int&gt; 428, 428, 428, 428, 428, 428, 428, 428, 428,... ## $ TailNum &lt;chr&gt; \"N576AA\", \"N557AA\", \"N541AA\", \"N403AA\", \"N49... ## $ ActualElapsedTime &lt;int&gt; 60, 60, 70, 70, 62, 64, 70, 59, 71, 70, 70, ... ## $ AirTime &lt;int&gt; 40, 45, 48, 39, 44, 45, 43, 40, 41, 45, 42, ... ## $ ArrDelay &lt;int&gt; -10, -9, -8, 3, -3, -7, -1, -16, 44, 43, 29,... ## $ DepDelay &lt;int&gt; 0, 1, -8, 3, 5, -1, -1, -5, 43, 43, 29, 19, ... ## $ Origin &lt;chr&gt; \"IAH\", \"IAH\", \"IAH\", \"IAH\", \"IAH\", \"IAH\", \"I... ## $ Dest &lt;chr&gt; \"DFW\", \"DFW\", \"DFW\", \"DFW\", \"DFW\", \"DFW\", \"D... ## $ Distance &lt;int&gt; 224, 224, 224, 224, 224, 224, 224, 224, 224,... ## $ TaxiIn &lt;int&gt; 7, 6, 5, 9, 9, 6, 12, 7, 8, 6, 8, 4, 6, 5, 6... ## $ TaxiOut &lt;int&gt; 13, 9, 17, 22, 9, 13, 15, 12, 22, 19, 20, 11... ## $ Cancelled &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,... ## $ CancellationCode &lt;chr&gt; \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", ... ## $ Diverted &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,... Changing labels of hflights , part 2 of 2 1 2 3 4 5 6 7 8 # Build the lookup table: lut lut <- c ( \"A\" = \"carrier\" , \"B\" = \"weather\" , \"C\" = \"FFA\" , \"D\" = \"security\" , \"E\" = \"not cancelled\" ) # Add the Code column hflights $ Code <- lut[hflights $ CancellationCode] # Glimpse at hflights glimpse ( hflights ) Result. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 Observations: 227,496 Variables: 22 $ Year <int> 2011, 2011, 2011, 2011, 2011, 2011,... $ Month <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,... $ DayofMonth <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, ... $ DayOfWeek <int> 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3,... $ DepTime <int> 1400, 1401, 1352, 1403, 1405, 1359,... $ ArrTime <int> 1500, 1501, 1502, 1513, 1507, 1503,... $ UniqueCarrier <chr> \"American\", \"American\", \"American\",... $ FlightNum <int> 428, 428, 428, 428, 428, 428, 428, ... $ TailNum <chr> \"N576AA\", \"N557AA\", \"N541AA\", \"N403... $ ActualElapsedTime <int> 60, 60, 70, 70, 62, 64, 70, 59, 71,... $ AirTime <int> 40, 45, 48, 39, 44, 45, 43, 40, 41,... $ ArrDelay <int> -10, -9, -8, 3, -3, -7, -1, -16, 44... $ DepDelay <int> 0, 1, -8, 3, 5, -1, -1, -5, 43, 43,... $ Origin <chr> \"IAH\", \"IAH\", \"IAH\", \"IAH\", \"IAH\", ... $ Dest <chr> \"DFW\", \"DFW\", \"DFW\", \"DFW\", \"DFW\", ... $ Distance <int> 224, 224, 224, 224, 224, 224, 224, ... $ TaxiIn <int> 7, 6, 5, 9, 9, 6, 12, 7, 8, 6, 8, 4... $ TaxiOut <int> 13, 9, 17, 22, 9, 13, 15, 12, 22, 1... $ Cancelled <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,... $ CancellationCode <chr> \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\",... $ Diverted <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,... $ Code <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA,...","title":"1, Introduction to dplyr"},{"location":"data_wrangling/#2-select-and-mutate","text":"The five verbs and their meaning select ; which returns a subset of the columns. filter ; that is able to return a subset of the rows. arrange ; that reorders the rows according to single or multiple variables. mutate ; used to add columns from existing data. summarise ; which reduces each group to a single row by calculating aggregate measures. Choosing is not losing! The select verb 1 2 # Print out a tbl with the four columns of hflights related to delay select ( hflights , ActualElapsedTime , AirTime , ArrDelay , DepDelay ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 ## # A tibble: 227,496 \u00d7 4 ## ActualElapsedTime AirTime ArrDelay DepDelay ## * &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 60 40 -10 0 ## 2 60 45 -9 1 ## 3 70 48 -8 -8 ## 4 70 39 3 3 ## 5 62 44 -3 5 ## 6 64 45 -7 -1 ## 7 70 43 -1 -1 ## 8 59 40 -16 -5 ## 9 71 41 44 43 ## 10 70 45 43 43 ## # ... with 227,486 more rows 1 2 # Print out hflights, nothing has changed! hflights 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 ## # A tibble : 227 , 496 \u00d7 21 ## Year Month DayofMonth DayOfWeek DepTime ArrTime UniqueCarrier ## * & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; chr & gt ; ## 1 2011 1 1 6 1400 1500 American ## 2 2011 1 2 7 1401 1501 American ## 3 2011 1 3 1 1352 1502 American ## 4 2011 1 4 2 1403 1513 American ## 5 2011 1 5 3 1405 1507 American ## 6 2011 1 6 4 1359 1503 American ## 7 2011 1 7 5 1359 1509 American ## 8 2011 1 8 6 1355 1454 American ## 9 2011 1 9 7 1443 1554 American ## 10 2011 1 10 1 1443 1553 American ## # ... with 227 , 486 more rows , and 14 more variables : FlightNum & lt ; int & gt ;, ## # TailNum & lt ; chr & gt ;, ActualElapsedTime & lt ; int & gt ;, AirTime & lt ; int & gt ;, ArrDelay & lt ; int & gt ;, ## # DepDelay & lt ; int & gt ;, Origin & lt ; chr & gt ;, Dest & lt ; chr & gt ;, Distance & lt ; int & gt ;, ## # TaxiIn & lt ; int & gt ;, TaxiOut & lt ; int & gt ;, Cancelled & lt ; int & gt ;, CancellationCode & lt ; chr & gt ;, ## # Diverted & lt ; int & gt ; 1 2 # Print out the columns Origin up to Cancelled of hflights select ( hflights , 14 : 19 ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 ## # A tibble: 227,496 \u00d7 6 ## Origin Dest Distance TaxiIn TaxiOut Cancelled ## * &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 IAH DFW 224 7 13 0 ## 2 IAH DFW 224 6 9 0 ## 3 IAH DFW 224 5 17 0 ## 4 IAH DFW 224 9 22 0 ## 5 IAH DFW 224 9 9 0 ## 6 IAH DFW 224 6 13 0 ## 7 IAH DFW 224 12 15 0 ## 8 IAH DFW 224 7 12 0 ## 9 IAH DFW 224 8 22 0 ## 10 IAH DFW 224 6 19 0 ## # ... with 227,486 more rows 1 2 # Answer to last question: be concise! select ( hflights , 1 : 4 , 12 : 21 ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 ## # A tibble : 227 , 496 \u00d7 14 ## Year Month DayofMonth DayOfWeek ArrDelay DepDelay Origin Dest ## * & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; chr & gt ; & lt ; chr & gt ; ## 1 2011 1 1 6 -10 0 IAH DFW ## 2 2011 1 2 7 -9 1 IAH DFW ## 3 2011 1 3 1 -8 -8 IAH DFW ## 4 2011 1 4 2 3 3 IAH DFW ## 5 2011 1 5 3 -3 5 IAH DFW ## 6 2011 1 6 4 -7 -1 IAH DFW ## 7 2011 1 7 5 -1 -1 IAH DFW ## 8 2011 1 8 6 -16 -5 IAH DFW ## 9 2011 1 9 7 44 43 IAH DFW ## 10 2011 1 10 1 43 43 IAH DFW ## # ... with 227 , 486 more rows , and 6 more variables : Distance & lt ; int & gt ;, ## # TaxiIn & lt ; int & gt ;, TaxiOut & lt ; int & gt ;, Cancelled & lt ; int & gt ;, CancellationCode & lt ; chr & gt ;, ## # Diverted & lt ; int & gt ; Helper functions for variable selection select : starts_with(\"X\") ; every name that starts with \"X\" , ends_with(\"X\") ; every name that ends with \"X\" , contains(\"X\") ; every name that contains \"X\" , matches(\"X\") ; every name that matches \"X\" , where \"X\" can be a regular expression, num_range(\"x\", 1:5) ; the variables named x01 , x02 , x03 , - x04 and x05 , one_of(x) ; every name that appears in x , which should be a character vector. 1 2 # Print out a tbl containing just ArrDelay and DepDelay select ( hflights , ArrDelay , DepDelay ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 ## # A tibble: 227,496 \u00d7 2 ## ArrDelay DepDelay ## * &lt;int&gt; &lt;int&gt; ## 1 -10 0 ## 2 -9 1 ## 3 -8 -8 ## 4 3 3 ## 5 -3 5 ## 6 -7 -1 ## 7 -1 -1 ## 8 -16 -5 ## 9 44 43 ## 10 43 43 ## # ... with 227,486 more rows 1 2 # Print out a tbl as described in the second instruction, using both helper functions and variable names select ( hflights , UniqueCarrier , ends_with ( 'Num' ), starts_with ( 'Cancel' )) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 ## # A tibble: 227,496 \u00d7 5 ## UniqueCarrier FlightNum TailNum Cancelled CancellationCode ## * &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; ## 1 American 428 N576AA 0 ## 2 American 428 N557AA 0 ## 3 American 428 N541AA 0 ## 4 American 428 N403AA 0 ## 5 American 428 N492AA 0 ## 6 American 428 N262AA 0 ## 7 American 428 N493AA 0 ## 8 American 428 N477AA 0 ## 9 American 428 N476AA 0 ## 10 American 428 N504AA 0 ## # ... with 227,486 more rows 1 2 # Print out a tbl as described in the third instruction, using only helper functions. select ( hflights , ends_with ( 'Time' ), ends_with ( 'Delay' )) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 ## # A tibble: 227,496 \u00d7 6 ## DepTime ArrTime ActualElapsedTime AirTime ArrDelay DepDelay ## * &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 1400 1500 60 40 -10 0 ## 2 1401 1501 60 45 -9 1 ## 3 1352 1502 70 48 -8 -8 ## 4 1403 1513 70 39 3 3 ## 5 1405 1507 62 44 -3 5 ## 6 1359 1503 64 45 -7 -1 ## 7 1359 1509 70 43 -1 -1 ## 8 1355 1454 59 40 -16 -5 ## 9 1443 1554 71 41 44 43 ## 10 1443 1553 70 45 43 43 ## # ... with 227,486 more rows Comparison to basic R 1 2 3 4 5 6 7 8 9 10 11 12 # add ex1r <- hflights [c ( 'TaxiIn' , 'TaxiOut' , 'Distance' ) ] ex1d <- select ( hflights , starts_with ( 'Taxi' ), Distance ) ex2r <- hflights [c ( 'Year' , 'Month' , 'DayOfWeek' , 'DepTime' , 'ArrTime' ) ] ex2d <- select ( hflights , Year , Month , DayOfWeek , DepTime , ArrTime ) ex3r <- hflights [c ( 'TailNum' , 'TaxiIn' , 'TaxiOut' ) ] ex3d <- select ( hflights , TailNum , starts_with ( 'Taxi' )) Mutating is creating 1 2 3 # Add the new variable ActualGroundTime to a copy of hflights and save the result as g1 g1 <- mutate ( hflights , ActualGroundTime = ActualElapsedTime - AirTime ) glimpse ( hflights ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 ## Observations: 227,496 ## Variables: 21 ## $ Year &lt;int&gt; 2011, 2011, 2011, 2011, 2011, 2011, 2011, 20... ## $ Month &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,... ## $ DayofMonth &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 1... ## $ DayOfWeek &lt;int&gt; 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6,... ## $ DepTime &lt;int&gt; 1400, 1401, 1352, 1403, 1405, 1359, 1359, 13... ## $ ArrTime &lt;int&gt; 1500, 1501, 1502, 1513, 1507, 1503, 1509, 14... ## $ UniqueCarrier &lt;chr&gt; \"American\", \"American\", \"American\", \"America... ## $ FlightNum &lt;int&gt; 428, 428, 428, 428, 428, 428, 428, 428, 428,... ## $ TailNum &lt;chr&gt; \"N576AA\", \"N557AA\", \"N541AA\", \"N403AA\", \"N49... ## $ ActualElapsedTime &lt;int&gt; 60, 60, 70, 70, 62, 64, 70, 59, 71, 70, 70, ... ## $ AirTime &lt;int&gt; 40, 45, 48, 39, 44, 45, 43, 40, 41, 45, 42, ... ## $ ArrDelay &lt;int&gt; -10, -9, -8, 3, -3, -7, -1, -16, 44, 43, 29,... ## $ DepDelay &lt;int&gt; 0, 1, -8, 3, 5, -1, -1, -5, 43, 43, 29, 19, ... ## $ Origin &lt;chr&gt; \"IAH\", \"IAH\", \"IAH\", \"IAH\", \"IAH\", \"IAH\", \"I... ## $ Dest &lt;chr&gt; \"DFW\", \"DFW\", \"DFW\", \"DFW\", \"DFW\", \"DFW\", \"D... ## $ Distance &lt;int&gt; 224, 224, 224, 224, 224, 224, 224, 224, 224,... ## $ TaxiIn &lt;int&gt; 7, 6, 5, 9, 9, 6, 12, 7, 8, 6, 8, 4, 6, 5, 6... ## $ TaxiOut &lt;int&gt; 13, 9, 17, 22, 9, 13, 15, 12, 22, 19, 20, 11... ## $ Cancelled &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,... ## $ CancellationCode &lt;chr&gt; \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", ... ## $ Diverted &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,... 1 glimpse ( g1 ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 ## Observations: 227,496 ## Variables: 22 ## $ Year &lt;int&gt; 2011, 2011, 2011, 2011, 2011, 2011, 2011, 20... ## $ Month &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,... ## $ DayofMonth &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 1... ## $ DayOfWeek &lt;int&gt; 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6,... ## $ DepTime &lt;int&gt; 1400, 1401, 1352, 1403, 1405, 1359, 1359, 13... ## $ ArrTime &lt;int&gt; 1500, 1501, 1502, 1513, 1507, 1503, 1509, 14... ## $ UniqueCarrier &lt;chr&gt; \"American\", \"American\", \"American\", \"America... ## $ FlightNum &lt;int&gt; 428, 428, 428, 428, 428, 428, 428, 428, 428,... ## $ TailNum &lt;chr&gt; \"N576AA\", \"N557AA\", \"N541AA\", \"N403AA\", \"N49... ## $ ActualElapsedTime &lt;int&gt; 60, 60, 70, 70, 62, 64, 70, 59, 71, 70, 70, ... ## $ AirTime &lt;int&gt; 40, 45, 48, 39, 44, 45, 43, 40, 41, 45, 42, ... ## $ ArrDelay &lt;int&gt; -10, -9, -8, 3, -3, -7, -1, -16, 44, 43, 29,... ## $ DepDelay &lt;int&gt; 0, 1, -8, 3, 5, -1, -1, -5, 43, 43, 29, 19, ... ## $ Origin &lt;chr&gt; \"IAH\", \"IAH\", \"IAH\", \"IAH\", \"IAH\", \"IAH\", \"I... ## $ Dest &lt;chr&gt; \"DFW\", \"DFW\", \"DFW\", \"DFW\", \"DFW\", \"DFW\", \"D... ## $ Distance &lt;int&gt; 224, 224, 224, 224, 224, 224, 224, 224, 224,... ## $ TaxiIn &lt;int&gt; 7, 6, 5, 9, 9, 6, 12, 7, 8, 6, 8, 4, 6, 5, 6... ## $ TaxiOut &lt;int&gt; 13, 9, 17, 22, 9, 13, 15, 12, 22, 19, 20, 11... ## $ Cancelled &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,... ## $ CancellationCode &lt;chr&gt; \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", ... ## $ Diverted &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,... ## $ ActualGroundTime &lt;int&gt; 20, 15, 22, 31, 18, 19, 27, 19, 30, 25, 28, ... 1 2 3 4 # Add the new variable GroundTime to a g1; save the result as g2 g2 <- mutate ( g1 , GroundTime = TaxiIn + TaxiOut ) head ( g1 $ ActualGroundTime == g2 $ GroundTime , 20 ) 1 2 ## [1] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE ## [15] TRUE TRUE TRUE TRUE TRUE TRUE 1 2 3 # Add the new variable AverageSpeed to g2; save the result as g3 g3 <- mutate ( g2 , AverageSpeed = Distance / AirTime * 60 ) g3 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 ## # A tibble : 227 , 496 \u00d7 24 ## Year Month DayofMonth DayOfWeek DepTime ArrTime UniqueCarrier ## & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; chr & gt ; ## 1 2011 1 1 6 1400 1500 American ## 2 2011 1 2 7 1401 1501 American ## 3 2011 1 3 1 1352 1502 American ## 4 2011 1 4 2 1403 1513 American ## 5 2011 1 5 3 1405 1507 American ## 6 2011 1 6 4 1359 1503 American ## 7 2011 1 7 5 1359 1509 American ## 8 2011 1 8 6 1355 1454 American ## 9 2011 1 9 7 1443 1554 American ## 10 2011 1 10 1 1443 1553 American ## # ... with 227 , 486 more rows , and 17 more variables : FlightNum & lt ; int & gt ;, ## # TailNum & lt ; chr & gt ;, ActualElapsedTime & lt ; int & gt ;, AirTime & lt ; int & gt ;, ArrDelay & lt ; int & gt ;, ## # DepDelay & lt ; int & gt ;, Origin & lt ; chr & gt ;, Dest & lt ; chr & gt ;, Distance & lt ; int & gt ;, ## # TaxiIn & lt ; int & gt ;, TaxiOut & lt ; int & gt ;, Cancelled & lt ; int & gt ;, CancellationCode & lt ; chr & gt ;, ## # Diverted & lt ; int & gt ;, ActualGroundTime & lt ; int & gt ;, GroundTime & lt ; int & gt ;, ## # AverageSpeed & lt ; dbl & gt ; Add multiple variables using mutate 1 2 3 4 5 6 7 8 9 # Add a second variable loss_percent to the dataset: m1 m1 <- mutate ( hflights , loss = ArrDelay - DepDelay , loss_percent = ( ArrDelay - DepDelay ) / DepDelay * 100 ) # Copy and adapt the previous command to reduce redendancy: m2 m2 <- mutate ( hflights , loss = ArrDelay - DepDelay , loss_percent = loss / DepDelay * 100 ) # Add the three variables as described in the third instruction: m3 m3 <- mutate ( hflights , TotalTaxi = TaxiIn + TaxiOut , ActualGroundTime = ActualElapsedTime - AirTime , Diff = TotalTaxi - ActualGroundTime ) glimpse ( m3 ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 ## Observations: 227,496 ## Variables: 24 ## $ Year &lt;int&gt; 2011, 2011, 2011, 2011, 2011, 2011, 2011, 20... ## $ Month &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,... ## $ DayofMonth &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 1... ## $ DayOfWeek &lt;int&gt; 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6,... ## $ DepTime &lt;int&gt; 1400, 1401, 1352, 1403, 1405, 1359, 1359, 13... ## $ ArrTime &lt;int&gt; 1500, 1501, 1502, 1513, 1507, 1503, 1509, 14... ## $ UniqueCarrier &lt;chr&gt; \"American\", \"American\", \"American\", \"America... ## $ FlightNum &lt;int&gt; 428, 428, 428, 428, 428, 428, 428, 428, 428,... ## $ TailNum &lt;chr&gt; \"N576AA\", \"N557AA\", \"N541AA\", \"N403AA\", \"N49... ## $ ActualElapsedTime &lt;int&gt; 60, 60, 70, 70, 62, 64, 70, 59, 71, 70, 70, ... ## $ AirTime &lt;int&gt; 40, 45, 48, 39, 44, 45, 43, 40, 41, 45, 42, ... ## $ ArrDelay &lt;int&gt; -10, -9, -8, 3, -3, -7, -1, -16, 44, 43, 29,... ## $ DepDelay &lt;int&gt; 0, 1, -8, 3, 5, -1, -1, -5, 43, 43, 29, 19, ... ## $ Origin &lt;chr&gt; \"IAH\", \"IAH\", \"IAH\", \"IAH\", \"IAH\", \"IAH\", \"I... ## $ Dest &lt;chr&gt; \"DFW\", \"DFW\", \"DFW\", \"DFW\", \"DFW\", \"DFW\", \"D... ## $ Distance &lt;int&gt; 224, 224, 224, 224, 224, 224, 224, 224, 224,... ## $ TaxiIn &lt;int&gt; 7, 6, 5, 9, 9, 6, 12, 7, 8, 6, 8, 4, 6, 5, 6... ## $ TaxiOut &lt;int&gt; 13, 9, 17, 22, 9, 13, 15, 12, 22, 19, 20, 11... ## $ Cancelled &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,... ## $ CancellationCode &lt;chr&gt; \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", ... ## $ Diverted &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,... ## $ TotalTaxi &lt;int&gt; 20, 15, 22, 31, 18, 19, 27, 19, 30, 25, 28, ... ## $ ActualGroundTime &lt;int&gt; 20, 15, 22, 31, 18, 19, 27, 19, 30, 25, 28, ... ## $ Diff &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...","title":"2, select and mutate"},{"location":"data_wrangling/#3-filter-and-arrange","text":"Logical operators filter : x < y ; TRUE if x is less than y . x <= y ; TRUE if x is less than or equal to y . x == y ; TRUE if x equals y . x != y ; TRUE if x does not equal y . x >= y ; TRUE if x is greater than or equal to y . x > y ; TRUE if x is greater than y . x %in% c(a, b, c) ; TRUE if x is in the vector c(a, b, c) . 1 2 # All flights that traveled 3000 miles or more filter ( hflights , Distance >= 3000 ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 ## # A tibble : 527 \u00d7 21 ## Year Month DayofMonth DayOfWeek DepTime ArrTime UniqueCarrier ## & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; chr & gt ; ## 1 2011 1 31 1 924 1413 Continental ## 2 2011 1 30 7 925 1410 Continental ## 3 2011 1 29 6 1045 1445 Continental ## 4 2011 1 28 5 1516 1916 Continental ## 5 2011 1 27 4 950 1344 Continental ## 6 2011 1 26 3 944 1350 Continental ## 7 2011 1 25 2 924 1337 Continental ## 8 2011 1 24 1 1144 1605 Continental ## 9 2011 1 23 7 926 1335 Continental ## 10 2011 1 22 6 942 1340 Continental ## # ... with 517 more rows , and 14 more variables : FlightNum & lt ; int & gt ;, ## # TailNum & lt ; chr & gt ;, ActualElapsedTime & lt ; int & gt ;, AirTime & lt ; int & gt ;, ArrDelay & lt ; int & gt ;, ## # DepDelay & lt ; int & gt ;, Origin & lt ; chr & gt ;, Dest & lt ; chr & gt ;, Distance & lt ; int & gt ;, ## # TaxiIn & lt ; int & gt ;, TaxiOut & lt ; int & gt ;, Cancelled & lt ; int & gt ;, CancellationCode & lt ; chr & gt ;, ## # Diverted & lt ; int & gt ; 1 2 # All flights flown by one of JetBlue, Southwest, or Delta filter ( hflights , UniqueCarrier %in% c ( 'JetBlue' , 'Southwest' , 'Delta' )) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 ## # A tibble : 48 , 679 \u00d7 21 ## Year Month DayofMonth DayOfWeek DepTime ArrTime UniqueCarrier ## & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; chr & gt ; ## 1 2011 1 1 6 654 1124 JetBlue ## 2 2011 1 1 6 1639 2110 JetBlue ## 3 2011 1 2 7 703 1113 JetBlue ## 4 2011 1 2 7 1604 2040 JetBlue ## 5 2011 1 3 1 659 1100 JetBlue ## 6 2011 1 3 1 1801 2200 JetBlue ## 7 2011 1 4 2 654 1103 JetBlue ## 8 2011 1 4 2 1608 2034 JetBlue ## 9 2011 1 5 3 700 1103 JetBlue ## 10 2011 1 5 3 1544 1954 JetBlue ## # ... with 48 , 669 more rows , and 14 more variables : FlightNum & lt ; int & gt ;, ## # TailNum & lt ; chr & gt ;, ActualElapsedTime & lt ; int & gt ;, AirTime & lt ; int & gt ;, ArrDelay & lt ; int & gt ;, ## # DepDelay & lt ; int & gt ;, Origin & lt ; chr & gt ;, Dest & lt ; chr & gt ;, Distance & lt ; int & gt ;, ## # TaxiIn & lt ; int & gt ;, TaxiOut & lt ; int & gt ;, Cancelled & lt ; int & gt ;, CancellationCode & lt ; chr & gt ;, ## # Diverted & lt ; int & gt ; 1 2 # All flights where taxiing took longer than flying filter ( hflights , ( TaxiIn + TaxiOut ) > AirTime ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 ## # A tibble : 1 , 389 \u00d7 21 ## Year Month DayofMonth DayOfWeek DepTime ArrTime UniqueCarrier ## & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; chr & gt ; ## 1 2011 1 24 1 731 904 American ## 2 2011 1 30 7 1959 2132 American ## 3 2011 1 24 1 1621 1749 American ## 4 2011 1 10 1 941 1113 American ## 5 2011 1 31 1 1301 1356 Continental ## 6 2011 1 31 1 2113 2215 Continental ## 7 2011 1 31 1 1434 1539 Continental ## 8 2011 1 31 1 900 1006 Continental ## 9 2011 1 30 7 1304 1408 Continental ## 10 2011 1 30 7 2004 2128 Continental ## # ... with 1 , 379 more rows , and 14 more variables : FlightNum & lt ; int & gt ;, ## # TailNum & lt ; chr & gt ;, ActualElapsedTime & lt ; int & gt ;, AirTime & lt ; int & gt ;, ArrDelay & lt ; int & gt ;, ## # DepDelay & lt ; int & gt ;, Origin & lt ; chr & gt ;, Dest & lt ; chr & gt ;, Distance & lt ; int & gt ;, ## # TaxiIn & lt ; int & gt ;, TaxiOut & lt ; int & gt ;, Cancelled & lt ; int & gt ;, CancellationCode & lt ; chr & gt ;, ## # Diverted & lt ; int & gt ; Combining tests using boolean operators 1 2 # All flights that departed before 5am or arrived after 10pm filter ( hflights , DepTime < 500 | ArrTime > 2200 ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 ## # A tibble : 27 , 799 \u00d7 21 ## Year Month DayofMonth DayOfWeek DepTime ArrTime UniqueCarrier ## & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; chr & gt ; ## 1 2011 1 4 2 2100 2207 American ## 2 2011 1 14 5 2119 2229 American ## 3 2011 1 10 1 1934 2235 American ## 4 2011 1 26 3 1905 2211 American ## 5 2011 1 30 7 1856 2209 American ## 6 2011 1 9 7 1938 2228 Alaska ## 7 2011 1 31 1 1919 2231 Continental ## 8 2011 1 31 1 2116 2344 Continental ## 9 2011 1 31 1 1850 2211 Continental ## 10 2011 1 31 1 2102 2216 Continental ## # ... with 27 , 789 more rows , and 14 more variables : FlightNum & lt ; int & gt ;, ## # TailNum & lt ; chr & gt ;, ActualElapsedTime & lt ; int & gt ;, AirTime & lt ; int & gt ;, ArrDelay & lt ; int & gt ;, ## # DepDelay & lt ; int & gt ;, Origin & lt ; chr & gt ;, Dest & lt ; chr & gt ;, Distance & lt ; int & gt ;, ## # TaxiIn & lt ; int & gt ;, TaxiOut & lt ; int & gt ;, Cancelled & lt ; int & gt ;, CancellationCode & lt ; chr & gt ;, ## # Diverted & lt ; int & gt ; 1 2 # All flights that departed late but arrived ahead of schedule filter ( hflights , DepDelay > 0 & ArrDelay < 0 ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 ## # A tibble : 27 , 712 \u00d7 21 ## Year Month DayofMonth DayOfWeek DepTime ArrTime UniqueCarrier ## & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; chr & gt ; ## 1 2011 1 2 7 1401 1501 American ## 2 2011 1 5 3 1405 1507 American ## 3 2011 1 18 2 1408 1508 American ## 4 2011 1 18 2 721 827 American ## 5 2011 1 12 3 2015 2113 American ## 6 2011 1 13 4 2020 2116 American ## 7 2011 1 26 3 2009 2103 American ## 8 2011 1 1 6 1631 1736 American ## 9 2011 1 10 1 1639 1740 American ## 10 2011 1 12 3 1631 1739 American ## # ... with 27 , 702 more rows , and 14 more variables : FlightNum & lt ; int & gt ;, ## # TailNum & lt ; chr & gt ;, ActualElapsedTime & lt ; int & gt ;, AirTime & lt ; int & gt ;, ArrDelay & lt ; int & gt ;, ## # DepDelay & lt ; int & gt ;, Origin & lt ; chr & gt ;, Dest & lt ; chr & gt ;, Distance & lt ; int & gt ;, ## # TaxiIn & lt ; int & gt ;, TaxiOut & lt ; int & gt ;, Cancelled & lt ; int & gt ;, CancellationCode & lt ; chr & gt ;, ## # Diverted & lt ; int & gt ; 1 2 # All cancelled weekend flights filter ( hflights , DayOfWeek %in% c ( 6 , 7 ) & Cancelled == 1 ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 ## # A tibble : 585 \u00d7 21 ## Year Month DayofMonth DayOfWeek DepTime ArrTime UniqueCarrier ## & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; chr & gt ; ## 1 2011 1 9 7 NA NA American ## 2 2011 1 29 6 NA NA Continental ## 3 2011 1 9 7 NA NA Continental ## 4 2011 1 9 7 NA NA Delta ## 5 2011 1 9 7 NA NA SkyWest ## 6 2011 1 2 7 NA NA Southwest ## 7 2011 1 29 6 NA NA Delta ## 8 2011 1 9 7 NA NA Atlantic_Southeast ## 9 2011 1 1 6 NA NA AirTran ## 10 2011 1 9 7 NA NA AirTran ## # ... with 575 more rows , and 14 more variables : FlightNum & lt ; int & gt ;, ## # TailNum & lt ; chr & gt ;, ActualElapsedTime & lt ; int & gt ;, AirTime & lt ; int & gt ;, ArrDelay & lt ; int & gt ;, ## # DepDelay & lt ; int & gt ;, Origin & lt ; chr & gt ;, Dest & lt ; chr & gt ;, Distance & lt ; int & gt ;, ## # TaxiIn & lt ; int & gt ;, TaxiOut & lt ; int & gt ;, Cancelled & lt ; int & gt ;, CancellationCode & lt ; chr & gt ;, ## # Diverted & lt ; int & gt ; 1 2 # All flights that were cancelled after being delayed filter ( hflights , DepDelay > 0 & Cancelled == 1 ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 ## # A tibble : 40 \u00d7 21 ## Year Month DayofMonth DayOfWeek DepTime ArrTime UniqueCarrier ## & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; chr & gt ; ## 1 2011 1 26 3 1926 NA Continental ## 2 2011 1 11 2 1100 NA US_Airways ## 3 2011 1 19 3 1811 NA ExpressJet ## 4 2011 1 7 5 2028 NA ExpressJet ## 5 2011 2 4 5 1638 NA American ## 6 2011 2 8 2 1057 NA Continental ## 7 2011 2 2 3 802 NA ExpressJet ## 8 2011 2 9 3 904 NA ExpressJet ## 9 2011 2 1 2 1508 NA SkyWest ## 10 2011 3 31 4 1016 NA Continental ## # ... with 30 more rows , and 14 more variables : FlightNum & lt ; int & gt ;, ## # TailNum & lt ; chr & gt ;, ActualElapsedTime & lt ; int & gt ;, AirTime & lt ; int & gt ;, ArrDelay & lt ; int & gt ;, ## # DepDelay & lt ; int & gt ;, Origin & lt ; chr & gt ;, Dest & lt ; chr & gt ;, Distance & lt ; int & gt ;, ## # TaxiIn & lt ; int & gt ;, TaxiOut & lt ; int & gt ;, Cancelled & lt ; int & gt ;, CancellationCode & lt ; chr & gt ;, ## # Diverted & lt ; int & gt ; Blend together what you\u2019ve learned! 1 2 3 4 5 6 7 8 # Select the flights that had JFK as their destination: c1 c1 <- filter ( hflights , Dest == 'JFK' ) # Combine the Year, Month and DayofMonth variables to create a Date column: c2 c2 <- mutate ( c1 , Date = paste ( Year , Month , DayofMonth , sep = '-' )) # Print out a selection of columns of c2 select ( c2 , Date , DepTime , ArrTime , TailNum ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 ## # A tibble: 695 \u00d7 4 ## Date DepTime ArrTime TailNum ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; ## 1 2011-1-1 654 1124 N324JB ## 2 2011-1-1 1639 2110 N324JB ## 3 2011-1-2 703 1113 N324JB ## 4 2011-1-2 1604 2040 N324JB ## 5 2011-1-3 659 1100 N229JB ## 6 2011-1-3 1801 2200 N206JB ## 7 2011-1-4 654 1103 N267JB ## 8 2011-1-4 1608 2034 N267JB ## 9 2011-1-5 700 1103 N708JB ## 10 2011-1-5 1544 1954 N644JB ## # ... with 685 more rows Arranging your data 1 2 3 4 5 # Definition of dtc dtc <- filter ( hflights , Cancelled == 1 , ! is.na ( DepDelay )) # Arrange dtc by departure delays arrange ( dtc , DepDelay ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 ## # A tibble : 68 \u00d7 21 ## Year Month DayofMonth DayOfWeek DepTime ArrTime UniqueCarrier ## & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; chr & gt ; ## 1 2011 7 23 6 605 NA Frontier ## 2 2011 1 17 1 916 NA ExpressJet ## 3 2011 12 1 4 541 NA US_Airways ## 4 2011 10 12 3 2022 NA American_Eagle ## 5 2011 7 29 5 1424 NA Continental ## 6 2011 9 29 4 1639 NA SkyWest ## 7 2011 2 9 3 555 NA American_Eagle ## 8 2011 5 9 1 715 NA SkyWest ## 9 2011 1 20 4 1413 NA United ## 10 2011 1 17 1 831 NA Southwest ## # ... with 58 more rows , and 14 more variables : FlightNum & lt ; int & gt ;, ## # TailNum & lt ; chr & gt ;, ActualElapsedTime & lt ; int & gt ;, AirTime & lt ; int & gt ;, ArrDelay & lt ; int & gt ;, ## # DepDelay & lt ; int & gt ;, Origin & lt ; chr & gt ;, Dest & lt ; chr & gt ;, Distance & lt ; int & gt ;, ## # TaxiIn & lt ; int & gt ;, TaxiOut & lt ; int & gt ;, Cancelled & lt ; int & gt ;, CancellationCode & lt ; chr & gt ;, ## # Diverted & lt ; int & gt ; 1 2 # Arrange dtc so that cancellation reasons are grouped arrange ( dtc , CancellationCode ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 ## # A tibble : 68 \u00d7 21 ## Year Month DayofMonth DayOfWeek DepTime ArrTime UniqueCarrier ## & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; chr & gt ; ## 1 2011 1 20 4 1413 NA United ## 2 2011 1 7 5 2028 NA ExpressJet ## 3 2011 2 4 5 1638 NA American ## 4 2011 2 8 2 1057 NA Continental ## 5 2011 2 1 2 1508 NA SkyWest ## 6 2011 2 21 1 2257 NA SkyWest ## 7 2011 2 9 3 555 NA American_Eagle ## 8 2011 3 18 5 727 NA United ## 9 2011 4 4 1 1632 NA Delta ## 10 2011 4 8 5 1608 NA Southwest ## # ... with 58 more rows , and 14 more variables : FlightNum & lt ; int & gt ;, ## # TailNum & lt ; chr & gt ;, ActualElapsedTime & lt ; int & gt ;, AirTime & lt ; int & gt ;, ArrDelay & lt ; int & gt ;, ## # DepDelay & lt ; int & gt ;, Origin & lt ; chr & gt ;, Dest & lt ; chr & gt ;, Distance & lt ; int & gt ;, ## # TaxiIn & lt ; int & gt ;, TaxiOut & lt ; int & gt ;, Cancelled & lt ; int & gt ;, CancellationCode & lt ; chr & gt ;, ## # Diverted & lt ; int & gt ; 1 2 # Arrange dtc according to carrier and departure delays arrange ( dtc , UniqueCarrier , DepDelay ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 ## # A tibble : 68 \u00d7 21 ## Year Month DayofMonth DayOfWeek DepTime ArrTime UniqueCarrier ## & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; chr & gt ; ## 1 2011 6 11 6 1649 NA AirTran ## 2 2011 8 18 4 1808 NA American ## 3 2011 2 4 5 1638 NA American ## 4 2011 10 12 3 2022 NA American_Eagle ## 5 2011 2 9 3 555 NA American_Eagle ## 6 2011 7 17 7 1917 NA American_Eagle ## 7 2011 4 30 6 612 NA Atlantic_Southeast ## 8 2011 4 10 7 1147 NA Atlantic_Southeast ## 9 2011 5 23 1 657 NA Atlantic_Southeast ## 10 2011 9 29 4 723 NA Atlantic_Southeast ## # ... with 58 more rows , and 14 more variables : FlightNum & lt ; int & gt ;, ## # TailNum & lt ; chr & gt ;, ActualElapsedTime & lt ; int & gt ;, AirTime & lt ; int & gt ;, ArrDelay & lt ; int & gt ;, ## # DepDelay & lt ; int & gt ;, Origin & lt ; chr & gt ;, Dest & lt ; chr & gt ;, Distance & lt ; int & gt ;, ## # TaxiIn & lt ; int & gt ;, TaxiOut & lt ; int & gt ;, Cancelled & lt ; int & gt ;, CancellationCode & lt ; chr & gt ;, ## # Diverted & lt ; int & gt ; Reverse the order of arranging 1 2 # Arrange according to carrier and decreasing departure delays arrange ( hflights , UniqueCarrier , desc ( DepDelay )) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 ## # A tibble : 227 , 496 \u00d7 21 ## Year Month DayofMonth DayOfWeek DepTime ArrTime UniqueCarrier ## & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; chr & gt ; ## 1 2011 2 19 6 1902 2143 AirTran ## 2 2011 3 14 1 2024 2309 AirTran ## 3 2011 2 16 3 2349 227 AirTran ## 4 2011 11 13 7 2312 213 AirTran ## 5 2011 5 26 4 2353 305 AirTran ## 6 2011 5 26 4 1922 2229 AirTran ## 7 2011 4 28 4 1045 1328 AirTran ## 8 2011 6 5 7 2207 52 AirTran ## 9 2011 5 7 6 1009 1256 AirTran ## 10 2011 7 25 1 2107 14 AirTran ## # ... with 227 , 486 more rows , and 14 more variables : FlightNum & lt ; int & gt ;, ## # TailNum & lt ; chr & gt ;, ActualElapsedTime & lt ; int & gt ;, AirTime & lt ; int & gt ;, ArrDelay & lt ; int & gt ;, ## # DepDelay & lt ; int & gt ;, Origin & lt ; chr & gt ;, Dest & lt ; chr & gt ;, Distance & lt ; int & gt ;, ## # TaxiIn & lt ; int & gt ;, TaxiOut & lt ; int & gt ;, Cancelled & lt ; int & gt ;, CancellationCode & lt ; chr & gt ;, ## # Diverted & lt ; int & gt ; 1 2 # Arrange flights by total delay (normal order). arrange ( hflights , ( ArrDelay + DepDelay )) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 ## # A tibble : 227 , 496 \u00d7 21 ## Year Month DayofMonth DayOfWeek DepTime ArrTime UniqueCarrier ## & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; chr & gt ; ## 1 2011 7 3 7 1914 2039 ExpressJet ## 2 2011 8 31 3 934 1039 SkyWest ## 3 2011 8 21 7 935 1039 SkyWest ## 4 2011 8 28 7 2059 2206 SkyWest ## 5 2011 8 29 1 935 1041 SkyWest ## 6 2011 12 25 7 741 926 SkyWest ## 7 2011 1 30 7 620 812 SkyWest ## 8 2011 8 3 3 1741 1810 ExpressJet ## 9 2011 8 4 4 930 1041 SkyWest ## 10 2011 8 18 4 939 1043 SkyWest ## # ... with 227 , 486 more rows , and 14 more variables : FlightNum & lt ; int & gt ;, ## # TailNum & lt ; chr & gt ;, ActualElapsedTime & lt ; int & gt ;, AirTime & lt ; int & gt ;, ArrDelay & lt ; int & gt ;, ## # DepDelay & lt ; int & gt ;, Origin & lt ; chr & gt ;, Dest & lt ; chr & gt ;, Distance & lt ; int & gt ;, ## # TaxiIn & lt ; int & gt ;, TaxiOut & lt ; int & gt ;, Cancelled & lt ; int & gt ;, CancellationCode & lt ; chr & gt ;, ## # Diverted & lt ; int & gt ; 1 2 # Keep flights leaving to DFW before 8am and arrange according to decreasing AirTime arrange ( filter ( hflights , Dest == 'DFW' & DepTime < 800 ), desc ( AirTime )) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 ## # A tibble : 799 \u00d7 21 ## Year Month DayofMonth DayOfWeek DepTime ArrTime UniqueCarrier ## & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; int & gt ; & lt ; chr & gt ; ## 1 2011 11 22 2 635 825 American ## 2 2011 8 25 4 602 758 American_Eagle ## 3 2011 10 12 3 559 738 American_Eagle ## 4 2011 5 2 1 716 854 American ## 5 2011 4 4 1 741 949 American ## 6 2011 4 4 1 627 742 American_Eagle ## 7 2011 6 21 2 726 848 ExpressJet ## 8 2011 9 1 4 715 844 American ## 9 2011 3 14 1 729 917 Continental ## 10 2011 12 5 1 724 847 Continental ## # ... with 789 more rows , and 14 more variables : FlightNum & lt ; int & gt ;, ## # TailNum & lt ; chr & gt ;, ActualElapsedTime & lt ; int & gt ;, AirTime & lt ; int & gt ;, ArrDelay & lt ; int & gt ;, ## # DepDelay & lt ; int & gt ;, Origin & lt ; chr & gt ;, Dest & lt ; chr & gt ;, Distance & lt ; int & gt ;, ## # TaxiIn & lt ; int & gt ;, TaxiOut & lt ; int & gt ;, Cancelled & lt ; int & gt ;, CancellationCode & lt ; chr & gt ;, ## # Diverted & lt ; int & gt ;","title":"3, filter and arrange"},{"location":"data_wrangling/#4-summarise-and-the-pipe-operator","text":"The syntax of summarise 1 2 # Print out a summary with variables min_dist and max_dist summarise ( hflights , min_dist = min ( Distance ), max_dist = max ( Distance )) 1 2 3 4 ## # A tibble: 1 \u00d7 2 ## min_dist max_dist ## &lt;int&gt; &lt;int&gt; ## 1 79 3904 1 2 # Print out a summary with variable max_div summarise ( filter ( hflights , Diverted == 1 ), max_div = max ( Distance )) 1 2 3 4 ## # A tibble: 1 \u00d7 1 ## max_div ## &lt;int&gt; ## 1 3904 Aggregate functions summarise : min(x) ; minimum value of vector x. max(x) ; maximum value of vector x. mean(x) ; mean value of vector x. median(x) ; median value of vector x. quantile(x, p) ; pth quantile of vector x. sd(x) ; standard deviation of vector x. var(x) ; variance of vector x. IQR(x) ; Inter Quartile Range (IQR) of vector x. diff(range(x)) ; total range of vector x. 1 2 3 4 5 # Remove rows that have NA ArrDelay: temp1 temp1 <- filter ( hflights , ! is.na ( ArrDelay )) # Generate summary about ArrDelay column of temp1 summarise ( temp1 , earliest = min ( ArrDelay ), average = mean ( ArrDelay ), latest = max ( ArrDelay ), sd = sd ( ArrDelay )) 1 2 3 4 ## # A tibble: 1 \u00d7 4 ## earliest average latest sd ## &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 -70 7.094334 978 30.70852 1 2 3 4 5 # Keep rows that have no NA TaxiIn and no NA TaxiOut: temp2 temp2 <- filter ( hflights , ! is.na ( TaxiIn ), ! is.na ( TaxiOut )) # Print the maximum taxiing difference of temp2 with summarise() summarise ( temp2 , max_taxi_diff = max ( abs ( TaxiIn - TaxiOut ))) 1 2 3 4 ## # A tibble: 1 \u00d7 1 ## max_taxi_diff ## &lt;int&gt; ## 1 160 dplyr aggregate functions first(x) ; the first element of vector x . last(x) ; the last element of vector x . nth(x, n) ; The n th element of vector x . n() ; The number of rows in the data.frame or group of observations that summarise() describes. n_distinct(x) ; The number of unique values in vector x . 1 2 # Generate summarizing statistics for hflights summarise ( hflights , n_obs = n (), n_carrier = n_distinct ( UniqueCarrier ), n_dest = n_distinct ( Dest ), dest100 = nth ( Dest , 100 )) 1 2 3 4 ## # A tibble: 1 \u00d7 4 ## n_obs n_carrier n_dest dest100 ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; ## 1 227496 15 116 DFW 1 2 3 4 5 # Filter hflights to keep all American Airline flights: aa aa <- filter ( hflights , UniqueCarrier == 'American' ) # Generate summarizing statistics for aa summarise ( aa , n_flights = n (), n_canc = sum ( Cancelled ), p_canc = n_canc / n_flights * 100 , avg_delay = mean ( ArrDelay , na.rm = TRUE )) 1 2 3 4 ## # A tibble: 1 \u00d7 4 ## n_flights n_canc p_canc avg_delay ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 3244 60 1.849568 0.8917558 Overview of syntax 1 2 3 4 5 # Write the 'piped' version of the English sentences hflights %>% mutate ( diff = TaxiOut - TaxiIn ) %>% filter ( ! is.na ( diff )) %>% summarise ( avg = mean ( diff )) 1 2 3 4 ## # A tibble: 1 \u00d7 1 ## avg ## &lt;dbl&gt; ## 1 8.992064 Drive or fly? Part 1 of 2 1 2 3 4 5 6 7 8 9 10 11 12 # Part 1, concerning the selection and creation of columns d <- hflights %>% select ( Dest , UniqueCarrier , Distance , ActualElapsedTime ) %>% mutate ( RealTime = ActualElapsedTime + 100 , mph = Distance / RealTime * 60 ) # Part 2, concerning flights that had an actual average speed of < 70 mph. d %>% filter ( ! is.na ( mph ), mph < 70 ) %>% summarise ( n_less = n (), n_dest = n_distinct ( Dest ), min_dist = min ( Distance ), max_dist = max ( Distance )) 1 2 3 4 ## # A tibble: 1 \u00d7 4 ## n_less n_dest min_dist max_dist ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 6726 13 79 305 Drive or fly? Part 2 of 2 1 2 3 4 5 # Solve the exercise using a combination of dplyr verbs and %>% hflights %>% #summarise(all_flights = n()) %>% filter ((( Distance / ( ActualElapsedTime + 100 ) * 60 ) < 105 ) | Cancelled == 1 | Diverted == 1 ) %>% summarise ( n_non = n (), p_non = n_non / 22751 * 100 , n_dest = n_distinct ( Dest ), min_dist = min ( Distance ), max_dist = max ( Distance )) 1 2 3 4 ## # A tibble: 1 \u00d7 5 ## n_non p_non n_dest min_dist max_dist ## &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 42400 186.3654 113 79 3904 Advanced piping exercise 1 2 3 4 # Count the number of overnight flights hflights %>% filter ( ArrTime < DepTime & ! is.na ( DepTime ) & ! is.na ( ArrTime )) %>% summarise ( n = n ()) 1 2 3 4 ## # A tibble: 1 \u00d7 1 ## n ## &lt;int&gt; ## 1 2718","title":"4, summarise and the Pipe Operator"},{"location":"data_wrangling/#5-group_by-and-working-with-data","text":"Unite and conquer using group_by 1 2 3 4 5 6 7 8 # Make an ordered per-carrier summary of hflights hflights %>% group_by ( UniqueCarrier ) %>% summarise ( n_flights = n (), n_canc = sum ( Cancelled == 1 ), p_canc = mean ( Cancelled == 1 ) * 100 , avg_delay = mean ( ArrDelay , na.rm = TRUE )) %>% arrange ( avg_delay , p_canc ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 ## # A tibble: 15 \u00d7 5 ## UniqueCarrier n_flights n_canc p_canc avg_delay ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 US_Airways 4082 46 1.1268986 -0.6307692 ## 2 American 3244 60 1.8495684 0.8917558 ## 3 AirTran 2139 21 0.9817672 1.8536239 ## 4 Alaska 365 0 0.0000000 3.1923077 ## 5 Mesa 79 1 1.2658228 4.0128205 ## 6 Delta 2641 42 1.5903067 6.0841374 ## 7 Continental 70032 475 0.6782614 6.0986983 ## 8 American_Eagle 4648 135 2.9044750 7.1529751 ## 9 Atlantic_Southeast 2204 76 3.4482759 7.2569543 ## 10 Southwest 45343 703 1.5504047 7.5871430 ## 11 Frontier 838 6 0.7159905 7.6682692 ## 12 ExpressJet 73053 1132 1.5495599 8.1865242 ## 13 SkyWest 16061 224 1.3946828 8.6934922 ## 14 JetBlue 695 18 2.5899281 9.8588410 ## 15 United 2072 34 1.6409266 10.4628628 1 2 3 4 5 # Make an ordered per-day summary of hflights hflights %>% group_by ( DayOfWeek ) %>% summarise ( avg_taxi = mean ( TaxiIn + TaxiOut , na.rm = TRUE )) %>% arrange ( desc ( avg_taxi )) 1 2 3 4 5 6 7 8 9 10 ## # A tibble: 7 \u00d7 2 ## DayOfWeek avg_taxi ## &lt;int&gt; &lt;dbl&gt; ## 1 1 21.77027 ## 2 2 21.43505 ## 3 4 21.26076 ## 4 3 21.19055 ## 5 5 21.15805 ## 6 7 20.93726 ## 7 6 20.43061 Combine group_by with mutate 1 2 3 4 5 6 7 # Solution to first instruction hflights %>% filter ( ! is.na ( ArrDelay )) %>% group_by ( UniqueCarrier ) %>% summarise ( p_delay = sum ( ArrDelay > 0 ) / n ()) %>% mutate ( rank = rank ( p_delay )) %>% arrange ( rank ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 ## # A tibble: 15 \u00d7 3 ## UniqueCarrier p_delay rank ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 American 0.3030208 1 ## 2 AirTran 0.3112269 2 ## 3 US_Airways 0.3267990 3 ## 4 Atlantic_Southeast 0.3677511 4 ## 5 American_Eagle 0.3696714 5 ## 6 Delta 0.3871092 6 ## 7 JetBlue 0.3952452 7 ## 8 Alaska 0.4368132 8 ## 9 Southwest 0.4644557 9 ## 10 Mesa 0.4743590 10 ## 11 Continental 0.4907385 11 ## 12 ExpressJet 0.4943420 12 ## 13 United 0.4963109 13 ## 14 SkyWest 0.5350105 14 ## 15 Frontier 0.5564904 15 1 2 3 4 5 6 7 # Solution to second instruction hflights %>% filter ( ! is.na ( ArrDelay ), ArrDelay > 0 ) %>% group_by ( UniqueCarrier ) %>% summarise ( avg = mean ( ArrDelay )) %>% mutate ( rank = rank ( avg )) %>% arrange ( rank ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 ## # A tibble: 15 \u00d7 3 ## UniqueCarrier avg rank ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Mesa 18.67568 1 ## 2 Frontier 18.68683 2 ## 3 US_Airways 20.70235 3 ## 4 Continental 22.13374 4 ## 5 Alaska 22.91195 5 ## 6 SkyWest 24.14663 6 ## 7 ExpressJet 24.19337 7 ## 8 Southwest 25.27750 8 ## 9 AirTran 27.85693 9 ## 10 American 28.49740 10 ## 11 Delta 32.12463 11 ## 12 United 32.48067 12 ## 13 American_Eagle 38.75135 13 ## 14 Atlantic_Southeast 40.24231 14 ## 15 JetBlue 45.47744 15 Advanced group_by exercises 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 # Which plane (by tail number) flew out of Houston the most times? How many times? adv1 adv1 <- hflights %>% group_by ( TailNum ) %>% summarise ( n = n ()) %>% filter ( n == max ( n )) # How many airplanes only flew to one destination from Houston? adv2 adv2 <- hflights %>% group_by ( TailNum ) %>% summarise ( ndest = n_distinct ( Dest )) %>% filter ( ndest == 1 ) %>% summarise ( nplanes = n ()) # Find the most visited destination for each carrier: adv3 adv3 <- hflights %>% group_by ( UniqueCarrier , Dest ) %>% summarise ( n = n ()) %>% mutate ( rank = rank ( desc ( n ))) %>% filter ( rank == 1 ) # Find the carrier that travels to each destination the most: adv4 adv4 <- hflights %>% group_by ( Dest , UniqueCarrier ) %>% summarise ( n = n ()) %>% mutate ( rank = rank ( desc ( n ))) %>% filter ( rank == 1 ) dplyr deals with different types 1 2 3 # Use summarise to calculate n_carrier s2 <- hflights %>% summarise ( n_carrier = n_distinct ( UniqueCarrier )) dplyr and mySQL databases Code only. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 # set up a src that connects to the mysql database (src_mysql is provided by dplyr) my_db <- src_mysql ( dbname = 'dplyr' , host = 'dplyr.csrrinzqubik.us-east-1.rds.amazonaws.com' , port = 3306 , user = 'dplyr' , password = 'dplyr' ) # and reference a table within that src: nycflights is now available as an R object that references to the remote nycflights table nycflights <- tbl ( my_db , 'dplyr' ) # glimpse at nycflights glimpse ( nycflights ) # Calculate the grouped summaries detailed in the instructions nycflights %>% group_by ( carrier ) %>% summarise ( n_flights = n (), avg_delay = mean ( arr_delay )) %>% arrange ( avg_delay )","title":"5, group_by and working with data"},{"location":"data_wrangling/#adding-tidyr-functions","text":"complete drop_na expand extract extract_numeric complete fill full_seq gather nest replace_na separate separate_rows separate_rows_ smiths spread table1 unite unnest who","title":"Adding tidyr Functions"},{"location":"data_wrangling/#extension-joining-data-in-r-with-dplyr","text":"1 2 3 4 5 6 7 8 9 ## 1, Mutating Joins ## 2, Filtering Joins and Set Operations ## 3, Assembling Data ## 4, Advanced Joining ## 5, Case Study","title":"Extension: 'Joining Data in R with dplyr'"},{"location":"embedding/","text":"Embedding HTML outputs in general \u00b6 We can embed just about any HTML snippet with <iframe> or <embed> . Embedding static visualizations from packages such as ggplot2 , ggmap or rworldmap , to name a few, can be done with image outputs (.png, .pdf) or HTML outputs. For htmlwidgets, we can work with HTML outputs, as we demonstrate further down, or with the Shiny server. In both cases, the final results are interactive. Some interactive visualization packages, like ggvis , require the Shiny server to be fully interactive. The HTML document heading: 1 2 3 4 5 --- output: html_document: code_folding: hide --- If the figures are generated with fig.width=5, fig.height=5 ; they should fit in a 520x520px box. In some case, we might need more room\u2026 Embedding outputs from\u2026 \u00b6 \u2026the leaflet package \u00b6 1 <iframe seamless src= \"../img/leaflet_frag.html\" width= 520px height= 520px ></iframe> 1 <embed seamless src= \"../img/leaflet_frag.html\" width= 520px height= 520px ></embed> \u2026the dygraphs package \u00b6 1 <embed seamless src= \"../img/dygraphs_frag.html\" width= 520px height= 520px ></embed> \u2026the plotly package \u00b6 1 <embed seamless src= \"../img/plotly_frag.html\" width= 520px height= 520px ></embed> \u2026the rbokeh package \u00b6 1 <embed seamless src= \"../img/rbokeh_frag.html\" width= 540px height= 540px ></embed> \u2026the highcharters package \u00b6 Highcharts (www.highcharts.com) is a Highsoft software product which is not free for commercial and Governmental use. 1 <embed seamless src= \"../img/highcharters_frag.html\" width= 600px height= 600px ></embed> \u2026the datatables package \u00b6 1 <embed seamless src= \"../img/datatable_frag.html\" width= 700px height= 600px ></embed>","title":"Embedding HTML into HTML"},{"location":"embedding/#embedding-html-outputs-in-general","text":"We can embed just about any HTML snippet with <iframe> or <embed> . Embedding static visualizations from packages such as ggplot2 , ggmap or rworldmap , to name a few, can be done with image outputs (.png, .pdf) or HTML outputs. For htmlwidgets, we can work with HTML outputs, as we demonstrate further down, or with the Shiny server. In both cases, the final results are interactive. Some interactive visualization packages, like ggvis , require the Shiny server to be fully interactive. The HTML document heading: 1 2 3 4 5 --- output: html_document: code_folding: hide --- If the figures are generated with fig.width=5, fig.height=5 ; they should fit in a 520x520px box. In some case, we might need more room\u2026","title":"Embedding HTML outputs in general"},{"location":"embedding/#embedding-outputs-from","text":"","title":"Embedding outputs from..."},{"location":"embedding/#the-leaflet-package","text":"1 <iframe seamless src= \"../img/leaflet_frag.html\" width= 520px height= 520px ></iframe> 1 <embed seamless src= \"../img/leaflet_frag.html\" width= 520px height= 520px ></embed>","title":"...the leaflet package"},{"location":"embedding/#the-dygraphs-package","text":"1 <embed seamless src= \"../img/dygraphs_frag.html\" width= 520px height= 520px ></embed>","title":"...the dygraphs package"},{"location":"embedding/#the-plotly-package","text":"1 <embed seamless src= \"../img/plotly_frag.html\" width= 520px height= 520px ></embed>","title":"...the plotly package"},{"location":"embedding/#the-rbokeh-package","text":"1 <embed seamless src= \"../img/rbokeh_frag.html\" width= 540px height= 540px ></embed>","title":"...the rbokeh package"},{"location":"embedding/#the-highcharters-package","text":"Highcharts (www.highcharts.com) is a Highsoft software product which is not free for commercial and Governmental use. 1 <embed seamless src= \"../img/highcharters_frag.html\" width= 600px height= 600px ></embed>","title":"...the highcharters package"},{"location":"embedding/#the-datatables-package","text":"1 <embed seamless src= \"../img/datatable_frag.html\" width= 700px height= 600px ></embed>","title":"...the datatables package"},{"location":"embedding_2/","text":"Stand-alone .Rdm document \u00b6 It is one document with a header; the settings determine the output (HTML, Word, PDF) style and appearance: 1 2 3 4 --- title: \"Document only\" output: html_document --- Embedding .Rmd sub-documents into a .Rmd document \u00b6 It is also possible to create sub-documents to be casted in the main document. The sub-documents have to headers, no settings. They are not even html_fragment . They simply contain Markdown text strings and code chunks. The main document is like a stand-alone document. Its settings are applied to itself and its \u2018children\u2019. Within the main document, we call sub-documents with this code chunk (path and filename): 1 2 ```{r, child=\"sub1.Rmd\"} ``` We can add extra options: eval=FALSE , echo=FALSE , include=FALSE , etc. to bypass the main settings. We can compile sub-documents to test them. We compile the main document to wrap up the complete document (\u2018parent + children\u2019).","title":"Embedding Rmd into Rmd"},{"location":"embedding_2/#stand-alone-rdm-document","text":"It is one document with a header; the settings determine the output (HTML, Word, PDF) style and appearance: 1 2 3 4 --- title: \"Document only\" output: html_document ---","title":"Stand-alone .Rdm document"},{"location":"embedding_2/#embedding-rmd-sub-documents-into-a-rmd-document","text":"It is also possible to create sub-documents to be casted in the main document. The sub-documents have to headers, no settings. They are not even html_fragment . They simply contain Markdown text strings and code chunks. The main document is like a stand-alone document. Its settings are applied to itself and its \u2018children\u2019. Within the main document, we call sub-documents with this code chunk (path and filename): 1 2 ```{r, child=\"sub1.Rmd\"} ``` We can add extra options: eval=FALSE , echo=FALSE , include=FALSE , etc. to bypass the main settings. We can compile sub-documents to test them. We compile the main document to wrap up the complete document (\u2018parent + children\u2019).","title":"Embedding .Rmd sub-documents into a .Rmd document"},{"location":"fast_and_frugal_decision_trees_in_r_with_fftrees/","text":"Foreword Code snippets and excerpts from the tutorial. From DataCamp. Open the HTML file in a new tab. Why? \u00b6 Quickly decide: use an algorithm that can quickly understand and apply with minimal effort. For this reason, complex algorithms such as random forests, and even regression are not viable. A fast-and-frugal tree is an extremely simple decision tree that anyone can easily understand, learn, and use to make fast decisions with minimal effort. Explore the Heart Disease Data \u00b6 The FFTrees package contains two datasets: heart.train to create (aka. train) fast-and-frugal trees, and heart.test to test their prediction performance. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 # Load the package library(\"FFTrees\") # Print the first few rows of the training dataframe head(heart.train, 3) ## age sex cp trestbps chol fbs restecg thalach exang oldpeak slope ## 94 44 0 np 108 141 0 normal 175 0 0.6 flat ## 78 51 0 np 140 308 0 hypertrophy 142 0 1.5 up ## 167 52 1 np 138 223 0 normal 169 0 0.0 up ## ca thal diagnosis ## 94 0 normal 0 ## 78 1 normal 0 ## 167 1 normal 0 The train dataframe contains data from several patients, each categorized by demographic features such as age and sex, as well as the results of medical tests and measures such as their cholesterol level ( chol ) and their type of chest pain ( cp ). The key variable you want to predict is diagnosis , which is 1 for patients who are truly having heart attacks, and 0 for those who are not. The goal of your fast-and-frugal tree will be to find a few key variables in the data that can quickly and accurately predict diagnosis. Create an FFTrees object \u00b6 As with any algorithm, we need a function (formula), a train set, and a test set. We add a title and labels to the results. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 # Create an FFTrees object called `heart_FFT` heart_FFT &lt;- FFTrees(formula = diagnosis ~ ., data = heart.train, data.test = heart.test, main = \"ER Decisions\", decision.labels = c(\"Stable\", \"H Attack\")) # Print the summary statistics heart_FFT ## ER Decisions ## FFT #1 predicts diagnosis using 3 cues: {thal,cp,ca} ## ## [1] If thal = {rd,fd}, predict H Attack. ## [2] If cp != {a}, predict Stable. ## [3] If ca &lt;= 0, predict Stable, otherwise, predict H Attack. ## ## train test ## cases :n 150.00 153.00 ## speed :mcu 1.74 1.73 ## frugality :pci 0.88 0.88 ## accuracy :acc 0.80 0.82 ## weighted :wacc 0.80 0.82 ## sensitivity :sens 0.82 0.88 ## specificity :spec 0.79 0.76 ## ## pars: algorithm = 'ifan', goal = 'wacc', goal.chase = 'bacc', sens.w = 0.5, max.levels = 4 thal is either rd or fd , the dependent variable H Attack . cp is not equal to a and the model has converged towards an optimum ( Stable ). The tree used an average of 1.74 pieces of information to classify cases with the train set. The tree has then an accuracy of 82% with the test set. Plotting Fast-And-Frugal Trees \u00b6 The visualization greatly improves the understanding. 1 2 # Visualise the tree applied to the test data heart.test plot(heart_FFT, data = \"test\") On the top row, you can see that there were 153 patients (cases) were in the training data, where 73 patients were truly having heart attacks (48%), and 80 patients were not (52%). In the middle row, you see exactly how the tree makes decisions for each of the patients using easy\u2013to\u2013understand icon arrays. For example, you see that 72 patients suspected of having heart attacks were (virtually) sent to the CCU after the first question, where 18 were not having heart attacks (false\u2013alarms), and 54 were having heart attacks (hits). In the bottom row of the plot, you can see aggregate summary statistics for the tree. On the bottom row, you have a 2 x 2 confusion matrix, which shows you a summary of how well the tree was able to classify patients, levels indicating overall summary statistics, and an ROC curve which compares the accuracy of the tree to other algorithms such as logistic regression (LR) and random forests (RF). Here, where the fast-and-frugal tree is represented by the green circle \u201c1\u201d, you can see that the fast-and-frugal tree had a higher sensitivity than logistic regression and random forests, but at a cost of a lower specificity. Creating and Testing a Custom Tree \u00b6 We can easily describe fast-and-frugal trees \u2018in words\u2019: \u201cA better rule would be to use the cues cholesterol, age, and slope.\u201d If cholesterol > 300, decide Heart Attack. If age < 50, decide Stable. If slope is either up or flat, predict Attack, otherwise, predict Stable. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 # Create Heidi's custom FFT custom_FFT &lt;- FFTrees(formula = diagnosis ~ ., data = heart.train, data.test = heart.test, main = \"Heidi's Tree\", decision.labels = c(\"Stable\", \"Attack\"), my.tree = \"If chol &gt; 300, predict Attack. If age &lt; 50, predict Stable. If slope = {up, flat} predict Attack. Otherwise, predict Stable.\") # Print the summary statistics custom_FFT ## Heidi's Tree ## FFT #1 predicts diagnosis using 3 cues: {chol,age,slope} ## ## [1] If chol &gt; 300, predict Attack. ## [2] If age &lt; 50, predict Stable. ## [3] If slope != {up,flat}, predict Stable, otherwise, predict Attack. ## ## train test ## cases :n 150.00 153.00 ## speed :mcu 2.41 2.49 ## frugality :pci 0.83 0.82 ## accuracy :acc 0.59 0.54 ## weighted :wacc 0.61 0.55 ## sensitivity :sens 0.80 0.77 ## specificity :spec 0.42 0.34 ## ## pars: algorithm = 'ifan', goal = 'wacc', goal.chase = 'bacc', sens.w = 0.5, max.levels = 4 # Plot Heidi's tree and accuracy statistics plot(custom_FFT, data = \"test\") The new tree is much, much worse than the tree the internal algorithm came up with. While the tree generated by FFTrees had an overall accuracy of 82%, the new tree is only 54% accurate! Moreover, you can see very few patients (only 21) are classified as having a heart attack after the first node based on their cholesterol level, and of those, only 12 / 21 (57%) were really having heart attacks. In contrast, for the tree created by FFTrees , a full 72 patients are classified after the first node based on their value of thal , and of these, 75% were truly having heart attacks. More \u00b6 We can use trees to predict classes (and their probabilities) for new datasets, and create trees that minimise different classification error costs (for example, when the cost of a miss is much higher than the cost of a false alarm). Check out the package vignette by running FFTrees.guide() . 1 FFTrees.guide() Summary \u00b6 Fast-and-frugal decision trees are great options when you need a simple, transparent decision algorithm that can easily be communicated and applied, either by a person or a computer. Although fast-and-frugal trees are great for medical decisions (Green & Mehr, 1997), they can be created from any dataset with a binary criterion, from predicting whether or not a bank will fail (Neth et al., 2014), to predicting a judge\u2019s bailing decisions (Dhami & Ayton, 2001). Dhami, Mandeep K, and Peter Ayton. 2001. \u201cBailing and Jailing the Fast and Frugal Way.\u201d Journal of Behavioral Decision Making 14 (2). Wiley Online Library: 141 - 168. Galesic, Mirta, Rocio Garcia-Retamero, and Gerd Gigerenzer. 2009. \u201cUsing Icon Arrays to Communicate Medical Risks: Overcoming Low Numeracy.\u201d Health Psychology 28 (2). American Psychological Association: 210. Green, Lee, and David R Mehr. 1997. \u201cWhat Alters Physicians\u2019 Decisions to Admit to the Coronary Care Unit\u201d. Journal of Family Practice 45 (3). [New York, Appleton-Century-Crofts]: 219 - 226. Martignon, Laura, Oliver Vitouch, Masanori Takezawa, and Malcolm R Forster. 2003. \u201cNaive and yet Enlightened: From Natural Frequencies to Fast and Frugal Decision Trees.\u201d Thinking: Pychological Perspectives on Reasoning, Judgment and Decision Making. John Wiley & Sons, Ltd, 189 - 211. Neth, Hansj\u00f6rg, Bj\u00f6rn Meder, Amit Kothiyal, and Gerd Gigerenzer. 2014. \u201cHomo Heuristicus in the Financial World: From Risk Management to Managing Uncertainty.\u201d Journal of Risk Management in Financial Institutions 7 (2). Henry Stewart Publications: 134 - 144. Phillips, Nathaniel D, Hansj\u00f6rg Neth, Jan K Woike, and Wolfgang Gaissmaier. 2017. \u201cFFTrees: A Toolbox to Create, Visualize, and Evaluate Fast-and-Frugal Decision Trees.\u201d Judgment and Decision Making 12 (4). Society for Judgment & Decision Making: 344.","title":"Fast-and-Frugal Decision Trees"},{"location":"fast_and_frugal_decision_trees_in_r_with_fftrees/#why","text":"Quickly decide: use an algorithm that can quickly understand and apply with minimal effort. For this reason, complex algorithms such as random forests, and even regression are not viable. A fast-and-frugal tree is an extremely simple decision tree that anyone can easily understand, learn, and use to make fast decisions with minimal effort.","title":"Why?"},{"location":"fast_and_frugal_decision_trees_in_r_with_fftrees/#explore-the-heart-disease-data","text":"The FFTrees package contains two datasets: heart.train to create (aka. train) fast-and-frugal trees, and heart.test to test their prediction performance. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 # Load the package library(\"FFTrees\") # Print the first few rows of the training dataframe head(heart.train, 3) ## age sex cp trestbps chol fbs restecg thalach exang oldpeak slope ## 94 44 0 np 108 141 0 normal 175 0 0.6 flat ## 78 51 0 np 140 308 0 hypertrophy 142 0 1.5 up ## 167 52 1 np 138 223 0 normal 169 0 0.0 up ## ca thal diagnosis ## 94 0 normal 0 ## 78 1 normal 0 ## 167 1 normal 0 The train dataframe contains data from several patients, each categorized by demographic features such as age and sex, as well as the results of medical tests and measures such as their cholesterol level ( chol ) and their type of chest pain ( cp ). The key variable you want to predict is diagnosis , which is 1 for patients who are truly having heart attacks, and 0 for those who are not. The goal of your fast-and-frugal tree will be to find a few key variables in the data that can quickly and accurately predict diagnosis.","title":"Explore the Heart Disease Data"},{"location":"fast_and_frugal_decision_trees_in_r_with_fftrees/#create-an-fftrees-object","text":"As with any algorithm, we need a function (formula), a train set, and a test set. We add a title and labels to the results. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 # Create an FFTrees object called `heart_FFT` heart_FFT &lt;- FFTrees(formula = diagnosis ~ ., data = heart.train, data.test = heart.test, main = \"ER Decisions\", decision.labels = c(\"Stable\", \"H Attack\")) # Print the summary statistics heart_FFT ## ER Decisions ## FFT #1 predicts diagnosis using 3 cues: {thal,cp,ca} ## ## [1] If thal = {rd,fd}, predict H Attack. ## [2] If cp != {a}, predict Stable. ## [3] If ca &lt;= 0, predict Stable, otherwise, predict H Attack. ## ## train test ## cases :n 150.00 153.00 ## speed :mcu 1.74 1.73 ## frugality :pci 0.88 0.88 ## accuracy :acc 0.80 0.82 ## weighted :wacc 0.80 0.82 ## sensitivity :sens 0.82 0.88 ## specificity :spec 0.79 0.76 ## ## pars: algorithm = 'ifan', goal = 'wacc', goal.chase = 'bacc', sens.w = 0.5, max.levels = 4 thal is either rd or fd , the dependent variable H Attack . cp is not equal to a and the model has converged towards an optimum ( Stable ). The tree used an average of 1.74 pieces of information to classify cases with the train set. The tree has then an accuracy of 82% with the test set.","title":"Create an FFTrees object"},{"location":"fast_and_frugal_decision_trees_in_r_with_fftrees/#plotting-fast-and-frugal-trees","text":"The visualization greatly improves the understanding. 1 2 # Visualise the tree applied to the test data heart.test plot(heart_FFT, data = \"test\") On the top row, you can see that there were 153 patients (cases) were in the training data, where 73 patients were truly having heart attacks (48%), and 80 patients were not (52%). In the middle row, you see exactly how the tree makes decisions for each of the patients using easy\u2013to\u2013understand icon arrays. For example, you see that 72 patients suspected of having heart attacks were (virtually) sent to the CCU after the first question, where 18 were not having heart attacks (false\u2013alarms), and 54 were having heart attacks (hits). In the bottom row of the plot, you can see aggregate summary statistics for the tree. On the bottom row, you have a 2 x 2 confusion matrix, which shows you a summary of how well the tree was able to classify patients, levels indicating overall summary statistics, and an ROC curve which compares the accuracy of the tree to other algorithms such as logistic regression (LR) and random forests (RF). Here, where the fast-and-frugal tree is represented by the green circle \u201c1\u201d, you can see that the fast-and-frugal tree had a higher sensitivity than logistic regression and random forests, but at a cost of a lower specificity.","title":"Plotting Fast-And-Frugal Trees"},{"location":"fast_and_frugal_decision_trees_in_r_with_fftrees/#creating-and-testing-a-custom-tree","text":"We can easily describe fast-and-frugal trees \u2018in words\u2019: \u201cA better rule would be to use the cues cholesterol, age, and slope.\u201d If cholesterol > 300, decide Heart Attack. If age < 50, decide Stable. If slope is either up or flat, predict Attack, otherwise, predict Stable. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 # Create Heidi's custom FFT custom_FFT &lt;- FFTrees(formula = diagnosis ~ ., data = heart.train, data.test = heart.test, main = \"Heidi's Tree\", decision.labels = c(\"Stable\", \"Attack\"), my.tree = \"If chol &gt; 300, predict Attack. If age &lt; 50, predict Stable. If slope = {up, flat} predict Attack. Otherwise, predict Stable.\") # Print the summary statistics custom_FFT ## Heidi's Tree ## FFT #1 predicts diagnosis using 3 cues: {chol,age,slope} ## ## [1] If chol &gt; 300, predict Attack. ## [2] If age &lt; 50, predict Stable. ## [3] If slope != {up,flat}, predict Stable, otherwise, predict Attack. ## ## train test ## cases :n 150.00 153.00 ## speed :mcu 2.41 2.49 ## frugality :pci 0.83 0.82 ## accuracy :acc 0.59 0.54 ## weighted :wacc 0.61 0.55 ## sensitivity :sens 0.80 0.77 ## specificity :spec 0.42 0.34 ## ## pars: algorithm = 'ifan', goal = 'wacc', goal.chase = 'bacc', sens.w = 0.5, max.levels = 4 # Plot Heidi's tree and accuracy statistics plot(custom_FFT, data = \"test\") The new tree is much, much worse than the tree the internal algorithm came up with. While the tree generated by FFTrees had an overall accuracy of 82%, the new tree is only 54% accurate! Moreover, you can see very few patients (only 21) are classified as having a heart attack after the first node based on their cholesterol level, and of those, only 12 / 21 (57%) were really having heart attacks. In contrast, for the tree created by FFTrees , a full 72 patients are classified after the first node based on their value of thal , and of these, 75% were truly having heart attacks.","title":"Creating and Testing a Custom Tree"},{"location":"fast_and_frugal_decision_trees_in_r_with_fftrees/#more","text":"We can use trees to predict classes (and their probabilities) for new datasets, and create trees that minimise different classification error costs (for example, when the cost of a miss is much higher than the cost of a false alarm). Check out the package vignette by running FFTrees.guide() . 1 FFTrees.guide()","title":"More"},{"location":"fast_and_frugal_decision_trees_in_r_with_fftrees/#summary","text":"Fast-and-frugal decision trees are great options when you need a simple, transparent decision algorithm that can easily be communicated and applied, either by a person or a computer. Although fast-and-frugal trees are great for medical decisions (Green & Mehr, 1997), they can be created from any dataset with a binary criterion, from predicting whether or not a bank will fail (Neth et al., 2014), to predicting a judge\u2019s bailing decisions (Dhami & Ayton, 2001). Dhami, Mandeep K, and Peter Ayton. 2001. \u201cBailing and Jailing the Fast and Frugal Way.\u201d Journal of Behavioral Decision Making 14 (2). Wiley Online Library: 141 - 168. Galesic, Mirta, Rocio Garcia-Retamero, and Gerd Gigerenzer. 2009. \u201cUsing Icon Arrays to Communicate Medical Risks: Overcoming Low Numeracy.\u201d Health Psychology 28 (2). American Psychological Association: 210. Green, Lee, and David R Mehr. 1997. \u201cWhat Alters Physicians\u2019 Decisions to Admit to the Coronary Care Unit\u201d. Journal of Family Practice 45 (3). [New York, Appleton-Century-Crofts]: 219 - 226. Martignon, Laura, Oliver Vitouch, Masanori Takezawa, and Malcolm R Forster. 2003. \u201cNaive and yet Enlightened: From Natural Frequencies to Fast and Frugal Decision Trees.\u201d Thinking: Pychological Perspectives on Reasoning, Judgment and Decision Making. John Wiley & Sons, Ltd, 189 - 211. Neth, Hansj\u00f6rg, Bj\u00f6rn Meder, Amit Kothiyal, and Gerd Gigerenzer. 2014. \u201cHomo Heuristicus in the Financial World: From Risk Management to Managing Uncertainty.\u201d Journal of Risk Management in Financial Institutions 7 (2). Henry Stewart Publications: 134 - 144. Phillips, Nathaniel D, Hansj\u00f6rg Neth, Jan K Woike, and Wolfgang Gaissmaier. 2017. \u201cFFTrees: A Toolbox to Create, Visualize, and Evaluate Fast-and-Frugal Decision Trees.\u201d Judgment and Decision Making 12 (4). Society for Judgment & Decision Making: 344.","title":"Summary"},{"location":"frequentist_bayesian/","text":"Foreword Notes. So what does Bayesian statistics mean for A/B testing? First, let\u2019s summarize Bayesian and Frequentist approaches, and what the difference between them is. Frequentist \u00b6 Using a Frequentist method means making predictions on underlying truths of the experiment using only data from the current experiment. We learn frequentist statistics in entry-level statistics courses. A t-test, where we ask, \u201cIs this variation different from the control?\u201d is a basic building block of this approach. The focus is on the observations alone; no other data is used and no judgment. The procedure is objective and based solely on the test data and the assumed model. Bayesian \u00b6 In Bayesian statistics, past knowledge of similar experiments is encoded into a statistical device known as a prior, and this prior is combined with current experiment data to make a conclusion on the test at hand. So, the biggest distinction is that Bayesian probability specifies that there is some prior probability. A fundamental aspect of Bayesian inference is updating your beliefs in light of new evidence. Essentially, you start out with a prior belief and then update it in light of new evidence. For example, restaurant owners know that if by 5 p.m. there are 50 reservations, then they can predict there will be around 250 covers for the night. This is a prior and can be updated with new sets of data. Conclusion \u00b6 In the Bayesian approach, the parameters that we are trying to estimate, are treated as random variables having some known prior distribution. In the Frequentist approach, they are a fixed but unknown; there is no probability associated with it. Random variables are governed by their parameters (mean, variance, etc), and distributions (Gaussian, Poisson, binomial, etc). The prior is just the prior belief about these parameters. In this way, we can think of the Bayesian approach to treating probabilities as degrees of belief, rather than as frequencies generated by some unknown process. In summary, the difference is that in the Bayesian view, a probability is assigned to a hypothesis. In the Frequentist view, a hypothesis is tested without being assigned a probability. Frequentist Bayesian Definition of probability Long-run expected frequency in repeated (actual or hypothetical) experiments (law of large numbers). Relative degree of belief in the state of the world. Point estimate Maximum likelihood estimate. Mean, mode, or median of the posterior probability distribution. Confidence intervals for parameters Based on the likelihood ratio test; i.e., the expected probability distribution of the maximum likelihood estimate over many experiments. Credible intervals based on the posterior probability distribution. Confidence intervals for non-parameters Based on likelihood profile/ratio test, or by resampling from the sampling distribution of the parameter. Calculated directly from the distribution of parameters. Model selection Discard terms that are not significantly different from a nested (null) model at a previously set confidence interval level. Retain terms in models, on the argument that processes are not absent simply because they are not statistically significant. Difficulties Confidence intervals are confusing (range that will contain the true value in a proportion alpha or repeated experiments); rejection of model terms of non-significance. Subjectivity; need to specify priors.","title":"Frequentist vs. Bayesian"},{"location":"frequentist_bayesian/#frequentist","text":"Using a Frequentist method means making predictions on underlying truths of the experiment using only data from the current experiment. We learn frequentist statistics in entry-level statistics courses. A t-test, where we ask, \u201cIs this variation different from the control?\u201d is a basic building block of this approach. The focus is on the observations alone; no other data is used and no judgment. The procedure is objective and based solely on the test data and the assumed model.","title":"Frequentist"},{"location":"frequentist_bayesian/#bayesian","text":"In Bayesian statistics, past knowledge of similar experiments is encoded into a statistical device known as a prior, and this prior is combined with current experiment data to make a conclusion on the test at hand. So, the biggest distinction is that Bayesian probability specifies that there is some prior probability. A fundamental aspect of Bayesian inference is updating your beliefs in light of new evidence. Essentially, you start out with a prior belief and then update it in light of new evidence. For example, restaurant owners know that if by 5 p.m. there are 50 reservations, then they can predict there will be around 250 covers for the night. This is a prior and can be updated with new sets of data.","title":"Bayesian"},{"location":"frequentist_bayesian/#conclusion","text":"In the Bayesian approach, the parameters that we are trying to estimate, are treated as random variables having some known prior distribution. In the Frequentist approach, they are a fixed but unknown; there is no probability associated with it. Random variables are governed by their parameters (mean, variance, etc), and distributions (Gaussian, Poisson, binomial, etc). The prior is just the prior belief about these parameters. In this way, we can think of the Bayesian approach to treating probabilities as degrees of belief, rather than as frequencies generated by some unknown process. In summary, the difference is that in the Bayesian view, a probability is assigned to a hypothesis. In the Frequentist view, a hypothesis is tested without being assigned a probability. Frequentist Bayesian Definition of probability Long-run expected frequency in repeated (actual or hypothetical) experiments (law of large numbers). Relative degree of belief in the state of the world. Point estimate Maximum likelihood estimate. Mean, mode, or median of the posterior probability distribution. Confidence intervals for parameters Based on the likelihood ratio test; i.e., the expected probability distribution of the maximum likelihood estimate over many experiments. Credible intervals based on the posterior probability distribution. Confidence intervals for non-parameters Based on likelihood profile/ratio test, or by resampling from the sampling distribution of the parameter. Calculated directly from the distribution of parameters. Model selection Discard terms that are not significantly different from a nested (null) model at a previously set confidence interval level. Retain terms in models, on the argument that processes are not absent simply because they are not statistically significant. Difficulties Confidence intervals are confusing (range that will contain the true value in a proportion alpha or repeated experiments); rejection of model terms of non-significance. Subjectivity; need to specify priors.","title":"Conclusion"},{"location":"gganimate/","text":"Foreword Snippets and results. The gganimate package \u00b6 The package does not make interactive charts, but introduces a time dimension in the display of static charts. Static chart \u00b6 We add a time dimension with frame = year . However, we are not using it and the data points look bunched up! 1 2 3 4 5 6 7 8 9 library(gapminder) library(ggplot2) theme_set(theme_bw()) p &lt;- ggplot(gapminder, aes(gdpPercap, lifeExp, size = pop, color = continent, frame = year)) + geom_point() + scale_x_log10() p Dynamising a static chart \u00b6 Moving frames can turn into a motion pictures! Let\u2019s use frame = year . 1 2 3 library(gganimate) gganimate(p) We can see the results further down. Saving the animation \u00b6 1 2 3 4 5 gganimate(p, \"img/gganimate/output.mp4\") gganimate(p, \"img/gganimate/output.swf\") gganimate(p, \"img/gganimate/output.html\") gganimate(p, \"output.gif\") Rendering the animation (.gif): And again\u2026 \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 library(googleVis) head(Fruits, 3) ## Fruit Year Location Sales Expenses Profit Date ## 1 Apples 2008 West 98 78 20 2008-12-31 ## 2 Apples 2009 West 111 79 32 2009-12-31 ## 3 Apples 2010 West 89 76 13 2010-12-31 library(ggplot2) library(gganimate) Fruits_a &lt;- ggplot(Fruits, aes(x=Sales, y=Expenses, size = Profit, color = Fruit, frame = Year)) + geom_point() + geom_path(aes(cumulative = TRUE, group = Fruit)) + facet_wrap(~Fruit) Fruits_a 1 2 3 gganimate(Fruits_a, interval = 5) gganimate(Fruits_a, \"Fruits_a.gif\") Rendering the animation (.gif):","title":"gganimate"},{"location":"gganimate/#the-gganimate-package","text":"The package does not make interactive charts, but introduces a time dimension in the display of static charts.","title":"The gganimate package"},{"location":"gganimate/#static-chart","text":"We add a time dimension with frame = year . However, we are not using it and the data points look bunched up! 1 2 3 4 5 6 7 8 9 library(gapminder) library(ggplot2) theme_set(theme_bw()) p &lt;- ggplot(gapminder, aes(gdpPercap, lifeExp, size = pop, color = continent, frame = year)) + geom_point() + scale_x_log10() p","title":"Static chart"},{"location":"gganimate/#dynamising-a-static-chart","text":"Moving frames can turn into a motion pictures! Let\u2019s use frame = year . 1 2 3 library(gganimate) gganimate(p) We can see the results further down.","title":"Dynamising a static chart"},{"location":"gganimate/#saving-the-animation","text":"1 2 3 4 5 gganimate(p, \"img/gganimate/output.mp4\") gganimate(p, \"img/gganimate/output.swf\") gganimate(p, \"img/gganimate/output.html\") gganimate(p, \"output.gif\") Rendering the animation (.gif):","title":"Saving the animation"},{"location":"gganimate/#and-again","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 library(googleVis) head(Fruits, 3) ## Fruit Year Location Sales Expenses Profit Date ## 1 Apples 2008 West 98 78 20 2008-12-31 ## 2 Apples 2009 West 111 79 32 2009-12-31 ## 3 Apples 2010 West 89 76 13 2010-12-31 library(ggplot2) library(gganimate) Fruits_a &lt;- ggplot(Fruits, aes(x=Sales, y=Expenses, size = Profit, color = Fruit, frame = Year)) + geom_point() + geom_path(aes(cumulative = TRUE, group = Fruit)) + facet_wrap(~Fruit) Fruits_a 1 2 3 gganimate(Fruits_a, interval = 5) gganimate(Fruits_a, \"Fruits_a.gif\") Rendering the animation (.gif):","title":"And again..."},{"location":"googlevis/","text":"The googleVis package \u00b6 The package provides an interface to Google\u2019s chart tools, allowing users to create interactive charts based on data frames. It included maps. The interactive maps are displayed in a browser. We can plot a complete set of interactive graphs and embed them into a web page. Some motion charts cannot be displays in tablets and mobile phones (using HTML5) because they are rendered with Flash; Flash has to be installed on a PC. Examples . Charts: line, bar, column, area, stepped area, combo, scatter, bubble, customizing, candlestick (or boxplot), pie, gauge, annotation, Sankey, histogram, and motion (GapMinder-like) Maps: intensity, geo, choropleth, marker, Google Maps. Table, organizational chart, tree map, calendar, timeline, merging. Motion charts and some maps only work in Flash, not in HTML5 as with tablets and mobile phones). Gallery . Documentation . As above. Introduction . Roles . Trendlines . Markdown . In R, run a demo with demo(googleVis) . Always cite the package: 1 citation ( \"googleVis\" ) Suppress the message \u00b6 We normally load the library\u2026 1 library ( googleVis ) \u2026but we can also suppress the message when loading the package. 1 suppressPackageStartupMessages ( library ( googleVis )) Printing \u00b6 Generate the chart. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 library ( googleVis ) head ( Fruits , 3 ) M <- gvisMotionChart ( Fruits , idvar = 'Fruit' , timevar = 'Year' , options = list ( width = 400 , height = 350 )) # xvar = # yvar = # colorvar = # sizevar = # date.format = Emulate the chart\u2026 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 # In the browser, with a tag underneath plot ( M ) # In the console (code only), with a tag underneath M # Header part # Basic html and formatting tags print ( M , tag = 'header' ) # Actual Google visualization code # Can be copy-paste in a markdown or html document print ( M , tag = 'chart' ) # Header + visualization code = what we see in the browser # Components of the chart print ( M , tag = 'jsChart' ) # Basic chart caption and html footer (what is underneath) print ( M , tag = 'caption' ) # Save it locally print ( M , file = \"GoogleVis/M.html\" ) # Or #cat(M$html$chart, file = \"GoogleVis/M.html\") Embedding a chart/map into a static website \u00b6 The chart can be standing alone as an image file; even be embedded within a text. In RStudio, set the working directory with setwd(' ') . Set the option to print the output in a html file. 1 2 3 4 { r , message = FALSE } library ( googleVis ) op <- options ( gvis.plot.tag = 'chart' ) Set options back to original options. 1 options ( op ) Generate the chart. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 library ( googleVis ) # Set the option to print here (alternative) options ( gvis.plot.tag = 'chart' ) # Create the chart M <- gvisMotionChart ( Fruits , idvar = 'Fruit' , timevar = 'Year' , options = list ( width = 400 , height = 350 )) # Do not print the chart in the browser #plot(M) # in the browser # Print the chart as a html file cat ( M $ html $ chart , file = \"M.html\" ) The file has several parts: JavaScript and HTML. The part that displays the chart is in the end. 1 2 3 4 5 <!-- divChart --> < div id = \"MotionChartID2cd02805862d\" style = \"width: 400; height: 350;\" > </ div > Following this bullet, in the Markdown document, paste the M.html code. First, open the HTML file to copy the code. Modify the markup language with more styling parameters. 1 2 3 < div id = \"MotionChartID2cd02805862d\" style = \"width: 400; height: 350; float:left;\" ; > </ div > The HTML snippet must follow the JavaScript snippet in the document. Move the HTML snippet within a text (the text can separate both snippets). 1 2 3 4 5 Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. < div id = \"MotionChartID2cd02805862d\" style = \"width: 400; height: 350; float:left;\" ; > </ div > Duis aute irure dolor... Here is the result: // jsData function gvisDataMotionChartID2cd02805862d () { var data = new google.visualization.DataTable(); var datajson = [ [ \"Apples\", 2008, \"West\", 98, 78, 20, \"2008-12-31\" ], [ \"Apples\", 2009, \"West\", 111, 79, 32, \"2009-12-31\" ], [ \"Apples\", 2010, \"West\", 89, 76, 13, \"2010-12-31\" ], [ \"Oranges\", 2008, \"East\", 96, 81, 15, \"2008-12-31\" ], [ \"Bananas\", 2008, \"East\", 85, 76, 9, \"2008-12-31\" ], [ \"Oranges\", 2009, \"East\", 93, 80, 13, \"2009-12-31\" ], [ \"Bananas\", 2009, \"East\", 94, 78, 16, \"2009-12-31\" ], [ \"Oranges\", 2010, \"East\", 98, 91, 7, \"2010-12-31\" ], [ \"Bananas\", 2010, \"East\", 81, 71, 10, \"2010-12-31\" ] ]; data.addColumn('string','Fruit'); data.addColumn('number','Year'); data.addColumn('string','Location'); data.addColumn('number','Sales'); data.addColumn('number','Expenses'); data.addColumn('number','Profit'); data.addColumn('string','Date'); data.addRows(datajson); return(data); } // jsDrawChart function drawChartMotionChartID2cd02805862d() { var data = gvisDataMotionChartID2cd02805862d(); var options = {}; options[\"width\"] = 400; options[\"height\"] = 350; options[\"state\"] = \"\"; var chart = new google.visualization.MotionChart( document.getElementById('MotionChartID2cd02805862d') ); chart.draw(data,options); } // jsDisplayChart (function() { var pkgs = window.__gvisPackages = window.__gvisPackages || []; var callbacks = window.__gvisCallbacks = window.__gvisCallbacks || []; var chartid = \"motionchart\"; // Manually see if chartid is in pkgs (not all browsers support Array.indexOf) var i, newPackage = true; for (i = 0; newPackage && i < pkgs.length; i++) { if (pkgs[i] === chartid) newPackage = false; } if (newPackage) pkgs.push(chartid); // Add the drawChart function to the global list of callbacks callbacks.push(drawChartMotionChartID2cd02805862d); })(); function displayChartMotionChartID2cd02805862d() { var pkgs = window.__gvisPackages = window.__gvisPackages || []; var callbacks = window.__gvisCallbacks = window.__gvisCallbacks || []; window.clearTimeout(window.__gvisLoad); // The timeout is set to 100 because otherwise the container div we are // targeting might not be part of the document yet window.__gvisLoad = setTimeout(function() { var pkgCount = pkgs.length; google.load(\"visualization\", \"1\", { packages:pkgs, callback: function() { if (pkgCount != pkgs.length) { // Race condition where another setTimeout call snuck in after us; if // that call added a package, we must not shift its callback return; } while (callbacks.length > 0) callbacks.shift()(); } }); }, 100); } // jsFooter Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum. Sed ut perspiciatis unde omnis iste natus error sit voluptatem accusantium doloremque laudantium, totam rem aperiam, eaque ipsa quae ab illo inventore veritatis et quasi architecto beatae vitae dicta sunt explicabo. Nemo enim ipsam voluptatem quia voluptas sit aspernatur aut odit aut fugit, sed quia consequuntur magni dolores eos qui ratione voluptatem sequi nesciunt. Neque porro quisquam est, qui dolorem ipsum quia dolor sit amet, consectetur, adipisci velit, sed quia non numquam eius modi tempora incidunt ut labore et dolore magnam aliquam quaerat voluptatem. Other static websites \u00b6 There is a procedure for embedding graphics in: WordPress. Google Sites. Blogger. Google Code wiki pages. Wikipedia. others websites. Dynamic websites \u00b6 The R.rsp package allows the integration of R code into html code. The rApache and brew packages support web application development using R and the Apache HTTP server. Rook is a lightweight web server interface for R. The shiny package builds interactive web application with R. Charts \u00b6 Consult the examples (further above); charts are similar to what we find in other packages. gvisMotionChart is exclusive to googleVis . gvisMotionChart \u00b6 1 head ( Fruits , 3 ) 1 2 3 4 5 # Output Fruit Year Location Sales Expenses Profit Date 1 Apples 2008 West 98 78 20 2008-12-31 2 Apples 2009 West 111 79 32 2009-12-31 3 Apples 2010 West 89 76 13 2010-12-31 1 2 3 4 5 6 7 8 9 10 11 12 M <- gvisMotionChart ( Fruits , idvar = 'Fruit' , timevar = 'Year' , options = list ( width = 400 , height = 350 )) # xvar = # yvar = # colorvar = # sizevar = # date.format = plot ( M ) // jsData function gvisDataMotionChartID336e668836ab () { var data = new google.visualization.DataTable(); var datajson = [ [ \"Apples\", 2008, \"West\", 98, 78, 20, \"2008-12-31\" ], [ \"Apples\", 2009, \"West\", 111, 79, 32, \"2009-12-31\" ], [ \"Apples\", 2010, \"West\", 89, 76, 13, \"2010-12-31\" ], [ \"Oranges\", 2008, \"East\", 96, 81, 15, \"2008-12-31\" ], [ \"Bananas\", 2008, \"East\", 85, 76, 9, \"2008-12-31\" ], [ \"Oranges\", 2009, \"East\", 93, 80, 13, \"2009-12-31\" ], [ \"Bananas\", 2009, \"East\", 94, 78, 16, \"2009-12-31\" ], [ \"Oranges\", 2010, \"East\", 98, 91, 7, \"2010-12-31\" ], [ \"Bananas\", 2010, \"East\", 81, 71, 10, \"2010-12-31\" ] ]; data.addColumn('string','Fruit'); data.addColumn('number','Year'); data.addColumn('string','Location'); data.addColumn('number','Sales'); data.addColumn('number','Expenses'); data.addColumn('number','Profit'); data.addColumn('string','Date'); data.addRows(datajson); return(data); } // jsDrawChart function drawChartMotionChartID336e668836ab() { var data = gvisDataMotionChartID336e668836ab(); var options = {}; options[\"width\"] = 400; options[\"height\"] = 350; options[\"state\"] = \"\"; var chart = new google.visualization.MotionChart( document.getElementById('MotionChartID336e668836ab') ); chart.draw(data,options); } // jsDisplayChart (function() { var pkgs = window.__gvisPackages = window.__gvisPackages || []; var callbacks = window.__gvisCallbacks = window.__gvisCallbacks || []; var chartid = \"motionchart\"; // Manually see if chartid is in pkgs (not all browsers support Array.indexOf) var i, newPackage = true; for (i = 0; newPackage && i < pkgs.length; i++) { if (pkgs[i] === chartid) newPackage = false; } if (newPackage) pkgs.push(chartid); // Add the drawChart function to the global list of callbacks callbacks.push(drawChartMotionChartID336e668836ab); })(); function displayChartMotionChartID336e668836ab() { var pkgs = window.__gvisPackages = window.__gvisPackages || []; var callbacks = window.__gvisCallbacks = window.__gvisCallbacks || []; window.clearTimeout(window.__gvisLoad); // The timeout is set to 100 because otherwise the container div we are // targeting might not be part of the document yet window.__gvisLoad = setTimeout(function() { var pkgCount = pkgs.length; google.load(\"visualization\", \"1\", { packages:pkgs, callback: function() { if (pkgCount != pkgs.length) { // Race condition where another setTimeout call snuck in after us; if // that call added a package, we must not shift its callback return; } while (callbacks.length > 0) callbacks.shift()(); } }); }, 100); } // jsFooter Embedding with <iframe> or <embed> \u00b6 After we generate the HTML chart or map, save the object as a HTML file. 1 2 3 4 library ( htmlwidgets ) library ( DT ) saveWidget ( object , \"googleVis.html\" ) Add these HTML snippets to the Markdown/HTML document. 1 2 < iframe seamless src = \"../img/googlevis.html\" width = 600px height = 400px ></ iframe > 1 < embed seamless src = \"../img/googlevis.html\" width = 600px height = 400px ></ embed > In other words, we can either write the code or embed a file into the HTML/Markdown document. Information from the object \u00b6 1 2 M $ type M $ chartid 1 2 \"MotionChart\" \"MotionChartID336e668836ab\" Maps \u00b6 gvisGeoChart \u00b6 1 head ( Exports , 3 ) 1 2 3 4 5 # Output Country Profit Online 1 Germany 3 TRUE 2 Brazil 4 FALSE 3 United States 5 TRUE 1 2 3 4 5 6 7 8 9 10 Geo <- gvisGeoChart ( Exports , locationvar = \"Country\" , colorvar = \"Profit\" , sizevar = \"\" , # size of markers hovervar = \"\" , # text options = list ( projection = \"kavrayskiy-vii\" )) # locationvar can be lat:long or address, country name, region, state, city plot ( Geo ) // jsData function gvisDataGeoChartID49617fef373 () { var data = new google.visualization.DataTable(); var datajson = [ [ \"Germany\", 3 ], [ \"Brazil\", 4 ], [ \"United States\", 5 ], [ \"France\", 4 ], [ \"Hungary\", 3 ], [ \"India\", 2 ], [ \"Iceland\", 1 ], [ \"Norway\", 4 ], [ \"Spain\", 5 ], [ \"Turkey\", 1 ] ]; data.addColumn('string','Country'); data.addColumn('number','Profit'); data.addRows(datajson); return(data); } // jsDrawChart function drawChartGeoChartID49617fef373() { var data = gvisDataGeoChartID49617fef373(); var options = {}; options[\"width\"] = 556; options[\"height\"] = 347; options[\"projection\"] = \"kavrayskiy-vii\"; var chart = new google.visualization.GeoChart( document.getElementById('GeoChartID49617fef373') ); chart.draw(data,options); } // jsDisplayChart (function() { var pkgs = window.__gvisPackages = window.__gvisPackages || []; var callbacks = window.__gvisCallbacks = window.__gvisCallbacks || []; var chartid = \"geochart\"; // Manually see if chartid is in pkgs (not all browsers support Array.indexOf) var i, newPackage = true; for (i = 0; newPackage && i < pkgs.length; i++) { if (pkgs[i] === chartid) newPackage = false; } if (newPackage) pkgs.push(chartid); // Add the drawChart function to the global list of callbacks callbacks.push(drawChartGeoChartID49617fef373); })(); function displayChartGeoChartID49617fef373() { var pkgs = window.__gvisPackages = window.__gvisPackages || []; var callbacks = window.__gvisCallbacks = window.__gvisCallbacks || []; window.clearTimeout(window.__gvisLoad); // The timeout is set to 100 because otherwise the container div we are // targeting might not be part of the document yet window.__gvisLoad = setTimeout(function() { var pkgCount = pkgs.length; google.load(\"visualization\", \"1\", { packages:pkgs, callback: function() { if (pkgCount != pkgs.length) { // Race condition where another setTimeout call snuck in after us; if // that call added a package, we must not shift its callback return; } while (callbacks.length > 0) callbacks.shift()(); } }); }, 100); } // jsFooter 1 head ( CityPopularity , 3 ) 1 2 3 4 5 # Output City Popularity 1 New York 200 2 Boston 300 3 Miami 400 1 2 3 4 5 6 7 8 9 Geo2 <- gvisGeoChart ( CityPopularity , locationvar = 'City' , colorvar = 'Popularity' , options = list ( region = 'US' , height = 350 , displayMode = 'markers' , colorAxis = \"{values:[200,400,600,800], colors:[\\'red', \\'pink\\', \\'orange',\\'green']}\" )) plot ( Geo2 ) // jsData function gvisDataGeoChartID4961741b9385 () { var data = new google.visualization.DataTable(); var datajson = [ [ \"New York\", 200 ], [ \"Boston\", 300 ], [ \"Miami\", 400 ], [ \"Chicago\", 500 ], [ \"Los Angeles\", 600 ], [ \"Houston\", 700 ] ]; data.addColumn('string','City'); data.addColumn('number','Popularity'); data.addRows(datajson); return(data); } // jsDrawChart function drawChartGeoChartID4961741b9385() { var data = gvisDataGeoChartID4961741b9385(); var options = {}; options[\"width\"] = 556; options[\"height\"] = 350; options[\"region\"] = \"US\"; options[\"displayMode\"] = \"markers\"; options[\"colorAxis\"] = {values:[200,400,600,800], colors:['red', 'pink', 'orange','green']}; var chart = new google.visualization.GeoChart( document.getElementById('GeoChartID4961741b9385') ); chart.draw(data,options); } // jsDisplayChart (function() { var pkgs = window.__gvisPackages = window.__gvisPackages || []; var callbacks = window.__gvisCallbacks = window.__gvisCallbacks || []; var chartid = \"geochart\"; // Manually see if chartid is in pkgs (not all browsers support Array.indexOf) var i, newPackage = true; for (i = 0; newPackage && i < pkgs.length; i++) { if (pkgs[i] === chartid) newPackage = false; } if (newPackage) pkgs.push(chartid); // Add the drawChart function to the global list of callbacks callbacks.push(drawChartGeoChartID4961741b9385); })(); function displayChartGeoChartID4961741b9385() { var pkgs = window.__gvisPackages = window.__gvisPackages || []; var callbacks = window.__gvisCallbacks = window.__gvisCallbacks || []; window.clearTimeout(window.__gvisLoad); // The timeout is set to 100 because otherwise the container div we are // targeting might not be part of the document yet window.__gvisLoad = setTimeout(function() { var pkgCount = pkgs.length; google.load(\"visualization\", \"1\", { packages:pkgs, callback: function() { if (pkgCount != pkgs.length) { // Race condition where another setTimeout call snuck in after us; if // that call added a package, we must not shift its callback return; } while (callbacks.length > 0) callbacks.shift()(); } }); }, 100); } // jsFooter 1 2 3 4 5 6 7 8 9 10 11 12 CityPopularity3 <- data.frame ( City = c ( 'Montreal' , 'Toronto' ), Popularity = c ( 400 , 200 )) Geo3 <- gvisGeoChart ( CityPopularity3 , locationvar = 'City' , colorvar = 'Popularity' , options = list ( region = 'CA' , height = 350 , displayMode = 'markers' , colorAxis = \"{values:[200,400,600,800], colors:[\\'red', \\'pink\\', \\'orange',\\'green']}\" )) plot ( Geo3 ) // jsData function gvisDataGeoChartID49614f24acff () { var data = new google.visualization.DataTable(); var datajson = [ [ \"Montreal\", 700 ], [ \"Toronto\", 200 ] ]; data.addColumn('string','City'); data.addColumn('number','Popularity'); data.addRows(datajson); return(data); } // jsDrawChart function drawChartGeoChartID49614f24acff() { var data = gvisDataGeoChartID49614f24acff(); var options = {}; options[\"width\"] = 556; options[\"height\"] = 350; options[\"region\"] = \"CA\"; options[\"displayMode\"] = \"markers\"; options[\"colorAxis\"] = {values:[200,400,600,800], colors:['red', 'pink', 'orange','green']}; var chart = new google.visualization.GeoChart( document.getElementById('GeoChartID49614f24acff') ); chart.draw(data,options); } // jsDisplayChart (function() { var pkgs = window.__gvisPackages = window.__gvisPackages || []; var callbacks = window.__gvisCallbacks = window.__gvisCallbacks || []; var chartid = \"geochart\"; // Manually see if chartid is in pkgs (not all browsers support Array.indexOf) var i, newPackage = true; for (i = 0; newPackage && i < pkgs.length; i++) { if (pkgs[i] === chartid) newPackage = false; } if (newPackage) pkgs.push(chartid); // Add the drawChart function to the global list of callbacks callbacks.push(drawChartGeoChartID49614f24acff); })(); function displayChartGeoChartID49614f24acff() { var pkgs = window.__gvisPackages = window.__gvisPackages || []; var callbacks = window.__gvisCallbacks = window.__gvisCallbacks || []; window.clearTimeout(window.__gvisLoad); // The timeout is set to 100 because otherwise the container div we are // targeting might not be part of the document yet window.__gvisLoad = setTimeout(function() { var pkgCount = pkgs.length; google.load(\"visualization\", \"1\", { packages:pkgs, callback: function() { if (pkgCount != pkgs.length) { // Race condition where another setTimeout call snuck in after us; if // that call added a package, we must not shift its callback return; } while (callbacks.length > 0) callbacks.shift()(); } }); }, 100); } // jsFooter 1 head ( Andrew , 3 ) 1 2 3 4 5 6 7 8 9 # Output Date / Time UTC Lat Long Pressure_mb Speed_kt Category 1 1992 - 08 - 16 18 : 00 : 00 10.8 - 35.5 1010 25 Tropical Depression 2 1992 - 08 - 17 00 : 00 : 00 11.2 - 37.4 1009 30 Tropical Depression 3 1992 - 08 - 17 06 : 00 : 00 11.7 - 39.6 1008 30 Tropical Depression LatLong Tip 1 10.8 :- 35.5 Tropical Depression < BR > Pressure = 1010 < BR > Speed = 25 2 11.2 :- 37.4 Tropical Depression < BR > Pressure = 1009 < BR > Speed = 30 3 11.7 :- 39.6 Tropical Depression < BR > Pressure = 1008 < BR > Speed = 30 1 2 3 4 5 6 7 GeoMarker <- gvisGeoChart ( Andrew , locationvar = \"LatLong\" , sizevar = 'Speed_kt' , colorvar = \"Pressure_mb\" , options = list ( region = \"US\" )) plot ( GeoMarker ) // jsData function gvisDataGeoChartID2cd02f295f7 () { var data = new google.visualization.DataTable(); var datajson = [ [ 10.8, -35.5, 1010, 25 ], [ 11.2, -37.4, 1009, 30 ], [ 11.7, -39.6, 1008, 30 ], [ 12.3, -42, 1006, 35 ], [ 13.1, -44.2, 1003, 35 ], [ 13.6, -46.2, 1002, 40 ], [ 14.1, -48, 1001, 45 ], [ 14.6, -49.9, 1000, 45 ], [ 15.4, -51.8, 1000, 45 ], [ 16.3, -53.5, 1001, 45 ], [ 17.2, -55.3, 1002, 45 ], [ 18, -56.9, 1005, 45 ], [ 18.8, -58.3, 1007, 45 ], [ 19.8, -59.3, 1011, 40 ], [ 20.7, -60, 1013, 40 ], [ 21.7, -60.7, 1015, 40 ], [ 22.5, -61.5, 1014, 40 ], [ 23.2, -62.4, 1014, 45 ], [ 23.9, -63.3, 1010, 45 ], [ 24.4, -64.2, 1007, 50 ], [ 24.8, -64.9, 1004, 50 ], [ 25.3, -65.9, 1000, 55 ], [ 25.6, -67, 994, 60 ], [ 25.8, -68.3, 981, 70 ], [ 25.7, -69.7, 969, 80 ], [ 25.6, -71.1, 961, 90 ], [ 25.5, -72.5, 947, 105 ], [ 25.4, -74.2, 933, 120 ], [ 25.4, -75.8, 922, 135 ], [ 25.4, -77.5, 930, 125 ], [ 25.4, -79.3, 937, 120 ], [ 25.6, -81.2, 951, 110 ], [ 25.8, -83.1, 947, 115 ], [ 26.2, -85, 943, 115 ], [ 26.6, -86.7, 948, 115 ], [ 27.2, -88.2, 946, 115 ], [ 27.8, -89.6, 941, 120 ], [ 28.5, -90.5, 937, 120 ], [ 29.2, -91.3, 955, 115 ], [ 30.1, -91.7, 973, 80 ], [ 30.9, -91.6, 991, 50 ], [ 31.5, -91.1, 995, 35 ], [ 32.1, -90.5, 997, 30 ], [ 32.8, -89.6, 998, 30 ], [ 33.6, -88.4, 999, 25 ], [ 34.4, -86.7, 1000, 20 ], [ 35.4, -84, 1000, 20 ] ]; data.addColumn('number','Latitude'); data.addColumn('number','Longitude'); data.addColumn('number','Pressure_mb'); data.addColumn('number','Speed_kt'); data.addRows(datajson); return(data); } // jsDrawChart function drawChartGeoChartID2cd02f295f7() { var data = gvisDataGeoChartID2cd02f295f7(); var options = {}; options[\"width\"] = 556; options[\"height\"] = 347; options[\"region\"] = \"US\"; var chart = new google.visualization.GeoChart( document.getElementById('GeoChartID2cd02f295f7') ); chart.draw(data,options); } // jsDisplayChart (function() { var pkgs = window.__gvisPackages = window.__gvisPackages || []; var callbacks = window.__gvisCallbacks = window.__gvisCallbacks || []; var chartid = \"geochart\"; // Manually see if chartid is in pkgs (not all browsers support Array.indexOf) var i, newPackage = true; for (i = 0; newPackage && i < pkgs.length; i++) { if (pkgs[i] === chartid) newPackage = false; } if (newPackage) pkgs.push(chartid); // Add the drawChart function to the global list of callbacks callbacks.push(drawChartGeoChartID2cd02f295f7); })(); function displayChartGeoChartID2cd02f295f7() { var pkgs = window.__gvisPackages = window.__gvisPackages || []; var callbacks = window.__gvisCallbacks = window.__gvisCallbacks || []; window.clearTimeout(window.__gvisLoad); // The timeout is set to 100 because otherwise the container div we are // targeting might not be part of the document yet window.__gvisLoad = setTimeout(function() { var pkgCount = pkgs.length; google.load(\"visualization\", \"1\", { packages:pkgs, callback: function() { if (pkgCount != pkgs.length) { // Race condition where another setTimeout call snuck in after us; if // that call added a package, we must not shift its callback return; } while (callbacks.length > 0) callbacks.shift()(); } }); }, 100); } // jsFooter gvisMap or Google Maps \u00b6 1 2 3 4 5 6 7 8 9 10 AndrewMap <- gvisMap ( Andrew , locationvar = \"LatLong\" , tipvar = \"Tip\" , # text displayed over the tip icon options = list ( showTip = TRUE , showLine = TRUE , enableScrollWheel = TRUE , mapType = 'terrain' , useMapTypeControl = TRUE )) plot ( AndrewMap ) // jsData function gvisDataMapID496129f7ada () { var data = new google.visualization.DataTable(); var datajson = [ [ 10.8, -35.5, \"Tropical Depression<BR>Pressure=1010<BR>Speed=25\" ], [ 11.2, -37.4, \"Tropical Depression<BR>Pressure=1009<BR>Speed=30\" ], [ 11.7, -39.6, \"Tropical Depression<BR>Pressure=1008<BR>Speed=30\" ], [ 12.3, -42, \"Tropical Storm<BR>Pressure=1006<BR>Speed=35\" ], [ 13.1, -44.2, \"Tropical Storm<BR>Pressure=1003<BR>Speed=35\" ], [ 13.6, -46.2, \"Tropical Storm<BR>Pressure=1002<BR>Speed=40\" ], [ 14.1, -48, \"Tropical Storm<BR>Pressure=1001<BR>Speed=45\" ], [ 14.6, -49.9, \"Tropical Storm<BR>Pressure=1000<BR>Speed=45\" ], [ 15.4, -51.8, \"Tropical Storm<BR>Pressure=1000<BR>Speed=45\" ], [ 16.3, -53.5, \"Tropical Storm<BR>Pressure=1001<BR>Speed=45\" ], [ 17.2, -55.3, \"Tropical Storm<BR>Pressure=1002<BR>Speed=45\" ], [ 18, -56.9, \"Tropical Storm<BR>Pressure=1005<BR>Speed=45\" ], [ 18.8, -58.3, \"Tropical Storm<BR>Pressure=1007<BR>Speed=45\" ], [ 19.8, -59.3, \"Tropical Storm<BR>Pressure=1011<BR>Speed=40\" ], [ 20.7, -60, \"Tropical Storm<BR>Pressure=1013<BR>Speed=40\" ], [ 21.7, -60.7, \"Tropical Storm<BR>Pressure=1015<BR>Speed=40\" ], [ 22.5, -61.5, \"Tropical Storm<BR>Pressure=1014<BR>Speed=40\" ], [ 23.2, -62.4, \"Tropical Storm<BR>Pressure=1014<BR>Speed=45\" ], [ 23.9, -63.3, \"Tropical Storm<BR>Pressure=1010<BR>Speed=45\" ], [ 24.4, -64.2, \"Tropical Storm<BR>Pressure=1007<BR>Speed=50\" ], [ 24.8, -64.9, \"Tropical Storm<BR>Pressure=1004<BR>Speed=50\" ], [ 25.3, -65.9, \"Tropical Storm<BR>Pressure=1000<BR>Speed=55\" ], [ 25.6, -67, \"Tropical Storm<BR>Pressure=994<BR>Speed=60\" ], [ 25.8, -68.3, \"Hurricane<BR>Pressure=981<BR>Speed=70\" ], [ 25.7, -69.7, \"Hurricane<BR>Pressure=969<BR>Speed=80\" ], [ 25.6, -71.1, \"Hurricane<BR>Pressure=961<BR>Speed=90\" ], [ 25.5, -72.5, \"Hurricane<BR>Pressure=947<BR>Speed=105\" ], [ 25.4, -74.2, \"Hurricane<BR>Pressure=933<BR>Speed=120\" ], [ 25.4, -75.8, \"Hurricane<BR>Pressure=922<BR>Speed=135\" ], [ 25.4, -77.5, \"Hurricane<BR>Pressure=930<BR>Speed=125\" ], [ 25.4, -79.3, \"Hurricane<BR>Pressure=937<BR>Speed=120\" ], [ 25.6, -81.2, \"Hurricane<BR>Pressure=951<BR>Speed=110\" ], [ 25.8, -83.1, \"Hurricane<BR>Pressure=947<BR>Speed=115\" ], [ 26.2, -85, \"Hurricane<BR>Pressure=943<BR>Speed=115\" ], [ 26.6, -86.7, \"Hurricane<BR>Pressure=948<BR>Speed=115\" ], [ 27.2, -88.2, \"Hurricane<BR>Pressure=946<BR>Speed=115\" ], [ 27.8, -89.6, \"Hurricane<BR>Pressure=941<BR>Speed=120\" ], [ 28.5, -90.5, \"Hurricane<BR>Pressure=937<BR>Speed=120\" ], [ 29.2, -91.3, \"Hurricane<BR>Pressure=955<BR>Speed=115\" ], [ 30.1, -91.7, \"Tropical Storm<BR>Pressure=973<BR>Speed=80\" ], [ 30.9, -91.6, \"Tropical Storm<BR>Pressure=991<BR>Speed=50\" ], [ 31.5, -91.1, \"Tropical Depression<BR>Pressure=995<BR>Speed=35\" ], [ 32.1, -90.5, \"Tropical Depression<BR>Pressure=997<BR>Speed=30\" ], [ 32.8, -89.6, \"Tropical Depression<BR>Pressure=998<BR>Speed=30\" ], [ 33.6, -88.4, \"Tropical Depression<BR>Pressure=999<BR>Speed=25\" ], [ 34.4, -86.7, \"Tropical Depression<BR>Pressure=1000<BR>Speed=20\" ], [ 35.4, -84, \"Tropical Depression<BR>Pressure=1000<BR>Speed=20\" ] ]; data.addColumn('number','Latitude'); data.addColumn('number','Longitude'); data.addColumn('string','Tip'); data.addRows(datajson); return(data); } // jsDrawChart function drawChartMapID496129f7ada() { var data = gvisDataMapID496129f7ada(); var options = {}; options[\"showTip\"] = true; options[\"showLine\"] = true; options[\"enableScrollWheel\"] = true; options[\"mapType\"] = \"terrain\"; options[\"useMapTypeControl\"] = true; var chart = new google.visualization.Map( document.getElementById('MapID496129f7ada') ); chart.draw(data,options); } // jsDisplayChart (function() { var pkgs = window.__gvisPackages = window.__gvisPackages || []; var callbacks = window.__gvisCallbacks = window.__gvisCallbacks || []; var chartid = \"map\"; // Manually see if chartid is in pkgs (not all browsers support Array.indexOf) var i, newPackage = true; for (i = 0; newPackage && i < pkgs.length; i++) { if (pkgs[i] === chartid) newPackage = false; } if (newPackage) pkgs.push(chartid); // Add the drawChart function to the global list of callbacks callbacks.push(drawChartMapID496129f7ada); })(); function displayChartMapID496129f7ada() { var pkgs = window.__gvisPackages = window.__gvisPackages || []; var callbacks = window.__gvisCallbacks = window.__gvisCallbacks || []; window.clearTimeout(window.__gvisLoad); // The timeout is set to 100 because otherwise the container div we are // targeting might not be part of the document yet window.__gvisLoad = setTimeout(function() { var pkgCount = pkgs.length; google.load(\"visualization\", \"1\", { packages:pkgs, callback: function() { if (pkgCount != pkgs.length) { // Race condition where another setTimeout call snuck in after us; if // that call added a package, we must not shift its callback return; } while (callbacks.length > 0) callbacks.shift()(); } }); }, 100); } // jsFooter Editor \u00b6 This is a fantastic option that let us start with one chart or map and change everything with a menu. 1 2 3 4 5 6 Editor <- gvisGeoChart ( Exports , locationvar = \"Country\" , colorvar = \"Profit\" , options = list ( gvis.editor = 'Edit me!' )) plot ( Editor ) // jsData function gvisDataGeoChartID49613aa5c54a () { var data = new google.visualization.DataTable(); var datajson = [ [ \"Germany\", 3 ], [ \"Brazil\", 4 ], [ \"United States\", 5 ], [ \"France\", 4 ], [ \"Hungary\", 3 ], [ \"India\", 2 ], [ \"Iceland\", 1 ], [ \"Norway\", 4 ], [ \"Spain\", 5 ], [ \"Turkey\", 1 ] ]; data.addColumn('string','Country'); data.addColumn('number','Profit'); data.addRows(datajson); return(data); } // jsDrawChart function drawChartGeoChartID49613aa5c54a() { var data = gvisDataGeoChartID49613aa5c54a(); var options = {}; options[\"width\"] = 556; options[\"height\"] = 347; chartGeoChartID49613aa5c54a = new google.visualization.ChartWrapper({ dataTable: data, chartType: 'GeoChart', containerId: 'GeoChartID49613aa5c54a', options: options }); chartGeoChartID49613aa5c54a.draw(); } function openEditorGeoChartID49613aa5c54a() { var editor = new google.visualization.ChartEditor(); google.visualization.events.addListener(editor, 'ok', function() { chartGeoChartID49613aa5c54a = editor.getChartWrapper(); chartGeoChartID49613aa5c54a.draw(document.getElementById('GeoChartID49613aa5c54a')); }); editor.openDialog(chartGeoChartID49613aa5c54a); } // jsDisplayChart (function() { var pkgs = window.__gvisPackages = window.__gvisPackages || []; var callbacks = window.__gvisCallbacks = window.__gvisCallbacks || []; var chartid = \"charteditor\"; // Manually see if chartid is in pkgs (not all browsers support Array.indexOf) var i, newPackage = true; for (i = 0; newPackage && i < pkgs.length; i++) { if (pkgs[i] === chartid) newPackage = false; } if (newPackage) pkgs.push(chartid); // Add the drawChart function to the global list of callbacks callbacks.push(drawChartGeoChartID49613aa5c54a); })(); function displayChartGeoChartID49613aa5c54a() { var pkgs = window.__gvisPackages = window.__gvisPackages || []; var callbacks = window.__gvisCallbacks = window.__gvisCallbacks || []; window.clearTimeout(window.__gvisLoad); // The timeout is set to 100 because otherwise the container div we are // targeting might not be part of the document yet window.__gvisLoad = setTimeout(function() { var pkgCount = pkgs.length; google.load(\"visualization\", \"1\", { packages:pkgs, callback: function() { if (pkgCount != pkgs.length) { // Race condition where another setTimeout call snuck in after us; if // that call added a package, we must not shift its callback return; } while (callbacks.length > 0) callbacks.shift()(); } }); }, 100); } // jsFooter Tables with gvisTable \u00b6 1 head ( Stock , 3 ) 1 2 3 4 5 # Output Date Device Value Title Annotation 1 2008-01-01 Pencils 3000 <NA> <NA> 2 2008-01-02 Pencils 14045 <NA> <NA> 3 2008-01-03 Pencils 5502 <NA> <NA> 1 2 3 4 Table <- gvisTable ( Stock , formats = list ( Value = \"#,###\" )) plot ( Table ) // jsData function gvisDataTableID2cd03b735929 () { var data = new google.visualization.DataTable(); var datajson = [ [ new Date(2008,0,1), \"Pencils\", 3000, null, null ], [ new Date(2008,0,2), \"Pencils\", 14045, null, null ], [ new Date(2008,0,3), \"Pencils\", 5502, null, null ], [ new Date(2008,0,4), \"Pencils\", 75284, null, null ], [ new Date(2008,0,5), \"Pencils\", 41476, \"Bought pencils\", \"Bought 200k pencils\" ], [ new Date(2008,0,6), \"Pencils\", 333222, null, null ], [ new Date(2008,0,1), \"Pens\", 40645, null, null ], [ new Date(2008,0,2), \"Pens\", 20374, null, null ], [ new Date(2008,0,3), \"Pens\", 50766, null, null ], [ new Date(2008,0,4), \"Pens\", 14334, \"Out of stock\", \"Ran out of stock of pens at 4pm\" ], [ new Date(2008,0,5), \"Pens\", 66467, null, null ], [ new Date(2008,0,6), \"Pens\", 39463, null, null ] ]; data.addColumn('date','Date'); data.addColumn('string','Device'); data.addColumn('number','Value'); data.addColumn('string','Title'); data.addColumn('string','Annotation'); data.addRows(datajson); return(data); } // jsDrawChart function drawChartTableID2cd03b735929() { var data = gvisDataTableID2cd03b735929(); var options = {}; options[\"allowHtml\"] = true; var dataFormat1 = new google.visualization.NumberFormat({pattern:\"#,###\"}); dataFormat1.format(data, 2); var chart = new google.visualization.Table( document.getElementById('TableID2cd03b735929') ); chart.draw(data,options); } // jsDisplayChart (function() { var pkgs = window.__gvisPackages = window.__gvisPackages || []; var callbacks = window.__gvisCallbacks = window.__gvisCallbacks || []; var chartid = \"table\"; // Manually see if chartid is in pkgs (not all browsers support Array.indexOf) var i, newPackage = true; for (i = 0; newPackage && i < pkgs.length; i++) { if (pkgs[i] === chartid) newPackage = false; } if (newPackage) pkgs.push(chartid); // Add the drawChart function to the global list of callbacks callbacks.push(drawChartTableID2cd03b735929); })(); function displayChartTableID2cd03b735929() { var pkgs = window.__gvisPackages = window.__gvisPackages || []; var callbacks = window.__gvisCallbacks = window.__gvisCallbacks || []; window.clearTimeout(window.__gvisLoad); // The timeout is set to 100 because otherwise the container div we are // targeting might not be part of the document yet window.__gvisLoad = setTimeout(function() { var pkgCount = pkgs.length; google.load(\"visualization\", \"1\", { packages:pkgs, callback: function() { if (pkgCount != pkgs.length) { // Race condition where another setTimeout call snuck in after us; if // that call added a package, we must not shift its callback return; } while (callbacks.length > 0) callbacks.shift()(); } }); }, 100); } // jsFooter 1 head ( Population , 3 ) 1 2 3 4 5 6 7 8 9 10 11 12 13 # Output Rank Country Population % of World Population 1 1 China 1339940000 0.1950 2 2 India 1188650000 0.1730 3 3 United States 310438000 0.0452 Flag 1 <img src=\"http://upload.wikimedia.org/wikipedia/commons/thumb/f/fa/Flag_of_the_People%27s_Republic_of_China.svg/22px-Flag_of_the_People%27s_Republic_of_China.svg.png\"> 2 <img src=\"http://upload.wikimedia.org/wikipedia/commons/thumb/4/41/Flag_of_India.svg/22px-Flag_of_India.svg.png\"> 3 <img src=\"http://upload.wikimedia.org/wikipedia/commons/thumb/a/a4/Flag_of_the_United_States.svg/22px-Flag_of_the_United_States.svg.png\"> Mode Date 1 TRUE 2010-10-09 2 TRUE 2010-10-09 3 TRUE 2010-10-09 1 2 3 4 5 6 PopTable <- gvisTable ( Population , formats = list ( Population = \"#,###\" , '% of World Population' = '#.#%' ), options = list ( page = 'enable' )) plot ( PopTable ) // jsData function gvisDataTableID2cd013050362 () { var data = new google.visualization.DataTable(); var datajson = [ [ \"1\", \"China\", 1339940000, 0.195, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/f/fa/Flag_of_the_People%27s_Republic_of_China.svg/22px-Flag_of_the_People%27s_Republic_of_China.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"2\", \"India\", 1188650000, 0.173, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/4/41/Flag_of_India.svg/22px-Flag_of_India.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"3\", \"United States\", 310438000, 0.0452, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/a/a4/Flag_of_the_United_States.svg/22px-Flag_of_the_United_States.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"4\", \"Indonesia\", 237556363, 0.0346, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/9/9f/Flag_of_Indonesia.svg/22px-Flag_of_Indonesia.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"5\", \"Brazil\", 193626000, 0.0282, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/0/05/Flag_of_Brazil.svg/22px-Flag_of_Brazil.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"6\", \"Pakistan\", 170745000, 0.0248, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/3/32/Flag_of_Pakistan.svg/22px-Flag_of_Pakistan.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"7\", \"Bangladesh\", 164425000, 0.0239, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/f/f9/Flag_of_Bangladesh.svg/22px-Flag_of_Bangladesh.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"8\", \"Nigeria\", 158259000, 0.023, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/7/79/Flag_of_Nigeria.svg/22px-Flag_of_Nigeria.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"9\", \"Russia\", 141927297, 0.0206, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/f/f3/Flag_of_Russia.svg/22px-Flag_of_Russia.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"10\", \"Japan\", 127390000, 0.0185, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/9/9e/Flag_of_Japan.svg/22px-Flag_of_Japan.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"11\", \"Mexico\", 108396211, 0.0158, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/f/fc/Flag_of_Mexico.svg/22px-Flag_of_Mexico.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"12\", \"Philippines\", 94013200, 0.0137, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/9/99/Flag_of_the_Philippines.svg/22px-Flag_of_the_Philippines.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"13\", \"Vietnam\", 85846997, 0.0125, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/2/21/Flag_of_Vietnam.svg/22px-Flag_of_Vietnam.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"14\", \"Ethiopia\", 84976000, 0.0124, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/7/71/Flag_of_Ethiopia.svg/22px-Flag_of_Ethiopia.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"15\", \"Germany\", 81802257, 0.0119, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/b/ba/Flag_of_Germany.svg/22px-Flag_of_Germany.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"16\", \"Egypt\", 79135000, 0.0115, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/f/fe/Flag_of_Egypt.svg/22px-Flag_of_Egypt.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"17\", \"Iran\", 75078000, 0.0109, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/c/ca/Flag_of_Iran.svg/22px-Flag_of_Iran.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"18\", \"Turkey\", 72561312, 0.0106, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/b/b4/Flag_of_Turkey.svg/22px-Flag_of_Turkey.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"19\", \"Dem. Rep. of Congo\", 67827000, 0.0099, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/6/6f/Flag_of_the_Democratic_Republic_of_the_Congo.svg/22px-Flag_of_the_Democratic_Republic_of_the_Congo.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"20\", \"Thailand\", 67070000, 0.01, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/a/a9/Flag_of_Thailand.svg/22px-Flag_of_Thailand.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"21\", \"France\", 65447374, 0.0095, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/c/c3/Flag_of_France.svg/22px-Flag_of_France.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"22\", \"United Kingdom\", 62008049, 0.009, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/a/ae/Flag_of_the_United_Kingdom.svg/22px-Flag_of_the_United_Kingdom.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"23\", \"Italy\", 60402499, 0.0088, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/0/03/Flag_of_Italy.svg/22px-Flag_of_Italy.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"24\", \"Myanmar\", 50496000, 0.0073, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/8/8c/Flag_of_Myanmar.svg/22px-Flag_of_Myanmar.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"25\", \"South Africa\", 49991300, 0.0073, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/a/af/Flag_of_South_Africa.svg/22px-Flag_of_South_Africa.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"26\", \"South Korea\", 49773145, 0.0072, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/0/09/Flag_of_South_Korea.svg/22px-Flag_of_South_Korea.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"27\", \"Spain\", 46072834, 0.0067, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/9/9a/Flag_of_Spain.svg/22px-Flag_of_Spain.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"28\", \"Ukraine\", 45871738, 0.0067, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/4/49/Flag_of_Ukraine.svg/22px-Flag_of_Ukraine.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"29\", \"Colombia\", 45655000, 0.0066, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/2/21/Flag_of_Colombia.svg/22px-Flag_of_Colombia.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"30\", \"Tanzania\", 45040000, 0.0066, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/3/38/Flag_of_Tanzania.svg/22px-Flag_of_Tanzania.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"31\", \"Sudan\", 43192000, 0.0063, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/0/01/Flag_of_Sudan.svg/22px-Flag_of_Sudan.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"32\", \"Argentina\", 40518951, 0.0059, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/1/1a/Flag_of_Argentina.svg/22px-Flag_of_Argentina.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"33\", \"Kenya\", 38610097, 0.0056, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/4/49/Flag_of_Kenya.svg/22px-Flag_of_Kenya.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"34\", \"Poland\", 38167329, 0.0056, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/1/12/Flag_of_Poland.svg/22px-Flag_of_Poland.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"35\", \"Algeria\", 35423000, 0.0052, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/7/77/Flag_of_Algeria.svg/22px-Flag_of_Algeria.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"36\", \"Canada\", 34272000, 0.005, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/c/cf/Flag_of_Canada.svg/22px-Flag_of_Canada.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"37\", \"Uganda\", 33796000, 0.0049, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/4/4e/Flag_of_Uganda.svg/22px-Flag_of_Uganda.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"38\", \"Morocco\", 31944000, 0.0046, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/2/2c/Flag_of_Morocco.svg/22px-Flag_of_Morocco.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"39\", \"Iraq\", 31467000, 0.0046, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/f/f6/Flag_of_Iraq.svg/22px-Flag_of_Iraq.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"40\", \"Nepal\", 29853000, 0.0043, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/9/9b/Flag_of_Nepal.svg/16px-Flag_of_Nepal.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"41\", \"Peru\", 29461933, 0.0043, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/c/cf/Flag_of_Peru.svg/22px-Flag_of_Peru.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"42\", \"Afghanistan\", 29117000, 0.0042, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/9/9a/Flag_of_Afghanistan.svg/22px-Flag_of_Afghanistan.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"43\", \"Venezuela\", 28958000, 0.0042, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/0/06/Flag_of_Venezuela.svg/22px-Flag_of_Venezuela.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"44\", \"Malaysia\", 28250500, 0.0041, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/6/66/Flag_of_Malaysia.svg/22px-Flag_of_Malaysia.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"45\", \"Uzbekistan\", 27794000, 0.004, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/8/84/Flag_of_Uzbekistan.svg/22px-Flag_of_Uzbekistan.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"46\", \"Saudi Arabia\", 27136977, 0.0039, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/0/0d/Flag_of_Saudi_Arabia.svg/22px-Flag_of_Saudi_Arabia.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"47\", \"Ghana\", 24333000, 0.0035, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/1/19/Flag_of_Ghana.svg/22px-Flag_of_Ghana.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"48\", \"Yemen\", 24256000, 0.0035, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/8/89/Flag_of_Yemen.svg/22px-Flag_of_Yemen.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"49\", \"North Korea\", 23991000, 0.0035, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/5/51/Flag_of_North_Korea.svg/22px-Flag_of_North_Korea.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"50\", \"Mozambique\", 23406000, 0.0034, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/d/d0/Flag_of_Mozambique.svg/22px-Flag_of_Mozambique.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"52\", \"Syria\", 22505000, 0.0033, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/5/53/Flag_of_Syria.svg/22px-Flag_of_Syria.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"53\", \"Australia\", 22483305, 0.0033, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/b/b9/Flag_of_Australia.svg/22px-Flag_of_Australia.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"54\", \"Cote d'Ivoire\", 21571000, 0.0031, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/8/86/Flag_of_Cote_d%27Ivoire.svg/22px-Flag_of_Cote_d%27Ivoire.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"55\", \"Romania\", 21466174, 0.0031, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/7/73/Flag_of_Romania.svg/22px-Flag_of_Romania.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"56\", \"Sri Lanka\", 20410000, 0.003, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/1/11/Flag_of_Sri_Lanka.svg/22px-Flag_of_Sri_Lanka.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"57\", \"Madagascar\", 20146000, 0.0029, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/b/bc/Flag_of_Madagascar.svg/22px-Flag_of_Madagascar.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"58\", \"Cameroon\", 19958000, 0.0029, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/4/4f/Flag_of_Cameroon.svg/22px-Flag_of_Cameroon.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"59\", \"Angola\", 18993000, 0.0028, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/9/9d/Flag_of_Angola.svg/22px-Flag_of_Angola.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"60\", \"Chile\", 17140000, 0.0025, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/7/78/Flag_of_Chile.svg/22px-Flag_of_Chile.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"61\", \"Netherlands\", 16619500, 0.00242, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/2/20/Flag_of_the_Netherlands.svg/22px-Flag_of_the_Netherlands.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"62\", \"Burkina Faso\", 16287000, 0.0024, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/3/31/Flag_of_Burkina_Faso.svg/22px-Flag_of_Burkina_Faso.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"63\", \"Kazakhstan\", 16197000, 0.0024, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/d/d3/Flag_of_Kazakhstan.svg/22px-Flag_of_Kazakhstan.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"64\", \"Niger\", 15891000, 0.0023, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/f/f4/Flag_of_Niger.svg/22px-Flag_of_Niger.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"65\", \"Malawi\", 15692000, 0.0023, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/d/d1/Flag_of_Malawi.svg/22px-Flag_of_Malawi.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"66\", \"Mali\", 14517176, 0.0021, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/9/92/Flag_of_Mali.svg/22px-Flag_of_Mali.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"67\", \"Guatemala\", 14377000, 0.0021, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/e/ec/Flag_of_Guatemala.svg/22px-Flag_of_Guatemala.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"68\", \"Ecuador\", 14259000, 0.0021, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/e/e8/Flag_of_Ecuador.svg/22px-Flag_of_Ecuador.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"69\", \"Cambodia\", 13395682, 0.0019, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/8/83/Flag_of_Cambodia.svg/22px-Flag_of_Cambodia.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"70\", \"Zambia\", 13257000, 0.0019, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/0/06/Flag_of_Zambia.svg/22px-Flag_of_Zambia.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"71\", \"Senegal\", 12861000, 0.0019, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/f/fd/Flag_of_Senegal.svg/22px-Flag_of_Senegal.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"72\", \"Zimbabwe\", 12644000, 0.0018, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/6/6a/Flag_of_Zimbabwe.svg/22px-Flag_of_Zimbabwe.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"73\", \"Chad\", 11506000, 0.0017, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/4/4b/Flag_of_Chad.svg/22px-Flag_of_Chad.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"74\", \"Greece\", 11306183, 0.0016, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/5/5c/Flag_of_Greece.svg/22px-Flag_of_Greece.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"75\", \"Cuba\", 11204000, 0.0016, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/b/bd/Flag_of_Cuba.svg/22px-Flag_of_Cuba.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"76\", \"Belgium\", 10827519, 0.0016, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/9/92/Flag_of_Belgium_%28civil%29.svg/22px-Flag_of_Belgium_%28civil%29.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"77\", \"Portugal\", 10636888, 0.0015, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/5/5c/Flag_of_Portugal.svg/22px-Flag_of_Portugal.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"78\", \"Czech Republic\", 10512397, 0.0015, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/c/cb/Flag_of_the_Czech_Republic.svg/22px-Flag_of_the_Czech_Republic.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"79\", \"Tunisia\", 10432500, 0.0015, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/c/ce/Flag_of_Tunisia.svg/22px-Flag_of_Tunisia.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"80\", \"Guinea\", 10324000, 0.0015, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/e/ed/Flag_of_Guinea.svg/22px-Flag_of_Guinea.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"81\", \"Rwanda\", 10277000, 0.0015, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/1/17/Flag_of_Rwanda.svg/22px-Flag_of_Rwanda.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"82\", \"Dominican Republic\", 10225000, 0.0015, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/9/9f/Flag_of_the_Dominican_Republic.svg/22px-Flag_of_the_Dominican_Republic.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"83\", \"Haiti\", 10188000, 0.0015, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/5/56/Flag_of_Haiti.svg/22px-Flag_of_Haiti.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"84\", \"Bolivia\", 10031000, 0.0015, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/4/48/Flag_of_Bolivia.svg/22px-Flag_of_Bolivia.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"85\", \"Hungary\", 10013628, 0.0015, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/c/c1/Flag_of_Hungary.svg/22px-Flag_of_Hungary.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"86\", \"Serbia\", 9856000, 0.0014, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/f/ff/Flag_of_Serbia.svg/22px-Flag_of_Serbia.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"87\", \"Belarus\", 9467700, 0.0014, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/8/85/Flag_of_Belarus.svg/22px-Flag_of_Belarus.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"88\", \"Sweden\", 9393648, 0.0014, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/4/4c/Flag_of_Sweden.svg/22px-Flag_of_Sweden.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"89\", \"Somalia\", 9359000, 0.0014, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/a/a0/Flag_of_Somalia.svg/22px-Flag_of_Somalia.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"90\", \"Benin\", 9212000, 0.0013, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/0/0a/Flag_of_Benin.svg/22px-Flag_of_Benin.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"91\", \"Azerbaijan\", 8997400, 0.0013, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Flag_of_Azerbaijan.svg/22px-Flag_of_Azerbaijan.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"92\", \"Burundi\", 8519000, 0.0012, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/5/50/Flag_of_Burundi.svg/22px-Flag_of_Burundi.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"93\", \"Austria\", 8372930, 0.0012, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/4/41/Flag_of_Austria.svg/22px-Flag_of_Austria.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"94\", \"Switzerland\", 7782900, 0.0011, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/f/f3/Flag_of_Switzerland.svg/20px-Flag_of_Switzerland.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"95\", \"Israel\", 7640800, 0.0011, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/d/d4/Flag_of_Israel.svg/22px-Flag_of_Israel.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"96\", \"Honduras\", 7616000, 0.0011, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/8/82/Flag_of_Honduras.svg/22px-Flag_of_Honduras.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"97\", \"Bulgaria\", 7576751, 0.0011, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/9/9a/Flag_of_Bulgaria.svg/22px-Flag_of_Bulgaria.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"98\", \"Tajikistan\", 7075000, 0.00103, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/d/d0/Flag_of_Tajikistan.svg/22px-Flag_of_Tajikistan.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"100\", \"Papua New Guinea\", 6888000, 0.001, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/e/e3/Flag_of_Papua_New_Guinea.svg/22px-Flag_of_Papua_New_Guinea.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"101\", \"Togo\", 6780000, 0.00099, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/6/68/Flag_of_Togo.svg/22px-Flag_of_Togo.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"102\", \"Libya\", 6546000, 0.00095, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/0/05/Flag_of_Libya.svg/22px-Flag_of_Libya.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"103\", \"Jordan\", 6472000, 0.00094, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/c/c0/Flag_of_Jordan.svg/22px-Flag_of_Jordan.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"104\", \"Paraguay\", 6460000, 0.00094, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/2/27/Flag_of_Paraguay.svg/22px-Flag_of_Paraguay.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"105\", \"Laos\", 6436000, 0.00094, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/5/56/Flag_of_Laos.svg/22px-Flag_of_Laos.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"106\", \"El Salvador\", 6194000, 9e-04, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/3/34/Flag_of_El_Salvador.svg/22px-Flag_of_El_Salvador.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"107\", \"Sierra Leone\", 5836000, 0.00085, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/1/17/Flag_of_Sierra_Leone.svg/22px-Flag_of_Sierra_Leone.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"108\", \"Nicaragua\", 5822000, 0.00085, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/1/19/Flag_of_Nicaragua.svg/22px-Flag_of_Nicaragua.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"109\", \"Kyrgyzstan\", 5550000, 0.00081, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/c/c7/Flag_of_Kyrgyzstan.svg/22px-Flag_of_Kyrgyzstan.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"110\", \"Denmark\", 5543819, 8e-04, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/9/9c/Flag_of_Denmark.svg/22px-Flag_of_Denmark.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"111\", \"Slovakia\", 5429763, 0.00079, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/e/e6/Flag_of_Slovakia.svg/22px-Flag_of_Slovakia.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"112\", \"Finland\", 5370000, 0.00078, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/b/bc/Flag_of_Finland.svg/22px-Flag_of_Finland.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"113\", \"Eritrea\", 5224000, 0.00076, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/2/29/Flag_of_Eritrea.svg/22px-Flag_of_Eritrea.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"114\", \"Turkmenistan\", 5177000, 0.00075, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/1/1b/Flag_of_Turkmenistan.svg/22px-Flag_of_Turkmenistan.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"115\", \"Singapore\", 5076700, 0.00074, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/4/48/Flag_of_Singapore.svg/22px-Flag_of_Singapore.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"116\", \"Norway\", 4906500, 0.00071, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/d/d9/Flag_of_Norway.svg/22px-Flag_of_Norway.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"117\", \"United Arab Emirates\", 4707000, 0.00068, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/c/cb/Flag_of_the_United_Arab_Emirates.svg/22px-Flag_of_the_United_Arab_Emirates.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"118\", \"Costa Rica\", 4640000, 0.00068, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/f/f2/Flag_of_Costa_Rica.svg/22px-Flag_of_Costa_Rica.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"119\", \"Central African Republic\", 4506000, 0.00066, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/6/6f/Flag_of_the_Central_African_Republic.svg/22px-Flag_of_the_Central_African_Republic.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"120\", \"Ireland\", 4470700, 0.00064, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/4/45/Flag_of_Ireland.svg/22px-Flag_of_Ireland.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"121\", \"Georgia\", 4436000, 0.00065, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/0/0f/Flag_of_Georgia.svg/22px-Flag_of_Georgia.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"122\", \"Croatia\", 4435056, 0.00065, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/1/1b/Flag_of_Croatia.svg/22px-Flag_of_Croatia.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"123\", \"New Zealand\", 4393000, 0.00064, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/3/3e/Flag_of_New_Zealand.svg/22px-Flag_of_New_Zealand.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"124\", \"Lebanon\", 4255000, 0.00062, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/5/59/Flag_of_Lebanon.svg/22px-Flag_of_Lebanon.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"125\", \"Liberia\", 4102000, 6e-04, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/b/b8/Flag_of_Liberia.svg/22px-Flag_of_Liberia.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"127\", \"Palestinian territories\", 3935249, 0.00055, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/0/00/Flag_of_Palestine.svg/22px-Flag_of_Palestine.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"128\", \"Bosnia and Herzegovina\", 3760000, 0.00055, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/b/bf/Flag_of_Bosnia_and_Herzegovina.svg/22px-Flag_of_Bosnia_and_Herzegovina.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"129\", \"Republic of the Congo\", 3759000, 0.00055, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/9/92/Flag_of_the_Republic_of_the_Congo.svg/22px-Flag_of_the_Republic_of_the_Congo.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"130\", \"Moldova\", 3563800, 0.00052, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/2/27/Flag_of_Moldova.svg/22px-Flag_of_Moldova.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"131\", \"Uruguay\", 3372000, 0.00049, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/f/fe/Flag_of_Uruguay.svg/22px-Flag_of_Uruguay.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"132\", \"Mauritania\", 3366000, 0.00049, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/4/43/Flag_of_Mauritania.svg/22px-Flag_of_Mauritania.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"133\", \"Lithuania\", 3329227, 0.00048, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/1/11/Flag_of_Lithuania.svg/22px-Flag_of_Lithuania.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"134\", \"Panama\", 3322576, 0.00048, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/a/ab/Flag_of_Panama.svg/22px-Flag_of_Panama.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"135\", \"Armenia\", 3238000, 0.00047, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/2/2f/Flag_of_Armenia.svg/22px-Flag_of_Armenia.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"136\", \"Albania\", 3195000, 0.00046, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/3/36/Flag_of_Albania.svg/22px-Flag_of_Albania.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"137\", \"Kuwait\", 3051000, 0.00044, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/a/aa/Flag_of_Kuwait.svg/22px-Flag_of_Kuwait.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"138\", \"Oman\", 2905000, 0.00042, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Flag_of_Oman.svg/22px-Flag_of_Oman.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"139\", \"Mongolia\", 2776500, 4e-04, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/4/4c/Flag_of_Mongolia.svg/22px-Flag_of_Mongolia.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"140\", \"Jamaica\", 2730000, 4e-04, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/0/0a/Flag_of_Jamaica.svg/22px-Flag_of_Jamaica.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"141\", \"Latvia\", 2236300, 0.00033, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/8/84/Flag_of_Latvia.svg/22px-Flag_of_Latvia.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"142\", \"Namibia\", 2212000, 0.00032, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/0/00/Flag_of_Namibia.svg/22px-Flag_of_Namibia.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"143\", \"Lesotho\", 2084000, 3e-04, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/4/4a/Flag_of_Lesotho.svg/22px-Flag_of_Lesotho.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"144\", \"Slovenia\", 2065720, 3e-04, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/f/f0/Flag_of_Slovenia.svg/22px-Flag_of_Slovenia.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"145\", \"Republic of Macedonia\", 2048620, 3e-04, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/f/f8/Flag_of_Macedonia.svg/22px-Flag_of_Macedonia.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"146\", \"Botswana\", 1978000, 0.00029, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/f/fa/Flag_of_Botswana.svg/22px-Flag_of_Botswana.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"147\", \"Gambia\", 1751000, 0.00025, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/7/77/Flag_of_The_Gambia.svg/22px-Flag_of_The_Gambia.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"148\", \"Qatar\", 1696563, 0.00025, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/6/65/Flag_of_Qatar.svg/22px-Flag_of_Qatar.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"149\", \"Guinea-Bissau\", 1647000, 0.00024, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/0/01/Flag_of_Guinea-Bissau.svg/22px-Flag_of_Guinea-Bissau.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"150\", \"Gabon\", 1501000, 0.00022, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/0/04/Flag_of_Gabon.svg/22px-Flag_of_Gabon.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"151\", \"Trinidad and Tobago\", 1344000, 2e-04, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/6/64/Flag_of_Trinidad_and_Tobago.svg/22px-Flag_of_Trinidad_and_Tobago.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"152\", \"Estonia\", 1340127, 0.00019, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/8/8f/Flag_of_Estonia.svg/22px-Flag_of_Estonia.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"153\", \"Mauritius\", 1297000, 0.00019, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/7/77/Flag_of_Mauritius.svg/22px-Flag_of_Mauritius.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"154\", \"Swaziland\", 1202000, 0.00017, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/1/1e/Flag_of_Swaziland.svg/22px-Flag_of_Swaziland.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"155\", \"East Timor\", 1171000, 0.00017, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/2/26/Flag_of_East_Timor.svg/22px-Flag_of_East_Timor.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"156\", \"Djibouti\", 879000, 0.00013, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/3/34/Flag_of_Djibouti.svg/22px-Flag_of_Djibouti.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"157\", \"Fiji\", 854000, 0.00012, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/b/ba/Flag_of_Fiji.svg/22px-Flag_of_Fiji.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"158\", \"Bahrain\", 807000, 0.00012, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/2/2c/Flag_of_Bahrain.svg/22px-Flag_of_Bahrain.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"159\", \"Cyprus\", 801851, 0.00012, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/d/d4/Flag_of_Cyprus.svg/22px-Flag_of_Cyprus.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"160\", \"Guyana\", 761000, 0.00011, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/9/99/Flag_of_Guyana.svg/22px-Flag_of_Guyana.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"161\", \"Bhutan\", 708000, 1e-04, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/9/91/Flag_of_Bhutan.svg/22px-Flag_of_Bhutan.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"162\", \"Equatorial Guinea\", 693000, 1e-04, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/3/31/Flag_of_Equatorial_Guinea.svg/22px-Flag_of_Equatorial_Guinea.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"163\", \"Comoros\", 691000, 1e-04, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/9/94/Flag_of_the_Comoros.svg/22px-Flag_of_the_Comoros.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"164\", \"Montenegro\", 626000, 9e-05, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/6/64/Flag_of_Montenegro.svg/22px-Flag_of_Montenegro.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"166\", \"Solomon Islands\", 536000, 8e-05, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/7/74/Flag_of_the_Solomon_Islands.svg/22px-Flag_of_the_Solomon_Islands.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"167\", \"Western Sahara\", 530000, 8e-05, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/c/c8/Flag_of_Western_Sahara.svg/22px-Flag_of_Western_Sahara.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"168\", \"Suriname\", 524000, 8e-05, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/6/60/Flag_of_Suriname.svg/22px-Flag_of_Suriname.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"169\", \"Cape Verde\", 513000, 7e-05, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/3/38/Flag_of_Cape_Verde.svg/22px-Flag_of_Cape_Verde.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"170\", \"Luxembourg\", 502207, 7e-05, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/d/da/Flag_of_Luxembourg.svg/22px-Flag_of_Luxembourg.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"171\", \"Malta\", 416333, 6e-05, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/7/73/Flag_of_Malta.svg/22px-Flag_of_Malta.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"172\", \"Brunei\", 407000, 6e-05, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/9/9c/Flag_of_Brunei.svg/22px-Flag_of_Brunei.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"173\", \"Bahamas\", 346000, 5e-05, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/9/93/Flag_of_the_Bahamas.svg/22px-Flag_of_the_Bahamas.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"174\", \"Belize\", 322100, 5e-05, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/e/e7/Flag_of_Belize.svg/22px-Flag_of_Belize.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"175\", \"Iceland\", 318006, 5e-05, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/c/ce/Flag_of_Iceland.svg/22px-Flag_of_Iceland.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"176\", \"Maldives\", 314000, 5e-05, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/0/0f/Flag_of_Maldives.svg/22px-Flag_of_Maldives.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"177\", \"Barbados\", 257000, 4e-05, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/e/ef/Flag_of_Barbados.svg/22px-Flag_of_Barbados.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"178\", \"Vanuatu\", 246000, 4e-05, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/b/bc/Flag_of_Vanuatu.svg/22px-Flag_of_Vanuatu.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"181\", \"Samoa\", 179000, 3e-05, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/3/31/Flag_of_Samoa.svg/22px-Flag_of_Samoa.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"182\", \"Saint Lucia\", 174000, 3e-05, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/9/9f/Flag_of_Saint_Lucia.svg/22px-Flag_of_Saint_Lucia.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"183\", \"Sao Tome and Principe\", 165000, 2e-05, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/4/4f/Flag_of_Sao_Tome_and_Principe.svg/22px-Flag_of_Sao_Tome_and_Principe.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"184\", \"Federated States of Micronesia\", 111000, 2e-05, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/4/4c/Flag_of_Federated_States_of_Micronesia.svg/22px-Flag_of_Federated_States_of_Micronesia.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"186\", \"Saint Vincent and the Grenadines\", 109000, 2e-05, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/6/6d/Flag_of_Saint_Vincent_and_the_Grenadines.svg/22px-Flag_of_Saint_Vincent_and_the_Grenadines.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"188\", \"Grenada\", 104000, 2e-05, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/b/bc/Flag_of_Grenada.svg/22px-Flag_of_Grenada.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"189\", \"Tonga\", 104000, 2e-05, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/9/9a/Flag_of_Tonga.svg/22px-Flag_of_Tonga.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"190\", \"Kiribati\", 100000, 1e-05, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/d/d3/Flag_of_Kiribati.svg/22px-Flag_of_Kiribati.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"192\", \"Antigua and Barbuda\", 89000, 1e-05, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/8/89/Flag_of_Antigua_and_Barbuda.svg/22px-Flag_of_Antigua_and_Barbuda.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"194\", \"Seychelles\", 85000, 1e-05, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/9/92/Flag_of_the_Seychelles.svg/22px-Flag_of_the_Seychelles.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"195\", \"Andorra\", 84082, 1e-05, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/1/19/Flag_of_Andorra.svg/22px-Flag_of_Andorra.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"198\", \"Dominica\", 67000, 1e-05, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/c/c4/Flag_of_Dominica.svg/22px-Flag_of_Dominica.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"200\", \"Marshall Islands\", 63000, 1e-05, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/2/2e/Flag_of_the_Marshall_Islands.svg/22px-Flag_of_the_Marshall_Islands.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"204\", \"Saint Kitts and Nevis\", 52000, 1e-05, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/f/fe/Flag_of_Saint_Kitts_and_Nevis.svg/22px-Flag_of_Saint_Kitts_and_Nevis.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"206\", \"Liechtenstein\", 35904, 5e-06, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/4/47/Flag_of_Liechtenstein.svg/22px-Flag_of_Liechtenstein.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"207\", \"Monaco\", 33000, 5e-06, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/e/ea/Flag_of_Monaco.svg/22px-Flag_of_Monaco.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"209\", \"San Marino\", 31794, 5e-06, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/b/b1/Flag_of_San_Marino.svg/22px-Flag_of_San_Marino.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"213\", \"Palau\", 20000, 3e-06, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/4/48/Flag_of_Palau.svg/22px-Flag_of_Palau.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"215\", \"Tuvalu\", 10000, 1e-06, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/3/38/Flag_of_Tuvalu.svg/22px-Flag_of_Tuvalu.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"216\", \"Nauru\", 10000, 1e-06, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/3/30/Flag_of_Nauru.svg/22px-Flag_of_Nauru.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"222\", \"Vatican City\", 800, 2e-07, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/0/00/Flag_of_the_Vatican_City.svg/20px-Flag_of_the_Vatican_City.svg.png\\\">\", true, new Date(2010,9,9) ] ]; data.addColumn('string','Rank'); data.addColumn('string','Country'); data.addColumn('number','Population'); data.addColumn('number','% of World Population'); data.addColumn('string','Flag'); data.addColumn('boolean','Mode'); data.addColumn('date','Date'); data.addRows(datajson); return(data); } // jsDrawChart function drawChartTableID2cd013050362() { var data = gvisDataTableID2cd013050362(); var options = {}; options[\"allowHtml\"] = true; options[\"page\"] = \"enable\"; var dataFormat1 = new google.visualization.NumberFormat({pattern:\"#,###\"}); dataFormat1.format(data, 2); var dataFormat2 = new google.visualization.NumberFormat({pattern:\"#.#%\"}); dataFormat2.format(data, 3); var chart = new google.visualization.Table( document.getElementById('TableID2cd013050362') ); chart.draw(data,options); } // jsDisplayChart (function() { var pkgs = window.__gvisPackages = window.__gvisPackages || []; var callbacks = window.__gvisCallbacks = window.__gvisCallbacks || []; var chartid = \"table\"; // Manually see if chartid is in pkgs (not all browsers support Array.indexOf) var i, newPackage = true; for (i = 0; newPackage && i < pkgs.length; i++) { if (pkgs[i] === chartid) newPackage = false; } if (newPackage) pkgs.push(chartid); // Add the drawChart function to the global list of callbacks callbacks.push(drawChartTableID2cd013050362); })(); function displayChartTableID2cd013050362() { var pkgs = window.__gvisPackages = window.__gvisPackages || []; var callbacks = window.__gvisCallbacks = window.__gvisCallbacks || []; window.clearTimeout(window.__gvisLoad); // The timeout is set to 100 because otherwise the container div we are // targeting might not be part of the document yet window.__gvisLoad = setTimeout(function() { var pkgCount = pkgs.length; google.load(\"visualization\", \"1\", { packages:pkgs, callback: function() { if (pkgCount != pkgs.length) { // Race condition where another setTimeout call snuck in after us; if // that call added a package, we must not shift its callback return; } while (callbacks.length > 0) callbacks.shift()(); } }); }, 100); } // jsFooter Dashboards with gvisMerge \u00b6 1 2 3 4 5 6 7 G <- gvisGeoChart ( Exports , locationvar = \"Country\" , colorvar = \"Profit\" , options = list ( width = 300 , height = 200 )) T <- gvisTable ( Exports , options = list ( width = 300 , height = 370 )) 1 2 3 GT <- gvisMerge ( G , T , horizontal = FALSE ) plot ( GT ) // jsData function gvisDataGeoChartID49611ffdda49 () { var data = new google.visualization.DataTable(); var datajson = [ [ \"Germany\", 3 ], [ \"Brazil\", 4 ], [ \"United States\", 5 ], [ \"France\", 4 ], [ \"Hungary\", 3 ], [ \"India\", 2 ], [ \"Iceland\", 1 ], [ \"Norway\", 4 ], [ \"Spain\", 5 ], [ \"Turkey\", 1 ] ]; data.addColumn('string','Country'); data.addColumn('number','Profit'); data.addRows(datajson); return(data); } // jsData function gvisDataTableID496155ab963a () { var data = new google.visualization.DataTable(); var datajson = [ [ \"Germany\", 3, true ], [ \"Brazil\", 4, false ], [ \"United States\", 5, true ], [ \"France\", 4, true ], [ \"Hungary\", 3, false ], [ \"India\", 2, true ], [ \"Iceland\", 1, false ], [ \"Norway\", 4, true ], [ \"Spain\", 5, true ], [ \"Turkey\", 1, false ] ]; data.addColumn('string','Country'); data.addColumn('number','Profit'); data.addColumn('boolean','Online'); data.addRows(datajson); return(data); } // jsDrawChart function drawChartGeoChartID49611ffdda49() { var data = gvisDataGeoChartID49611ffdda49(); var options = {}; options[\"width\"] = 300; options[\"height\"] = 200; var chart = new google.visualization.GeoChart( document.getElementById('GeoChartID49611ffdda49') ); chart.draw(data,options); } // jsDrawChart function drawChartTableID496155ab963a() { var data = gvisDataTableID496155ab963a(); var options = {}; options[\"allowHtml\"] = true; options[\"width\"] = 300; options[\"height\"] = 370; var chart = new google.visualization.Table( document.getElementById('TableID496155ab963a') ); chart.draw(data,options); } // jsDisplayChart (function() { var pkgs = window.__gvisPackages = window.__gvisPackages || []; var callbacks = window.__gvisCallbacks = window.__gvisCallbacks || []; var chartid = \"geochart\"; // Manually see if chartid is in pkgs (not all browsers support Array.indexOf) var i, newPackage = true; for (i = 0; newPackage && i < pkgs.length; i++) { if (pkgs[i] === chartid) newPackage = false; } if (newPackage) pkgs.push(chartid); // Add the drawChart function to the global list of callbacks callbacks.push(drawChartGeoChartID49611ffdda49); })(); function displayChartGeoChartID49611ffdda49() { var pkgs = window.__gvisPackages = window.__gvisPackages || []; var callbacks = window.__gvisCallbacks = window.__gvisCallbacks || []; window.clearTimeout(window.__gvisLoad); // The timeout is set to 100 because otherwise the container div we are // targeting might not be part of the document yet window.__gvisLoad = setTimeout(function() { var pkgCount = pkgs.length; google.load(\"visualization\", \"1\", { packages:pkgs, callback: function() { if (pkgCount != pkgs.length) { // Race condition where another setTimeout call snuck in after us; if // that call added a package, we must not shift its callback return; } while (callbacks.length > 0) callbacks.shift()(); } }); }, 100); } // jsDisplayChart (function() { var pkgs = window.__gvisPackages = window.__gvisPackages || []; var callbacks = window.__gvisCallbacks = window.__gvisCallbacks || []; var chartid = \"table\"; // Manually see if chartid is in pkgs (not all browsers support Array.indexOf) var i, newPackage = true; for (i = 0; newPackage && i < pkgs.length; i++) { if (pkgs[i] === chartid) newPackage = false; } if (newPackage) pkgs.push(chartid); // Add the drawChart function to the global list of callbacks callbacks.push(drawChartTableID496155ab963a); })(); function displayChartTableID496155ab963a() { var pkgs = window.__gvisPackages = window.__gvisPackages || []; var callbacks = window.__gvisCallbacks = window.__gvisCallbacks || []; window.clearTimeout(window.__gvisLoad); // The timeout is set to 100 because otherwise the container div we are // targeting might not be part of the document yet window.__gvisLoad = setTimeout(function() { var pkgCount = pkgs.length; google.load(\"visualization\", \"1\", { packages:pkgs, callback: function() { if (pkgCount != pkgs.length) { // Race condition where another setTimeout call snuck in after us; if // that call added a package, we must not shift its callback return; } while (callbacks.length > 0) callbacks.shift()(); } }); }, 100); } // jsFooter 1 2 3 4 G <- gvisGeoChart ( Exports , locationvar = \"Country\" , colorvar = \"Profit\" , options = list ( width = 300 , height = 370 )) 1 2 3 GT <- gvisMerge ( G , T , horizontal = TRUE ) plot ( GT ) // jsData function gvisDataGeoChartID114f1fa5c040 () { var data = new google.visualization.DataTable(); var datajson = [ [ \"Germany\", 3 ], [ \"Brazil\", 4 ], [ \"United States\", 5 ], [ \"France\", 4 ], [ \"Hungary\", 3 ], [ \"India\", 2 ], [ \"Iceland\", 1 ], [ \"Norway\", 4 ], [ \"Spain\", 5 ], [ \"Turkey\", 1 ] ]; data.addColumn('string','Country'); data.addColumn('number','Profit'); data.addRows(datajson); return(data); } // jsData function gvisDataTableID114f79fb112f () { var data = new google.visualization.DataTable(); var datajson = [ [ \"Germany\", 3, true ], [ \"Brazil\", 4, false ], [ \"United States\", 5, true ], [ \"France\", 4, true ], [ \"Hungary\", 3, false ], [ \"India\", 2, true ], [ \"Iceland\", 1, false ], [ \"Norway\", 4, true ], [ \"Spain\", 5, true ], [ \"Turkey\", 1, false ] ]; data.addColumn('string','Country'); data.addColumn('number','Profit'); data.addColumn('boolean','Online'); data.addRows(datajson); return(data); } // jsDrawChart function drawChartGeoChartID114f1fa5c040() { var data = gvisDataGeoChartID114f1fa5c040(); var options = {}; options[\"width\"] = 300; options[\"height\"] = 370; var chart = new google.visualization.GeoChart( document.getElementById('GeoChartID114f1fa5c040') ); chart.draw(data,options); } // jsDrawChart function drawChartTableID114f79fb112f() { var data = gvisDataTableID114f79fb112f(); var options = {}; options[\"allowHtml\"] = true; options[\"width\"] = 200; options[\"height\"] = 270; var chart = new google.visualization.Table( document.getElementById('TableID114f79fb112f') ); chart.draw(data,options); } // jsDisplayChart (function() { var pkgs = window.__gvisPackages = window.__gvisPackages || []; var callbacks = window.__gvisCallbacks = window.__gvisCallbacks || []; var chartid = \"geochart\"; // Manually see if chartid is in pkgs (not all browsers support Array.indexOf) var i, newPackage = true; for (i = 0; newPackage && i < pkgs.length; i++) { if (pkgs[i] === chartid) newPackage = false; } if (newPackage) pkgs.push(chartid); // Add the drawChart function to the global list of callbacks callbacks.push(drawChartGeoChartID114f1fa5c040); })(); function displayChartGeoChartID114f1fa5c040() { var pkgs = window.__gvisPackages = window.__gvisPackages || []; var callbacks = window.__gvisCallbacks = window.__gvisCallbacks || []; window.clearTimeout(window.__gvisLoad); // The timeout is set to 100 because otherwise the container div we are // targeting might not be part of the document yet window.__gvisLoad = setTimeout(function() { var pkgCount = pkgs.length; google.load(\"visualization\", \"1\", { packages:pkgs, callback: function() { if (pkgCount != pkgs.length) { // Race condition where another setTimeout call snuck in after us; if // that call added a package, we must not shift its callback return; } while (callbacks.length > 0) callbacks.shift()(); } }); }, 100); } // jsDisplayChart (function() { var pkgs = window.__gvisPackages = window.__gvisPackages || []; var callbacks = window.__gvisCallbacks = window.__gvisCallbacks || []; var chartid = \"table\"; // Manually see if chartid is in pkgs (not all browsers support Array.indexOf) var i, newPackage = true; for (i = 0; newPackage && i < pkgs.length; i++) { if (pkgs[i] === chartid) newPackage = false; } if (newPackage) pkgs.push(chartid); // Add the drawChart function to the global list of callbacks callbacks.push(drawChartTableID114f79fb112f); })(); function displayChartTableID114f79fb112f() { var pkgs = window.__gvisPackages = window.__gvisPackages || []; var callbacks = window.__gvisCallbacks = window.__gvisCallbacks || []; window.clearTimeout(window.__gvisLoad); // The timeout is set to 100 because otherwise the container div we are // targeting might not be part of the document yet window.__gvisLoad = setTimeout(function() { var pkgCount = pkgs.length; google.load(\"visualization\", \"1\", { packages:pkgs, callback: function() { if (pkgCount != pkgs.length) { // Race condition where another setTimeout call snuck in after us; if // that call added a package, we must not shift its callback return; } while (callbacks.length > 0) callbacks.shift()(); } }); }, 100); } // jsFooter Options \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 df <- data.frame ( country = c ( \"US\" , \"GB\" , \"BR\" ), val1 = c ( 1 , 3 , 4 ), val2 = c ( 23 , 12 , 32 )) Line <- gvisLineChart ( df , xvar = \"country\" , yvar = c ( \"val1\" , \"val2\" ), options = list ( title = \"Hello World\" , titleTextStyle = \"{color:'red', fontName:'Courier', fontSize:16}\" , backgroundColor = \"#D3D3D3\" , vAxis = \"{gridlines:{color:'red', count:3}}\" , hAxis = \"{title:'Country', titleTextStyle:{color:'blue'}}\" , series = \"[{color:'green', targetAxisIndex: 0}, {color: 'orange',targetAxisIndex:1}]\" , vAxes = \"[{title:'val1'}, {title:'val2'}]\" , legend = \"bottom\" , curveType = \"function\" , width = 500 , height = 300 )) // jsData function gvisDataLineChartID584f5d6f811a () { var data = new google.visualization.DataTable(); var datajson = [ [ \"US\", 1, 23 ], [ \"GB\", 3, 12 ], [ \"BR\", 4, 32 ] ]; data.addColumn('string','country'); data.addColumn('number','val1'); data.addColumn('number','val2'); data.addRows(datajson); return(data); } // jsDrawChart function drawChartLineChartID584f5d6f811a() { var data = gvisDataLineChartID584f5d6f811a(); var options = {}; options[\"allowHtml\"] = true; options[\"title\"] = \"Hello World\"; options[\"titleTextStyle\"] = {color:'red', fontName:'Courier', fontSize:16}; options[\"backgroundColor\"] = \"#D3D3D3\"; options[\"vAxis\"] = {gridlines:{color:'red', count:3}}; options[\"hAxis\"] = {title:'Country', titleTextStyle:{color:'blue'}}; options[\"series\"] = [{color:'green', targetAxisIndex: 0}, {color: 'orange',targetAxisIndex:1}]; options[\"vAxes\"] = [{title:'val1'}, {title:'val2'}]; options[\"legend\"] = \"bottom\"; options[\"curveType\"] = \"function\"; options[\"width\"] = 500; options[\"height\"] = 300; var chart = new google.visualization.LineChart( document.getElementById('LineChartID584f5d6f811a') ); chart.draw(data,options); } // jsDisplayChart (function() { var pkgs = window.__gvisPackages = window.__gvisPackages || []; var callbacks = window.__gvisCallbacks = window.__gvisCallbacks || []; var chartid = \"corechart\"; // Manually see if chartid is in pkgs (not all browsers support Array.indexOf) var i, newPackage = true; for (i = 0; newPackage && i < pkgs.length; i++) { if (pkgs[i] === chartid) newPackage = false; } if (newPackage) pkgs.push(chartid); // Add the drawChart function to the global list of callbacks callbacks.push(drawChartLineChartID584f5d6f811a); })(); function displayChartLineChartID584f5d6f811a() { var pkgs = window.__gvisPackages = window.__gvisPackages || []; var callbacks = window.__gvisCallbacks = window.__gvisCallbacks || []; window.clearTimeout(window.__gvisLoad); // The timeout is set to 100 because otherwise the container div we are // targeting might not be part of the document yet window.__gvisLoad = setTimeout(function() { var pkgCount = pkgs.length; google.load(\"visualization\", \"1\", { packages:pkgs, callback: function() { if (pkgCount != pkgs.length) { // Race condition where another setTimeout call snuck in after us; if // that call added a package, we must not shift its callback return; } while (callbacks.length > 0) callbacks.shift()(); } }); }, 100); } // jsFooter Apostrophes \u00b6 1 2 3 4 5 6 7 8 9 10 df <- data.frame ( \"Year\" = c ( 2009 , 2010 ), \"Lloyd\\\\'s\" = c ( 86.1 , 93.3 ), \"Munich Re\\\\'s R/I\" = c ( 95.3 , 100.5 ), check.names = FALSE ) R <- gvisColumnChart ( df , options = list ( vAxis = '{baseline:0}' , title = \"Combined Ratio %\" , legend = \"{position:'bottom'}\" )) cat ( R $ html $ chart , file = \"GoogleVis/R.html\" ) # save // jsData function gvisDataColumnChartID336e54fe1b42 () { var data = new google.visualization.DataTable(); var datajson = [ [ 2009, 86.1, 95.3 ], [ 2010, 93.3, 100.5 ] ]; data.addColumn('number','Year'); data.addColumn('number','Lloyd\\'s'); data.addColumn('number','Munich Re\\'s R/I'); data.addRows(datajson); return(data); } // jsDrawChart function drawChartColumnChartID336e54fe1b42() { var data = gvisDataColumnChartID336e54fe1b42(); var options = {}; options[\"allowHtml\"] = true; options[\"vAxis\"] = {baseline:0}; options[\"title\"] = \"Combined Ratio %\"; options[\"legend\"] = {position:'bottom'}; var chart = new google.visualization.ColumnChart( document.getElementById('ColumnChartID336e54fe1b42') ); chart.draw(data,options); } // jsDisplayChart (function() { var pkgs = window.__gvisPackages = window.__gvisPackages || []; var callbacks = window.__gvisCallbacks = window.__gvisCallbacks || []; var chartid = \"corechart\"; // Manually see if chartid is in pkgs (not all browsers support Array.indexOf) var i, newPackage = true; for (i = 0; newPackage && i < pkgs.length; i++) { if (pkgs[i] === chartid) newPackage = false; } if (newPackage) pkgs.push(chartid); // Add the drawChart function to the global list of callbacks callbacks.push(drawChartColumnChartID336e54fe1b42); })(); function displayChartColumnChartID336e54fe1b42() { var pkgs = window.__gvisPackages = window.__gvisPackages || []; var callbacks = window.__gvisCallbacks = window.__gvisCallbacks || []; window.clearTimeout(window.__gvisLoad); // The timeout is set to 100 because otherwise the container div we are // targeting might not be part of the document yet window.__gvisLoad = setTimeout(function() { var pkgCount = pkgs.length; google.load(\"visualization\", \"1\", { packages:pkgs, callback: function() { if (pkgCount != pkgs.length) { // Race condition where another setTimeout call snuck in after us; if // that call added a package, we must not shift its callback return; } while (callbacks.length > 0) callbacks.shift()(); } }); }, 100); } // jsFooter","title":"googleVis (embed HTML plots)"},{"location":"googlevis/#the-googlevis-package","text":"The package provides an interface to Google\u2019s chart tools, allowing users to create interactive charts based on data frames. It included maps. The interactive maps are displayed in a browser. We can plot a complete set of interactive graphs and embed them into a web page. Some motion charts cannot be displays in tablets and mobile phones (using HTML5) because they are rendered with Flash; Flash has to be installed on a PC. Examples . Charts: line, bar, column, area, stepped area, combo, scatter, bubble, customizing, candlestick (or boxplot), pie, gauge, annotation, Sankey, histogram, and motion (GapMinder-like) Maps: intensity, geo, choropleth, marker, Google Maps. Table, organizational chart, tree map, calendar, timeline, merging. Motion charts and some maps only work in Flash, not in HTML5 as with tablets and mobile phones). Gallery . Documentation . As above. Introduction . Roles . Trendlines . Markdown . In R, run a demo with demo(googleVis) . Always cite the package: 1 citation ( \"googleVis\" )","title":"The googleVis package"},{"location":"googlevis/#suppress-the-message","text":"We normally load the library\u2026 1 library ( googleVis ) \u2026but we can also suppress the message when loading the package. 1 suppressPackageStartupMessages ( library ( googleVis ))","title":"Suppress the message"},{"location":"googlevis/#printing","text":"Generate the chart. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 library ( googleVis ) head ( Fruits , 3 ) M <- gvisMotionChart ( Fruits , idvar = 'Fruit' , timevar = 'Year' , options = list ( width = 400 , height = 350 )) # xvar = # yvar = # colorvar = # sizevar = # date.format = Emulate the chart\u2026 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 # In the browser, with a tag underneath plot ( M ) # In the console (code only), with a tag underneath M # Header part # Basic html and formatting tags print ( M , tag = 'header' ) # Actual Google visualization code # Can be copy-paste in a markdown or html document print ( M , tag = 'chart' ) # Header + visualization code = what we see in the browser # Components of the chart print ( M , tag = 'jsChart' ) # Basic chart caption and html footer (what is underneath) print ( M , tag = 'caption' ) # Save it locally print ( M , file = \"GoogleVis/M.html\" ) # Or #cat(M$html$chart, file = \"GoogleVis/M.html\")","title":"Printing"},{"location":"googlevis/#embedding-a-chartmap-into-a-static-website","text":"The chart can be standing alone as an image file; even be embedded within a text. In RStudio, set the working directory with setwd(' ') . Set the option to print the output in a html file. 1 2 3 4 { r , message = FALSE } library ( googleVis ) op <- options ( gvis.plot.tag = 'chart' ) Set options back to original options. 1 options ( op ) Generate the chart. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 library ( googleVis ) # Set the option to print here (alternative) options ( gvis.plot.tag = 'chart' ) # Create the chart M <- gvisMotionChart ( Fruits , idvar = 'Fruit' , timevar = 'Year' , options = list ( width = 400 , height = 350 )) # Do not print the chart in the browser #plot(M) # in the browser # Print the chart as a html file cat ( M $ html $ chart , file = \"M.html\" ) The file has several parts: JavaScript and HTML. The part that displays the chart is in the end. 1 2 3 4 5 <!-- divChart --> < div id = \"MotionChartID2cd02805862d\" style = \"width: 400; height: 350;\" > </ div > Following this bullet, in the Markdown document, paste the M.html code. First, open the HTML file to copy the code. Modify the markup language with more styling parameters. 1 2 3 < div id = \"MotionChartID2cd02805862d\" style = \"width: 400; height: 350; float:left;\" ; > </ div > The HTML snippet must follow the JavaScript snippet in the document. Move the HTML snippet within a text (the text can separate both snippets). 1 2 3 4 5 Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. < div id = \"MotionChartID2cd02805862d\" style = \"width: 400; height: 350; float:left;\" ; > </ div > Duis aute irure dolor... Here is the result: // jsData function gvisDataMotionChartID2cd02805862d () { var data = new google.visualization.DataTable(); var datajson = [ [ \"Apples\", 2008, \"West\", 98, 78, 20, \"2008-12-31\" ], [ \"Apples\", 2009, \"West\", 111, 79, 32, \"2009-12-31\" ], [ \"Apples\", 2010, \"West\", 89, 76, 13, \"2010-12-31\" ], [ \"Oranges\", 2008, \"East\", 96, 81, 15, \"2008-12-31\" ], [ \"Bananas\", 2008, \"East\", 85, 76, 9, \"2008-12-31\" ], [ \"Oranges\", 2009, \"East\", 93, 80, 13, \"2009-12-31\" ], [ \"Bananas\", 2009, \"East\", 94, 78, 16, \"2009-12-31\" ], [ \"Oranges\", 2010, \"East\", 98, 91, 7, \"2010-12-31\" ], [ \"Bananas\", 2010, \"East\", 81, 71, 10, \"2010-12-31\" ] ]; data.addColumn('string','Fruit'); data.addColumn('number','Year'); data.addColumn('string','Location'); data.addColumn('number','Sales'); data.addColumn('number','Expenses'); data.addColumn('number','Profit'); data.addColumn('string','Date'); data.addRows(datajson); return(data); } // jsDrawChart function drawChartMotionChartID2cd02805862d() { var data = gvisDataMotionChartID2cd02805862d(); var options = {}; options[\"width\"] = 400; options[\"height\"] = 350; options[\"state\"] = \"\"; var chart = new google.visualization.MotionChart( document.getElementById('MotionChartID2cd02805862d') ); chart.draw(data,options); } // jsDisplayChart (function() { var pkgs = window.__gvisPackages = window.__gvisPackages || []; var callbacks = window.__gvisCallbacks = window.__gvisCallbacks || []; var chartid = \"motionchart\"; // Manually see if chartid is in pkgs (not all browsers support Array.indexOf) var i, newPackage = true; for (i = 0; newPackage && i < pkgs.length; i++) { if (pkgs[i] === chartid) newPackage = false; } if (newPackage) pkgs.push(chartid); // Add the drawChart function to the global list of callbacks callbacks.push(drawChartMotionChartID2cd02805862d); })(); function displayChartMotionChartID2cd02805862d() { var pkgs = window.__gvisPackages = window.__gvisPackages || []; var callbacks = window.__gvisCallbacks = window.__gvisCallbacks || []; window.clearTimeout(window.__gvisLoad); // The timeout is set to 100 because otherwise the container div we are // targeting might not be part of the document yet window.__gvisLoad = setTimeout(function() { var pkgCount = pkgs.length; google.load(\"visualization\", \"1\", { packages:pkgs, callback: function() { if (pkgCount != pkgs.length) { // Race condition where another setTimeout call snuck in after us; if // that call added a package, we must not shift its callback return; } while (callbacks.length > 0) callbacks.shift()(); } }); }, 100); } // jsFooter Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum. Sed ut perspiciatis unde omnis iste natus error sit voluptatem accusantium doloremque laudantium, totam rem aperiam, eaque ipsa quae ab illo inventore veritatis et quasi architecto beatae vitae dicta sunt explicabo. Nemo enim ipsam voluptatem quia voluptas sit aspernatur aut odit aut fugit, sed quia consequuntur magni dolores eos qui ratione voluptatem sequi nesciunt. Neque porro quisquam est, qui dolorem ipsum quia dolor sit amet, consectetur, adipisci velit, sed quia non numquam eius modi tempora incidunt ut labore et dolore magnam aliquam quaerat voluptatem.","title":"Embedding a chart/map into a static website"},{"location":"googlevis/#other-static-websites","text":"There is a procedure for embedding graphics in: WordPress. Google Sites. Blogger. Google Code wiki pages. Wikipedia. others websites.","title":"Other static websites"},{"location":"googlevis/#dynamic-websites","text":"The R.rsp package allows the integration of R code into html code. The rApache and brew packages support web application development using R and the Apache HTTP server. Rook is a lightweight web server interface for R. The shiny package builds interactive web application with R.","title":"Dynamic websites"},{"location":"googlevis/#charts","text":"Consult the examples (further above); charts are similar to what we find in other packages. gvisMotionChart is exclusive to googleVis .","title":"Charts"},{"location":"googlevis/#gvismotionchart","text":"1 head ( Fruits , 3 ) 1 2 3 4 5 # Output Fruit Year Location Sales Expenses Profit Date 1 Apples 2008 West 98 78 20 2008-12-31 2 Apples 2009 West 111 79 32 2009-12-31 3 Apples 2010 West 89 76 13 2010-12-31 1 2 3 4 5 6 7 8 9 10 11 12 M <- gvisMotionChart ( Fruits , idvar = 'Fruit' , timevar = 'Year' , options = list ( width = 400 , height = 350 )) # xvar = # yvar = # colorvar = # sizevar = # date.format = plot ( M ) // jsData function gvisDataMotionChartID336e668836ab () { var data = new google.visualization.DataTable(); var datajson = [ [ \"Apples\", 2008, \"West\", 98, 78, 20, \"2008-12-31\" ], [ \"Apples\", 2009, \"West\", 111, 79, 32, \"2009-12-31\" ], [ \"Apples\", 2010, \"West\", 89, 76, 13, \"2010-12-31\" ], [ \"Oranges\", 2008, \"East\", 96, 81, 15, \"2008-12-31\" ], [ \"Bananas\", 2008, \"East\", 85, 76, 9, \"2008-12-31\" ], [ \"Oranges\", 2009, \"East\", 93, 80, 13, \"2009-12-31\" ], [ \"Bananas\", 2009, \"East\", 94, 78, 16, \"2009-12-31\" ], [ \"Oranges\", 2010, \"East\", 98, 91, 7, \"2010-12-31\" ], [ \"Bananas\", 2010, \"East\", 81, 71, 10, \"2010-12-31\" ] ]; data.addColumn('string','Fruit'); data.addColumn('number','Year'); data.addColumn('string','Location'); data.addColumn('number','Sales'); data.addColumn('number','Expenses'); data.addColumn('number','Profit'); data.addColumn('string','Date'); data.addRows(datajson); return(data); } // jsDrawChart function drawChartMotionChartID336e668836ab() { var data = gvisDataMotionChartID336e668836ab(); var options = {}; options[\"width\"] = 400; options[\"height\"] = 350; options[\"state\"] = \"\"; var chart = new google.visualization.MotionChart( document.getElementById('MotionChartID336e668836ab') ); chart.draw(data,options); } // jsDisplayChart (function() { var pkgs = window.__gvisPackages = window.__gvisPackages || []; var callbacks = window.__gvisCallbacks = window.__gvisCallbacks || []; var chartid = \"motionchart\"; // Manually see if chartid is in pkgs (not all browsers support Array.indexOf) var i, newPackage = true; for (i = 0; newPackage && i < pkgs.length; i++) { if (pkgs[i] === chartid) newPackage = false; } if (newPackage) pkgs.push(chartid); // Add the drawChart function to the global list of callbacks callbacks.push(drawChartMotionChartID336e668836ab); })(); function displayChartMotionChartID336e668836ab() { var pkgs = window.__gvisPackages = window.__gvisPackages || []; var callbacks = window.__gvisCallbacks = window.__gvisCallbacks || []; window.clearTimeout(window.__gvisLoad); // The timeout is set to 100 because otherwise the container div we are // targeting might not be part of the document yet window.__gvisLoad = setTimeout(function() { var pkgCount = pkgs.length; google.load(\"visualization\", \"1\", { packages:pkgs, callback: function() { if (pkgCount != pkgs.length) { // Race condition where another setTimeout call snuck in after us; if // that call added a package, we must not shift its callback return; } while (callbacks.length > 0) callbacks.shift()(); } }); }, 100); } // jsFooter","title":"gvisMotionChart"},{"location":"googlevis/#embedding-with-iframe-or-embed","text":"After we generate the HTML chart or map, save the object as a HTML file. 1 2 3 4 library ( htmlwidgets ) library ( DT ) saveWidget ( object , \"googleVis.html\" ) Add these HTML snippets to the Markdown/HTML document. 1 2 < iframe seamless src = \"../img/googlevis.html\" width = 600px height = 400px ></ iframe > 1 < embed seamless src = \"../img/googlevis.html\" width = 600px height = 400px ></ embed > In other words, we can either write the code or embed a file into the HTML/Markdown document.","title":"Embedding with &lt;iframe&gt; or &lt;embed&gt;"},{"location":"googlevis/#information-from-the-object","text":"1 2 M $ type M $ chartid 1 2 \"MotionChart\" \"MotionChartID336e668836ab\"","title":"Information from the object"},{"location":"googlevis/#maps","text":"","title":"Maps"},{"location":"googlevis/#gvisgeochart","text":"1 head ( Exports , 3 ) 1 2 3 4 5 # Output Country Profit Online 1 Germany 3 TRUE 2 Brazil 4 FALSE 3 United States 5 TRUE 1 2 3 4 5 6 7 8 9 10 Geo <- gvisGeoChart ( Exports , locationvar = \"Country\" , colorvar = \"Profit\" , sizevar = \"\" , # size of markers hovervar = \"\" , # text options = list ( projection = \"kavrayskiy-vii\" )) # locationvar can be lat:long or address, country name, region, state, city plot ( Geo ) // jsData function gvisDataGeoChartID49617fef373 () { var data = new google.visualization.DataTable(); var datajson = [ [ \"Germany\", 3 ], [ \"Brazil\", 4 ], [ \"United States\", 5 ], [ \"France\", 4 ], [ \"Hungary\", 3 ], [ \"India\", 2 ], [ \"Iceland\", 1 ], [ \"Norway\", 4 ], [ \"Spain\", 5 ], [ \"Turkey\", 1 ] ]; data.addColumn('string','Country'); data.addColumn('number','Profit'); data.addRows(datajson); return(data); } // jsDrawChart function drawChartGeoChartID49617fef373() { var data = gvisDataGeoChartID49617fef373(); var options = {}; options[\"width\"] = 556; options[\"height\"] = 347; options[\"projection\"] = \"kavrayskiy-vii\"; var chart = new google.visualization.GeoChart( document.getElementById('GeoChartID49617fef373') ); chart.draw(data,options); } // jsDisplayChart (function() { var pkgs = window.__gvisPackages = window.__gvisPackages || []; var callbacks = window.__gvisCallbacks = window.__gvisCallbacks || []; var chartid = \"geochart\"; // Manually see if chartid is in pkgs (not all browsers support Array.indexOf) var i, newPackage = true; for (i = 0; newPackage && i < pkgs.length; i++) { if (pkgs[i] === chartid) newPackage = false; } if (newPackage) pkgs.push(chartid); // Add the drawChart function to the global list of callbacks callbacks.push(drawChartGeoChartID49617fef373); })(); function displayChartGeoChartID49617fef373() { var pkgs = window.__gvisPackages = window.__gvisPackages || []; var callbacks = window.__gvisCallbacks = window.__gvisCallbacks || []; window.clearTimeout(window.__gvisLoad); // The timeout is set to 100 because otherwise the container div we are // targeting might not be part of the document yet window.__gvisLoad = setTimeout(function() { var pkgCount = pkgs.length; google.load(\"visualization\", \"1\", { packages:pkgs, callback: function() { if (pkgCount != pkgs.length) { // Race condition where another setTimeout call snuck in after us; if // that call added a package, we must not shift its callback return; } while (callbacks.length > 0) callbacks.shift()(); } }); }, 100); } // jsFooter 1 head ( CityPopularity , 3 ) 1 2 3 4 5 # Output City Popularity 1 New York 200 2 Boston 300 3 Miami 400 1 2 3 4 5 6 7 8 9 Geo2 <- gvisGeoChart ( CityPopularity , locationvar = 'City' , colorvar = 'Popularity' , options = list ( region = 'US' , height = 350 , displayMode = 'markers' , colorAxis = \"{values:[200,400,600,800], colors:[\\'red', \\'pink\\', \\'orange',\\'green']}\" )) plot ( Geo2 ) // jsData function gvisDataGeoChartID4961741b9385 () { var data = new google.visualization.DataTable(); var datajson = [ [ \"New York\", 200 ], [ \"Boston\", 300 ], [ \"Miami\", 400 ], [ \"Chicago\", 500 ], [ \"Los Angeles\", 600 ], [ \"Houston\", 700 ] ]; data.addColumn('string','City'); data.addColumn('number','Popularity'); data.addRows(datajson); return(data); } // jsDrawChart function drawChartGeoChartID4961741b9385() { var data = gvisDataGeoChartID4961741b9385(); var options = {}; options[\"width\"] = 556; options[\"height\"] = 350; options[\"region\"] = \"US\"; options[\"displayMode\"] = \"markers\"; options[\"colorAxis\"] = {values:[200,400,600,800], colors:['red', 'pink', 'orange','green']}; var chart = new google.visualization.GeoChart( document.getElementById('GeoChartID4961741b9385') ); chart.draw(data,options); } // jsDisplayChart (function() { var pkgs = window.__gvisPackages = window.__gvisPackages || []; var callbacks = window.__gvisCallbacks = window.__gvisCallbacks || []; var chartid = \"geochart\"; // Manually see if chartid is in pkgs (not all browsers support Array.indexOf) var i, newPackage = true; for (i = 0; newPackage && i < pkgs.length; i++) { if (pkgs[i] === chartid) newPackage = false; } if (newPackage) pkgs.push(chartid); // Add the drawChart function to the global list of callbacks callbacks.push(drawChartGeoChartID4961741b9385); })(); function displayChartGeoChartID4961741b9385() { var pkgs = window.__gvisPackages = window.__gvisPackages || []; var callbacks = window.__gvisCallbacks = window.__gvisCallbacks || []; window.clearTimeout(window.__gvisLoad); // The timeout is set to 100 because otherwise the container div we are // targeting might not be part of the document yet window.__gvisLoad = setTimeout(function() { var pkgCount = pkgs.length; google.load(\"visualization\", \"1\", { packages:pkgs, callback: function() { if (pkgCount != pkgs.length) { // Race condition where another setTimeout call snuck in after us; if // that call added a package, we must not shift its callback return; } while (callbacks.length > 0) callbacks.shift()(); } }); }, 100); } // jsFooter 1 2 3 4 5 6 7 8 9 10 11 12 CityPopularity3 <- data.frame ( City = c ( 'Montreal' , 'Toronto' ), Popularity = c ( 400 , 200 )) Geo3 <- gvisGeoChart ( CityPopularity3 , locationvar = 'City' , colorvar = 'Popularity' , options = list ( region = 'CA' , height = 350 , displayMode = 'markers' , colorAxis = \"{values:[200,400,600,800], colors:[\\'red', \\'pink\\', \\'orange',\\'green']}\" )) plot ( Geo3 ) // jsData function gvisDataGeoChartID49614f24acff () { var data = new google.visualization.DataTable(); var datajson = [ [ \"Montreal\", 700 ], [ \"Toronto\", 200 ] ]; data.addColumn('string','City'); data.addColumn('number','Popularity'); data.addRows(datajson); return(data); } // jsDrawChart function drawChartGeoChartID49614f24acff() { var data = gvisDataGeoChartID49614f24acff(); var options = {}; options[\"width\"] = 556; options[\"height\"] = 350; options[\"region\"] = \"CA\"; options[\"displayMode\"] = \"markers\"; options[\"colorAxis\"] = {values:[200,400,600,800], colors:['red', 'pink', 'orange','green']}; var chart = new google.visualization.GeoChart( document.getElementById('GeoChartID49614f24acff') ); chart.draw(data,options); } // jsDisplayChart (function() { var pkgs = window.__gvisPackages = window.__gvisPackages || []; var callbacks = window.__gvisCallbacks = window.__gvisCallbacks || []; var chartid = \"geochart\"; // Manually see if chartid is in pkgs (not all browsers support Array.indexOf) var i, newPackage = true; for (i = 0; newPackage && i < pkgs.length; i++) { if (pkgs[i] === chartid) newPackage = false; } if (newPackage) pkgs.push(chartid); // Add the drawChart function to the global list of callbacks callbacks.push(drawChartGeoChartID49614f24acff); })(); function displayChartGeoChartID49614f24acff() { var pkgs = window.__gvisPackages = window.__gvisPackages || []; var callbacks = window.__gvisCallbacks = window.__gvisCallbacks || []; window.clearTimeout(window.__gvisLoad); // The timeout is set to 100 because otherwise the container div we are // targeting might not be part of the document yet window.__gvisLoad = setTimeout(function() { var pkgCount = pkgs.length; google.load(\"visualization\", \"1\", { packages:pkgs, callback: function() { if (pkgCount != pkgs.length) { // Race condition where another setTimeout call snuck in after us; if // that call added a package, we must not shift its callback return; } while (callbacks.length > 0) callbacks.shift()(); } }); }, 100); } // jsFooter 1 head ( Andrew , 3 ) 1 2 3 4 5 6 7 8 9 # Output Date / Time UTC Lat Long Pressure_mb Speed_kt Category 1 1992 - 08 - 16 18 : 00 : 00 10.8 - 35.5 1010 25 Tropical Depression 2 1992 - 08 - 17 00 : 00 : 00 11.2 - 37.4 1009 30 Tropical Depression 3 1992 - 08 - 17 06 : 00 : 00 11.7 - 39.6 1008 30 Tropical Depression LatLong Tip 1 10.8 :- 35.5 Tropical Depression < BR > Pressure = 1010 < BR > Speed = 25 2 11.2 :- 37.4 Tropical Depression < BR > Pressure = 1009 < BR > Speed = 30 3 11.7 :- 39.6 Tropical Depression < BR > Pressure = 1008 < BR > Speed = 30 1 2 3 4 5 6 7 GeoMarker <- gvisGeoChart ( Andrew , locationvar = \"LatLong\" , sizevar = 'Speed_kt' , colorvar = \"Pressure_mb\" , options = list ( region = \"US\" )) plot ( GeoMarker ) // jsData function gvisDataGeoChartID2cd02f295f7 () { var data = new google.visualization.DataTable(); var datajson = [ [ 10.8, -35.5, 1010, 25 ], [ 11.2, -37.4, 1009, 30 ], [ 11.7, -39.6, 1008, 30 ], [ 12.3, -42, 1006, 35 ], [ 13.1, -44.2, 1003, 35 ], [ 13.6, -46.2, 1002, 40 ], [ 14.1, -48, 1001, 45 ], [ 14.6, -49.9, 1000, 45 ], [ 15.4, -51.8, 1000, 45 ], [ 16.3, -53.5, 1001, 45 ], [ 17.2, -55.3, 1002, 45 ], [ 18, -56.9, 1005, 45 ], [ 18.8, -58.3, 1007, 45 ], [ 19.8, -59.3, 1011, 40 ], [ 20.7, -60, 1013, 40 ], [ 21.7, -60.7, 1015, 40 ], [ 22.5, -61.5, 1014, 40 ], [ 23.2, -62.4, 1014, 45 ], [ 23.9, -63.3, 1010, 45 ], [ 24.4, -64.2, 1007, 50 ], [ 24.8, -64.9, 1004, 50 ], [ 25.3, -65.9, 1000, 55 ], [ 25.6, -67, 994, 60 ], [ 25.8, -68.3, 981, 70 ], [ 25.7, -69.7, 969, 80 ], [ 25.6, -71.1, 961, 90 ], [ 25.5, -72.5, 947, 105 ], [ 25.4, -74.2, 933, 120 ], [ 25.4, -75.8, 922, 135 ], [ 25.4, -77.5, 930, 125 ], [ 25.4, -79.3, 937, 120 ], [ 25.6, -81.2, 951, 110 ], [ 25.8, -83.1, 947, 115 ], [ 26.2, -85, 943, 115 ], [ 26.6, -86.7, 948, 115 ], [ 27.2, -88.2, 946, 115 ], [ 27.8, -89.6, 941, 120 ], [ 28.5, -90.5, 937, 120 ], [ 29.2, -91.3, 955, 115 ], [ 30.1, -91.7, 973, 80 ], [ 30.9, -91.6, 991, 50 ], [ 31.5, -91.1, 995, 35 ], [ 32.1, -90.5, 997, 30 ], [ 32.8, -89.6, 998, 30 ], [ 33.6, -88.4, 999, 25 ], [ 34.4, -86.7, 1000, 20 ], [ 35.4, -84, 1000, 20 ] ]; data.addColumn('number','Latitude'); data.addColumn('number','Longitude'); data.addColumn('number','Pressure_mb'); data.addColumn('number','Speed_kt'); data.addRows(datajson); return(data); } // jsDrawChart function drawChartGeoChartID2cd02f295f7() { var data = gvisDataGeoChartID2cd02f295f7(); var options = {}; options[\"width\"] = 556; options[\"height\"] = 347; options[\"region\"] = \"US\"; var chart = new google.visualization.GeoChart( document.getElementById('GeoChartID2cd02f295f7') ); chart.draw(data,options); } // jsDisplayChart (function() { var pkgs = window.__gvisPackages = window.__gvisPackages || []; var callbacks = window.__gvisCallbacks = window.__gvisCallbacks || []; var chartid = \"geochart\"; // Manually see if chartid is in pkgs (not all browsers support Array.indexOf) var i, newPackage = true; for (i = 0; newPackage && i < pkgs.length; i++) { if (pkgs[i] === chartid) newPackage = false; } if (newPackage) pkgs.push(chartid); // Add the drawChart function to the global list of callbacks callbacks.push(drawChartGeoChartID2cd02f295f7); })(); function displayChartGeoChartID2cd02f295f7() { var pkgs = window.__gvisPackages = window.__gvisPackages || []; var callbacks = window.__gvisCallbacks = window.__gvisCallbacks || []; window.clearTimeout(window.__gvisLoad); // The timeout is set to 100 because otherwise the container div we are // targeting might not be part of the document yet window.__gvisLoad = setTimeout(function() { var pkgCount = pkgs.length; google.load(\"visualization\", \"1\", { packages:pkgs, callback: function() { if (pkgCount != pkgs.length) { // Race condition where another setTimeout call snuck in after us; if // that call added a package, we must not shift its callback return; } while (callbacks.length > 0) callbacks.shift()(); } }); }, 100); } // jsFooter","title":"gvisGeoChart"},{"location":"googlevis/#gvismap-or-google-maps","text":"1 2 3 4 5 6 7 8 9 10 AndrewMap <- gvisMap ( Andrew , locationvar = \"LatLong\" , tipvar = \"Tip\" , # text displayed over the tip icon options = list ( showTip = TRUE , showLine = TRUE , enableScrollWheel = TRUE , mapType = 'terrain' , useMapTypeControl = TRUE )) plot ( AndrewMap ) // jsData function gvisDataMapID496129f7ada () { var data = new google.visualization.DataTable(); var datajson = [ [ 10.8, -35.5, \"Tropical Depression<BR>Pressure=1010<BR>Speed=25\" ], [ 11.2, -37.4, \"Tropical Depression<BR>Pressure=1009<BR>Speed=30\" ], [ 11.7, -39.6, \"Tropical Depression<BR>Pressure=1008<BR>Speed=30\" ], [ 12.3, -42, \"Tropical Storm<BR>Pressure=1006<BR>Speed=35\" ], [ 13.1, -44.2, \"Tropical Storm<BR>Pressure=1003<BR>Speed=35\" ], [ 13.6, -46.2, \"Tropical Storm<BR>Pressure=1002<BR>Speed=40\" ], [ 14.1, -48, \"Tropical Storm<BR>Pressure=1001<BR>Speed=45\" ], [ 14.6, -49.9, \"Tropical Storm<BR>Pressure=1000<BR>Speed=45\" ], [ 15.4, -51.8, \"Tropical Storm<BR>Pressure=1000<BR>Speed=45\" ], [ 16.3, -53.5, \"Tropical Storm<BR>Pressure=1001<BR>Speed=45\" ], [ 17.2, -55.3, \"Tropical Storm<BR>Pressure=1002<BR>Speed=45\" ], [ 18, -56.9, \"Tropical Storm<BR>Pressure=1005<BR>Speed=45\" ], [ 18.8, -58.3, \"Tropical Storm<BR>Pressure=1007<BR>Speed=45\" ], [ 19.8, -59.3, \"Tropical Storm<BR>Pressure=1011<BR>Speed=40\" ], [ 20.7, -60, \"Tropical Storm<BR>Pressure=1013<BR>Speed=40\" ], [ 21.7, -60.7, \"Tropical Storm<BR>Pressure=1015<BR>Speed=40\" ], [ 22.5, -61.5, \"Tropical Storm<BR>Pressure=1014<BR>Speed=40\" ], [ 23.2, -62.4, \"Tropical Storm<BR>Pressure=1014<BR>Speed=45\" ], [ 23.9, -63.3, \"Tropical Storm<BR>Pressure=1010<BR>Speed=45\" ], [ 24.4, -64.2, \"Tropical Storm<BR>Pressure=1007<BR>Speed=50\" ], [ 24.8, -64.9, \"Tropical Storm<BR>Pressure=1004<BR>Speed=50\" ], [ 25.3, -65.9, \"Tropical Storm<BR>Pressure=1000<BR>Speed=55\" ], [ 25.6, -67, \"Tropical Storm<BR>Pressure=994<BR>Speed=60\" ], [ 25.8, -68.3, \"Hurricane<BR>Pressure=981<BR>Speed=70\" ], [ 25.7, -69.7, \"Hurricane<BR>Pressure=969<BR>Speed=80\" ], [ 25.6, -71.1, \"Hurricane<BR>Pressure=961<BR>Speed=90\" ], [ 25.5, -72.5, \"Hurricane<BR>Pressure=947<BR>Speed=105\" ], [ 25.4, -74.2, \"Hurricane<BR>Pressure=933<BR>Speed=120\" ], [ 25.4, -75.8, \"Hurricane<BR>Pressure=922<BR>Speed=135\" ], [ 25.4, -77.5, \"Hurricane<BR>Pressure=930<BR>Speed=125\" ], [ 25.4, -79.3, \"Hurricane<BR>Pressure=937<BR>Speed=120\" ], [ 25.6, -81.2, \"Hurricane<BR>Pressure=951<BR>Speed=110\" ], [ 25.8, -83.1, \"Hurricane<BR>Pressure=947<BR>Speed=115\" ], [ 26.2, -85, \"Hurricane<BR>Pressure=943<BR>Speed=115\" ], [ 26.6, -86.7, \"Hurricane<BR>Pressure=948<BR>Speed=115\" ], [ 27.2, -88.2, \"Hurricane<BR>Pressure=946<BR>Speed=115\" ], [ 27.8, -89.6, \"Hurricane<BR>Pressure=941<BR>Speed=120\" ], [ 28.5, -90.5, \"Hurricane<BR>Pressure=937<BR>Speed=120\" ], [ 29.2, -91.3, \"Hurricane<BR>Pressure=955<BR>Speed=115\" ], [ 30.1, -91.7, \"Tropical Storm<BR>Pressure=973<BR>Speed=80\" ], [ 30.9, -91.6, \"Tropical Storm<BR>Pressure=991<BR>Speed=50\" ], [ 31.5, -91.1, \"Tropical Depression<BR>Pressure=995<BR>Speed=35\" ], [ 32.1, -90.5, \"Tropical Depression<BR>Pressure=997<BR>Speed=30\" ], [ 32.8, -89.6, \"Tropical Depression<BR>Pressure=998<BR>Speed=30\" ], [ 33.6, -88.4, \"Tropical Depression<BR>Pressure=999<BR>Speed=25\" ], [ 34.4, -86.7, \"Tropical Depression<BR>Pressure=1000<BR>Speed=20\" ], [ 35.4, -84, \"Tropical Depression<BR>Pressure=1000<BR>Speed=20\" ] ]; data.addColumn('number','Latitude'); data.addColumn('number','Longitude'); data.addColumn('string','Tip'); data.addRows(datajson); return(data); } // jsDrawChart function drawChartMapID496129f7ada() { var data = gvisDataMapID496129f7ada(); var options = {}; options[\"showTip\"] = true; options[\"showLine\"] = true; options[\"enableScrollWheel\"] = true; options[\"mapType\"] = \"terrain\"; options[\"useMapTypeControl\"] = true; var chart = new google.visualization.Map( document.getElementById('MapID496129f7ada') ); chart.draw(data,options); } // jsDisplayChart (function() { var pkgs = window.__gvisPackages = window.__gvisPackages || []; var callbacks = window.__gvisCallbacks = window.__gvisCallbacks || []; var chartid = \"map\"; // Manually see if chartid is in pkgs (not all browsers support Array.indexOf) var i, newPackage = true; for (i = 0; newPackage && i < pkgs.length; i++) { if (pkgs[i] === chartid) newPackage = false; } if (newPackage) pkgs.push(chartid); // Add the drawChart function to the global list of callbacks callbacks.push(drawChartMapID496129f7ada); })(); function displayChartMapID496129f7ada() { var pkgs = window.__gvisPackages = window.__gvisPackages || []; var callbacks = window.__gvisCallbacks = window.__gvisCallbacks || []; window.clearTimeout(window.__gvisLoad); // The timeout is set to 100 because otherwise the container div we are // targeting might not be part of the document yet window.__gvisLoad = setTimeout(function() { var pkgCount = pkgs.length; google.load(\"visualization\", \"1\", { packages:pkgs, callback: function() { if (pkgCount != pkgs.length) { // Race condition where another setTimeout call snuck in after us; if // that call added a package, we must not shift its callback return; } while (callbacks.length > 0) callbacks.shift()(); } }); }, 100); } // jsFooter","title":"gvisMap or Google Maps"},{"location":"googlevis/#editor","text":"This is a fantastic option that let us start with one chart or map and change everything with a menu. 1 2 3 4 5 6 Editor <- gvisGeoChart ( Exports , locationvar = \"Country\" , colorvar = \"Profit\" , options = list ( gvis.editor = 'Edit me!' )) plot ( Editor ) // jsData function gvisDataGeoChartID49613aa5c54a () { var data = new google.visualization.DataTable(); var datajson = [ [ \"Germany\", 3 ], [ \"Brazil\", 4 ], [ \"United States\", 5 ], [ \"France\", 4 ], [ \"Hungary\", 3 ], [ \"India\", 2 ], [ \"Iceland\", 1 ], [ \"Norway\", 4 ], [ \"Spain\", 5 ], [ \"Turkey\", 1 ] ]; data.addColumn('string','Country'); data.addColumn('number','Profit'); data.addRows(datajson); return(data); } // jsDrawChart function drawChartGeoChartID49613aa5c54a() { var data = gvisDataGeoChartID49613aa5c54a(); var options = {}; options[\"width\"] = 556; options[\"height\"] = 347; chartGeoChartID49613aa5c54a = new google.visualization.ChartWrapper({ dataTable: data, chartType: 'GeoChart', containerId: 'GeoChartID49613aa5c54a', options: options }); chartGeoChartID49613aa5c54a.draw(); } function openEditorGeoChartID49613aa5c54a() { var editor = new google.visualization.ChartEditor(); google.visualization.events.addListener(editor, 'ok', function() { chartGeoChartID49613aa5c54a = editor.getChartWrapper(); chartGeoChartID49613aa5c54a.draw(document.getElementById('GeoChartID49613aa5c54a')); }); editor.openDialog(chartGeoChartID49613aa5c54a); } // jsDisplayChart (function() { var pkgs = window.__gvisPackages = window.__gvisPackages || []; var callbacks = window.__gvisCallbacks = window.__gvisCallbacks || []; var chartid = \"charteditor\"; // Manually see if chartid is in pkgs (not all browsers support Array.indexOf) var i, newPackage = true; for (i = 0; newPackage && i < pkgs.length; i++) { if (pkgs[i] === chartid) newPackage = false; } if (newPackage) pkgs.push(chartid); // Add the drawChart function to the global list of callbacks callbacks.push(drawChartGeoChartID49613aa5c54a); })(); function displayChartGeoChartID49613aa5c54a() { var pkgs = window.__gvisPackages = window.__gvisPackages || []; var callbacks = window.__gvisCallbacks = window.__gvisCallbacks || []; window.clearTimeout(window.__gvisLoad); // The timeout is set to 100 because otherwise the container div we are // targeting might not be part of the document yet window.__gvisLoad = setTimeout(function() { var pkgCount = pkgs.length; google.load(\"visualization\", \"1\", { packages:pkgs, callback: function() { if (pkgCount != pkgs.length) { // Race condition where another setTimeout call snuck in after us; if // that call added a package, we must not shift its callback return; } while (callbacks.length > 0) callbacks.shift()(); } }); }, 100); } // jsFooter","title":"Editor"},{"location":"googlevis/#tables-with-gvistable","text":"1 head ( Stock , 3 ) 1 2 3 4 5 # Output Date Device Value Title Annotation 1 2008-01-01 Pencils 3000 <NA> <NA> 2 2008-01-02 Pencils 14045 <NA> <NA> 3 2008-01-03 Pencils 5502 <NA> <NA> 1 2 3 4 Table <- gvisTable ( Stock , formats = list ( Value = \"#,###\" )) plot ( Table ) // jsData function gvisDataTableID2cd03b735929 () { var data = new google.visualization.DataTable(); var datajson = [ [ new Date(2008,0,1), \"Pencils\", 3000, null, null ], [ new Date(2008,0,2), \"Pencils\", 14045, null, null ], [ new Date(2008,0,3), \"Pencils\", 5502, null, null ], [ new Date(2008,0,4), \"Pencils\", 75284, null, null ], [ new Date(2008,0,5), \"Pencils\", 41476, \"Bought pencils\", \"Bought 200k pencils\" ], [ new Date(2008,0,6), \"Pencils\", 333222, null, null ], [ new Date(2008,0,1), \"Pens\", 40645, null, null ], [ new Date(2008,0,2), \"Pens\", 20374, null, null ], [ new Date(2008,0,3), \"Pens\", 50766, null, null ], [ new Date(2008,0,4), \"Pens\", 14334, \"Out of stock\", \"Ran out of stock of pens at 4pm\" ], [ new Date(2008,0,5), \"Pens\", 66467, null, null ], [ new Date(2008,0,6), \"Pens\", 39463, null, null ] ]; data.addColumn('date','Date'); data.addColumn('string','Device'); data.addColumn('number','Value'); data.addColumn('string','Title'); data.addColumn('string','Annotation'); data.addRows(datajson); return(data); } // jsDrawChart function drawChartTableID2cd03b735929() { var data = gvisDataTableID2cd03b735929(); var options = {}; options[\"allowHtml\"] = true; var dataFormat1 = new google.visualization.NumberFormat({pattern:\"#,###\"}); dataFormat1.format(data, 2); var chart = new google.visualization.Table( document.getElementById('TableID2cd03b735929') ); chart.draw(data,options); } // jsDisplayChart (function() { var pkgs = window.__gvisPackages = window.__gvisPackages || []; var callbacks = window.__gvisCallbacks = window.__gvisCallbacks || []; var chartid = \"table\"; // Manually see if chartid is in pkgs (not all browsers support Array.indexOf) var i, newPackage = true; for (i = 0; newPackage && i < pkgs.length; i++) { if (pkgs[i] === chartid) newPackage = false; } if (newPackage) pkgs.push(chartid); // Add the drawChart function to the global list of callbacks callbacks.push(drawChartTableID2cd03b735929); })(); function displayChartTableID2cd03b735929() { var pkgs = window.__gvisPackages = window.__gvisPackages || []; var callbacks = window.__gvisCallbacks = window.__gvisCallbacks || []; window.clearTimeout(window.__gvisLoad); // The timeout is set to 100 because otherwise the container div we are // targeting might not be part of the document yet window.__gvisLoad = setTimeout(function() { var pkgCount = pkgs.length; google.load(\"visualization\", \"1\", { packages:pkgs, callback: function() { if (pkgCount != pkgs.length) { // Race condition where another setTimeout call snuck in after us; if // that call added a package, we must not shift its callback return; } while (callbacks.length > 0) callbacks.shift()(); } }); }, 100); } // jsFooter 1 head ( Population , 3 ) 1 2 3 4 5 6 7 8 9 10 11 12 13 # Output Rank Country Population % of World Population 1 1 China 1339940000 0.1950 2 2 India 1188650000 0.1730 3 3 United States 310438000 0.0452 Flag 1 <img src=\"http://upload.wikimedia.org/wikipedia/commons/thumb/f/fa/Flag_of_the_People%27s_Republic_of_China.svg/22px-Flag_of_the_People%27s_Republic_of_China.svg.png\"> 2 <img src=\"http://upload.wikimedia.org/wikipedia/commons/thumb/4/41/Flag_of_India.svg/22px-Flag_of_India.svg.png\"> 3 <img src=\"http://upload.wikimedia.org/wikipedia/commons/thumb/a/a4/Flag_of_the_United_States.svg/22px-Flag_of_the_United_States.svg.png\"> Mode Date 1 TRUE 2010-10-09 2 TRUE 2010-10-09 3 TRUE 2010-10-09 1 2 3 4 5 6 PopTable <- gvisTable ( Population , formats = list ( Population = \"#,###\" , '% of World Population' = '#.#%' ), options = list ( page = 'enable' )) plot ( PopTable ) // jsData function gvisDataTableID2cd013050362 () { var data = new google.visualization.DataTable(); var datajson = [ [ \"1\", \"China\", 1339940000, 0.195, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/f/fa/Flag_of_the_People%27s_Republic_of_China.svg/22px-Flag_of_the_People%27s_Republic_of_China.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"2\", \"India\", 1188650000, 0.173, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/4/41/Flag_of_India.svg/22px-Flag_of_India.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"3\", \"United States\", 310438000, 0.0452, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/a/a4/Flag_of_the_United_States.svg/22px-Flag_of_the_United_States.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"4\", \"Indonesia\", 237556363, 0.0346, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/9/9f/Flag_of_Indonesia.svg/22px-Flag_of_Indonesia.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"5\", \"Brazil\", 193626000, 0.0282, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/0/05/Flag_of_Brazil.svg/22px-Flag_of_Brazil.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"6\", \"Pakistan\", 170745000, 0.0248, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/3/32/Flag_of_Pakistan.svg/22px-Flag_of_Pakistan.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"7\", \"Bangladesh\", 164425000, 0.0239, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/f/f9/Flag_of_Bangladesh.svg/22px-Flag_of_Bangladesh.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"8\", \"Nigeria\", 158259000, 0.023, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/7/79/Flag_of_Nigeria.svg/22px-Flag_of_Nigeria.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"9\", \"Russia\", 141927297, 0.0206, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/f/f3/Flag_of_Russia.svg/22px-Flag_of_Russia.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"10\", \"Japan\", 127390000, 0.0185, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/9/9e/Flag_of_Japan.svg/22px-Flag_of_Japan.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"11\", \"Mexico\", 108396211, 0.0158, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/f/fc/Flag_of_Mexico.svg/22px-Flag_of_Mexico.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"12\", \"Philippines\", 94013200, 0.0137, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/9/99/Flag_of_the_Philippines.svg/22px-Flag_of_the_Philippines.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"13\", \"Vietnam\", 85846997, 0.0125, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/2/21/Flag_of_Vietnam.svg/22px-Flag_of_Vietnam.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"14\", \"Ethiopia\", 84976000, 0.0124, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/7/71/Flag_of_Ethiopia.svg/22px-Flag_of_Ethiopia.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"15\", \"Germany\", 81802257, 0.0119, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/b/ba/Flag_of_Germany.svg/22px-Flag_of_Germany.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"16\", \"Egypt\", 79135000, 0.0115, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/f/fe/Flag_of_Egypt.svg/22px-Flag_of_Egypt.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"17\", \"Iran\", 75078000, 0.0109, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/c/ca/Flag_of_Iran.svg/22px-Flag_of_Iran.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"18\", \"Turkey\", 72561312, 0.0106, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/b/b4/Flag_of_Turkey.svg/22px-Flag_of_Turkey.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"19\", \"Dem. Rep. of Congo\", 67827000, 0.0099, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/6/6f/Flag_of_the_Democratic_Republic_of_the_Congo.svg/22px-Flag_of_the_Democratic_Republic_of_the_Congo.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"20\", \"Thailand\", 67070000, 0.01, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/a/a9/Flag_of_Thailand.svg/22px-Flag_of_Thailand.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"21\", \"France\", 65447374, 0.0095, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/c/c3/Flag_of_France.svg/22px-Flag_of_France.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"22\", \"United Kingdom\", 62008049, 0.009, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/a/ae/Flag_of_the_United_Kingdom.svg/22px-Flag_of_the_United_Kingdom.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"23\", \"Italy\", 60402499, 0.0088, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/0/03/Flag_of_Italy.svg/22px-Flag_of_Italy.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"24\", \"Myanmar\", 50496000, 0.0073, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/8/8c/Flag_of_Myanmar.svg/22px-Flag_of_Myanmar.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"25\", \"South Africa\", 49991300, 0.0073, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/a/af/Flag_of_South_Africa.svg/22px-Flag_of_South_Africa.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"26\", \"South Korea\", 49773145, 0.0072, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/0/09/Flag_of_South_Korea.svg/22px-Flag_of_South_Korea.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"27\", \"Spain\", 46072834, 0.0067, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/9/9a/Flag_of_Spain.svg/22px-Flag_of_Spain.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"28\", \"Ukraine\", 45871738, 0.0067, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/4/49/Flag_of_Ukraine.svg/22px-Flag_of_Ukraine.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"29\", \"Colombia\", 45655000, 0.0066, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/2/21/Flag_of_Colombia.svg/22px-Flag_of_Colombia.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"30\", \"Tanzania\", 45040000, 0.0066, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/3/38/Flag_of_Tanzania.svg/22px-Flag_of_Tanzania.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"31\", \"Sudan\", 43192000, 0.0063, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/0/01/Flag_of_Sudan.svg/22px-Flag_of_Sudan.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"32\", \"Argentina\", 40518951, 0.0059, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/1/1a/Flag_of_Argentina.svg/22px-Flag_of_Argentina.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"33\", \"Kenya\", 38610097, 0.0056, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/4/49/Flag_of_Kenya.svg/22px-Flag_of_Kenya.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"34\", \"Poland\", 38167329, 0.0056, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/1/12/Flag_of_Poland.svg/22px-Flag_of_Poland.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"35\", \"Algeria\", 35423000, 0.0052, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/7/77/Flag_of_Algeria.svg/22px-Flag_of_Algeria.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"36\", \"Canada\", 34272000, 0.005, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/c/cf/Flag_of_Canada.svg/22px-Flag_of_Canada.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"37\", \"Uganda\", 33796000, 0.0049, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/4/4e/Flag_of_Uganda.svg/22px-Flag_of_Uganda.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"38\", \"Morocco\", 31944000, 0.0046, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/2/2c/Flag_of_Morocco.svg/22px-Flag_of_Morocco.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"39\", \"Iraq\", 31467000, 0.0046, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/f/f6/Flag_of_Iraq.svg/22px-Flag_of_Iraq.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"40\", \"Nepal\", 29853000, 0.0043, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/9/9b/Flag_of_Nepal.svg/16px-Flag_of_Nepal.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"41\", \"Peru\", 29461933, 0.0043, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/c/cf/Flag_of_Peru.svg/22px-Flag_of_Peru.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"42\", \"Afghanistan\", 29117000, 0.0042, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/9/9a/Flag_of_Afghanistan.svg/22px-Flag_of_Afghanistan.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"43\", \"Venezuela\", 28958000, 0.0042, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/0/06/Flag_of_Venezuela.svg/22px-Flag_of_Venezuela.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"44\", \"Malaysia\", 28250500, 0.0041, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/6/66/Flag_of_Malaysia.svg/22px-Flag_of_Malaysia.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"45\", \"Uzbekistan\", 27794000, 0.004, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/8/84/Flag_of_Uzbekistan.svg/22px-Flag_of_Uzbekistan.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"46\", \"Saudi Arabia\", 27136977, 0.0039, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/0/0d/Flag_of_Saudi_Arabia.svg/22px-Flag_of_Saudi_Arabia.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"47\", \"Ghana\", 24333000, 0.0035, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/1/19/Flag_of_Ghana.svg/22px-Flag_of_Ghana.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"48\", \"Yemen\", 24256000, 0.0035, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/8/89/Flag_of_Yemen.svg/22px-Flag_of_Yemen.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"49\", \"North Korea\", 23991000, 0.0035, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/5/51/Flag_of_North_Korea.svg/22px-Flag_of_North_Korea.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"50\", \"Mozambique\", 23406000, 0.0034, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/d/d0/Flag_of_Mozambique.svg/22px-Flag_of_Mozambique.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"52\", \"Syria\", 22505000, 0.0033, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/5/53/Flag_of_Syria.svg/22px-Flag_of_Syria.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"53\", \"Australia\", 22483305, 0.0033, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/b/b9/Flag_of_Australia.svg/22px-Flag_of_Australia.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"54\", \"Cote d'Ivoire\", 21571000, 0.0031, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/8/86/Flag_of_Cote_d%27Ivoire.svg/22px-Flag_of_Cote_d%27Ivoire.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"55\", \"Romania\", 21466174, 0.0031, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/7/73/Flag_of_Romania.svg/22px-Flag_of_Romania.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"56\", \"Sri Lanka\", 20410000, 0.003, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/1/11/Flag_of_Sri_Lanka.svg/22px-Flag_of_Sri_Lanka.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"57\", \"Madagascar\", 20146000, 0.0029, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/b/bc/Flag_of_Madagascar.svg/22px-Flag_of_Madagascar.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"58\", \"Cameroon\", 19958000, 0.0029, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/4/4f/Flag_of_Cameroon.svg/22px-Flag_of_Cameroon.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"59\", \"Angola\", 18993000, 0.0028, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/9/9d/Flag_of_Angola.svg/22px-Flag_of_Angola.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"60\", \"Chile\", 17140000, 0.0025, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/7/78/Flag_of_Chile.svg/22px-Flag_of_Chile.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"61\", \"Netherlands\", 16619500, 0.00242, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/2/20/Flag_of_the_Netherlands.svg/22px-Flag_of_the_Netherlands.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"62\", \"Burkina Faso\", 16287000, 0.0024, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/3/31/Flag_of_Burkina_Faso.svg/22px-Flag_of_Burkina_Faso.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"63\", \"Kazakhstan\", 16197000, 0.0024, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/d/d3/Flag_of_Kazakhstan.svg/22px-Flag_of_Kazakhstan.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"64\", \"Niger\", 15891000, 0.0023, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/f/f4/Flag_of_Niger.svg/22px-Flag_of_Niger.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"65\", \"Malawi\", 15692000, 0.0023, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/d/d1/Flag_of_Malawi.svg/22px-Flag_of_Malawi.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"66\", \"Mali\", 14517176, 0.0021, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/9/92/Flag_of_Mali.svg/22px-Flag_of_Mali.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"67\", \"Guatemala\", 14377000, 0.0021, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/e/ec/Flag_of_Guatemala.svg/22px-Flag_of_Guatemala.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"68\", \"Ecuador\", 14259000, 0.0021, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/e/e8/Flag_of_Ecuador.svg/22px-Flag_of_Ecuador.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"69\", \"Cambodia\", 13395682, 0.0019, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/8/83/Flag_of_Cambodia.svg/22px-Flag_of_Cambodia.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"70\", \"Zambia\", 13257000, 0.0019, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/0/06/Flag_of_Zambia.svg/22px-Flag_of_Zambia.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"71\", \"Senegal\", 12861000, 0.0019, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/f/fd/Flag_of_Senegal.svg/22px-Flag_of_Senegal.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"72\", \"Zimbabwe\", 12644000, 0.0018, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/6/6a/Flag_of_Zimbabwe.svg/22px-Flag_of_Zimbabwe.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"73\", \"Chad\", 11506000, 0.0017, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/4/4b/Flag_of_Chad.svg/22px-Flag_of_Chad.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"74\", \"Greece\", 11306183, 0.0016, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/5/5c/Flag_of_Greece.svg/22px-Flag_of_Greece.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"75\", \"Cuba\", 11204000, 0.0016, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/b/bd/Flag_of_Cuba.svg/22px-Flag_of_Cuba.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"76\", \"Belgium\", 10827519, 0.0016, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/9/92/Flag_of_Belgium_%28civil%29.svg/22px-Flag_of_Belgium_%28civil%29.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"77\", \"Portugal\", 10636888, 0.0015, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/5/5c/Flag_of_Portugal.svg/22px-Flag_of_Portugal.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"78\", \"Czech Republic\", 10512397, 0.0015, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/c/cb/Flag_of_the_Czech_Republic.svg/22px-Flag_of_the_Czech_Republic.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"79\", \"Tunisia\", 10432500, 0.0015, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/c/ce/Flag_of_Tunisia.svg/22px-Flag_of_Tunisia.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"80\", \"Guinea\", 10324000, 0.0015, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/e/ed/Flag_of_Guinea.svg/22px-Flag_of_Guinea.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"81\", \"Rwanda\", 10277000, 0.0015, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/1/17/Flag_of_Rwanda.svg/22px-Flag_of_Rwanda.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"82\", \"Dominican Republic\", 10225000, 0.0015, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/9/9f/Flag_of_the_Dominican_Republic.svg/22px-Flag_of_the_Dominican_Republic.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"83\", \"Haiti\", 10188000, 0.0015, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/5/56/Flag_of_Haiti.svg/22px-Flag_of_Haiti.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"84\", \"Bolivia\", 10031000, 0.0015, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/4/48/Flag_of_Bolivia.svg/22px-Flag_of_Bolivia.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"85\", \"Hungary\", 10013628, 0.0015, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/c/c1/Flag_of_Hungary.svg/22px-Flag_of_Hungary.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"86\", \"Serbia\", 9856000, 0.0014, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/f/ff/Flag_of_Serbia.svg/22px-Flag_of_Serbia.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"87\", \"Belarus\", 9467700, 0.0014, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/8/85/Flag_of_Belarus.svg/22px-Flag_of_Belarus.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"88\", \"Sweden\", 9393648, 0.0014, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/4/4c/Flag_of_Sweden.svg/22px-Flag_of_Sweden.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"89\", \"Somalia\", 9359000, 0.0014, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/a/a0/Flag_of_Somalia.svg/22px-Flag_of_Somalia.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"90\", \"Benin\", 9212000, 0.0013, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/0/0a/Flag_of_Benin.svg/22px-Flag_of_Benin.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"91\", \"Azerbaijan\", 8997400, 0.0013, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Flag_of_Azerbaijan.svg/22px-Flag_of_Azerbaijan.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"92\", \"Burundi\", 8519000, 0.0012, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/5/50/Flag_of_Burundi.svg/22px-Flag_of_Burundi.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"93\", \"Austria\", 8372930, 0.0012, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/4/41/Flag_of_Austria.svg/22px-Flag_of_Austria.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"94\", \"Switzerland\", 7782900, 0.0011, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/f/f3/Flag_of_Switzerland.svg/20px-Flag_of_Switzerland.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"95\", \"Israel\", 7640800, 0.0011, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/d/d4/Flag_of_Israel.svg/22px-Flag_of_Israel.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"96\", \"Honduras\", 7616000, 0.0011, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/8/82/Flag_of_Honduras.svg/22px-Flag_of_Honduras.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"97\", \"Bulgaria\", 7576751, 0.0011, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/9/9a/Flag_of_Bulgaria.svg/22px-Flag_of_Bulgaria.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"98\", \"Tajikistan\", 7075000, 0.00103, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/d/d0/Flag_of_Tajikistan.svg/22px-Flag_of_Tajikistan.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"100\", \"Papua New Guinea\", 6888000, 0.001, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/e/e3/Flag_of_Papua_New_Guinea.svg/22px-Flag_of_Papua_New_Guinea.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"101\", \"Togo\", 6780000, 0.00099, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/6/68/Flag_of_Togo.svg/22px-Flag_of_Togo.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"102\", \"Libya\", 6546000, 0.00095, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/0/05/Flag_of_Libya.svg/22px-Flag_of_Libya.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"103\", \"Jordan\", 6472000, 0.00094, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/c/c0/Flag_of_Jordan.svg/22px-Flag_of_Jordan.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"104\", \"Paraguay\", 6460000, 0.00094, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/2/27/Flag_of_Paraguay.svg/22px-Flag_of_Paraguay.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"105\", \"Laos\", 6436000, 0.00094, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/5/56/Flag_of_Laos.svg/22px-Flag_of_Laos.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"106\", \"El Salvador\", 6194000, 9e-04, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/3/34/Flag_of_El_Salvador.svg/22px-Flag_of_El_Salvador.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"107\", \"Sierra Leone\", 5836000, 0.00085, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/1/17/Flag_of_Sierra_Leone.svg/22px-Flag_of_Sierra_Leone.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"108\", \"Nicaragua\", 5822000, 0.00085, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/1/19/Flag_of_Nicaragua.svg/22px-Flag_of_Nicaragua.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"109\", \"Kyrgyzstan\", 5550000, 0.00081, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/c/c7/Flag_of_Kyrgyzstan.svg/22px-Flag_of_Kyrgyzstan.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"110\", \"Denmark\", 5543819, 8e-04, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/9/9c/Flag_of_Denmark.svg/22px-Flag_of_Denmark.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"111\", \"Slovakia\", 5429763, 0.00079, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/e/e6/Flag_of_Slovakia.svg/22px-Flag_of_Slovakia.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"112\", \"Finland\", 5370000, 0.00078, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/b/bc/Flag_of_Finland.svg/22px-Flag_of_Finland.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"113\", \"Eritrea\", 5224000, 0.00076, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/2/29/Flag_of_Eritrea.svg/22px-Flag_of_Eritrea.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"114\", \"Turkmenistan\", 5177000, 0.00075, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/1/1b/Flag_of_Turkmenistan.svg/22px-Flag_of_Turkmenistan.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"115\", \"Singapore\", 5076700, 0.00074, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/4/48/Flag_of_Singapore.svg/22px-Flag_of_Singapore.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"116\", \"Norway\", 4906500, 0.00071, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/d/d9/Flag_of_Norway.svg/22px-Flag_of_Norway.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"117\", \"United Arab Emirates\", 4707000, 0.00068, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/c/cb/Flag_of_the_United_Arab_Emirates.svg/22px-Flag_of_the_United_Arab_Emirates.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"118\", \"Costa Rica\", 4640000, 0.00068, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/f/f2/Flag_of_Costa_Rica.svg/22px-Flag_of_Costa_Rica.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"119\", \"Central African Republic\", 4506000, 0.00066, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/6/6f/Flag_of_the_Central_African_Republic.svg/22px-Flag_of_the_Central_African_Republic.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"120\", \"Ireland\", 4470700, 0.00064, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/4/45/Flag_of_Ireland.svg/22px-Flag_of_Ireland.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"121\", \"Georgia\", 4436000, 0.00065, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/0/0f/Flag_of_Georgia.svg/22px-Flag_of_Georgia.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"122\", \"Croatia\", 4435056, 0.00065, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/1/1b/Flag_of_Croatia.svg/22px-Flag_of_Croatia.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"123\", \"New Zealand\", 4393000, 0.00064, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/3/3e/Flag_of_New_Zealand.svg/22px-Flag_of_New_Zealand.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"124\", \"Lebanon\", 4255000, 0.00062, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/5/59/Flag_of_Lebanon.svg/22px-Flag_of_Lebanon.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"125\", \"Liberia\", 4102000, 6e-04, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/b/b8/Flag_of_Liberia.svg/22px-Flag_of_Liberia.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"127\", \"Palestinian territories\", 3935249, 0.00055, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/0/00/Flag_of_Palestine.svg/22px-Flag_of_Palestine.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"128\", \"Bosnia and Herzegovina\", 3760000, 0.00055, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/b/bf/Flag_of_Bosnia_and_Herzegovina.svg/22px-Flag_of_Bosnia_and_Herzegovina.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"129\", \"Republic of the Congo\", 3759000, 0.00055, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/9/92/Flag_of_the_Republic_of_the_Congo.svg/22px-Flag_of_the_Republic_of_the_Congo.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"130\", \"Moldova\", 3563800, 0.00052, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/2/27/Flag_of_Moldova.svg/22px-Flag_of_Moldova.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"131\", \"Uruguay\", 3372000, 0.00049, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/f/fe/Flag_of_Uruguay.svg/22px-Flag_of_Uruguay.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"132\", \"Mauritania\", 3366000, 0.00049, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/4/43/Flag_of_Mauritania.svg/22px-Flag_of_Mauritania.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"133\", \"Lithuania\", 3329227, 0.00048, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/1/11/Flag_of_Lithuania.svg/22px-Flag_of_Lithuania.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"134\", \"Panama\", 3322576, 0.00048, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/a/ab/Flag_of_Panama.svg/22px-Flag_of_Panama.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"135\", \"Armenia\", 3238000, 0.00047, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/2/2f/Flag_of_Armenia.svg/22px-Flag_of_Armenia.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"136\", \"Albania\", 3195000, 0.00046, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/3/36/Flag_of_Albania.svg/22px-Flag_of_Albania.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"137\", \"Kuwait\", 3051000, 0.00044, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/a/aa/Flag_of_Kuwait.svg/22px-Flag_of_Kuwait.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"138\", \"Oman\", 2905000, 0.00042, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Flag_of_Oman.svg/22px-Flag_of_Oman.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"139\", \"Mongolia\", 2776500, 4e-04, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/4/4c/Flag_of_Mongolia.svg/22px-Flag_of_Mongolia.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"140\", \"Jamaica\", 2730000, 4e-04, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/0/0a/Flag_of_Jamaica.svg/22px-Flag_of_Jamaica.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"141\", \"Latvia\", 2236300, 0.00033, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/8/84/Flag_of_Latvia.svg/22px-Flag_of_Latvia.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"142\", \"Namibia\", 2212000, 0.00032, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/0/00/Flag_of_Namibia.svg/22px-Flag_of_Namibia.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"143\", \"Lesotho\", 2084000, 3e-04, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/4/4a/Flag_of_Lesotho.svg/22px-Flag_of_Lesotho.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"144\", \"Slovenia\", 2065720, 3e-04, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/f/f0/Flag_of_Slovenia.svg/22px-Flag_of_Slovenia.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"145\", \"Republic of Macedonia\", 2048620, 3e-04, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/f/f8/Flag_of_Macedonia.svg/22px-Flag_of_Macedonia.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"146\", \"Botswana\", 1978000, 0.00029, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/f/fa/Flag_of_Botswana.svg/22px-Flag_of_Botswana.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"147\", \"Gambia\", 1751000, 0.00025, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/7/77/Flag_of_The_Gambia.svg/22px-Flag_of_The_Gambia.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"148\", \"Qatar\", 1696563, 0.00025, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/6/65/Flag_of_Qatar.svg/22px-Flag_of_Qatar.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"149\", \"Guinea-Bissau\", 1647000, 0.00024, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/0/01/Flag_of_Guinea-Bissau.svg/22px-Flag_of_Guinea-Bissau.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"150\", \"Gabon\", 1501000, 0.00022, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/0/04/Flag_of_Gabon.svg/22px-Flag_of_Gabon.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"151\", \"Trinidad and Tobago\", 1344000, 2e-04, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/6/64/Flag_of_Trinidad_and_Tobago.svg/22px-Flag_of_Trinidad_and_Tobago.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"152\", \"Estonia\", 1340127, 0.00019, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/8/8f/Flag_of_Estonia.svg/22px-Flag_of_Estonia.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"153\", \"Mauritius\", 1297000, 0.00019, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/7/77/Flag_of_Mauritius.svg/22px-Flag_of_Mauritius.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"154\", \"Swaziland\", 1202000, 0.00017, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/1/1e/Flag_of_Swaziland.svg/22px-Flag_of_Swaziland.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"155\", \"East Timor\", 1171000, 0.00017, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/2/26/Flag_of_East_Timor.svg/22px-Flag_of_East_Timor.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"156\", \"Djibouti\", 879000, 0.00013, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/3/34/Flag_of_Djibouti.svg/22px-Flag_of_Djibouti.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"157\", \"Fiji\", 854000, 0.00012, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/b/ba/Flag_of_Fiji.svg/22px-Flag_of_Fiji.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"158\", \"Bahrain\", 807000, 0.00012, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/2/2c/Flag_of_Bahrain.svg/22px-Flag_of_Bahrain.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"159\", \"Cyprus\", 801851, 0.00012, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/d/d4/Flag_of_Cyprus.svg/22px-Flag_of_Cyprus.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"160\", \"Guyana\", 761000, 0.00011, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/9/99/Flag_of_Guyana.svg/22px-Flag_of_Guyana.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"161\", \"Bhutan\", 708000, 1e-04, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/9/91/Flag_of_Bhutan.svg/22px-Flag_of_Bhutan.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"162\", \"Equatorial Guinea\", 693000, 1e-04, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/3/31/Flag_of_Equatorial_Guinea.svg/22px-Flag_of_Equatorial_Guinea.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"163\", \"Comoros\", 691000, 1e-04, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/9/94/Flag_of_the_Comoros.svg/22px-Flag_of_the_Comoros.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"164\", \"Montenegro\", 626000, 9e-05, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/6/64/Flag_of_Montenegro.svg/22px-Flag_of_Montenegro.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"166\", \"Solomon Islands\", 536000, 8e-05, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/7/74/Flag_of_the_Solomon_Islands.svg/22px-Flag_of_the_Solomon_Islands.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"167\", \"Western Sahara\", 530000, 8e-05, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/c/c8/Flag_of_Western_Sahara.svg/22px-Flag_of_Western_Sahara.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"168\", \"Suriname\", 524000, 8e-05, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/6/60/Flag_of_Suriname.svg/22px-Flag_of_Suriname.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"169\", \"Cape Verde\", 513000, 7e-05, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/3/38/Flag_of_Cape_Verde.svg/22px-Flag_of_Cape_Verde.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"170\", \"Luxembourg\", 502207, 7e-05, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/d/da/Flag_of_Luxembourg.svg/22px-Flag_of_Luxembourg.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"171\", \"Malta\", 416333, 6e-05, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/7/73/Flag_of_Malta.svg/22px-Flag_of_Malta.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"172\", \"Brunei\", 407000, 6e-05, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/9/9c/Flag_of_Brunei.svg/22px-Flag_of_Brunei.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"173\", \"Bahamas\", 346000, 5e-05, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/9/93/Flag_of_the_Bahamas.svg/22px-Flag_of_the_Bahamas.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"174\", \"Belize\", 322100, 5e-05, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/e/e7/Flag_of_Belize.svg/22px-Flag_of_Belize.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"175\", \"Iceland\", 318006, 5e-05, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/c/ce/Flag_of_Iceland.svg/22px-Flag_of_Iceland.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"176\", \"Maldives\", 314000, 5e-05, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/0/0f/Flag_of_Maldives.svg/22px-Flag_of_Maldives.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"177\", \"Barbados\", 257000, 4e-05, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/e/ef/Flag_of_Barbados.svg/22px-Flag_of_Barbados.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"178\", \"Vanuatu\", 246000, 4e-05, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/b/bc/Flag_of_Vanuatu.svg/22px-Flag_of_Vanuatu.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"181\", \"Samoa\", 179000, 3e-05, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/3/31/Flag_of_Samoa.svg/22px-Flag_of_Samoa.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"182\", \"Saint Lucia\", 174000, 3e-05, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/9/9f/Flag_of_Saint_Lucia.svg/22px-Flag_of_Saint_Lucia.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"183\", \"Sao Tome and Principe\", 165000, 2e-05, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/4/4f/Flag_of_Sao_Tome_and_Principe.svg/22px-Flag_of_Sao_Tome_and_Principe.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"184\", \"Federated States of Micronesia\", 111000, 2e-05, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/4/4c/Flag_of_Federated_States_of_Micronesia.svg/22px-Flag_of_Federated_States_of_Micronesia.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"186\", \"Saint Vincent and the Grenadines\", 109000, 2e-05, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/6/6d/Flag_of_Saint_Vincent_and_the_Grenadines.svg/22px-Flag_of_Saint_Vincent_and_the_Grenadines.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"188\", \"Grenada\", 104000, 2e-05, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/b/bc/Flag_of_Grenada.svg/22px-Flag_of_Grenada.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"189\", \"Tonga\", 104000, 2e-05, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/9/9a/Flag_of_Tonga.svg/22px-Flag_of_Tonga.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"190\", \"Kiribati\", 100000, 1e-05, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/d/d3/Flag_of_Kiribati.svg/22px-Flag_of_Kiribati.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"192\", \"Antigua and Barbuda\", 89000, 1e-05, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/8/89/Flag_of_Antigua_and_Barbuda.svg/22px-Flag_of_Antigua_and_Barbuda.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"194\", \"Seychelles\", 85000, 1e-05, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/9/92/Flag_of_the_Seychelles.svg/22px-Flag_of_the_Seychelles.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"195\", \"Andorra\", 84082, 1e-05, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/1/19/Flag_of_Andorra.svg/22px-Flag_of_Andorra.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"198\", \"Dominica\", 67000, 1e-05, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/c/c4/Flag_of_Dominica.svg/22px-Flag_of_Dominica.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"200\", \"Marshall Islands\", 63000, 1e-05, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/2/2e/Flag_of_the_Marshall_Islands.svg/22px-Flag_of_the_Marshall_Islands.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"204\", \"Saint Kitts and Nevis\", 52000, 1e-05, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/f/fe/Flag_of_Saint_Kitts_and_Nevis.svg/22px-Flag_of_Saint_Kitts_and_Nevis.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"206\", \"Liechtenstein\", 35904, 5e-06, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/4/47/Flag_of_Liechtenstein.svg/22px-Flag_of_Liechtenstein.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"207\", \"Monaco\", 33000, 5e-06, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/e/ea/Flag_of_Monaco.svg/22px-Flag_of_Monaco.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"209\", \"San Marino\", 31794, 5e-06, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/b/b1/Flag_of_San_Marino.svg/22px-Flag_of_San_Marino.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"213\", \"Palau\", 20000, 3e-06, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/4/48/Flag_of_Palau.svg/22px-Flag_of_Palau.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"215\", \"Tuvalu\", 10000, 1e-06, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/3/38/Flag_of_Tuvalu.svg/22px-Flag_of_Tuvalu.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"216\", \"Nauru\", 10000, 1e-06, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/3/30/Flag_of_Nauru.svg/22px-Flag_of_Nauru.svg.png\\\">\", true, new Date(2010,9,9) ], [ \"222\", \"Vatican City\", 800, 2e-07, \"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/0/00/Flag_of_the_Vatican_City.svg/20px-Flag_of_the_Vatican_City.svg.png\\\">\", true, new Date(2010,9,9) ] ]; data.addColumn('string','Rank'); data.addColumn('string','Country'); data.addColumn('number','Population'); data.addColumn('number','% of World Population'); data.addColumn('string','Flag'); data.addColumn('boolean','Mode'); data.addColumn('date','Date'); data.addRows(datajson); return(data); } // jsDrawChart function drawChartTableID2cd013050362() { var data = gvisDataTableID2cd013050362(); var options = {}; options[\"allowHtml\"] = true; options[\"page\"] = \"enable\"; var dataFormat1 = new google.visualization.NumberFormat({pattern:\"#,###\"}); dataFormat1.format(data, 2); var dataFormat2 = new google.visualization.NumberFormat({pattern:\"#.#%\"}); dataFormat2.format(data, 3); var chart = new google.visualization.Table( document.getElementById('TableID2cd013050362') ); chart.draw(data,options); } // jsDisplayChart (function() { var pkgs = window.__gvisPackages = window.__gvisPackages || []; var callbacks = window.__gvisCallbacks = window.__gvisCallbacks || []; var chartid = \"table\"; // Manually see if chartid is in pkgs (not all browsers support Array.indexOf) var i, newPackage = true; for (i = 0; newPackage && i < pkgs.length; i++) { if (pkgs[i] === chartid) newPackage = false; } if (newPackage) pkgs.push(chartid); // Add the drawChart function to the global list of callbacks callbacks.push(drawChartTableID2cd013050362); })(); function displayChartTableID2cd013050362() { var pkgs = window.__gvisPackages = window.__gvisPackages || []; var callbacks = window.__gvisCallbacks = window.__gvisCallbacks || []; window.clearTimeout(window.__gvisLoad); // The timeout is set to 100 because otherwise the container div we are // targeting might not be part of the document yet window.__gvisLoad = setTimeout(function() { var pkgCount = pkgs.length; google.load(\"visualization\", \"1\", { packages:pkgs, callback: function() { if (pkgCount != pkgs.length) { // Race condition where another setTimeout call snuck in after us; if // that call added a package, we must not shift its callback return; } while (callbacks.length > 0) callbacks.shift()(); } }); }, 100); } // jsFooter","title":"Tables with gvisTable"},{"location":"googlevis/#dashboards-with-gvismerge","text":"1 2 3 4 5 6 7 G <- gvisGeoChart ( Exports , locationvar = \"Country\" , colorvar = \"Profit\" , options = list ( width = 300 , height = 200 )) T <- gvisTable ( Exports , options = list ( width = 300 , height = 370 )) 1 2 3 GT <- gvisMerge ( G , T , horizontal = FALSE ) plot ( GT ) // jsData function gvisDataGeoChartID49611ffdda49 () { var data = new google.visualization.DataTable(); var datajson = [ [ \"Germany\", 3 ], [ \"Brazil\", 4 ], [ \"United States\", 5 ], [ \"France\", 4 ], [ \"Hungary\", 3 ], [ \"India\", 2 ], [ \"Iceland\", 1 ], [ \"Norway\", 4 ], [ \"Spain\", 5 ], [ \"Turkey\", 1 ] ]; data.addColumn('string','Country'); data.addColumn('number','Profit'); data.addRows(datajson); return(data); } // jsData function gvisDataTableID496155ab963a () { var data = new google.visualization.DataTable(); var datajson = [ [ \"Germany\", 3, true ], [ \"Brazil\", 4, false ], [ \"United States\", 5, true ], [ \"France\", 4, true ], [ \"Hungary\", 3, false ], [ \"India\", 2, true ], [ \"Iceland\", 1, false ], [ \"Norway\", 4, true ], [ \"Spain\", 5, true ], [ \"Turkey\", 1, false ] ]; data.addColumn('string','Country'); data.addColumn('number','Profit'); data.addColumn('boolean','Online'); data.addRows(datajson); return(data); } // jsDrawChart function drawChartGeoChartID49611ffdda49() { var data = gvisDataGeoChartID49611ffdda49(); var options = {}; options[\"width\"] = 300; options[\"height\"] = 200; var chart = new google.visualization.GeoChart( document.getElementById('GeoChartID49611ffdda49') ); chart.draw(data,options); } // jsDrawChart function drawChartTableID496155ab963a() { var data = gvisDataTableID496155ab963a(); var options = {}; options[\"allowHtml\"] = true; options[\"width\"] = 300; options[\"height\"] = 370; var chart = new google.visualization.Table( document.getElementById('TableID496155ab963a') ); chart.draw(data,options); } // jsDisplayChart (function() { var pkgs = window.__gvisPackages = window.__gvisPackages || []; var callbacks = window.__gvisCallbacks = window.__gvisCallbacks || []; var chartid = \"geochart\"; // Manually see if chartid is in pkgs (not all browsers support Array.indexOf) var i, newPackage = true; for (i = 0; newPackage && i < pkgs.length; i++) { if (pkgs[i] === chartid) newPackage = false; } if (newPackage) pkgs.push(chartid); // Add the drawChart function to the global list of callbacks callbacks.push(drawChartGeoChartID49611ffdda49); })(); function displayChartGeoChartID49611ffdda49() { var pkgs = window.__gvisPackages = window.__gvisPackages || []; var callbacks = window.__gvisCallbacks = window.__gvisCallbacks || []; window.clearTimeout(window.__gvisLoad); // The timeout is set to 100 because otherwise the container div we are // targeting might not be part of the document yet window.__gvisLoad = setTimeout(function() { var pkgCount = pkgs.length; google.load(\"visualization\", \"1\", { packages:pkgs, callback: function() { if (pkgCount != pkgs.length) { // Race condition where another setTimeout call snuck in after us; if // that call added a package, we must not shift its callback return; } while (callbacks.length > 0) callbacks.shift()(); } }); }, 100); } // jsDisplayChart (function() { var pkgs = window.__gvisPackages = window.__gvisPackages || []; var callbacks = window.__gvisCallbacks = window.__gvisCallbacks || []; var chartid = \"table\"; // Manually see if chartid is in pkgs (not all browsers support Array.indexOf) var i, newPackage = true; for (i = 0; newPackage && i < pkgs.length; i++) { if (pkgs[i] === chartid) newPackage = false; } if (newPackage) pkgs.push(chartid); // Add the drawChart function to the global list of callbacks callbacks.push(drawChartTableID496155ab963a); })(); function displayChartTableID496155ab963a() { var pkgs = window.__gvisPackages = window.__gvisPackages || []; var callbacks = window.__gvisCallbacks = window.__gvisCallbacks || []; window.clearTimeout(window.__gvisLoad); // The timeout is set to 100 because otherwise the container div we are // targeting might not be part of the document yet window.__gvisLoad = setTimeout(function() { var pkgCount = pkgs.length; google.load(\"visualization\", \"1\", { packages:pkgs, callback: function() { if (pkgCount != pkgs.length) { // Race condition where another setTimeout call snuck in after us; if // that call added a package, we must not shift its callback return; } while (callbacks.length > 0) callbacks.shift()(); } }); }, 100); } // jsFooter 1 2 3 4 G <- gvisGeoChart ( Exports , locationvar = \"Country\" , colorvar = \"Profit\" , options = list ( width = 300 , height = 370 )) 1 2 3 GT <- gvisMerge ( G , T , horizontal = TRUE ) plot ( GT ) // jsData function gvisDataGeoChartID114f1fa5c040 () { var data = new google.visualization.DataTable(); var datajson = [ [ \"Germany\", 3 ], [ \"Brazil\", 4 ], [ \"United States\", 5 ], [ \"France\", 4 ], [ \"Hungary\", 3 ], [ \"India\", 2 ], [ \"Iceland\", 1 ], [ \"Norway\", 4 ], [ \"Spain\", 5 ], [ \"Turkey\", 1 ] ]; data.addColumn('string','Country'); data.addColumn('number','Profit'); data.addRows(datajson); return(data); } // jsData function gvisDataTableID114f79fb112f () { var data = new google.visualization.DataTable(); var datajson = [ [ \"Germany\", 3, true ], [ \"Brazil\", 4, false ], [ \"United States\", 5, true ], [ \"France\", 4, true ], [ \"Hungary\", 3, false ], [ \"India\", 2, true ], [ \"Iceland\", 1, false ], [ \"Norway\", 4, true ], [ \"Spain\", 5, true ], [ \"Turkey\", 1, false ] ]; data.addColumn('string','Country'); data.addColumn('number','Profit'); data.addColumn('boolean','Online'); data.addRows(datajson); return(data); } // jsDrawChart function drawChartGeoChartID114f1fa5c040() { var data = gvisDataGeoChartID114f1fa5c040(); var options = {}; options[\"width\"] = 300; options[\"height\"] = 370; var chart = new google.visualization.GeoChart( document.getElementById('GeoChartID114f1fa5c040') ); chart.draw(data,options); } // jsDrawChart function drawChartTableID114f79fb112f() { var data = gvisDataTableID114f79fb112f(); var options = {}; options[\"allowHtml\"] = true; options[\"width\"] = 200; options[\"height\"] = 270; var chart = new google.visualization.Table( document.getElementById('TableID114f79fb112f') ); chart.draw(data,options); } // jsDisplayChart (function() { var pkgs = window.__gvisPackages = window.__gvisPackages || []; var callbacks = window.__gvisCallbacks = window.__gvisCallbacks || []; var chartid = \"geochart\"; // Manually see if chartid is in pkgs (not all browsers support Array.indexOf) var i, newPackage = true; for (i = 0; newPackage && i < pkgs.length; i++) { if (pkgs[i] === chartid) newPackage = false; } if (newPackage) pkgs.push(chartid); // Add the drawChart function to the global list of callbacks callbacks.push(drawChartGeoChartID114f1fa5c040); })(); function displayChartGeoChartID114f1fa5c040() { var pkgs = window.__gvisPackages = window.__gvisPackages || []; var callbacks = window.__gvisCallbacks = window.__gvisCallbacks || []; window.clearTimeout(window.__gvisLoad); // The timeout is set to 100 because otherwise the container div we are // targeting might not be part of the document yet window.__gvisLoad = setTimeout(function() { var pkgCount = pkgs.length; google.load(\"visualization\", \"1\", { packages:pkgs, callback: function() { if (pkgCount != pkgs.length) { // Race condition where another setTimeout call snuck in after us; if // that call added a package, we must not shift its callback return; } while (callbacks.length > 0) callbacks.shift()(); } }); }, 100); } // jsDisplayChart (function() { var pkgs = window.__gvisPackages = window.__gvisPackages || []; var callbacks = window.__gvisCallbacks = window.__gvisCallbacks || []; var chartid = \"table\"; // Manually see if chartid is in pkgs (not all browsers support Array.indexOf) var i, newPackage = true; for (i = 0; newPackage && i < pkgs.length; i++) { if (pkgs[i] === chartid) newPackage = false; } if (newPackage) pkgs.push(chartid); // Add the drawChart function to the global list of callbacks callbacks.push(drawChartTableID114f79fb112f); })(); function displayChartTableID114f79fb112f() { var pkgs = window.__gvisPackages = window.__gvisPackages || []; var callbacks = window.__gvisCallbacks = window.__gvisCallbacks || []; window.clearTimeout(window.__gvisLoad); // The timeout is set to 100 because otherwise the container div we are // targeting might not be part of the document yet window.__gvisLoad = setTimeout(function() { var pkgCount = pkgs.length; google.load(\"visualization\", \"1\", { packages:pkgs, callback: function() { if (pkgCount != pkgs.length) { // Race condition where another setTimeout call snuck in after us; if // that call added a package, we must not shift its callback return; } while (callbacks.length > 0) callbacks.shift()(); } }); }, 100); } // jsFooter","title":"Dashboards with gvisMerge"},{"location":"googlevis/#options","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 df <- data.frame ( country = c ( \"US\" , \"GB\" , \"BR\" ), val1 = c ( 1 , 3 , 4 ), val2 = c ( 23 , 12 , 32 )) Line <- gvisLineChart ( df , xvar = \"country\" , yvar = c ( \"val1\" , \"val2\" ), options = list ( title = \"Hello World\" , titleTextStyle = \"{color:'red', fontName:'Courier', fontSize:16}\" , backgroundColor = \"#D3D3D3\" , vAxis = \"{gridlines:{color:'red', count:3}}\" , hAxis = \"{title:'Country', titleTextStyle:{color:'blue'}}\" , series = \"[{color:'green', targetAxisIndex: 0}, {color: 'orange',targetAxisIndex:1}]\" , vAxes = \"[{title:'val1'}, {title:'val2'}]\" , legend = \"bottom\" , curveType = \"function\" , width = 500 , height = 300 )) // jsData function gvisDataLineChartID584f5d6f811a () { var data = new google.visualization.DataTable(); var datajson = [ [ \"US\", 1, 23 ], [ \"GB\", 3, 12 ], [ \"BR\", 4, 32 ] ]; data.addColumn('string','country'); data.addColumn('number','val1'); data.addColumn('number','val2'); data.addRows(datajson); return(data); } // jsDrawChart function drawChartLineChartID584f5d6f811a() { var data = gvisDataLineChartID584f5d6f811a(); var options = {}; options[\"allowHtml\"] = true; options[\"title\"] = \"Hello World\"; options[\"titleTextStyle\"] = {color:'red', fontName:'Courier', fontSize:16}; options[\"backgroundColor\"] = \"#D3D3D3\"; options[\"vAxis\"] = {gridlines:{color:'red', count:3}}; options[\"hAxis\"] = {title:'Country', titleTextStyle:{color:'blue'}}; options[\"series\"] = [{color:'green', targetAxisIndex: 0}, {color: 'orange',targetAxisIndex:1}]; options[\"vAxes\"] = [{title:'val1'}, {title:'val2'}]; options[\"legend\"] = \"bottom\"; options[\"curveType\"] = \"function\"; options[\"width\"] = 500; options[\"height\"] = 300; var chart = new google.visualization.LineChart( document.getElementById('LineChartID584f5d6f811a') ); chart.draw(data,options); } // jsDisplayChart (function() { var pkgs = window.__gvisPackages = window.__gvisPackages || []; var callbacks = window.__gvisCallbacks = window.__gvisCallbacks || []; var chartid = \"corechart\"; // Manually see if chartid is in pkgs (not all browsers support Array.indexOf) var i, newPackage = true; for (i = 0; newPackage && i < pkgs.length; i++) { if (pkgs[i] === chartid) newPackage = false; } if (newPackage) pkgs.push(chartid); // Add the drawChart function to the global list of callbacks callbacks.push(drawChartLineChartID584f5d6f811a); })(); function displayChartLineChartID584f5d6f811a() { var pkgs = window.__gvisPackages = window.__gvisPackages || []; var callbacks = window.__gvisCallbacks = window.__gvisCallbacks || []; window.clearTimeout(window.__gvisLoad); // The timeout is set to 100 because otherwise the container div we are // targeting might not be part of the document yet window.__gvisLoad = setTimeout(function() { var pkgCount = pkgs.length; google.load(\"visualization\", \"1\", { packages:pkgs, callback: function() { if (pkgCount != pkgs.length) { // Race condition where another setTimeout call snuck in after us; if // that call added a package, we must not shift its callback return; } while (callbacks.length > 0) callbacks.shift()(); } }); }, 100); } // jsFooter","title":"Options"},{"location":"googlevis/#apostrophes","text":"1 2 3 4 5 6 7 8 9 10 df <- data.frame ( \"Year\" = c ( 2009 , 2010 ), \"Lloyd\\\\'s\" = c ( 86.1 , 93.3 ), \"Munich Re\\\\'s R/I\" = c ( 95.3 , 100.5 ), check.names = FALSE ) R <- gvisColumnChart ( df , options = list ( vAxis = '{baseline:0}' , title = \"Combined Ratio %\" , legend = \"{position:'bottom'}\" )) cat ( R $ html $ chart , file = \"GoogleVis/R.html\" ) # save // jsData function gvisDataColumnChartID336e54fe1b42 () { var data = new google.visualization.DataTable(); var datajson = [ [ 2009, 86.1, 95.3 ], [ 2010, 93.3, 100.5 ] ]; data.addColumn('number','Year'); data.addColumn('number','Lloyd\\'s'); data.addColumn('number','Munich Re\\'s R/I'); data.addRows(datajson); return(data); } // jsDrawChart function drawChartColumnChartID336e54fe1b42() { var data = gvisDataColumnChartID336e54fe1b42(); var options = {}; options[\"allowHtml\"] = true; options[\"vAxis\"] = {baseline:0}; options[\"title\"] = \"Combined Ratio %\"; options[\"legend\"] = {position:'bottom'}; var chart = new google.visualization.ColumnChart( document.getElementById('ColumnChartID336e54fe1b42') ); chart.draw(data,options); } // jsDisplayChart (function() { var pkgs = window.__gvisPackages = window.__gvisPackages || []; var callbacks = window.__gvisCallbacks = window.__gvisCallbacks || []; var chartid = \"corechart\"; // Manually see if chartid is in pkgs (not all browsers support Array.indexOf) var i, newPackage = true; for (i = 0; newPackage && i < pkgs.length; i++) { if (pkgs[i] === chartid) newPackage = false; } if (newPackage) pkgs.push(chartid); // Add the drawChart function to the global list of callbacks callbacks.push(drawChartColumnChartID336e54fe1b42); })(); function displayChartColumnChartID336e54fe1b42() { var pkgs = window.__gvisPackages = window.__gvisPackages || []; var callbacks = window.__gvisCallbacks = window.__gvisCallbacks || []; window.clearTimeout(window.__gvisLoad); // The timeout is set to 100 because otherwise the container div we are // targeting might not be part of the document yet window.__gvisLoad = setTimeout(function() { var pkgCount = pkgs.length; google.load(\"visualization\", \"1\", { packages:pkgs, callback: function() { if (pkgCount != pkgs.length) { // Race condition where another setTimeout call snuck in after us; if // that call added a package, we must not shift its callback return; } while (callbacks.length > 0) callbacks.shift()(); } }); }, 100); } // jsFooter","title":"Apostrophes"},{"location":"intermediate_r/","text":"Foreword Output options: the \u2018tango\u2019 syntax and the \u2018readable\u2019 theme. Snippets and results. Conditionals and Control Flow \u00b6 Equality (or not) \u00b6 1 2 # Comparison of logicals TRUE == FALSE 1 ## [1] FALSE 1 2 # Comparison of numerics ( -6 * 14 ) != ( 17 - 101 ) 1 ## [1] FALSE 1 2 # Comparison of character strings 'useR' == 'user' 1 ## [1] FALSE 1 2 # Compare a logical with a numeric TRUE == 1 1 ## [1] TRUE Greater and less than 1 2 # Comparison of numerics ( -6 * 5 + 2 ) >= ( -10 + 1 ) 1 ## [1] FALSE 1 2 # Comparison of character strings 'raining' <= 'raining dogs' 1 ## [1] TRUE 1 2 # Comparison of logicals TRUE > FALSE 1 ## [1] TRUE Compare vectors 1 2 3 4 5 6 # The linkedin and facebook vectors linkedin <- c ( 16 , 9 , 13 , 5 , 2 , 17 , 14 ) facebook <- c ( 17 , 7 , 5 , 16 , 8 , 13 , 14 ) # Popular days linkedin > 15 1 ## [1] TRUE FALSE FALSE FALSE FALSE TRUE FALSE 1 2 # Quiet days linkedin <= 5 1 ## [1] FALSE FALSE FALSE TRUE TRUE FALSE FALSE 1 2 # LinkedIn more popular than Facebook linkedin > facebook 1 ## [1] FALSE TRUE TRUE FALSE FALSE TRUE FALSE Compare matrices 1 2 3 4 views <- matrix ( c ( linkedin , facebook ), nrow = 2 , byrow = TRUE ) # When does views equal 13? views == 13 1 2 3 ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] ## [1,] FALSE FALSE TRUE FALSE FALSE FALSE FALSE ## [2,] FALSE FALSE FALSE FALSE FALSE TRUE FALSE 1 2 # When is views less than or equal to 14? views <= 14 1 2 3 ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] ## [1,] FALSE TRUE TRUE TRUE TRUE FALSE TRUE ## [2,] FALSE TRUE TRUE FALSE TRUE TRUE TRUE 1 2 # How often does facebook equal or exceed linkedin times two? sum ( facebook >= linkedin * 2 ) 1 ## [1] 2 & and | \u00b6 1 2 3 4 5 6 # The linkedin and last variable linkedin <- c ( 16 , 9 , 13 , 5 , 2 , 17 , 14 ) last <- tail ( linkedin , 1 ) # Is last under 5 or above 10? last < 5 | last > 10 1 ## [1] TRUE 1 2 # Is last between 15 (exclusive) and 20 (inclusive)? last > 15 & last <= 20 1 ## [1] FALSE 1 2 # Is last between 0 and 5 or between 10 and 15? ( last > 0 & last < 5 ) | ( last > 10 & last < 15 ) 1 ## [1] TRUE & and | (2) 1 2 # linkedin exceeds 10 but facebook below 10 linkedin > 10 & facebook < 10 1 ## [1] FALSE FALSE TRUE FALSE FALSE FALSE FALSE 1 2 # When were one or both visited at least 12 times? linkedin >= 12 | facebook >= 12 1 ## [1] TRUE FALSE TRUE TRUE FALSE TRUE TRUE 1 2 # When is views between 11 (exclusive) and 14 (inclusive)? views > 11 & views <= 14 1 2 3 ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] ## [1,] FALSE FALSE TRUE FALSE FALSE FALSE TRUE ## [2,] FALSE FALSE FALSE FALSE FALSE TRUE TRUE Blend it all together 1 2 3 4 5 6 7 8 # Select the second column, named day2, from li_df: second second <- li_df $ day2 # Build a logical vector, TRUE if value in second is extreme: extremes extremes <- ( second > 25 | second < 5 ) # Count the number of TRUEs in extremes sum ( extremes ) 1 ## [1] 16 The if statement (and more) \u00b6 1 2 3 4 5 6 7 8 # Variables related to your last day of recordings medium <- 'LinkedIn' num_views <- 14 # Examine the if statement for medium if ( medium == 'LinkedIn' ) { print ( 'Showing LinkedIn information' ) } 1 ## [1] \"Showing LinkedIn information\" 1 2 3 4 # Write the if statement for num_views if ( num_views > 15 ) { print ( 'You\\'re popular!' ) } Add an else 1 2 3 4 5 6 7 8 9 10 # Variables related to your last day of recordings medium <- 'LinkedIn' num_views <- 14 # Control structure for medium if ( medium == 'LinkedIn' ) { print ( 'Showing LinkedIn information' ) } else { print ( 'Unknown medium' ) } 1 ## [1] \"Showing LinkedIn information\" 1 2 3 4 5 6 # Control structure for num_views if ( num_views > 15 ) { print ( 'You\\'re popular!' ) } else { print ( 'Try to be more visible!' ) } 1 ## [1] \"Try to be more visible!\" Customize further: else if 1 2 3 4 5 6 7 8 9 10 11 12 # Variables related to your last day of recordings medium <- 'LinkedIn' num_views <- 14 # Control structure for medium if ( medium == 'LinkedIn' ) { print ( 'Showing LinkedIn information' ) } else if ( medium == 'Facebook' ) { print ( 'Showing Facebook information' ) } else { print ( 'Unknown medium' ) } 1 ## [1] \"Showing LinkedIn information\" 1 2 3 4 5 6 7 8 # Control structure for num_views if ( num_views > 15 ) { print ( 'You\\'re popular!' ) } else if ( num_views > 10 | num_views <= 15 ) { print ( 'Your number of views is average' ) } else { print ( 'Try to be more visible!' ) } 1 ## [1] \"Your number of views is average\" Take control! 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 # Variables related to your last day of recordings li <- 15 fb <- 9 # Code the control-flow construct if ( li >= 15 & fb >= 15 ) { sms <- 2 * ( li + fb ) } else if ( li < 10 & fb < 10 ) { sms <- ( li + fb ) / 2 } else { sms <- li + fb } # Print the resulting sms to the console sms 1 ## [1] 24 Loops \u00b6 Write a while loop \u00b6 1 2 3 4 5 6 7 8 # Initialize the speed variable speed <- 64 # Code the while loop while ( speed > 30 ) { print ( 'Slow down!' ) speed <- speed - 7 } 1 2 3 4 5 ## [1] \"Slow down!\" ## [1] \"Slow down!\" ## [1] \"Slow down!\" ## [1] \"Slow down!\" ## [1] \"Slow down!\" 1 2 # Print out the speed variable speed 1 ## [1] 29 Throw in more conditionals 1 2 3 4 5 6 7 8 9 10 11 12 13 14 # Initialize the speed variable speed <- 64 # Extend/adapt the while loop while ( speed > 30 ) { print ( paste ( 'Your speed is ' , speed )) if ( speed > 48 ) { print ( 'Slow down big time!' ) speed <- speed - 11 } else { print ( 'Slow down!' ) speed <- speed - 6 } } 1 2 3 4 5 6 7 8 ## [1] \"Your speed is 64\" ## [1] \"Slow down big time!\" ## [1] \"Your speed is 53\" ## [1] \"Slow down big time!\" ## [1] \"Your speed is 42\" ## [1] \"Slow down!\" ## [1] \"Your speed is 36\" ## [1] \"Slow down!\" Stop the while loop: break 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 # Initialize the speed variable speed <- 88 while ( speed > 30 ) { print ( paste ( 'Your speed is' , speed )) # Break the while loop when speed exceeds 80 if ( speed > 80 ) { break } else if ( speed > 48 ) { print ( 'Slow down big time!' ) speed <- speed - 11 } else { print ( 'Slow down!' ) speed <- speed - 6 } } 1 ## [1] \"Your speed is 88\" Build a while loop from scratch strsplit ; split up in a vector that contains separate letters. 1 2 3 4 5 6 7 8 9 10 11 # Initialize i i <- 1 # Code the while loop while ( i <= 10 ) { print ( i * 3 ) if ( ( i * 3 ) %% 8 == 0 ) { break } i <- i + 1 } 1 2 3 4 5 6 7 8 ## [1] 3 ## [1] 6 ## [1] 9 ## [1] 12 ## [1] 15 ## [1] 18 ## [1] 21 ## [1] 24 Write a for loop \u00b6 Loop over a vector 1 2 3 4 5 6 7 # The linkedin vector linkedin <- c ( 16 , 9 , 13 , 5 , 2 , 17 , 14 ) # Loop version 1 for ( lin in linkedin ) { print ( lin ) } 1 2 3 4 5 6 7 ## [1] 16 ## [1] 9 ## [1] 13 ## [1] 5 ## [1] 2 ## [1] 17 ## [1] 14 1 2 3 4 # Loop version 2 for ( i in 1 : length ( linkedin )) { print ( linkedin[i] ) } 1 2 3 4 5 6 7 ## [1] 16 ## [1] 9 ## [1] 13 ## [1] 5 ## [1] 2 ## [1] 17 ## [1] 14 Loop over a list [[]] ; list of list. 1 2 3 4 5 6 7 8 9 # The nyc list is already specified nyc <- list ( pop = 8405837 , boroughs = c ( 'Manhattan' , 'Bronx' , 'Brooklyn' , 'Queens' , 'Staten Island' ), capital = FALSE ) # Loop version 1 for ( item in nyc ) { print ( item ) } 1 2 3 4 ## [1] 8405837 ## [1] \"Manhattan\" \"Bronx\" \"Brooklyn\" \"Queens\" ## [5] \"Staten Island\" ## [1] FALSE 1 2 3 4 # Loop version 2 for ( i in 1 : length ( nyc )) { print ( nyc[[i]] ) } 1 2 3 4 ## [1] 8405837 ## [1] \"Manhattan\" \"Bronx\" \"Brooklyn\" \"Queens\" ## [5] \"Staten Island\" ## [1] FALSE Loop over a matrix 1 2 3 4 5 6 7 8 9 # The tic-tac-toe matrix has already been defined for you ttt <- matrix ( c ( 'O' , NA , 'X' , NA , 'O' , NA , 'X' , 'O' , 'X' ), nrow = 3 , ncol = 3 ) # define the double for loop for ( i in 1 : nrow ( ttt )) { for ( j in 1 : ncol ( ttt )) { print ( paste ( 'On row' , i , 'and column' , j , 'the board contains ' , ttt[i , j] )) } } 1 2 3 4 5 6 7 8 9 ## [1] \"On row 1 and column 1 the board contains O\" ## [1] \"On row 1 and column 2 the board contains NA\" ## [1] \"On row 1 and column 3 the board contains X\" ## [1] \"On row 2 and column 1 the board contains NA\" ## [1] \"On row 2 and column 2 the board contains O\" ## [1] \"On row 2 and column 3 the board contains O\" ## [1] \"On row 3 and column 1 the board contains X\" ## [1] \"On row 3 and column 2 the board contains NA\" ## [1] \"On row 3 and column 3 the board contains X\" Mix up loops with control flow \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 # The linkedin vector linkedin <- c ( 16 , 9 , 13 , 5 , 2 , 17 , 14 ) # Code the for loop with conditionals for ( i in 1 : length ( linkedin )) { if ( linkedin[i] > 10 ) { print ( 'You\\'re popular!' ) } else { print ( 'Be more visible!' ) } print ( linkedin[i] ) } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 ## [1] \"You're popular!\" ## [1] 16 ## [1] \"Be more visible!\" ## [1] 9 ## [1] \"You're popular!\" ## [1] 13 ## [1] \"Be more visible!\" ## [1] 5 ## [1] \"Be more visible!\" ## [1] 2 ## [1] \"You're popular!\" ## [1] 17 ## [1] \"You're popular!\" ## [1] 14 Next, you break it 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 # The linkedin vector linkedin <- c ( 16 , 9 , 13 , 5 , 2 , 17 , 14 ) # Extend the for loop for ( li in linkedin ) { if ( li > 10 ) { print ( 'You\\'re popular!' ) } else { print ( 'Be more visible!' ) } # Add code to conditionally break iteration if ( li > 16 ) { print ( 'This is ridiculous, I\\'m outta here!' ) break } # Add code to conditionally skip iteration if ( li < 5 ) { print ( 'This is too embarrassing!' ) next } print ( li ) } 1 2 3 4 5 6 7 8 9 10 11 12 ## [1] \"You're popular!\" ## [1] 16 ## [1] \"Be more visible!\" ## [1] 9 ## [1] \"You're popular!\" ## [1] 13 ## [1] \"Be more visible!\" ## [1] 5 ## [1] \"Be more visible!\" ## [1] \"This is too embarrassing!\" ## [1] \"You're popular!\" ## [1] \"This is ridiculous, I'm outta here!\" Build a for loop from scratch 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 # Pre-defined variables rquote <- 'R\\'s internals are irrefutably intriguing' chars <- strsplit ( rquote , split = '' ) [[1]] rcount <- 0 # Your solution here for ( i in 1 : length ( chars )) { if ( chars[i] == 'u' ) { break } if ( chars[i] == 'r' | chars[i] == 'R' ) { rcount <- rcount + 1 } } # Print the resulting rcount variable to the console print ( rcount ) 1 ## [1] 5 Functions \u00b6 Function documentation \u00b6 1 2 3 4 5 # Consult the documentation on the mean() function ? mean # Inspect the arguments of the mean() function args ( mean ) Use a function \u00b6 1 2 3 4 5 6 7 8 9 10 # The linkedin and facebook vectors linkedin <- c ( 16 , 9 , 13 , 5 , 2 , 17 , 14 ) facebook <- c ( 17 , 7 , 5 , 16 , 8 , 13 , 14 ) # Calculate average number of views avg_li <- mean ( linkedin ) avg_fb <- mean ( facebook ) # Inspect avg_li and avg_fb print ( avg_li ) 1 ## [1] 10.85714 1 print ( avg_fb ) 1 ## [1] 11.42857 1 avg_li 1 ## [1] 10.85714 1 2 # Calculate the mean of linkedin minus facebook print ( mean ( linkedin - facebook )) 1 ## [1] -0.5714286 Use a function (2) 1 2 3 4 5 6 7 8 9 10 11 12 # The linkedin and facebook vectors linkedin <- c ( 16 , 9 , 13 , 5 , 2 , 17 , 14 ) facebook <- c ( 17 , 7 , 5 , 16 , 8 , 13 , 14 ) # Calculate the mean of the sum avg_sum <- mean ( linkedin + facebook ) # Calculate the trimmed mean of the sum avg_sum_trimmed <- mean (( linkedin + facebook ), trim = 0.2 ) # Inspect both new variables avg_sum 1 ## [1] 22.28571 1 avg_sum_trimmed 1 ## [1] 22.6 Use a function (3) 1 2 3 4 5 6 # The linkedin and facebook vectors linkedin <- c ( 16 , 9 , 13 , 5 , NA , 17 , 14 ) facebook <- c ( 17 , NA , 5 , 16 , 8 , 13 , 14 ) # Basic average of linkedin print ( mean ( linkedin )) 1 ## [1] NA 1 2 # Advanced average of facebook print ( mean ( facebook , na.rm = TRUE )) 1 ## [1] 12.16667 Functions inside functions \u00b6 1 2 3 4 5 6 # The linkedin and facebook vectors linkedin <- c ( 16 , 9 , 13 , 5 , NA , 17 , 14 ) facebook <- c ( 17 , NA , 5 , 16 , 8 , 13 , 14 ) # Calculate the mean absolute deviation mean (( abs ( linkedin - facebook )), na.rm = TRUE ) 1 ## [1] 4.8 Write your own function \u00b6 1 2 3 4 5 6 7 # Create a function pow_two() pow_two <- function ( arg1 ) { arg1^2 } # Use the function pow_two ( 12 ) 1 ## [1] 144 1 2 3 4 5 6 7 # Create a function sum_abs() sum_abs <- function ( arg2 , arg3 ) { abs ( arg2 ) + abs ( arg3 ) } # Use the function sum_abs ( -2 , 3 ) 1 ## [1] 5 Write your own function (2) 1 2 3 4 5 6 7 8 # Define the function hello() hello <- function () { print ( 'Hi there!' ) return ( TRUE ) } # Call the function hello() hello () 1 2 3 ## [1] \"Hi there!\" ## [1] TRUE 1 2 3 4 5 6 7 8 9 10 11 # Define the function my_filter() my_filter <- function ( arg1 ) { if ( arg1 > 0 ) { return ( arg1 ) } else { return ( NULL ) } } # Call the function my_filter() twice my_filter ( 5 ) 1 ## [1] 5 1 my_filter ( -5 ) 1 ## NULL Write your own function (3) Variables inside a function are not in the Global Environment. 1 2 3 4 5 6 7 8 9 10 11 # Extend the pow_two() function pow_two <- function ( x , print_info = TRUE ) { y <- x ^ 2 if ( print_info ) { print ( paste ( x , 'to the power two equals' , y )) } return ( y ) } #pow_two(2) pow_two ( 2 , FALSE ) 1 ## [1] 4 R you functional? \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 # The linkedin and facebook vectors linkedin <- c ( 16 , 9 , 13 , 5 , NA , 17 , 14 ) facebook <- c ( 17 , 7 , 5 , 16 , 8 , 13 , 14 ) # Define the interpret function interpret <- function ( arg ) { if ( arg > 15 ) { print ( 'You\\'re popular!' ) return ( arg ) } else { print ( 'Try to be more visible!' ) return ( 0 ) } } interpret ( linkedin[1] ) 1 2 3 ## [1] \"You're popular!\" ## [1] 16 1 interpret ( facebook[2] ) 1 2 3 ## [1] \"Try to be more visible!\" ## [1] 0 R you functional? (2) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 # The linkedin and facebook vectors linkedin <- c ( 16 , 9 , 13 , 5 , 2 , 17 , 14 ) facebook <- c ( 17 , 7 , 5 , 16 , 8 , 13 , 14 ) # The interpret() can be used inside interpret_all() interpret <- function ( num_views ){ if ( num_views > 15 ) { print ( 'You\\'re popular!' ) return ( num_views ) } else { print ( 'Try to be more visible!' ) return ( 0 ) } } # Define the interpret_all() function interpret_all <- function ( data , logi = TRUE ){ yy <- 0 for ( i in data ) { yy <- yy + interpret ( i ) } if ( logi ) { return ( yy ) } else { return ( NULL ) } } # Call the interpret_all() function on both linkedin and facebook interpret_all ( linkedin ) 1 2 3 4 5 6 7 8 9 ## [1] \"You're popular!\" ## [1] \"Try to be more visible!\" ## [1] \"Try to be more visible!\" ## [1] \"Try to be more visible!\" ## [1] \"Try to be more visible!\" ## [1] \"You're popular!\" ## [1] \"Try to be more visible!\" ## [1] 33 1 interpret_all ( facebook ) 1 2 3 4 5 6 7 8 9 ## [1] \"You're popular!\" ## [1] \"Try to be more visible!\" ## [1] \"Try to be more visible!\" ## [1] \"You're popular!\" ## [1] \"Try to be more visible!\" ## [1] \"Try to be more visible!\" ## [1] \"Try to be more visible!\" ## [1] 33 Load an R package \u00b6 1 2 3 4 5 6 # The mtcars vectors have already been prepared for you wt <- mtcars $ wt hp <- mtcars $ hp # Request the currently attached packages search () 1 2 3 4 5 6 ## [1] \".GlobalEnv\" \"package:XLConnect\" ## [3] \"package:XLConnectJars\" \"package:stats\" ## [5] \"package:graphics\" \"package:grDevices\" ## [7] \"package:utils\" \"package:datasets\" ## [9] \"package:methods\" \"Autoloads\" ## [11] \"package:base\" 1 2 # Try the qplot() function with wt and hp plot ( wt , hp ) 1 2 3 4 5 6 7 # Load the ggplot2 package library ( 'ggplot2' ) # or require ( 'ggplot2' ) # Retry the qplot() function qplot ( wt , hp ) 1 2 # Check out the currently attached packages again search () 1 2 3 4 5 6 ## [1] \".GlobalEnv\" \"package:ggplot2\" ## [3] \"package:XLConnect\" \"package:XLConnectJars\" ## [5] \"package:stats\" \"package:graphics\" ## [7] \"package:grDevices\" \"package:utils\" ## [9] \"package:datasets\" \"package:methods\" ## [11] \"Autoloads\" \"package:base\" The apply Family \u00b6 Use lapply (with a built-in R function) \u00b6 1 2 3 4 5 6 7 8 9 10 11 # The vector pioneers pioneers <- c ( 'GAUSS:1777' , 'BAYES:1702' , 'PASCAL:1623' , 'PEARSON:1857' ) # Split names from birth year: split_math split_math <- strsplit ( pioneers , ':' ) # Convert to lowercase strings: split_low split_low <- lapply ( split_math , tolower ) # Take a look at the structure of split_low str ( split_low ) 1 2 3 4 5 ## List of 4 ## $ : chr [1:2] \"gauss\" \"1777\" ## $ : chr [1:2] \"bayes\" \"1702\" ## $ : chr [1:2] \"pascal\" \"1623\" ## $ : chr [1:2] \"pearson\" \"1857\" Use lapply with your own function 1 2 3 4 5 6 7 8 9 10 11 12 13 14 # Code from previous exercise pioneers <- c ( 'GAUSS:1777' , 'BAYES:1702' , 'PASCAL:1623' , 'PEARSON:1857' ) split <- strsplit ( pioneers , split = ':' ) split_low <- lapply ( split , tolower ) # Write function select_first() select_first <- function ( x ) { return ( x[1] ) } # Apply select_first() over split_low: names names <- lapply ( split_low , select_first ) print ( names ) 1 2 3 4 5 6 7 8 9 10 11 ## [[1]] ## [1] \"gauss\" ## ## [[2]] ## [1] \"bayes\" ## ## [[3]] ## [1] \"pascal\" ## ## [[4]] ## [1] \"pearson\" 1 2 3 4 5 6 7 8 # Write function select_second() select_second <- function ( x ) { return ( x[2] ) } # Apply select_second() over split_low: years years <- lapply ( split_low , select_second ) print ( years ) 1 2 3 4 5 6 7 8 9 10 11 ## [[1]] ## [1] \"1777\" ## ## [[2]] ## [1] \"1702\" ## ## [[3]] ## [1] \"1623\" ## ## [[4]] ## [1] \"1857\" lapply and anonymous functions Anonymous function == lambda function. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 # Definition of split_low pioneers <- c ( 'GAUSS:1777' , 'BAYES:1702' , 'PASCAL:1623' , 'PEARSON:1857' ) split <- strsplit ( pioneers , split = ':' ) split_low <- lapply ( split , tolower ) #select_first <- function(x) { # x[1] #} names <- lapply ( split_low , function ( x ) { x[1] }) #select_second <- function(x) { # x[2] #} years <- lapply ( split_low , function ( x ) { x[2] }) Use lapply with additional arguments 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 # Definition of split_low pioneers <- c ( 'GAUSS:1777' , 'BAYES:1702' , 'PASCAL:1623' , 'PEARSON:1857' ) split <- strsplit ( pioneers , split = ':' ) split_low <- lapply ( split , tolower ) # Replace the select_*() functions by a single function: select_el select_el <- function ( x , i ) { x[i] } #select_second <- function(x) { # x[2] #} # Call the select_el() function twice on split_low: names and years names <- lapply ( split_low , select_el , i = 1 ) years <- lapply ( split_low , select_el , 2 ) Use sapply \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 temp1 <- c ( 3 , 7 , 9 , 6 , -1 ) temp2 <- c ( 6 , 9 , 12 , 13 , 5 ) temp3 <- c ( 4 , 8 , 3 , -1 , -3 ) temp4 <- c ( 1 , 4 , 7 , 2 , -2 ) temp5 <- c ( 5 , 7 , 9 , 4 , 2 ) temp6 <- c ( -3 , 5 , 8 , 9 , 4 ) temp7 <- c ( 3 , 6 , 9 , 4 , 1 ) temp <- list ( temp1 , temp2 , temp3 , temp4 , temp5 , temp6 , temp7 ) # Use lapply() to find each day's minimum temperature lapply ( temp , min ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 ## [[1]] ## [1] -1 ## ## [[2]] ## [1] 5 ## ## [[3]] ## [1] -3 ## ## [[4]] ## [1] -2 ## ## [[5]] ## [1] 2 ## ## [[6]] ## [1] -3 ## ## [[7]] ## [1] 1 1 2 # Use sapply() to find each day's minimum temperature sapply ( temp , min ) 1 ## [1] -1 5 -3 -2 2 -3 1 1 2 # Use lapply() to find each day's maximum temperature lapply ( temp , max ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 ## [[1]] ## [1] 9 ## ## [[2]] ## [1] 13 ## ## [[3]] ## [1] 8 ## ## [[4]] ## [1] 7 ## ## [[5]] ## [1] 9 ## ## [[6]] ## [1] 9 ## ## [[7]] ## [1] 9 1 2 # Use sapply() to find each day's maximum temperature sapply ( temp , max ) 1 ## [1] 9 13 8 7 9 9 9 sapply with your own function 1 2 3 4 5 6 7 8 9 # temp is already defined in the workspace # Define a function calculates the average of the min and max of a vector: extremes_avg extremes_avg <- function ( x ) { return (( min ( x ) + max ( x )) / 2 ) } # Apply extremes_avg() over temp using sapply() sapply ( temp , extremes_avg ) 1 ## [1] 4.0 9.0 2.5 2.5 5.5 3.0 5.0 1 2 # Apply extremes_avg() over temp using lapply() lapply ( temp , extremes_avg ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 ## [[1]] ## [1] 4 ## ## [[2]] ## [1] 9 ## ## [[3]] ## [1] 2.5 ## ## [[4]] ## [1] 2.5 ## ## [[5]] ## [1] 5.5 ## ## [[6]] ## [1] 3 ## ## [[7]] ## [1] 5 sapply with function returning vector 1 2 3 4 5 6 7 8 9 # temp is already available in the workspace # Create a function that returns min and max of a vector: extremes extremes <- function ( x ) { c ( min ( x ), max ( x )) } # Apply extremes() over temp with sapply() sapply ( temp , extremes ) 1 2 3 ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] ## [1,] -1 5 -3 -2 2 -3 1 ## [2,] 9 13 8 7 9 9 9 1 2 # Apply extremes() over temp with lapply() lapply ( temp , extremes ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 ## [[1]] ## [1] -1 9 ## ## [[2]] ## [1] 5 13 ## ## [[3]] ## [1] -3 8 ## ## [[4]] ## [1] -2 7 ## ## [[5]] ## [1] 2 9 ## ## [[6]] ## [1] -3 9 ## ## [[7]] ## [1] 1 9 sapply can\u2019t simplify, now what? 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 # temp is already prepared for you in the workspace # Create a function that returns all values below zero: below_zero below_zero <- function ( x ) { x[x < 0 ] } #below_zero(temp) alone won't work!!! # Apply below_zero over temp using sapply(): freezing_s freezing_s <- sapply ( temp , below_zero ) # Apply below_zero over temp using lapply(): freezing_l freezing_l <- lapply ( temp , below_zero ) # Compare freezing_s to freezing_l using identical() identical ( freezing_s , freezing_l ) 1 ## [1] TRUE sapply with functions that return NULL 1 2 3 4 5 6 7 8 9 # temp is already available in the workspace # Write a function that 'cat()s' out the average temperatures: print_info print_info <- function ( x ) { cat ( 'The average temperature is' , mean ( x ), '\\n' ) } # Apply print_info() over temp using lapply() lapply ( temp , print_info ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 ## The average temperature is 4.8 ## The average temperature is 9 ## The average temperature is 2.2 ## The average temperature is 2.4 ## The average temperature is 5.4 ## The average temperature is 4.6 ## The average temperature is 4.6 ## [[1]] ## NULL ## ## [[2]] ## NULL ## ## [[3]] ## NULL ## ## [[4]] ## NULL ## ## [[5]] ## NULL ## ## [[6]] ## NULL ## ## [[7]] ## NULL 1 2 # Apply print_info() over temp using sapply() sapply ( temp , print_info ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 ## The average temperature is 4.8 ## The average temperature is 9 ## The average temperature is 2.2 ## The average temperature is 2.4 ## The average temperature is 5.4 ## The average temperature is 4.6 ## The average temperature is 4.6 ## [[1]] ## NULL ## ## [[2]] ## NULL ## ## [[3]] ## NULL ## ## [[4]] ## NULL ## ## [[5]] ## NULL ## ## [[6]] ## NULL ## ## [[7]] ## NULL Use vapply \u00b6 1 2 3 4 5 6 7 8 9 # temp is already available in the workspace # Code the basics() function basics <- function ( x ) { c ( minimum = min ( x ), average = mean ( x ), maximum = max ( x )) } # Apply basics() over temp using vapply() vapply ( temp , basics , numeric ( 3 )) 1 2 3 4 ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] ## minimum -1.0 5 -3.0 -2.0 2.0 -3.0 1.0 ## average 4.8 9 2.2 2.4 5.4 4.6 4.6 ## maximum 9.0 13 8.0 7.0 9.0 9.0 9.0 Use vapply (2) 1 2 3 4 5 6 7 8 9 # temp is already available in the workspace # Definition of the basics() function basics <- function ( x ) { c ( min = min ( x ), mean = mean ( x ), median = median ( x ), max = max ( x )) } # Fix the error: vapply ( temp , basics , numeric ( 4 )) 1 2 3 4 5 ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] ## min -1.0 5 -3.0 -2.0 2.0 -3.0 1.0 ## mean 4.8 9 2.2 2.4 5.4 4.6 4.6 ## median 6.0 9 3.0 2.0 5.0 5.0 4.0 ## max 9.0 13 8.0 7.0 9.0 9.0 9.0 From sapply to vapply 1 2 3 4 # temp is already defined in the workspace # Convert to vapply() expression vapply ( temp , max , numeric ( 1 )) 1 ## [1] 9 13 8 7 9 9 9 1 2 # Convert to vapply() expression vapply ( temp , function ( x , y ) { mean ( x ) > y }, y = 5 , logical ( 1 )) 1 ## [1] FALSE TRUE FALSE FALSE TRUE FALSE FALSE 1 2 3 4 5 6 7 8 9 10 11 # Definition of get_info (don't change) get_info <- function ( x , y ) { if ( mean ( x ) > y ) { return ( 'Not too cold!' ) } else { return ( 'Pretty cold!' ) } } # Convert to vapply() expression vapply ( temp , get_info , y = 5 , character ( 1 )) 1 2 ## [1] \"Pretty cold!\" \"Not too cold!\" \"Pretty cold!\" \"Pretty cold!\" ## [5] \"Not too cold!\" \"Pretty cold!\" \"Pretty cold!\" Apply your knowledge. Or better yet: sapply it? \u00b6 1 2 3 4 5 6 7 8 9 10 # work_todos and fun_todos have already been defined work_todos <- c ( 'Schedule call with team' , 'Fix error in Recommendation System' , 'Respond to Marc from IT' ) fun_todos <- c ( 'Sleep' , 'Make arrangements for summer trip' ) # Create a list: todos todos <- list ( work_todos , fun_todos ) todos 1 2 3 4 5 6 7 ## [[1]] ## [1] \"Schedule call with team\" ## [2] \"Fix error in Recommendation System\" ## [3] \"Respond to Marc from IT\" ## ## [[2]] ## [1] \"Sleep\" \"Make arrangements for summer trip\" 1 2 # Sort the vectors inside todos alphabetically lapply ( todos , sort ) 1 2 3 4 5 6 7 ## [[1]] ## [1] \"Fix error in Recommendation System\" ## [2] \"Respond to Marc from IT\" ## [3] \"Schedule call with team\" ## ## [[2]] ## [1] \"Make arrangements for summer trip\" \"Sleep\" Utilities \u00b6 Mathematical utilities \u00b6 abs ; calculate the absolute value. sum ; calculate the sum of all the values in a data structure. mean ; calculate the arithmetic mean. round ; round the values to 0 decimal places by default. Try out ?round in the console for variations of round and ways to change the number of digits to round to. 1 2 3 4 5 # The errors vector errors <- c ( 1.9 , -2.6 , 4.0 , -9.5 , -3.4 , 7.3 ) # Sum of absolute rounded values of errors sum ( abs ( round ( errors ))) 1 ## [1] 29 Find the error \u00b6 1 2 3 4 5 6 # Vectors vec1 <- c ( 1.5 , 2.5 , 8.4 , 3.7 , 6.3 ) vec2 <- rev ( vec1 ) # Fix the error mean ( abs ( append ( vec1 , vec2 ))) 1 ## [1] 4.48 Data utilities \u00b6 seq ; generate sequences, by specifying the from, to and by arguments. rep ; replicate elements of vectors and lists. sort ; sort a vector in ascending order. Works on numerics, but also on character strings and logicals. rev ; reverse the elements in a data structures for which reversal is defined. str ; display the structure of any R object. append; Merge vectors or lists. is.* ; check for the class of an R object. as.* ; convert an R object from one class to another. unlist ; flatten (possibly embedded) lists to produce a vector. 1 2 3 4 5 6 7 8 9 10 11 12 13 # The linkedin and facebook vectors linkedin <- list ( 16 , 9 , 13 , 5 , 2 , 17 , 14 ) facebook <- list ( 17 , 7 , 5 , 16 , 8 , 13 , 14 ) # Convert linkedin and facebook to a vector: li_vec and fb_vec li_vec <- unlist ( as.vector ( linkedin )) fb_vec <- unlist ( as.vector ( facebook )) # Append fb_vec to li_vec: social_vec social_vec <- append ( li_vec , fb_vec ) # Sort social_vec sort ( social_vec , decreasing = TRUE ) 1 ## [1] 17 17 16 16 14 14 13 13 9 8 7 5 5 2 Find the error (2) 1 2 # Fix me round ( sum ( unlist ( list ( 1.1 , 3 , 5 )))) 1 ## [1] 9 1 2 # Fix me rep ( seq ( 1 , 7 , by = 2 ), times = 7 ) 1 ## [1] 1 3 5 7 1 3 5 7 1 3 5 7 1 3 5 7 1 3 5 7 1 3 5 7 1 3 5 7 1 print ( rep ( seq ( 1 , 7 , by = 2 ), times = 7 )) 1 ## [1] 1 3 5 7 1 3 5 7 1 3 5 7 1 3 5 7 1 3 5 7 1 3 5 7 1 3 5 7 Beat Gauss using R \u00b6 1 2 3 # Create first sequence: seq1 seq1 <- seq ( 1 , 500 , by = 3 ) print ( seq1 ) 1 2 3 4 5 6 7 8 9 10 ## [1] 1 4 7 10 13 16 19 22 25 28 31 34 37 40 43 46 49 ## [18] 52 55 58 61 64 67 70 73 76 79 82 85 88 91 94 97 100 ## [35] 103 106 109 112 115 118 121 124 127 130 133 136 139 142 145 148 151 ## [52] 154 157 160 163 166 169 172 175 178 181 184 187 190 193 196 199 202 ## [69] 205 208 211 214 217 220 223 226 229 232 235 238 241 244 247 250 253 ## [86] 256 259 262 265 268 271 274 277 280 283 286 289 292 295 298 301 304 ## [103] 307 310 313 316 319 322 325 328 331 334 337 340 343 346 349 352 355 ## [120] 358 361 364 367 370 373 376 379 382 385 388 391 394 397 400 403 406 ## [137] 409 412 415 418 421 424 427 430 433 436 439 442 445 448 451 454 457 ## [154] 460 463 466 469 472 475 478 481 484 487 490 493 496 499 1 2 3 # Create second sequence: seq2 seq2 <- seq ( 1200 , 900 , by = -7 ) print ( seq2 ) 1 2 3 4 ## [1] 1200 1193 1186 1179 1172 1165 1158 1151 1144 1137 1130 1123 1116 1109 ## [15] 1102 1095 1088 1081 1074 1067 1060 1053 1046 1039 1032 1025 1018 1011 ## [29] 1004 997 990 983 976 969 962 955 948 941 934 927 920 913 ## [43] 906 1 2 # Calculate total sum of the sequences print ( sum ( append ( seq1 , seq2 ))) 1 ## [1] 87029 grepl & grep (and the likes) \u00b6 grepl ; return TRUE when a pattern is found in the corresponding character string. grep ; return a vector of indices of the character strings that contains the pattern. 1 2 3 4 5 6 # The emails vector has emails <- c ( 'john.doe@ivyleague.edu' , 'education@world.gov' , 'dalai.lama@peace.org' , 'invalid.edu' , 'quant@bigdatacollege.edu' , 'cookie.monster@sesame.tv' ) # Use grepl() to match for 'edu' print ( grepl ( pattern = 'edu' , x = emails )) 1 ## [1] TRUE TRUE FALSE TRUE TRUE FALSE 1 2 3 # Use grep() to match for 'edu', save result to hits hits <- grep ( pattern = 'edu' , x = emails ) hits 1 ## [1] 1 2 4 5 1 2 # Subset emails using hits print ( emails[hits] ) 1 2 ## [1] \"john.doe@ivyleague.edu\" \"education@world.gov\" ## [3] \"invalid.edu\" \"quant@bigdatacollege.edu\" grepl & grep (2) Consult a regex character chart for more. 1 2 3 4 5 6 # The emails vector emails <- c ( 'john.doe@ivyleague.edu' , 'education@world.gov' , 'dalai.lama@peace.org' , 'invalid.edu' , 'quant@bigdatacollege.edu' , 'cookie.monster@sesame.tv' ) # Use grep() to match for .edu addresses more robustly print ( grep ( pattern = '@.*\\\\.edu$' , x = emails )) 1 ## [1] 1 5 1 2 3 # Use grepl() to match for .edu addresses more robustly, save result to hits hits <- grepl ( pattern = '@.*\\\\.edu$' , x = emails ) hits 1 ## [1] TRUE FALSE FALSE FALSE TRUE FALSE 1 2 # Subset emails using hits print ( emails[hits] ) 1 ## [1] \"john.doe@ivyleague.edu\" \"quant@bigdatacollege.edu\" sub & gsub 1 2 3 4 5 6 # The emails vector emails <- c ( 'john.doe@ivyleague.edu' , 'education@world.gov' , 'dalai.lama@peace.org' , 'invalid.edu' , 'quant@bigdatacollege.edu' , 'cookie.monster@sesame.tv' ) # Use sub() to convert the email domains to datacamp.edu (attempt 1) print ( sub ( pattern = '@.*\\\\.edu$' , replacement = 'datacamp.edu' , x = emails )) 1 2 3 ## [1] \"john.doedatacamp.edu\" \"education@world.gov\" ## [3] \"dalai.lama@peace.org\" \"invalid.edu\" ## [5] \"quantdatacamp.edu\" \"cookie.monster@sesame.tv\" 1 2 # Use sub() to convert the email domains to datacamp.edu (attempt 2) print ( sub ( pattern = '@.*\\\\.edu$' , replacement = '@datacamp.edu' , x = emails )) 1 2 3 ## [1] \"john.doe@datacamp.edu\" \"education@world.gov\" ## [3] \"dalai.lama@peace.org\" \"invalid.edu\" ## [5] \"quant@datacamp.edu\" \"cookie.monster@sesame.tv\" Time is of the essence \u00b6 Right here, right now 1 2 3 # Get the current date: today today <- Sys.Date () today 1 ## [1] \"2017-04-14\" 1 2 # See what today looks like under the hood print ( unclass ( today )) 1 ## [1] 17270 1 2 3 # Get the current time: now now <- Sys.time () now 1 ## [1] \"2017-04-14 08:29:36 EDT\" 1 2 # See what now looks like under the hood print ( unclass ( now )) 1 ## [1] 1492172976 Create and format dates Symbol Meaning Example %d day as a number (0-31) 31-janv %a abbreviated weekday Mon %A unabbreviated weekday Monday %m month (00-12) 00-12 %b abbreviated month Jan %B unabbreviated month January %y 2-digit year 07 %Y 4-digit year 2007 %H hours as a decimal number 23 %M minutes as a decimal number 10 %S seconds as a decimal number 53 %T shorthand notation for the typical format %H:%M:%S 23:10:53 Find out more with ?strptime . R offer default functions for dealing with time and dates. There are better packages: date and lubridate . lubridate enhances time-series packages such as zoo and xts , and works well with dplyr for data wrangling. 1 2 3 4 5 6 7 8 9 10 library ( date ) # Definition of character strings representing dates str1 <- \"May 23, 96\" str2 <- \"2012-3-15\" str3 <- \"30/January/2006\" # Convert the strings to dates: date1, date2, date3 date1 <- as.date ( str1 , order = \"mdy\" ) date1 1 ## [1] 23May96 1 2 date1 <- as.POSIXct ( date1 , format = \"%d %m %y\" ) date1 1 ## [1] \"1996-05-22 20:00:00 EDT\" 1 2 date2 <- as.date ( str2 , order = \"ymd\" ) date2 1 ## [1] 15Mar2012 1 2 date2 <- as.POSIXct ( date2 , format = \"%d %m %y\" ) date2 1 ## [1] \"2012-03-14 20:00:00 EDT\" 1 2 date3 <- as.date ( str3 , order = \"dmy\" ) date3 1 ## [1] 30Jan2006 1 2 date3 <- as.POSIXct ( date3 , format = \"%d %m %y\" ) date3 1 ## [1] \"2006-01-29 19:00:00 EST\" 1 2 # Convert dates to formatted strings format ( date1 , \"%A\" ) 1 ## [1] \"mercredi\" 1 format ( date2 , \"%d\" ) 1 ## [1] \"14\" 1 format ( date3 , \"%b %Y\" ) 1 ## [1] \"janv. 2006\" 1 2 3 # convert dates to character data strDate2 <- as.character ( date2 ) strDate2 1 ## [1] \"2012-03-14 20:00:00\" Create and format times 1 2 3 4 5 6 7 8 9 10 # Definition of character strings representing times str1 <- \"2012-3-12 14:23:08\" # Convert the strings to POSIXct objects: time1, time2 time1 <- as.POSIXct ( str2 , format = \"%Y-%m-%d %H:%M:%S\" ) # Convert times to formatted strings # Definition of character strings representing dates format ( time1 , \"%M\" ) 1 ## [1] NA 1 format ( time1 , \"%I:%M %p\" ) 1 ## [1] NA Calculations with dates 1 2 3 4 5 6 7 8 9 # day1, day2, day3, day4 and day5 day1 <- as.Date ( \"2016-11-21\" ) day2 <- as.Date ( \"2016-11-16\" ) day3 <- as.Date ( \"2016-11-27\" ) day4 <- as.Date ( \"2016-11-14\" ) day5 <- as.Date ( \"2016-12-02\" ) # Difference between last and first pizza day print ( day5 - day1 ) 1 ## Time difference of 11 days 1 2 3 4 5 6 # Create vector pizza pizza <- c ( day1 , day2 , day3 , day4 , day5 ) # Create differences between consecutive pizza days: day_diff day_diff <- diff ( pizza , lag = 1 , differences = 1 ) day_diff 1 2 ## Time differences in days ## [1] -5 11 -13 18 1 2 # Average period between two consecutive pizza days print ( mean ( day_diff )) 1 ## Time difference of 2.75 days Calculus with times 1 2 3 4 5 6 7 8 9 10 11 # login and logout login <- as.POSIXct ( c ( \"2016-11-18 10:18:04 UTC\" , \"2016-11-23 09:14:18 UTC\" , \"2016-11-23 12:21:51 UTC\" , \"2016-11-23 12:37:24 UTC\" , \"2016-11-25 21:37:55 UTC\" )) logout <- as.POSIXct ( c ( \"2016-11-18 10:56:29 UTC\" , \"2016-11-23 09:14:52 UTC\" , \"2016-11-23 12:35:48 UTC\" , \"2016-11-23 13:17:22 UTC\" , \"2016-11-25 22:08:47 UTC\" )) # Calculate the difference between login and logout: time_online time_online <- logout - login # Inspect the variable time_online #class(time_online) time_online 1 2 ## Time differences in secs ## [1] 2305 34 837 2398 1852 1 2 # Calculate the total time online print ( sum ( time_online )) 1 ## Time difference of 7426 secs 1 2 # Calculate the average time online print ( mean ( time_online )) 1 ## Time difference of 1485.2 secs","title":"Intermediate R"},{"location":"intermediate_r/#conditionals-and-control-flow","text":"","title":"Conditionals and Control Flow"},{"location":"intermediate_r/#equality-or-not","text":"1 2 # Comparison of logicals TRUE == FALSE 1 ## [1] FALSE 1 2 # Comparison of numerics ( -6 * 14 ) != ( 17 - 101 ) 1 ## [1] FALSE 1 2 # Comparison of character strings 'useR' == 'user' 1 ## [1] FALSE 1 2 # Compare a logical with a numeric TRUE == 1 1 ## [1] TRUE Greater and less than 1 2 # Comparison of numerics ( -6 * 5 + 2 ) >= ( -10 + 1 ) 1 ## [1] FALSE 1 2 # Comparison of character strings 'raining' <= 'raining dogs' 1 ## [1] TRUE 1 2 # Comparison of logicals TRUE > FALSE 1 ## [1] TRUE Compare vectors 1 2 3 4 5 6 # The linkedin and facebook vectors linkedin <- c ( 16 , 9 , 13 , 5 , 2 , 17 , 14 ) facebook <- c ( 17 , 7 , 5 , 16 , 8 , 13 , 14 ) # Popular days linkedin > 15 1 ## [1] TRUE FALSE FALSE FALSE FALSE TRUE FALSE 1 2 # Quiet days linkedin <= 5 1 ## [1] FALSE FALSE FALSE TRUE TRUE FALSE FALSE 1 2 # LinkedIn more popular than Facebook linkedin > facebook 1 ## [1] FALSE TRUE TRUE FALSE FALSE TRUE FALSE Compare matrices 1 2 3 4 views <- matrix ( c ( linkedin , facebook ), nrow = 2 , byrow = TRUE ) # When does views equal 13? views == 13 1 2 3 ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] ## [1,] FALSE FALSE TRUE FALSE FALSE FALSE FALSE ## [2,] FALSE FALSE FALSE FALSE FALSE TRUE FALSE 1 2 # When is views less than or equal to 14? views <= 14 1 2 3 ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] ## [1,] FALSE TRUE TRUE TRUE TRUE FALSE TRUE ## [2,] FALSE TRUE TRUE FALSE TRUE TRUE TRUE 1 2 # How often does facebook equal or exceed linkedin times two? sum ( facebook >= linkedin * 2 ) 1 ## [1] 2","title":"Equality (or not)"},{"location":"intermediate_r/#and","text":"1 2 3 4 5 6 # The linkedin and last variable linkedin <- c ( 16 , 9 , 13 , 5 , 2 , 17 , 14 ) last <- tail ( linkedin , 1 ) # Is last under 5 or above 10? last < 5 | last > 10 1 ## [1] TRUE 1 2 # Is last between 15 (exclusive) and 20 (inclusive)? last > 15 & last <= 20 1 ## [1] FALSE 1 2 # Is last between 0 and 5 or between 10 and 15? ( last > 0 & last < 5 ) | ( last > 10 & last < 15 ) 1 ## [1] TRUE & and | (2) 1 2 # linkedin exceeds 10 but facebook below 10 linkedin > 10 & facebook < 10 1 ## [1] FALSE FALSE TRUE FALSE FALSE FALSE FALSE 1 2 # When were one or both visited at least 12 times? linkedin >= 12 | facebook >= 12 1 ## [1] TRUE FALSE TRUE TRUE FALSE TRUE TRUE 1 2 # When is views between 11 (exclusive) and 14 (inclusive)? views > 11 & views <= 14 1 2 3 ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] ## [1,] FALSE FALSE TRUE FALSE FALSE FALSE TRUE ## [2,] FALSE FALSE FALSE FALSE FALSE TRUE TRUE Blend it all together 1 2 3 4 5 6 7 8 # Select the second column, named day2, from li_df: second second <- li_df $ day2 # Build a logical vector, TRUE if value in second is extreme: extremes extremes <- ( second > 25 | second < 5 ) # Count the number of TRUEs in extremes sum ( extremes ) 1 ## [1] 16","title":"&amp; and |"},{"location":"intermediate_r/#the-if-statement-and-more","text":"1 2 3 4 5 6 7 8 # Variables related to your last day of recordings medium <- 'LinkedIn' num_views <- 14 # Examine the if statement for medium if ( medium == 'LinkedIn' ) { print ( 'Showing LinkedIn information' ) } 1 ## [1] \"Showing LinkedIn information\" 1 2 3 4 # Write the if statement for num_views if ( num_views > 15 ) { print ( 'You\\'re popular!' ) } Add an else 1 2 3 4 5 6 7 8 9 10 # Variables related to your last day of recordings medium <- 'LinkedIn' num_views <- 14 # Control structure for medium if ( medium == 'LinkedIn' ) { print ( 'Showing LinkedIn information' ) } else { print ( 'Unknown medium' ) } 1 ## [1] \"Showing LinkedIn information\" 1 2 3 4 5 6 # Control structure for num_views if ( num_views > 15 ) { print ( 'You\\'re popular!' ) } else { print ( 'Try to be more visible!' ) } 1 ## [1] \"Try to be more visible!\" Customize further: else if 1 2 3 4 5 6 7 8 9 10 11 12 # Variables related to your last day of recordings medium <- 'LinkedIn' num_views <- 14 # Control structure for medium if ( medium == 'LinkedIn' ) { print ( 'Showing LinkedIn information' ) } else if ( medium == 'Facebook' ) { print ( 'Showing Facebook information' ) } else { print ( 'Unknown medium' ) } 1 ## [1] \"Showing LinkedIn information\" 1 2 3 4 5 6 7 8 # Control structure for num_views if ( num_views > 15 ) { print ( 'You\\'re popular!' ) } else if ( num_views > 10 | num_views <= 15 ) { print ( 'Your number of views is average' ) } else { print ( 'Try to be more visible!' ) } 1 ## [1] \"Your number of views is average\" Take control! 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 # Variables related to your last day of recordings li <- 15 fb <- 9 # Code the control-flow construct if ( li >= 15 & fb >= 15 ) { sms <- 2 * ( li + fb ) } else if ( li < 10 & fb < 10 ) { sms <- ( li + fb ) / 2 } else { sms <- li + fb } # Print the resulting sms to the console sms 1 ## [1] 24","title":"The if statement (and more)"},{"location":"intermediate_r/#loops","text":"","title":"Loops"},{"location":"intermediate_r/#write-a-while-loop","text":"1 2 3 4 5 6 7 8 # Initialize the speed variable speed <- 64 # Code the while loop while ( speed > 30 ) { print ( 'Slow down!' ) speed <- speed - 7 } 1 2 3 4 5 ## [1] \"Slow down!\" ## [1] \"Slow down!\" ## [1] \"Slow down!\" ## [1] \"Slow down!\" ## [1] \"Slow down!\" 1 2 # Print out the speed variable speed 1 ## [1] 29 Throw in more conditionals 1 2 3 4 5 6 7 8 9 10 11 12 13 14 # Initialize the speed variable speed <- 64 # Extend/adapt the while loop while ( speed > 30 ) { print ( paste ( 'Your speed is ' , speed )) if ( speed > 48 ) { print ( 'Slow down big time!' ) speed <- speed - 11 } else { print ( 'Slow down!' ) speed <- speed - 6 } } 1 2 3 4 5 6 7 8 ## [1] \"Your speed is 64\" ## [1] \"Slow down big time!\" ## [1] \"Your speed is 53\" ## [1] \"Slow down big time!\" ## [1] \"Your speed is 42\" ## [1] \"Slow down!\" ## [1] \"Your speed is 36\" ## [1] \"Slow down!\" Stop the while loop: break 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 # Initialize the speed variable speed <- 88 while ( speed > 30 ) { print ( paste ( 'Your speed is' , speed )) # Break the while loop when speed exceeds 80 if ( speed > 80 ) { break } else if ( speed > 48 ) { print ( 'Slow down big time!' ) speed <- speed - 11 } else { print ( 'Slow down!' ) speed <- speed - 6 } } 1 ## [1] \"Your speed is 88\" Build a while loop from scratch strsplit ; split up in a vector that contains separate letters. 1 2 3 4 5 6 7 8 9 10 11 # Initialize i i <- 1 # Code the while loop while ( i <= 10 ) { print ( i * 3 ) if ( ( i * 3 ) %% 8 == 0 ) { break } i <- i + 1 } 1 2 3 4 5 6 7 8 ## [1] 3 ## [1] 6 ## [1] 9 ## [1] 12 ## [1] 15 ## [1] 18 ## [1] 21 ## [1] 24","title":"Write a while loop"},{"location":"intermediate_r/#write-a-for-loop","text":"Loop over a vector 1 2 3 4 5 6 7 # The linkedin vector linkedin <- c ( 16 , 9 , 13 , 5 , 2 , 17 , 14 ) # Loop version 1 for ( lin in linkedin ) { print ( lin ) } 1 2 3 4 5 6 7 ## [1] 16 ## [1] 9 ## [1] 13 ## [1] 5 ## [1] 2 ## [1] 17 ## [1] 14 1 2 3 4 # Loop version 2 for ( i in 1 : length ( linkedin )) { print ( linkedin[i] ) } 1 2 3 4 5 6 7 ## [1] 16 ## [1] 9 ## [1] 13 ## [1] 5 ## [1] 2 ## [1] 17 ## [1] 14 Loop over a list [[]] ; list of list. 1 2 3 4 5 6 7 8 9 # The nyc list is already specified nyc <- list ( pop = 8405837 , boroughs = c ( 'Manhattan' , 'Bronx' , 'Brooklyn' , 'Queens' , 'Staten Island' ), capital = FALSE ) # Loop version 1 for ( item in nyc ) { print ( item ) } 1 2 3 4 ## [1] 8405837 ## [1] \"Manhattan\" \"Bronx\" \"Brooklyn\" \"Queens\" ## [5] \"Staten Island\" ## [1] FALSE 1 2 3 4 # Loop version 2 for ( i in 1 : length ( nyc )) { print ( nyc[[i]] ) } 1 2 3 4 ## [1] 8405837 ## [1] \"Manhattan\" \"Bronx\" \"Brooklyn\" \"Queens\" ## [5] \"Staten Island\" ## [1] FALSE Loop over a matrix 1 2 3 4 5 6 7 8 9 # The tic-tac-toe matrix has already been defined for you ttt <- matrix ( c ( 'O' , NA , 'X' , NA , 'O' , NA , 'X' , 'O' , 'X' ), nrow = 3 , ncol = 3 ) # define the double for loop for ( i in 1 : nrow ( ttt )) { for ( j in 1 : ncol ( ttt )) { print ( paste ( 'On row' , i , 'and column' , j , 'the board contains ' , ttt[i , j] )) } } 1 2 3 4 5 6 7 8 9 ## [1] \"On row 1 and column 1 the board contains O\" ## [1] \"On row 1 and column 2 the board contains NA\" ## [1] \"On row 1 and column 3 the board contains X\" ## [1] \"On row 2 and column 1 the board contains NA\" ## [1] \"On row 2 and column 2 the board contains O\" ## [1] \"On row 2 and column 3 the board contains O\" ## [1] \"On row 3 and column 1 the board contains X\" ## [1] \"On row 3 and column 2 the board contains NA\" ## [1] \"On row 3 and column 3 the board contains X\"","title":"Write a for loop"},{"location":"intermediate_r/#mix-up-loops-with-control-flow","text":"1 2 3 4 5 6 7 8 9 10 11 12 # The linkedin vector linkedin <- c ( 16 , 9 , 13 , 5 , 2 , 17 , 14 ) # Code the for loop with conditionals for ( i in 1 : length ( linkedin )) { if ( linkedin[i] > 10 ) { print ( 'You\\'re popular!' ) } else { print ( 'Be more visible!' ) } print ( linkedin[i] ) } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 ## [1] \"You're popular!\" ## [1] 16 ## [1] \"Be more visible!\" ## [1] 9 ## [1] \"You're popular!\" ## [1] 13 ## [1] \"Be more visible!\" ## [1] 5 ## [1] \"Be more visible!\" ## [1] 2 ## [1] \"You're popular!\" ## [1] 17 ## [1] \"You're popular!\" ## [1] 14 Next, you break it 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 # The linkedin vector linkedin <- c ( 16 , 9 , 13 , 5 , 2 , 17 , 14 ) # Extend the for loop for ( li in linkedin ) { if ( li > 10 ) { print ( 'You\\'re popular!' ) } else { print ( 'Be more visible!' ) } # Add code to conditionally break iteration if ( li > 16 ) { print ( 'This is ridiculous, I\\'m outta here!' ) break } # Add code to conditionally skip iteration if ( li < 5 ) { print ( 'This is too embarrassing!' ) next } print ( li ) } 1 2 3 4 5 6 7 8 9 10 11 12 ## [1] \"You're popular!\" ## [1] 16 ## [1] \"Be more visible!\" ## [1] 9 ## [1] \"You're popular!\" ## [1] 13 ## [1] \"Be more visible!\" ## [1] 5 ## [1] \"Be more visible!\" ## [1] \"This is too embarrassing!\" ## [1] \"You're popular!\" ## [1] \"This is ridiculous, I'm outta here!\" Build a for loop from scratch 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 # Pre-defined variables rquote <- 'R\\'s internals are irrefutably intriguing' chars <- strsplit ( rquote , split = '' ) [[1]] rcount <- 0 # Your solution here for ( i in 1 : length ( chars )) { if ( chars[i] == 'u' ) { break } if ( chars[i] == 'r' | chars[i] == 'R' ) { rcount <- rcount + 1 } } # Print the resulting rcount variable to the console print ( rcount ) 1 ## [1] 5","title":"Mix up loops with control flow"},{"location":"intermediate_r/#functions","text":"","title":"Functions"},{"location":"intermediate_r/#function-documentation","text":"1 2 3 4 5 # Consult the documentation on the mean() function ? mean # Inspect the arguments of the mean() function args ( mean )","title":"Function documentation"},{"location":"intermediate_r/#use-a-function","text":"1 2 3 4 5 6 7 8 9 10 # The linkedin and facebook vectors linkedin <- c ( 16 , 9 , 13 , 5 , 2 , 17 , 14 ) facebook <- c ( 17 , 7 , 5 , 16 , 8 , 13 , 14 ) # Calculate average number of views avg_li <- mean ( linkedin ) avg_fb <- mean ( facebook ) # Inspect avg_li and avg_fb print ( avg_li ) 1 ## [1] 10.85714 1 print ( avg_fb ) 1 ## [1] 11.42857 1 avg_li 1 ## [1] 10.85714 1 2 # Calculate the mean of linkedin minus facebook print ( mean ( linkedin - facebook )) 1 ## [1] -0.5714286 Use a function (2) 1 2 3 4 5 6 7 8 9 10 11 12 # The linkedin and facebook vectors linkedin <- c ( 16 , 9 , 13 , 5 , 2 , 17 , 14 ) facebook <- c ( 17 , 7 , 5 , 16 , 8 , 13 , 14 ) # Calculate the mean of the sum avg_sum <- mean ( linkedin + facebook ) # Calculate the trimmed mean of the sum avg_sum_trimmed <- mean (( linkedin + facebook ), trim = 0.2 ) # Inspect both new variables avg_sum 1 ## [1] 22.28571 1 avg_sum_trimmed 1 ## [1] 22.6 Use a function (3) 1 2 3 4 5 6 # The linkedin and facebook vectors linkedin <- c ( 16 , 9 , 13 , 5 , NA , 17 , 14 ) facebook <- c ( 17 , NA , 5 , 16 , 8 , 13 , 14 ) # Basic average of linkedin print ( mean ( linkedin )) 1 ## [1] NA 1 2 # Advanced average of facebook print ( mean ( facebook , na.rm = TRUE )) 1 ## [1] 12.16667","title":"Use a function"},{"location":"intermediate_r/#functions-inside-functions","text":"1 2 3 4 5 6 # The linkedin and facebook vectors linkedin <- c ( 16 , 9 , 13 , 5 , NA , 17 , 14 ) facebook <- c ( 17 , NA , 5 , 16 , 8 , 13 , 14 ) # Calculate the mean absolute deviation mean (( abs ( linkedin - facebook )), na.rm = TRUE ) 1 ## [1] 4.8","title":"Functions inside functions"},{"location":"intermediate_r/#write-your-own-function","text":"1 2 3 4 5 6 7 # Create a function pow_two() pow_two <- function ( arg1 ) { arg1^2 } # Use the function pow_two ( 12 ) 1 ## [1] 144 1 2 3 4 5 6 7 # Create a function sum_abs() sum_abs <- function ( arg2 , arg3 ) { abs ( arg2 ) + abs ( arg3 ) } # Use the function sum_abs ( -2 , 3 ) 1 ## [1] 5 Write your own function (2) 1 2 3 4 5 6 7 8 # Define the function hello() hello <- function () { print ( 'Hi there!' ) return ( TRUE ) } # Call the function hello() hello () 1 2 3 ## [1] \"Hi there!\" ## [1] TRUE 1 2 3 4 5 6 7 8 9 10 11 # Define the function my_filter() my_filter <- function ( arg1 ) { if ( arg1 > 0 ) { return ( arg1 ) } else { return ( NULL ) } } # Call the function my_filter() twice my_filter ( 5 ) 1 ## [1] 5 1 my_filter ( -5 ) 1 ## NULL Write your own function (3) Variables inside a function are not in the Global Environment. 1 2 3 4 5 6 7 8 9 10 11 # Extend the pow_two() function pow_two <- function ( x , print_info = TRUE ) { y <- x ^ 2 if ( print_info ) { print ( paste ( x , 'to the power two equals' , y )) } return ( y ) } #pow_two(2) pow_two ( 2 , FALSE ) 1 ## [1] 4","title":"Write your own function"},{"location":"intermediate_r/#r-you-functional","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 # The linkedin and facebook vectors linkedin <- c ( 16 , 9 , 13 , 5 , NA , 17 , 14 ) facebook <- c ( 17 , 7 , 5 , 16 , 8 , 13 , 14 ) # Define the interpret function interpret <- function ( arg ) { if ( arg > 15 ) { print ( 'You\\'re popular!' ) return ( arg ) } else { print ( 'Try to be more visible!' ) return ( 0 ) } } interpret ( linkedin[1] ) 1 2 3 ## [1] \"You're popular!\" ## [1] 16 1 interpret ( facebook[2] ) 1 2 3 ## [1] \"Try to be more visible!\" ## [1] 0 R you functional? (2) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 # The linkedin and facebook vectors linkedin <- c ( 16 , 9 , 13 , 5 , 2 , 17 , 14 ) facebook <- c ( 17 , 7 , 5 , 16 , 8 , 13 , 14 ) # The interpret() can be used inside interpret_all() interpret <- function ( num_views ){ if ( num_views > 15 ) { print ( 'You\\'re popular!' ) return ( num_views ) } else { print ( 'Try to be more visible!' ) return ( 0 ) } } # Define the interpret_all() function interpret_all <- function ( data , logi = TRUE ){ yy <- 0 for ( i in data ) { yy <- yy + interpret ( i ) } if ( logi ) { return ( yy ) } else { return ( NULL ) } } # Call the interpret_all() function on both linkedin and facebook interpret_all ( linkedin ) 1 2 3 4 5 6 7 8 9 ## [1] \"You're popular!\" ## [1] \"Try to be more visible!\" ## [1] \"Try to be more visible!\" ## [1] \"Try to be more visible!\" ## [1] \"Try to be more visible!\" ## [1] \"You're popular!\" ## [1] \"Try to be more visible!\" ## [1] 33 1 interpret_all ( facebook ) 1 2 3 4 5 6 7 8 9 ## [1] \"You're popular!\" ## [1] \"Try to be more visible!\" ## [1] \"Try to be more visible!\" ## [1] \"You're popular!\" ## [1] \"Try to be more visible!\" ## [1] \"Try to be more visible!\" ## [1] \"Try to be more visible!\" ## [1] 33","title":"R you functional?"},{"location":"intermediate_r/#load-an-r-package","text":"1 2 3 4 5 6 # The mtcars vectors have already been prepared for you wt <- mtcars $ wt hp <- mtcars $ hp # Request the currently attached packages search () 1 2 3 4 5 6 ## [1] \".GlobalEnv\" \"package:XLConnect\" ## [3] \"package:XLConnectJars\" \"package:stats\" ## [5] \"package:graphics\" \"package:grDevices\" ## [7] \"package:utils\" \"package:datasets\" ## [9] \"package:methods\" \"Autoloads\" ## [11] \"package:base\" 1 2 # Try the qplot() function with wt and hp plot ( wt , hp ) 1 2 3 4 5 6 7 # Load the ggplot2 package library ( 'ggplot2' ) # or require ( 'ggplot2' ) # Retry the qplot() function qplot ( wt , hp ) 1 2 # Check out the currently attached packages again search () 1 2 3 4 5 6 ## [1] \".GlobalEnv\" \"package:ggplot2\" ## [3] \"package:XLConnect\" \"package:XLConnectJars\" ## [5] \"package:stats\" \"package:graphics\" ## [7] \"package:grDevices\" \"package:utils\" ## [9] \"package:datasets\" \"package:methods\" ## [11] \"Autoloads\" \"package:base\"","title":"Load an R package"},{"location":"intermediate_r/#the-apply-family","text":"","title":"The apply Family"},{"location":"intermediate_r/#use-lapply-with-a-built-in-r-function","text":"1 2 3 4 5 6 7 8 9 10 11 # The vector pioneers pioneers <- c ( 'GAUSS:1777' , 'BAYES:1702' , 'PASCAL:1623' , 'PEARSON:1857' ) # Split names from birth year: split_math split_math <- strsplit ( pioneers , ':' ) # Convert to lowercase strings: split_low split_low <- lapply ( split_math , tolower ) # Take a look at the structure of split_low str ( split_low ) 1 2 3 4 5 ## List of 4 ## $ : chr [1:2] \"gauss\" \"1777\" ## $ : chr [1:2] \"bayes\" \"1702\" ## $ : chr [1:2] \"pascal\" \"1623\" ## $ : chr [1:2] \"pearson\" \"1857\" Use lapply with your own function 1 2 3 4 5 6 7 8 9 10 11 12 13 14 # Code from previous exercise pioneers <- c ( 'GAUSS:1777' , 'BAYES:1702' , 'PASCAL:1623' , 'PEARSON:1857' ) split <- strsplit ( pioneers , split = ':' ) split_low <- lapply ( split , tolower ) # Write function select_first() select_first <- function ( x ) { return ( x[1] ) } # Apply select_first() over split_low: names names <- lapply ( split_low , select_first ) print ( names ) 1 2 3 4 5 6 7 8 9 10 11 ## [[1]] ## [1] \"gauss\" ## ## [[2]] ## [1] \"bayes\" ## ## [[3]] ## [1] \"pascal\" ## ## [[4]] ## [1] \"pearson\" 1 2 3 4 5 6 7 8 # Write function select_second() select_second <- function ( x ) { return ( x[2] ) } # Apply select_second() over split_low: years years <- lapply ( split_low , select_second ) print ( years ) 1 2 3 4 5 6 7 8 9 10 11 ## [[1]] ## [1] \"1777\" ## ## [[2]] ## [1] \"1702\" ## ## [[3]] ## [1] \"1623\" ## ## [[4]] ## [1] \"1857\" lapply and anonymous functions Anonymous function == lambda function. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 # Definition of split_low pioneers <- c ( 'GAUSS:1777' , 'BAYES:1702' , 'PASCAL:1623' , 'PEARSON:1857' ) split <- strsplit ( pioneers , split = ':' ) split_low <- lapply ( split , tolower ) #select_first <- function(x) { # x[1] #} names <- lapply ( split_low , function ( x ) { x[1] }) #select_second <- function(x) { # x[2] #} years <- lapply ( split_low , function ( x ) { x[2] }) Use lapply with additional arguments 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 # Definition of split_low pioneers <- c ( 'GAUSS:1777' , 'BAYES:1702' , 'PASCAL:1623' , 'PEARSON:1857' ) split <- strsplit ( pioneers , split = ':' ) split_low <- lapply ( split , tolower ) # Replace the select_*() functions by a single function: select_el select_el <- function ( x , i ) { x[i] } #select_second <- function(x) { # x[2] #} # Call the select_el() function twice on split_low: names and years names <- lapply ( split_low , select_el , i = 1 ) years <- lapply ( split_low , select_el , 2 )","title":"Use lapply (with a built-in R function)"},{"location":"intermediate_r/#use-sapply","text":"1 2 3 4 5 6 7 8 9 10 11 12 temp1 <- c ( 3 , 7 , 9 , 6 , -1 ) temp2 <- c ( 6 , 9 , 12 , 13 , 5 ) temp3 <- c ( 4 , 8 , 3 , -1 , -3 ) temp4 <- c ( 1 , 4 , 7 , 2 , -2 ) temp5 <- c ( 5 , 7 , 9 , 4 , 2 ) temp6 <- c ( -3 , 5 , 8 , 9 , 4 ) temp7 <- c ( 3 , 6 , 9 , 4 , 1 ) temp <- list ( temp1 , temp2 , temp3 , temp4 , temp5 , temp6 , temp7 ) # Use lapply() to find each day's minimum temperature lapply ( temp , min ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 ## [[1]] ## [1] -1 ## ## [[2]] ## [1] 5 ## ## [[3]] ## [1] -3 ## ## [[4]] ## [1] -2 ## ## [[5]] ## [1] 2 ## ## [[6]] ## [1] -3 ## ## [[7]] ## [1] 1 1 2 # Use sapply() to find each day's minimum temperature sapply ( temp , min ) 1 ## [1] -1 5 -3 -2 2 -3 1 1 2 # Use lapply() to find each day's maximum temperature lapply ( temp , max ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 ## [[1]] ## [1] 9 ## ## [[2]] ## [1] 13 ## ## [[3]] ## [1] 8 ## ## [[4]] ## [1] 7 ## ## [[5]] ## [1] 9 ## ## [[6]] ## [1] 9 ## ## [[7]] ## [1] 9 1 2 # Use sapply() to find each day's maximum temperature sapply ( temp , max ) 1 ## [1] 9 13 8 7 9 9 9 sapply with your own function 1 2 3 4 5 6 7 8 9 # temp is already defined in the workspace # Define a function calculates the average of the min and max of a vector: extremes_avg extremes_avg <- function ( x ) { return (( min ( x ) + max ( x )) / 2 ) } # Apply extremes_avg() over temp using sapply() sapply ( temp , extremes_avg ) 1 ## [1] 4.0 9.0 2.5 2.5 5.5 3.0 5.0 1 2 # Apply extremes_avg() over temp using lapply() lapply ( temp , extremes_avg ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 ## [[1]] ## [1] 4 ## ## [[2]] ## [1] 9 ## ## [[3]] ## [1] 2.5 ## ## [[4]] ## [1] 2.5 ## ## [[5]] ## [1] 5.5 ## ## [[6]] ## [1] 3 ## ## [[7]] ## [1] 5 sapply with function returning vector 1 2 3 4 5 6 7 8 9 # temp is already available in the workspace # Create a function that returns min and max of a vector: extremes extremes <- function ( x ) { c ( min ( x ), max ( x )) } # Apply extremes() over temp with sapply() sapply ( temp , extremes ) 1 2 3 ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] ## [1,] -1 5 -3 -2 2 -3 1 ## [2,] 9 13 8 7 9 9 9 1 2 # Apply extremes() over temp with lapply() lapply ( temp , extremes ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 ## [[1]] ## [1] -1 9 ## ## [[2]] ## [1] 5 13 ## ## [[3]] ## [1] -3 8 ## ## [[4]] ## [1] -2 7 ## ## [[5]] ## [1] 2 9 ## ## [[6]] ## [1] -3 9 ## ## [[7]] ## [1] 1 9 sapply can\u2019t simplify, now what? 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 # temp is already prepared for you in the workspace # Create a function that returns all values below zero: below_zero below_zero <- function ( x ) { x[x < 0 ] } #below_zero(temp) alone won't work!!! # Apply below_zero over temp using sapply(): freezing_s freezing_s <- sapply ( temp , below_zero ) # Apply below_zero over temp using lapply(): freezing_l freezing_l <- lapply ( temp , below_zero ) # Compare freezing_s to freezing_l using identical() identical ( freezing_s , freezing_l ) 1 ## [1] TRUE sapply with functions that return NULL 1 2 3 4 5 6 7 8 9 # temp is already available in the workspace # Write a function that 'cat()s' out the average temperatures: print_info print_info <- function ( x ) { cat ( 'The average temperature is' , mean ( x ), '\\n' ) } # Apply print_info() over temp using lapply() lapply ( temp , print_info ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 ## The average temperature is 4.8 ## The average temperature is 9 ## The average temperature is 2.2 ## The average temperature is 2.4 ## The average temperature is 5.4 ## The average temperature is 4.6 ## The average temperature is 4.6 ## [[1]] ## NULL ## ## [[2]] ## NULL ## ## [[3]] ## NULL ## ## [[4]] ## NULL ## ## [[5]] ## NULL ## ## [[6]] ## NULL ## ## [[7]] ## NULL 1 2 # Apply print_info() over temp using sapply() sapply ( temp , print_info ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 ## The average temperature is 4.8 ## The average temperature is 9 ## The average temperature is 2.2 ## The average temperature is 2.4 ## The average temperature is 5.4 ## The average temperature is 4.6 ## The average temperature is 4.6 ## [[1]] ## NULL ## ## [[2]] ## NULL ## ## [[3]] ## NULL ## ## [[4]] ## NULL ## ## [[5]] ## NULL ## ## [[6]] ## NULL ## ## [[7]] ## NULL","title":"Use sapply"},{"location":"intermediate_r/#use-vapply","text":"1 2 3 4 5 6 7 8 9 # temp is already available in the workspace # Code the basics() function basics <- function ( x ) { c ( minimum = min ( x ), average = mean ( x ), maximum = max ( x )) } # Apply basics() over temp using vapply() vapply ( temp , basics , numeric ( 3 )) 1 2 3 4 ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] ## minimum -1.0 5 -3.0 -2.0 2.0 -3.0 1.0 ## average 4.8 9 2.2 2.4 5.4 4.6 4.6 ## maximum 9.0 13 8.0 7.0 9.0 9.0 9.0 Use vapply (2) 1 2 3 4 5 6 7 8 9 # temp is already available in the workspace # Definition of the basics() function basics <- function ( x ) { c ( min = min ( x ), mean = mean ( x ), median = median ( x ), max = max ( x )) } # Fix the error: vapply ( temp , basics , numeric ( 4 )) 1 2 3 4 5 ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] ## min -1.0 5 -3.0 -2.0 2.0 -3.0 1.0 ## mean 4.8 9 2.2 2.4 5.4 4.6 4.6 ## median 6.0 9 3.0 2.0 5.0 5.0 4.0 ## max 9.0 13 8.0 7.0 9.0 9.0 9.0 From sapply to vapply 1 2 3 4 # temp is already defined in the workspace # Convert to vapply() expression vapply ( temp , max , numeric ( 1 )) 1 ## [1] 9 13 8 7 9 9 9 1 2 # Convert to vapply() expression vapply ( temp , function ( x , y ) { mean ( x ) > y }, y = 5 , logical ( 1 )) 1 ## [1] FALSE TRUE FALSE FALSE TRUE FALSE FALSE 1 2 3 4 5 6 7 8 9 10 11 # Definition of get_info (don't change) get_info <- function ( x , y ) { if ( mean ( x ) > y ) { return ( 'Not too cold!' ) } else { return ( 'Pretty cold!' ) } } # Convert to vapply() expression vapply ( temp , get_info , y = 5 , character ( 1 )) 1 2 ## [1] \"Pretty cold!\" \"Not too cold!\" \"Pretty cold!\" \"Pretty cold!\" ## [5] \"Not too cold!\" \"Pretty cold!\" \"Pretty cold!\"","title":"Use vapply"},{"location":"intermediate_r/#apply-your-knowledge-or-better-yet-sapply-it","text":"1 2 3 4 5 6 7 8 9 10 # work_todos and fun_todos have already been defined work_todos <- c ( 'Schedule call with team' , 'Fix error in Recommendation System' , 'Respond to Marc from IT' ) fun_todos <- c ( 'Sleep' , 'Make arrangements for summer trip' ) # Create a list: todos todos <- list ( work_todos , fun_todos ) todos 1 2 3 4 5 6 7 ## [[1]] ## [1] \"Schedule call with team\" ## [2] \"Fix error in Recommendation System\" ## [3] \"Respond to Marc from IT\" ## ## [[2]] ## [1] \"Sleep\" \"Make arrangements for summer trip\" 1 2 # Sort the vectors inside todos alphabetically lapply ( todos , sort ) 1 2 3 4 5 6 7 ## [[1]] ## [1] \"Fix error in Recommendation System\" ## [2] \"Respond to Marc from IT\" ## [3] \"Schedule call with team\" ## ## [[2]] ## [1] \"Make arrangements for summer trip\" \"Sleep\"","title":"Apply your knowledge. Or better yet: sapply it?"},{"location":"intermediate_r/#utilities","text":"","title":"Utilities"},{"location":"intermediate_r/#mathematical-utilities","text":"abs ; calculate the absolute value. sum ; calculate the sum of all the values in a data structure. mean ; calculate the arithmetic mean. round ; round the values to 0 decimal places by default. Try out ?round in the console for variations of round and ways to change the number of digits to round to. 1 2 3 4 5 # The errors vector errors <- c ( 1.9 , -2.6 , 4.0 , -9.5 , -3.4 , 7.3 ) # Sum of absolute rounded values of errors sum ( abs ( round ( errors ))) 1 ## [1] 29","title":"Mathematical utilities"},{"location":"intermediate_r/#find-the-error","text":"1 2 3 4 5 6 # Vectors vec1 <- c ( 1.5 , 2.5 , 8.4 , 3.7 , 6.3 ) vec2 <- rev ( vec1 ) # Fix the error mean ( abs ( append ( vec1 , vec2 ))) 1 ## [1] 4.48","title":"Find the error"},{"location":"intermediate_r/#data-utilities","text":"seq ; generate sequences, by specifying the from, to and by arguments. rep ; replicate elements of vectors and lists. sort ; sort a vector in ascending order. Works on numerics, but also on character strings and logicals. rev ; reverse the elements in a data structures for which reversal is defined. str ; display the structure of any R object. append; Merge vectors or lists. is.* ; check for the class of an R object. as.* ; convert an R object from one class to another. unlist ; flatten (possibly embedded) lists to produce a vector. 1 2 3 4 5 6 7 8 9 10 11 12 13 # The linkedin and facebook vectors linkedin <- list ( 16 , 9 , 13 , 5 , 2 , 17 , 14 ) facebook <- list ( 17 , 7 , 5 , 16 , 8 , 13 , 14 ) # Convert linkedin and facebook to a vector: li_vec and fb_vec li_vec <- unlist ( as.vector ( linkedin )) fb_vec <- unlist ( as.vector ( facebook )) # Append fb_vec to li_vec: social_vec social_vec <- append ( li_vec , fb_vec ) # Sort social_vec sort ( social_vec , decreasing = TRUE ) 1 ## [1] 17 17 16 16 14 14 13 13 9 8 7 5 5 2 Find the error (2) 1 2 # Fix me round ( sum ( unlist ( list ( 1.1 , 3 , 5 )))) 1 ## [1] 9 1 2 # Fix me rep ( seq ( 1 , 7 , by = 2 ), times = 7 ) 1 ## [1] 1 3 5 7 1 3 5 7 1 3 5 7 1 3 5 7 1 3 5 7 1 3 5 7 1 3 5 7 1 print ( rep ( seq ( 1 , 7 , by = 2 ), times = 7 )) 1 ## [1] 1 3 5 7 1 3 5 7 1 3 5 7 1 3 5 7 1 3 5 7 1 3 5 7 1 3 5 7","title":"Data utilities"},{"location":"intermediate_r/#beat-gauss-using-r","text":"1 2 3 # Create first sequence: seq1 seq1 <- seq ( 1 , 500 , by = 3 ) print ( seq1 ) 1 2 3 4 5 6 7 8 9 10 ## [1] 1 4 7 10 13 16 19 22 25 28 31 34 37 40 43 46 49 ## [18] 52 55 58 61 64 67 70 73 76 79 82 85 88 91 94 97 100 ## [35] 103 106 109 112 115 118 121 124 127 130 133 136 139 142 145 148 151 ## [52] 154 157 160 163 166 169 172 175 178 181 184 187 190 193 196 199 202 ## [69] 205 208 211 214 217 220 223 226 229 232 235 238 241 244 247 250 253 ## [86] 256 259 262 265 268 271 274 277 280 283 286 289 292 295 298 301 304 ## [103] 307 310 313 316 319 322 325 328 331 334 337 340 343 346 349 352 355 ## [120] 358 361 364 367 370 373 376 379 382 385 388 391 394 397 400 403 406 ## [137] 409 412 415 418 421 424 427 430 433 436 439 442 445 448 451 454 457 ## [154] 460 463 466 469 472 475 478 481 484 487 490 493 496 499 1 2 3 # Create second sequence: seq2 seq2 <- seq ( 1200 , 900 , by = -7 ) print ( seq2 ) 1 2 3 4 ## [1] 1200 1193 1186 1179 1172 1165 1158 1151 1144 1137 1130 1123 1116 1109 ## [15] 1102 1095 1088 1081 1074 1067 1060 1053 1046 1039 1032 1025 1018 1011 ## [29] 1004 997 990 983 976 969 962 955 948 941 934 927 920 913 ## [43] 906 1 2 # Calculate total sum of the sequences print ( sum ( append ( seq1 , seq2 ))) 1 ## [1] 87029","title":"Beat Gauss using R"},{"location":"intermediate_r/#grepl-grep-and-the-likes","text":"grepl ; return TRUE when a pattern is found in the corresponding character string. grep ; return a vector of indices of the character strings that contains the pattern. 1 2 3 4 5 6 # The emails vector has emails <- c ( 'john.doe@ivyleague.edu' , 'education@world.gov' , 'dalai.lama@peace.org' , 'invalid.edu' , 'quant@bigdatacollege.edu' , 'cookie.monster@sesame.tv' ) # Use grepl() to match for 'edu' print ( grepl ( pattern = 'edu' , x = emails )) 1 ## [1] TRUE TRUE FALSE TRUE TRUE FALSE 1 2 3 # Use grep() to match for 'edu', save result to hits hits <- grep ( pattern = 'edu' , x = emails ) hits 1 ## [1] 1 2 4 5 1 2 # Subset emails using hits print ( emails[hits] ) 1 2 ## [1] \"john.doe@ivyleague.edu\" \"education@world.gov\" ## [3] \"invalid.edu\" \"quant@bigdatacollege.edu\" grepl & grep (2) Consult a regex character chart for more. 1 2 3 4 5 6 # The emails vector emails <- c ( 'john.doe@ivyleague.edu' , 'education@world.gov' , 'dalai.lama@peace.org' , 'invalid.edu' , 'quant@bigdatacollege.edu' , 'cookie.monster@sesame.tv' ) # Use grep() to match for .edu addresses more robustly print ( grep ( pattern = '@.*\\\\.edu$' , x = emails )) 1 ## [1] 1 5 1 2 3 # Use grepl() to match for .edu addresses more robustly, save result to hits hits <- grepl ( pattern = '@.*\\\\.edu$' , x = emails ) hits 1 ## [1] TRUE FALSE FALSE FALSE TRUE FALSE 1 2 # Subset emails using hits print ( emails[hits] ) 1 ## [1] \"john.doe@ivyleague.edu\" \"quant@bigdatacollege.edu\" sub & gsub 1 2 3 4 5 6 # The emails vector emails <- c ( 'john.doe@ivyleague.edu' , 'education@world.gov' , 'dalai.lama@peace.org' , 'invalid.edu' , 'quant@bigdatacollege.edu' , 'cookie.monster@sesame.tv' ) # Use sub() to convert the email domains to datacamp.edu (attempt 1) print ( sub ( pattern = '@.*\\\\.edu$' , replacement = 'datacamp.edu' , x = emails )) 1 2 3 ## [1] \"john.doedatacamp.edu\" \"education@world.gov\" ## [3] \"dalai.lama@peace.org\" \"invalid.edu\" ## [5] \"quantdatacamp.edu\" \"cookie.monster@sesame.tv\" 1 2 # Use sub() to convert the email domains to datacamp.edu (attempt 2) print ( sub ( pattern = '@.*\\\\.edu$' , replacement = '@datacamp.edu' , x = emails )) 1 2 3 ## [1] \"john.doe@datacamp.edu\" \"education@world.gov\" ## [3] \"dalai.lama@peace.org\" \"invalid.edu\" ## [5] \"quant@datacamp.edu\" \"cookie.monster@sesame.tv\"","title":"grepl &amp; grep (and the likes)"},{"location":"intermediate_r/#time-is-of-the-essence","text":"Right here, right now 1 2 3 # Get the current date: today today <- Sys.Date () today 1 ## [1] \"2017-04-14\" 1 2 # See what today looks like under the hood print ( unclass ( today )) 1 ## [1] 17270 1 2 3 # Get the current time: now now <- Sys.time () now 1 ## [1] \"2017-04-14 08:29:36 EDT\" 1 2 # See what now looks like under the hood print ( unclass ( now )) 1 ## [1] 1492172976 Create and format dates Symbol Meaning Example %d day as a number (0-31) 31-janv %a abbreviated weekday Mon %A unabbreviated weekday Monday %m month (00-12) 00-12 %b abbreviated month Jan %B unabbreviated month January %y 2-digit year 07 %Y 4-digit year 2007 %H hours as a decimal number 23 %M minutes as a decimal number 10 %S seconds as a decimal number 53 %T shorthand notation for the typical format %H:%M:%S 23:10:53 Find out more with ?strptime . R offer default functions for dealing with time and dates. There are better packages: date and lubridate . lubridate enhances time-series packages such as zoo and xts , and works well with dplyr for data wrangling. 1 2 3 4 5 6 7 8 9 10 library ( date ) # Definition of character strings representing dates str1 <- \"May 23, 96\" str2 <- \"2012-3-15\" str3 <- \"30/January/2006\" # Convert the strings to dates: date1, date2, date3 date1 <- as.date ( str1 , order = \"mdy\" ) date1 1 ## [1] 23May96 1 2 date1 <- as.POSIXct ( date1 , format = \"%d %m %y\" ) date1 1 ## [1] \"1996-05-22 20:00:00 EDT\" 1 2 date2 <- as.date ( str2 , order = \"ymd\" ) date2 1 ## [1] 15Mar2012 1 2 date2 <- as.POSIXct ( date2 , format = \"%d %m %y\" ) date2 1 ## [1] \"2012-03-14 20:00:00 EDT\" 1 2 date3 <- as.date ( str3 , order = \"dmy\" ) date3 1 ## [1] 30Jan2006 1 2 date3 <- as.POSIXct ( date3 , format = \"%d %m %y\" ) date3 1 ## [1] \"2006-01-29 19:00:00 EST\" 1 2 # Convert dates to formatted strings format ( date1 , \"%A\" ) 1 ## [1] \"mercredi\" 1 format ( date2 , \"%d\" ) 1 ## [1] \"14\" 1 format ( date3 , \"%b %Y\" ) 1 ## [1] \"janv. 2006\" 1 2 3 # convert dates to character data strDate2 <- as.character ( date2 ) strDate2 1 ## [1] \"2012-03-14 20:00:00\" Create and format times 1 2 3 4 5 6 7 8 9 10 # Definition of character strings representing times str1 <- \"2012-3-12 14:23:08\" # Convert the strings to POSIXct objects: time1, time2 time1 <- as.POSIXct ( str2 , format = \"%Y-%m-%d %H:%M:%S\" ) # Convert times to formatted strings # Definition of character strings representing dates format ( time1 , \"%M\" ) 1 ## [1] NA 1 format ( time1 , \"%I:%M %p\" ) 1 ## [1] NA Calculations with dates 1 2 3 4 5 6 7 8 9 # day1, day2, day3, day4 and day5 day1 <- as.Date ( \"2016-11-21\" ) day2 <- as.Date ( \"2016-11-16\" ) day3 <- as.Date ( \"2016-11-27\" ) day4 <- as.Date ( \"2016-11-14\" ) day5 <- as.Date ( \"2016-12-02\" ) # Difference between last and first pizza day print ( day5 - day1 ) 1 ## Time difference of 11 days 1 2 3 4 5 6 # Create vector pizza pizza <- c ( day1 , day2 , day3 , day4 , day5 ) # Create differences between consecutive pizza days: day_diff day_diff <- diff ( pizza , lag = 1 , differences = 1 ) day_diff 1 2 ## Time differences in days ## [1] -5 11 -13 18 1 2 # Average period between two consecutive pizza days print ( mean ( day_diff )) 1 ## Time difference of 2.75 days Calculus with times 1 2 3 4 5 6 7 8 9 10 11 # login and logout login <- as.POSIXct ( c ( \"2016-11-18 10:18:04 UTC\" , \"2016-11-23 09:14:18 UTC\" , \"2016-11-23 12:21:51 UTC\" , \"2016-11-23 12:37:24 UTC\" , \"2016-11-25 21:37:55 UTC\" )) logout <- as.POSIXct ( c ( \"2016-11-18 10:56:29 UTC\" , \"2016-11-23 09:14:52 UTC\" , \"2016-11-23 12:35:48 UTC\" , \"2016-11-23 13:17:22 UTC\" , \"2016-11-25 22:08:47 UTC\" )) # Calculate the difference between login and logout: time_online time_online <- logout - login # Inspect the variable time_online #class(time_online) time_online 1 2 ## Time differences in secs ## [1] 2305 34 837 2398 1852 1 2 # Calculate the total time online print ( sum ( time_online )) 1 ## Time difference of 7426 secs 1 2 # Calculate the average time online print ( mean ( time_online )) 1 ## Time difference of 1485.2 secs","title":"Time is of the essence"},{"location":"intermediate_r_the_apply_family/","text":"Foreword Output options: the \u2018tango\u2019 syntax and the \u2018readable\u2019 theme. Snippets and results. Manipulate all data classes. Transform one data class into another class Avoid explicit use of loop constructs and speed up the code. Aggregate or subset the data. apply to Matrices and Arrays \u00b6 Flatten a matrix into a vector. 1 2 3 # Dataset X <- matrix ( rnorm ( 30 ), nrow = 4 , ncol = 4 ) X 1 2 3 4 5 ## [,1] [,2] [,3] [,4] ## [1,] -0.1902972 -0.3250492 0.1597852 -0.4878550 ## [2,] -0.4195968 -1.8485172 -0.1185904 0.2307813 ## [3,] -0.6387653 -0.7848382 -0.4198662 -0.6314304 ## [4,] 0.7374748 -2.0634558 0.2374686 -1.6572091 1 2 # Sum the values of each column apply ( X , 2 , sum ) 1 ## [1] -0.5111845 -5.0218604 -0.1412028 -2.5457132 X is the array or matrix (2D, 3D, etc.). MARGIN=1 for row, 2 for column. FUN=sum , mean , median , min , max , var , sd , range , length , skew , kurtosis , abs , round , tolower , toupper , etc. user-defined function such as FUN = function(x) { c(m = mean(x), s = sd(x)) } . 1 2 # More examples apply ( X , 2 , max ) 1 ## [1] 0.7374748 -0.3250492 0.2374686 0.2307813 1 apply ( X , 2 , range ) 1 2 3 ## [,1] [,2] [,3] [,4] ## [1,] -0.6387653 -2.0634558 -0.4198662 -1.6572091 ## [2,] 0.7374748 -0.3250492 0.2374686 0.2307813 1 apply ( X , 2 , abs ) 1 2 3 4 5 ## [,1] [,2] [,3] [,4] ## [1,] 0.1902972 0.3250492 0.1597852 0.4878550 ## [2,] 0.4195968 1.8485172 0.1185904 0.2307813 ## [3,] 0.6387653 0.7848382 0.4198662 0.6314304 ## [4,] 0.7374748 2.0634558 0.2374686 1.6572091 1 apply ( X , 2 , round , 2 ) 1 2 3 4 5 ## [,1] [,2] [,3] [,4] ## [1,] -0.19 -0.33 0.16 -0.49 ## [2,] -0.42 -1.85 -0.12 0.23 ## [3,] -0.64 -0.78 -0.42 -0.63 ## [4,] 0.74 -2.06 0.24 -1.66 Lambda & custom function 1 2 # Dataset X 1 2 3 4 5 ## [,1] [,2] [,3] [,4] ## [1,] -0.1902972 -0.3250492 0.1597852 -0.4878550 ## [2,] -0.4195968 -1.8485172 -0.1185904 0.2307813 ## [3,] -0.6387653 -0.7848382 -0.4198662 -0.6314304 ## [4,] 0.7374748 -2.0634558 0.2374686 -1.6572091 1 2 # Built-in function apply ( X , 2 , max ) 1 ## [1] 0.7374748 -0.3250492 0.2374686 0.2307813 1 2 # Lambda function apply ( X , 2 , function ( x ) x + 10 ) 1 2 3 4 5 ## [,1] [,2] [,3] [,4] ## [1,] 9.809703 9.674951 10.159785 9.512145 ## [2,] 9.580403 8.151483 9.881410 10.230781 ## [3,] 9.361235 9.215162 9.580134 9.368570 ## [4,] 10.737475 7.936544 10.237469 8.342791 1 2 3 4 5 # Custom function select_first <- function ( x ) { return ( x[1] ) } apply ( X , 2 , select_first ) 1 ## [1] -0.1902972 -0.3250492 0.1597852 -0.4878550 1 2 3 4 5 # Custom function with two arguments select_el <- function ( x , i ) { x[i] } apply ( X , 2 , select_el , i = 2 ) 1 ## [1] -0.4195968 -1.8485172 -0.1185904 0.2307813 Strings 1 2 3 # Dataset Y <- matrix ( c ( 'a' , 'b' , 'c' , 'd' ), nrow = 2 , ncol = 2 ) Y 1 2 3 ## [,1] [,2] ## [1,] \"a\" \"c\" ## [2,] \"b\" \"d\" 1 2 # Change the format apply ( Y , 2 , toupper ) 1 2 3 ## [,1] [,2] ## [1,] \"A\" \"C\" ## [2,] \"B\" \"D\" sweep \u00b6 Several steps 1 2 3 # Dataset dataPoints <- matrix ( 4 : 15 , nrow = 4 , ncol = 3 ) dataPoints 1 2 3 4 5 ## [,1] [,2] [,3] ## [1,] 4 8 12 ## [2,] 5 9 13 ## [3,] 6 10 14 ## [4,] 7 11 15 1 2 3 # Find means (center) per column with `apply()` dataPoints_means <- apply ( dataPoints , 2 , mean ) dataPoints_means 1 ## [1] 5.5 9.5 13.5 1 2 3 # Find standard deviation (dispersion) with `apply()` dataPoints_sdev <- apply ( dataPoints , 2 , sd ) dataPoints_sdev 1 ## [1] 1.290994 1.290994 1.290994 1 2 3 # Center the points; shift all the points with respect to their center dataPoints_Trans1 <- sweep ( dataPoints , 2 , dataPoints_means , \"-\" ) dataPoints_Trans1 1 2 3 4 5 ## [,1] [,2] [,3] ## [1,] -1.5 -1.5 -1.5 ## [2,] -0.5 -0.5 -0.5 ## [3,] 0.5 0.5 0.5 ## [4,] 1.5 1.5 1.5 1 2 3 # Normalize dataPoints_Trans2 <- sweep ( dataPoints_Trans1 , 2 , dataPoints_sdev , \"/\" ) dataPoints_Trans2 1 2 3 4 5 ## [,1] [,2] [,3] ## [1,] -1.1618950 -1.1618950 -1.1618950 ## [2,] -0.3872983 -0.3872983 -0.3872983 ## [3,] 0.3872983 0.3872983 0.3872983 ## [4,] 1.1618950 1.1618950 1.1618950 1 sweep ( dataPoints , 2 , dataPoints_means , sum ) 1 ## [1] 228 X . MARGIN=1 for row, 2 for column. STATS for the summary statistics to be swept out: sum , mean , median , min , max , var , sd , range , length , skew , kurtosis , se , etc. FUN=\"-\" , \"+\" , \"/\" , \"*\" , \"**\" , etc. user-defined function such as FUN = function(x) { c(m = mean(x), s = sd(x)) } . One step 1 2 3 4 # Normalize the data with a nested call dataPoints_Trans <- sweep ( sweep ( dataPoints , 2 , dataPoints_means , \"-\" ), 2 , dataPoints_sdev , \"/\" ) dataPoints_Trans 1 2 3 4 5 ## [,1] [,2] [,3] ## [1,] -1.1618950 -1.1618950 -1.1618950 ## [2,] -0.3872983 -0.3872983 -0.3872983 ## [3,] 0.3872983 0.3872983 0.3872983 ## [4,] 1.1618950 1.1618950 1.1618950 Even simpler 1 2 # Automatic data scaling scale ( dataPoints ) 1 2 3 4 5 6 7 8 9 ## [,1] [,2] [,3] ## [1,] -1.1618950 -1.1618950 -1.1618950 ## [2,] -0.3872983 -0.3872983 -0.3872983 ## [3,] 0.3872983 0.3872983 0.3872983 ## [4,] 1.1618950 1.1618950 1.1618950 ## attr(,\"scaled:center\") ## [1] 5.5 9.5 13.5 ## attr(,\"scaled:scale\") ## [1] 1.290994 1.290994 1.290994 aggregate \u00b6 1 2 # Dataset head ( Mydf , 15 ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 ## X DepPC DProgr Qty Delivered ## 1 1 90 1 7 FALSE ## 2 2 91 2 8 TRUE ## 3 3 92 3 9 FALSE ## 4 4 93 4 10 TRUE ## 5 5 94 5 11 TRUE ## 6 6 75 6 12 FALSE ## 7 7 90 7 13 TRUE ## 8 8 91 8 14 TRUE ## 9 9 92 9 15 FALSE ## 10 10 93 10 16 FALSE ## 11 11 94 11 17 TRUE ## 12 12 75 12 18 FALSE ## 13 13 90 13 19 FALSE ## 14 14 91 14 20 TRUE ## 15 15 92 15 21 TRUE 1 2 # Show data types for each column sapply ( Mydf , class ) 1 2 ## X DepPC DProgr Qty Delivered ## \"integer\" \"integer\" \"integer\" \"integer\" \"logical\" 1 2 # Return number of rows and columns dim ( Mydf ) 1 ## [1] 120 5 1 nrow ( Mydf ) 1 ## [1] 120 1 ncol ( Mydf ) 1 ## [1] 5 1 2 # How many departments? unique ( Mydf $ DepPC ) 1 ## [1] 90 91 92 93 94 75 1 2 # Dataset head ( Mydf , 5 ) 1 2 3 4 5 6 ## X DepPC DProgr Qty Delivered ## 1 1 90 1 7 FALSE ## 2 2 91 2 8 TRUE ## 3 3 92 3 9 FALSE ## 4 4 93 4 10 TRUE ## 5 5 94 5 11 TRUE 1 2 # Aggregate by a variable (categories) and sum up another variable aggregate ( Mydf $ Qty , by = Mydf[ \"DepPC\" ] , FUN = sum ) 1 2 3 4 5 6 7 ## DepPC x ## 1 75 878 ## 2 90 689 ## 3 91 684 ## 4 92 701 ## 5 93 707 ## 6 94 802 1 2 # Aggregate by a variable (categories) and extract descriptive stats from another variable aggregate ( Mydf $ Qty , by = Mydf[ \"DepPC\" ] , FUN = summary ) 1 2 3 4 5 6 7 ## DepPC x.Min. x.1st Qu. x.Median x.Mean x.3rd Qu. x.Max. ## 1 75 4.00 15.25 23.50 43.90 74.50 124.00 ## 2 90 2.00 11.75 19.50 34.45 27.25 119.00 ## 3 91 3.00 9.00 19.00 34.20 26.25 120.00 ## 4 92 1.00 10.00 20.00 35.05 27.25 121.00 ## 5 93 2.00 11.00 19.00 35.35 27.25 122.00 ## 6 94 3.00 12.00 20.00 40.10 46.50 123.00 by \u00b6 An alternative to aggregate with pros and cons. 1 2 # Dataset head ( Mydf , 5 ) 1 2 3 4 5 6 ## X DepPC DProgr Qty Delivered ## 1 1 90 1 7 FALSE ## 2 2 91 2 8 TRUE ## 3 3 92 3 9 FALSE ## 4 4 93 4 10 TRUE ## 5 5 94 5 11 TRUE 1 by ( Mydf $ Qty , Mydf[ \"DepPC\" ] , FUN = sum ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 ## DepPC: 75 ## [1] 878 ## -------------------------------------------------------- ## DepPC: 90 ## [1] 689 ## -------------------------------------------------------- ## DepPC: 91 ## [1] 684 ## -------------------------------------------------------- ## DepPC: 92 ## [1] 701 ## -------------------------------------------------------- ## DepPC: 93 ## [1] 707 ## -------------------------------------------------------- ## DepPC: 94 ## [1] 802 1 by ( Mydf $ Qty , Mydf[ \"DepPC\" ] , FUN = summary ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 ## DepPC: 75 ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 4.00 15.25 23.50 43.90 74.50 124.00 ## -------------------------------------------------------- ## DepPC: 90 ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 2.00 11.75 19.50 34.45 27.25 119.00 ## -------------------------------------------------------- ## DepPC: 91 ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 3.00 9.00 19.00 34.20 26.25 120.00 ## -------------------------------------------------------- ## DepPC: 92 ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 1.00 10.00 20.00 35.05 27.25 121.00 ## -------------------------------------------------------- ## DepPC: 93 ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 2.00 11.00 19.00 35.35 27.25 122.00 ## -------------------------------------------------------- ## DepPC: 94 ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 3.0 12.0 20.0 40.1 46.5 123.0 split \u00b6 1 2 # Dataset head ( Mydf , 5 ) 1 2 3 4 5 6 ## X DepPC DProgr Qty Delivered ## 1 1 90 1 7 FALSE ## 2 2 91 2 8 TRUE ## 3 3 92 3 9 FALSE ## 4 4 93 4 10 TRUE ## 5 5 94 5 11 TRUE 1 nrow ( Mydf ) 1 ## [1] 120 1 2 # Split with a variable (categories) split ( Mydf $ Qty , Mydf[ \"DepPC\" ] ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 ## $`75` ## [1] 12 18 24 30 13 19 100 106 112 118 124 7 13 19 25 16 23 ## [18] 29 4 66 ## ## $`90` ## [1] 7 13 19 25 31 14 20 101 107 113 119 2 8 14 20 26 17 ## [18] 24 4 5 ## ## $`91` ## [1] 8 14 20 26 9 15 21 102 108 114 120 3 9 15 21 27 18 ## [18] 25 3 6 ## ## $`92` ## [1] 9 15 21 27 10 16 22 103 109 115 121 4 10 16 22 28 19 ## [18] 26 1 7 ## ## $`93` ## [1] 10 16 22 28 11 17 23 104 110 116 122 5 11 17 23 14 21 ## [18] 27 2 8 ## ## $`94` ## [1] 11 17 23 29 12 18 99 105 111 117 123 6 12 18 24 15 22 ## [18] 28 3 9 1 2 # Split by row unlist ( Mydf $ Qty ) 1 2 3 4 5 6 7 8 ## [1] 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 ## [18] 24 25 26 27 28 29 30 31 9 10 11 12 13 14 15 16 17 ## [35] 18 19 20 21 22 23 99 100 101 102 103 104 105 106 107 108 109 ## [52] 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 2 3 ## [69] 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 ## [86] 21 22 23 24 25 26 27 28 14 15 16 17 18 19 21 22 23 ## [103] 24 25 26 27 28 29 4 3 1 2 3 4 5 6 7 8 9 ## [120] 66 1 split ( Mydf $ Qty , c ( 60 , 120 )) 1 2 3 4 5 6 7 8 9 10 11 ## $`60` ## [1] 7 9 11 13 15 17 19 21 23 25 27 29 31 10 12 14 16 ## [18] 18 20 22 99 101 103 105 107 109 111 113 115 117 119 121 123 2 ## [35] 4 6 8 10 12 14 16 18 20 22 24 26 28 15 17 19 22 ## [52] 24 26 28 4 1 3 5 7 9 ## ## $`120` ## [1] 8 10 12 14 16 18 20 22 24 26 28 30 9 11 13 15 17 ## [18] 19 21 23 100 102 104 106 108 110 112 114 116 118 120 122 124 3 ## [35] 5 7 9 11 13 15 17 19 21 23 25 27 14 16 18 21 23 ## [52] 25 27 29 3 2 4 6 8 66 strsplit \u00b6 1 2 3 4 5 # The vector pioneers pioneers <- c ( 'GAUSS:1777' , 'BAYES:1702' , 'PASCAL:1623' , 'PEARSON:1857' ) # Split names from birth year: split_math split_math <- strsplit ( pioneers , ':' ) Vectorize \u00b6 Vectorize a scalar function. 1 2 # Scalar function rep ( c ( 1 , 2 ), 2 ) 1 ## [1] 1 2 1 2 1 2 3 # Vector function vrep <- Vectorize ( rep.int ) vrep ( c ( 1 , 2 ), 2 ) 1 2 3 ## [,1] [,2] ## [1,] 1 2 ## [2,] 1 2 1 2 3 # Scalar function f <- function ( x = 1 : 3 , y ) c ( x , y ) f ( 1 : 3 , 1 : 3 ) 1 ## [1] 1 2 3 1 2 3 1 2 3 # Vector function vf <- Vectorize ( f , SIMPLIFY = FALSE ) vf ( 1 : 3 , 1 : 3 ) 1 2 3 4 5 6 7 8 ## [[1]] ## [1] 1 1 ## ## [[2]] ## [1] 2 2 ## ## [[3]] ## [1] 3 3 lapply : list-apply \u00b6 For lists, vectors, and data frames. 1 2 3 4 5 6 7 8 # Dataset A <- matrix ( 1 : 9 , nrow = 3 , ncol = 3 ) B <- matrix ( 10 : 18 , nrow = 3 , ncol = 3 ) C <- matrix ( 19 : 28 , nrow = 3 , ncol = 3 ) # Create a list of matrices; an array (a 3D matrix) MyList <- list ( A , B , C ) MyList 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 ## [[1]] ## [,1] [,2] [,3] ## [1,] 1 4 7 ## [2,] 2 5 8 ## [3,] 3 6 9 ## ## [[2]] ## [,1] [,2] [,3] ## [1,] 10 13 16 ## [2,] 11 14 17 ## [3,] 12 15 18 ## ## [[3]] ## [,1] [,2] [,3] ## [1,] 19 22 25 ## [2,] 20 23 26 ## [3,] 21 24 27 1 2 # Extract values MyList[[1]] 1 2 3 4 ## [,1] [,2] [,3] ## [1,] 1 4 7 ## [2,] 2 5 8 ## [3,] 3 6 9 1 MyList[[1]][1 , ] 1 ## [1] 1 4 7 1 MyList[[1]][ , 1 ] 1 ## [1] 1 2 3 1 MyList[[1]][1 , 1 ] 1 ## [1] 1 1 2 # Extract the 2nd column from `MyList` with the selection operator `[` with `lapply()` lapply ( MyList , \"[\" , , 2 ) 1 2 3 4 5 6 7 8 ## [[1]] ## [1] 4 5 6 ## ## [[2]] ## [1] 13 14 15 ## ## [[3]] ## [1] 22 23 24 1 2 # Extract the 1st row from `MyList` lapply ( MyList , \"[\" , 1 , ) 1 2 3 4 5 6 7 8 ## [[1]] ## [1] 1 4 7 ## ## [[2]] ## [1] 10 13 16 ## ## [[3]] ## [1] 19 22 25 Lambda & custom function 1 2 # Dataset MyList 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 ## [[1]] ## [,1] [,2] [,3] ## [1,] 1 4 7 ## [2,] 2 5 8 ## [3,] 3 6 9 ## ## [[2]] ## [,1] [,2] [,3] ## [1,] 10 13 16 ## [2,] 11 14 17 ## [3,] 12 15 18 ## ## [[3]] ## [,1] [,2] [,3] ## [1,] 19 22 25 ## [2,] 20 23 26 ## [3,] 21 24 27 1 2 # Built-in function lapply ( MyList , max ) 1 2 3 4 5 6 7 8 ## [[1]] ## [1] 9 ## ## [[2]] ## [1] 18 ## ## [[3]] ## [1] 27 1 2 # Lambda function lapply ( MyList , function ( x ) max ( x ) + 10 ) 1 2 3 4 5 6 7 8 ## [[1]] ## [1] 19 ## ## [[2]] ## [1] 28 ## ## [[3]] ## [1] 37 1 2 3 4 5 # Custom function select_first <- function ( x ) { return ( x[1] ) } lapply ( MyList , select_first ) 1 2 3 4 5 6 7 8 ## [[1]] ## [1] 1 ## ## [[2]] ## [1] 10 ## ## [[3]] ## [1] 19 1 2 3 4 5 # Custom function with two arguments select_el <- function ( x , i ) { x[i] } lapply ( MyList , select_el , i = 2 ) 1 2 3 4 5 6 7 8 ## [[1]] ## [1] 2 ## ## [[2]] ## [1] 11 ## ## [[3]] ## [1] 20 Strings 1 2 # Dataset Y 1 2 3 ## [,1] [,2] ## [1,] \"a\" \"c\" ## [2,] \"b\" \"d\" 1 2 # Change the format lapply ( Y , toupper ) 1 2 3 4 5 6 7 8 9 10 11 ## [[1]] ## [1] \"A\" ## ## [[2]] ## [1] \"B\" ## ## [[3]] ## [1] \"C\" ## ## [[4]] ## [1] \"D\" sapply : simplify-list-apply \u00b6 A wrapper that simplifies lapply . 1 2 # Dataset MyList 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 ## [[1]] ## [,1] [,2] [,3] ## [1,] 1 4 7 ## [2,] 2 5 8 ## [3,] 3 6 9 ## ## [[2]] ## [,1] [,2] [,3] ## [1,] 10 13 16 ## [2,] 11 14 17 ## [3,] 12 15 18 ## ## [[3]] ## [,1] [,2] [,3] ## [1,] 19 22 25 ## [2,] 20 23 26 ## [3,] 21 24 27 1 2 # Return a list with `lapply()` lapply ( MyList , \"[\" , 2 , 1 ) 1 2 3 4 5 6 7 8 ## [[1]] ## [1] 2 ## ## [[2]] ## [1] 11 ## ## [[3]] ## [1] 20 1 2 # Return a vector with `sapply()` sapply ( MyList , \"[\" , 2 , 1 ) # same result, but simpler 1 ## [1] 2 11 20 1 2 # Return a list with `sapply()` sapply ( MyList , \"[\" , 2 , 1 , simplify = T ) # by default 1 ## [1] 2 11 20 1 sapply ( MyList , \"[\" , 2 , 1 , simplify = F ) 1 2 3 4 5 6 7 8 ## [[1]] ## [1] 2 ## ## [[2]] ## [1] 11 ## ## [[3]] ## [1] 20 1 2 # Return a vector with `unlist()` unlist ( lapply ( MyList , \"[\" , 2 , 1 )) # similar 1 ## [1] 2 11 20 Lambda & custom function 1 2 # Dataset MyList 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 ## [[1]] ## [,1] [,2] [,3] ## [1,] 1 4 7 ## [2,] 2 5 8 ## [3,] 3 6 9 ## ## [[2]] ## [,1] [,2] [,3] ## [1,] 10 13 16 ## [2,] 11 14 17 ## [3,] 12 15 18 ## ## [[3]] ## [,1] [,2] [,3] ## [1,] 19 22 25 ## [2,] 20 23 26 ## [3,] 21 24 27 1 2 # Built-in function sapply ( MyList , max ) 1 ## [1] 9 18 27 1 2 # Lambda function lapply ( MyList , function ( x ) max ( x ) + 10 ) 1 2 3 4 5 6 7 8 ## [[1]] ## [1] 19 ## ## [[2]] ## [1] 28 ## ## [[3]] ## [1] 37 1 2 3 4 5 # Custom function select_first <- function ( x ) { return ( x[1] ) } sapply ( MyList , select_first ) 1 ## [1] 1 10 19 1 2 3 4 5 # Custom function with two arguments select_el <- function ( x , i ) { x[i] } sapply ( MyList , select_el , i = 2 ) 1 ## [1] 2 11 20 Strings 1 2 # Dataset Y 1 2 3 ## [,1] [,2] ## [1,] \"a\" \"c\" ## [2,] \"b\" \"d\" 1 2 # Change the format sapply ( Y , toupper ) 1 2 ## a b c d ## \"A\" \"B\" \"C\" \"D\" vapply \u00b6 A variant. rapply : recursive-list-apply \u00b6 1 2 # Dataset MyList 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 ## [[1]] ## [,1] [,2] [,3] ## [1,] 1 4 7 ## [2,] 2 5 8 ## [3,] 3 6 9 ## ## [[2]] ## [,1] [,2] [,3] ## [1,] 10 13 16 ## [2,] 11 14 17 ## [3,] 12 15 18 ## ## [[3]] ## [,1] [,2] [,3] ## [1,] 19 22 25 ## [2,] 20 23 26 ## [3,] 21 24 27 1 2 # Build-in function rapply ( MyList , sqrt , classes = \"ANY\" , how = \"replace\" ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 ## [[1]] ## [,1] [,2] [,3] ## [1,] 1.000000 2.000000 2.645751 ## [2,] 1.414214 2.236068 2.828427 ## [3,] 1.732051 2.449490 3.000000 ## ## [[2]] ## [,1] [,2] [,3] ## [1,] 3.162278 3.605551 4.000000 ## [2,] 3.316625 3.741657 4.123106 ## [3,] 3.464102 3.872983 4.242641 ## ## [[3]] ## [,1] [,2] [,3] ## [1,] 4.358899 4.690416 5.000000 ## [2,] 4.472136 4.795832 5.099020 ## [3,] 4.582576 4.898979 5.196152 1 2 # Lambda or custom the function rapply ( MyList , function ( x ) x^2 , classes = \"ANY\" , how = \"replace\" ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 ## [[1]] ## [,1] [,2] [,3] ## [1,] 1 16 49 ## [2,] 4 25 64 ## [3,] 9 36 81 ## ## [[2]] ## [,1] [,2] [,3] ## [1,] 100 169 256 ## [2,] 121 196 289 ## [3,] 144 225 324 ## ## [[3]] ## [,1] [,2] [,3] ## [1,] 361 484 625 ## [2,] 400 529 676 ## [3,] 441 576 729 1 2 3 4 select_first <- function ( x ) { return ( x[1] ) } rapply ( MyList , select_first , classes = \"ANY\" , how = \"replace\" ) 1 2 3 4 5 6 7 8 ## [[1]] ## [1] 1 ## ## [[2]] ## [1] 10 ## ## [[3]] ## [1] 19 1 2 # Change classes = class ( MyList ) # the object 1 ## [1] \"list\" 1 class ( MyList[[1]][1] ) # each entry 1 ## [1] \"integer\" 1 rapply ( MyList , sqrt , classes = \"list\" , how = \"replace\" ) # not work 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 ## [[1]] ## [,1] [,2] [,3] ## [1,] 1 4 7 ## [2,] 2 5 8 ## [3,] 3 6 9 ## ## [[2]] ## [,1] [,2] [,3] ## [1,] 10 13 16 ## [2,] 11 14 17 ## [3,] 12 15 18 ## ## [[3]] ## [,1] [,2] [,3] ## [1,] 19 22 25 ## [2,] 20 23 26 ## [3,] 21 24 27 1 rapply ( MyList , sqrt , classes = \"ANY\" , how = \"replace\" ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 ## [[1]] ## [,1] [,2] [,3] ## [1,] 1.000000 2.000000 2.645751 ## [2,] 1.414214 2.236068 2.828427 ## [3,] 1.732051 2.449490 3.000000 ## ## [[2]] ## [,1] [,2] [,3] ## [1,] 3.162278 3.605551 4.000000 ## [2,] 3.316625 3.741657 4.123106 ## [3,] 3.464102 3.872983 4.242641 ## ## [[3]] ## [,1] [,2] [,3] ## [1,] 4.358899 4.690416 5.000000 ## [2,] 4.472136 4.795832 5.099020 ## [3,] 4.582576 4.898979 5.196152 1 2 # Change how = rapply ( MyList , sqrt , classes = \"ANY\" , how = \"list\" ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 ## [[1]] ## [,1] [,2] [,3] ## [1,] 1.000000 2.000000 2.645751 ## [2,] 1.414214 2.236068 2.828427 ## [3,] 1.732051 2.449490 3.000000 ## ## [[2]] ## [,1] [,2] [,3] ## [1,] 3.162278 3.605551 4.000000 ## [2,] 3.316625 3.741657 4.123106 ## [3,] 3.464102 3.872983 4.242641 ## ## [[3]] ## [,1] [,2] [,3] ## [1,] 4.358899 4.690416 5.000000 ## [2,] 4.472136 4.795832 5.099020 ## [3,] 4.582576 4.898979 5.196152 1 rapply ( MyList , sqrt , classes = \"ANY\" , how = \"unlist\" ) 1 2 3 4 ## [1] 1.000000 1.414214 1.732051 2.000000 2.236068 2.449490 2.645751 ## [8] 2.828427 3.000000 3.162278 3.316625 3.464102 3.605551 3.741657 ## [15] 3.872983 4.000000 4.123106 4.242641 4.358899 4.472136 4.582576 ## [22] 4.690416 4.795832 4.898979 5.000000 5.099020 5.196152 Built-in or custom function(x) x . classes = \"ANY\" for any object classes or a given object class. Useful when the data is mixed and we want to focus on one class only. how = \"replace\" or \"list\" , \"unlist\" mapply : multivariate-apply \u00b6 1 2 3 # Create a 4x4 matrix Q1 <- matrix ( c ( rep ( 1 , 4 ), rep ( 2 , 4 ), rep ( 3 , 4 ), rep ( 4 , 4 )), 4 , 4 ) Q1 1 2 3 4 5 ## [,1] [,2] [,3] [,4] ## [1,] 1 2 3 4 ## [2,] 1 2 3 4 ## [3,] 1 2 3 4 ## [4,] 1 2 3 4 1 2 3 # Or use `mapply()` Q2 <- mapply ( rep , 1 : 4 , 4 ) Q2 1 2 3 4 5 ## [,1] [,2] [,3] [,4] ## [1,] 1 2 3 4 ## [2,] 1 2 3 4 ## [3,] 1 2 3 4 ## [4,] 1 2 3 4 Vectorize arguments to a function, rep , that is not usually accepting vectors as arguments. Applies a function to multiple lists, 1 : 4 , or multiple vector, c() , arguments. Vectorize \u00b6 Vectorize a scaler function. 1 2 # Scalar function rep ( c ( 1 , 2 ), 2 ) 1 ## [1] 1 2 1 2 1 2 3 # Vector function vrep <- Vectorize ( rep.int ) vrep ( c ( 1 , 2 ), 2 ) 1 2 3 ## [,1] [,2] ## [1,] 1 2 ## [2,] 1 2 1 2 3 # Scalar function f <- function ( x = 1 : 3 , y ) c ( x , y ) f ( 1 : 3 , 1 : 3 ) 1 ## [1] 1 2 3 1 2 3 1 2 3 # Vector function vf <- Vectorize ( f , SIMPLIFY = FALSE ) vf ( 1 : 3 , 1 : 3 ) 1 2 3 4 5 6 7 8 ## [[1]] ## [1] 1 1 ## ## [[2]] ## [1] 2 2 ## ## [[3]] ## [1] 3 3 And more \u00b6 tapply . eapply .","title":"Intermediate R - The apply Family"},{"location":"intermediate_r_the_apply_family/#apply-to-matrices-and-arrays","text":"Flatten a matrix into a vector. 1 2 3 # Dataset X <- matrix ( rnorm ( 30 ), nrow = 4 , ncol = 4 ) X 1 2 3 4 5 ## [,1] [,2] [,3] [,4] ## [1,] -0.1902972 -0.3250492 0.1597852 -0.4878550 ## [2,] -0.4195968 -1.8485172 -0.1185904 0.2307813 ## [3,] -0.6387653 -0.7848382 -0.4198662 -0.6314304 ## [4,] 0.7374748 -2.0634558 0.2374686 -1.6572091 1 2 # Sum the values of each column apply ( X , 2 , sum ) 1 ## [1] -0.5111845 -5.0218604 -0.1412028 -2.5457132 X is the array or matrix (2D, 3D, etc.). MARGIN=1 for row, 2 for column. FUN=sum , mean , median , min , max , var , sd , range , length , skew , kurtosis , abs , round , tolower , toupper , etc. user-defined function such as FUN = function(x) { c(m = mean(x), s = sd(x)) } . 1 2 # More examples apply ( X , 2 , max ) 1 ## [1] 0.7374748 -0.3250492 0.2374686 0.2307813 1 apply ( X , 2 , range ) 1 2 3 ## [,1] [,2] [,3] [,4] ## [1,] -0.6387653 -2.0634558 -0.4198662 -1.6572091 ## [2,] 0.7374748 -0.3250492 0.2374686 0.2307813 1 apply ( X , 2 , abs ) 1 2 3 4 5 ## [,1] [,2] [,3] [,4] ## [1,] 0.1902972 0.3250492 0.1597852 0.4878550 ## [2,] 0.4195968 1.8485172 0.1185904 0.2307813 ## [3,] 0.6387653 0.7848382 0.4198662 0.6314304 ## [4,] 0.7374748 2.0634558 0.2374686 1.6572091 1 apply ( X , 2 , round , 2 ) 1 2 3 4 5 ## [,1] [,2] [,3] [,4] ## [1,] -0.19 -0.33 0.16 -0.49 ## [2,] -0.42 -1.85 -0.12 0.23 ## [3,] -0.64 -0.78 -0.42 -0.63 ## [4,] 0.74 -2.06 0.24 -1.66 Lambda & custom function 1 2 # Dataset X 1 2 3 4 5 ## [,1] [,2] [,3] [,4] ## [1,] -0.1902972 -0.3250492 0.1597852 -0.4878550 ## [2,] -0.4195968 -1.8485172 -0.1185904 0.2307813 ## [3,] -0.6387653 -0.7848382 -0.4198662 -0.6314304 ## [4,] 0.7374748 -2.0634558 0.2374686 -1.6572091 1 2 # Built-in function apply ( X , 2 , max ) 1 ## [1] 0.7374748 -0.3250492 0.2374686 0.2307813 1 2 # Lambda function apply ( X , 2 , function ( x ) x + 10 ) 1 2 3 4 5 ## [,1] [,2] [,3] [,4] ## [1,] 9.809703 9.674951 10.159785 9.512145 ## [2,] 9.580403 8.151483 9.881410 10.230781 ## [3,] 9.361235 9.215162 9.580134 9.368570 ## [4,] 10.737475 7.936544 10.237469 8.342791 1 2 3 4 5 # Custom function select_first <- function ( x ) { return ( x[1] ) } apply ( X , 2 , select_first ) 1 ## [1] -0.1902972 -0.3250492 0.1597852 -0.4878550 1 2 3 4 5 # Custom function with two arguments select_el <- function ( x , i ) { x[i] } apply ( X , 2 , select_el , i = 2 ) 1 ## [1] -0.4195968 -1.8485172 -0.1185904 0.2307813 Strings 1 2 3 # Dataset Y <- matrix ( c ( 'a' , 'b' , 'c' , 'd' ), nrow = 2 , ncol = 2 ) Y 1 2 3 ## [,1] [,2] ## [1,] \"a\" \"c\" ## [2,] \"b\" \"d\" 1 2 # Change the format apply ( Y , 2 , toupper ) 1 2 3 ## [,1] [,2] ## [1,] \"A\" \"C\" ## [2,] \"B\" \"D\"","title":"apply to Matrices and Arrays"},{"location":"intermediate_r_the_apply_family/#sweep","text":"Several steps 1 2 3 # Dataset dataPoints <- matrix ( 4 : 15 , nrow = 4 , ncol = 3 ) dataPoints 1 2 3 4 5 ## [,1] [,2] [,3] ## [1,] 4 8 12 ## [2,] 5 9 13 ## [3,] 6 10 14 ## [4,] 7 11 15 1 2 3 # Find means (center) per column with `apply()` dataPoints_means <- apply ( dataPoints , 2 , mean ) dataPoints_means 1 ## [1] 5.5 9.5 13.5 1 2 3 # Find standard deviation (dispersion) with `apply()` dataPoints_sdev <- apply ( dataPoints , 2 , sd ) dataPoints_sdev 1 ## [1] 1.290994 1.290994 1.290994 1 2 3 # Center the points; shift all the points with respect to their center dataPoints_Trans1 <- sweep ( dataPoints , 2 , dataPoints_means , \"-\" ) dataPoints_Trans1 1 2 3 4 5 ## [,1] [,2] [,3] ## [1,] -1.5 -1.5 -1.5 ## [2,] -0.5 -0.5 -0.5 ## [3,] 0.5 0.5 0.5 ## [4,] 1.5 1.5 1.5 1 2 3 # Normalize dataPoints_Trans2 <- sweep ( dataPoints_Trans1 , 2 , dataPoints_sdev , \"/\" ) dataPoints_Trans2 1 2 3 4 5 ## [,1] [,2] [,3] ## [1,] -1.1618950 -1.1618950 -1.1618950 ## [2,] -0.3872983 -0.3872983 -0.3872983 ## [3,] 0.3872983 0.3872983 0.3872983 ## [4,] 1.1618950 1.1618950 1.1618950 1 sweep ( dataPoints , 2 , dataPoints_means , sum ) 1 ## [1] 228 X . MARGIN=1 for row, 2 for column. STATS for the summary statistics to be swept out: sum , mean , median , min , max , var , sd , range , length , skew , kurtosis , se , etc. FUN=\"-\" , \"+\" , \"/\" , \"*\" , \"**\" , etc. user-defined function such as FUN = function(x) { c(m = mean(x), s = sd(x)) } . One step 1 2 3 4 # Normalize the data with a nested call dataPoints_Trans <- sweep ( sweep ( dataPoints , 2 , dataPoints_means , \"-\" ), 2 , dataPoints_sdev , \"/\" ) dataPoints_Trans 1 2 3 4 5 ## [,1] [,2] [,3] ## [1,] -1.1618950 -1.1618950 -1.1618950 ## [2,] -0.3872983 -0.3872983 -0.3872983 ## [3,] 0.3872983 0.3872983 0.3872983 ## [4,] 1.1618950 1.1618950 1.1618950 Even simpler 1 2 # Automatic data scaling scale ( dataPoints ) 1 2 3 4 5 6 7 8 9 ## [,1] [,2] [,3] ## [1,] -1.1618950 -1.1618950 -1.1618950 ## [2,] -0.3872983 -0.3872983 -0.3872983 ## [3,] 0.3872983 0.3872983 0.3872983 ## [4,] 1.1618950 1.1618950 1.1618950 ## attr(,\"scaled:center\") ## [1] 5.5 9.5 13.5 ## attr(,\"scaled:scale\") ## [1] 1.290994 1.290994 1.290994","title":"sweep"},{"location":"intermediate_r_the_apply_family/#aggregate","text":"1 2 # Dataset head ( Mydf , 15 ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 ## X DepPC DProgr Qty Delivered ## 1 1 90 1 7 FALSE ## 2 2 91 2 8 TRUE ## 3 3 92 3 9 FALSE ## 4 4 93 4 10 TRUE ## 5 5 94 5 11 TRUE ## 6 6 75 6 12 FALSE ## 7 7 90 7 13 TRUE ## 8 8 91 8 14 TRUE ## 9 9 92 9 15 FALSE ## 10 10 93 10 16 FALSE ## 11 11 94 11 17 TRUE ## 12 12 75 12 18 FALSE ## 13 13 90 13 19 FALSE ## 14 14 91 14 20 TRUE ## 15 15 92 15 21 TRUE 1 2 # Show data types for each column sapply ( Mydf , class ) 1 2 ## X DepPC DProgr Qty Delivered ## \"integer\" \"integer\" \"integer\" \"integer\" \"logical\" 1 2 # Return number of rows and columns dim ( Mydf ) 1 ## [1] 120 5 1 nrow ( Mydf ) 1 ## [1] 120 1 ncol ( Mydf ) 1 ## [1] 5 1 2 # How many departments? unique ( Mydf $ DepPC ) 1 ## [1] 90 91 92 93 94 75 1 2 # Dataset head ( Mydf , 5 ) 1 2 3 4 5 6 ## X DepPC DProgr Qty Delivered ## 1 1 90 1 7 FALSE ## 2 2 91 2 8 TRUE ## 3 3 92 3 9 FALSE ## 4 4 93 4 10 TRUE ## 5 5 94 5 11 TRUE 1 2 # Aggregate by a variable (categories) and sum up another variable aggregate ( Mydf $ Qty , by = Mydf[ \"DepPC\" ] , FUN = sum ) 1 2 3 4 5 6 7 ## DepPC x ## 1 75 878 ## 2 90 689 ## 3 91 684 ## 4 92 701 ## 5 93 707 ## 6 94 802 1 2 # Aggregate by a variable (categories) and extract descriptive stats from another variable aggregate ( Mydf $ Qty , by = Mydf[ \"DepPC\" ] , FUN = summary ) 1 2 3 4 5 6 7 ## DepPC x.Min. x.1st Qu. x.Median x.Mean x.3rd Qu. x.Max. ## 1 75 4.00 15.25 23.50 43.90 74.50 124.00 ## 2 90 2.00 11.75 19.50 34.45 27.25 119.00 ## 3 91 3.00 9.00 19.00 34.20 26.25 120.00 ## 4 92 1.00 10.00 20.00 35.05 27.25 121.00 ## 5 93 2.00 11.00 19.00 35.35 27.25 122.00 ## 6 94 3.00 12.00 20.00 40.10 46.50 123.00","title":"aggregate"},{"location":"intermediate_r_the_apply_family/#by","text":"An alternative to aggregate with pros and cons. 1 2 # Dataset head ( Mydf , 5 ) 1 2 3 4 5 6 ## X DepPC DProgr Qty Delivered ## 1 1 90 1 7 FALSE ## 2 2 91 2 8 TRUE ## 3 3 92 3 9 FALSE ## 4 4 93 4 10 TRUE ## 5 5 94 5 11 TRUE 1 by ( Mydf $ Qty , Mydf[ \"DepPC\" ] , FUN = sum ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 ## DepPC: 75 ## [1] 878 ## -------------------------------------------------------- ## DepPC: 90 ## [1] 689 ## -------------------------------------------------------- ## DepPC: 91 ## [1] 684 ## -------------------------------------------------------- ## DepPC: 92 ## [1] 701 ## -------------------------------------------------------- ## DepPC: 93 ## [1] 707 ## -------------------------------------------------------- ## DepPC: 94 ## [1] 802 1 by ( Mydf $ Qty , Mydf[ \"DepPC\" ] , FUN = summary ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 ## DepPC: 75 ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 4.00 15.25 23.50 43.90 74.50 124.00 ## -------------------------------------------------------- ## DepPC: 90 ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 2.00 11.75 19.50 34.45 27.25 119.00 ## -------------------------------------------------------- ## DepPC: 91 ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 3.00 9.00 19.00 34.20 26.25 120.00 ## -------------------------------------------------------- ## DepPC: 92 ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 1.00 10.00 20.00 35.05 27.25 121.00 ## -------------------------------------------------------- ## DepPC: 93 ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 2.00 11.00 19.00 35.35 27.25 122.00 ## -------------------------------------------------------- ## DepPC: 94 ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 3.0 12.0 20.0 40.1 46.5 123.0","title":"by"},{"location":"intermediate_r_the_apply_family/#split","text":"1 2 # Dataset head ( Mydf , 5 ) 1 2 3 4 5 6 ## X DepPC DProgr Qty Delivered ## 1 1 90 1 7 FALSE ## 2 2 91 2 8 TRUE ## 3 3 92 3 9 FALSE ## 4 4 93 4 10 TRUE ## 5 5 94 5 11 TRUE 1 nrow ( Mydf ) 1 ## [1] 120 1 2 # Split with a variable (categories) split ( Mydf $ Qty , Mydf[ \"DepPC\" ] ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 ## $`75` ## [1] 12 18 24 30 13 19 100 106 112 118 124 7 13 19 25 16 23 ## [18] 29 4 66 ## ## $`90` ## [1] 7 13 19 25 31 14 20 101 107 113 119 2 8 14 20 26 17 ## [18] 24 4 5 ## ## $`91` ## [1] 8 14 20 26 9 15 21 102 108 114 120 3 9 15 21 27 18 ## [18] 25 3 6 ## ## $`92` ## [1] 9 15 21 27 10 16 22 103 109 115 121 4 10 16 22 28 19 ## [18] 26 1 7 ## ## $`93` ## [1] 10 16 22 28 11 17 23 104 110 116 122 5 11 17 23 14 21 ## [18] 27 2 8 ## ## $`94` ## [1] 11 17 23 29 12 18 99 105 111 117 123 6 12 18 24 15 22 ## [18] 28 3 9 1 2 # Split by row unlist ( Mydf $ Qty ) 1 2 3 4 5 6 7 8 ## [1] 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 ## [18] 24 25 26 27 28 29 30 31 9 10 11 12 13 14 15 16 17 ## [35] 18 19 20 21 22 23 99 100 101 102 103 104 105 106 107 108 109 ## [52] 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 2 3 ## [69] 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 ## [86] 21 22 23 24 25 26 27 28 14 15 16 17 18 19 21 22 23 ## [103] 24 25 26 27 28 29 4 3 1 2 3 4 5 6 7 8 9 ## [120] 66 1 split ( Mydf $ Qty , c ( 60 , 120 )) 1 2 3 4 5 6 7 8 9 10 11 ## $`60` ## [1] 7 9 11 13 15 17 19 21 23 25 27 29 31 10 12 14 16 ## [18] 18 20 22 99 101 103 105 107 109 111 113 115 117 119 121 123 2 ## [35] 4 6 8 10 12 14 16 18 20 22 24 26 28 15 17 19 22 ## [52] 24 26 28 4 1 3 5 7 9 ## ## $`120` ## [1] 8 10 12 14 16 18 20 22 24 26 28 30 9 11 13 15 17 ## [18] 19 21 23 100 102 104 106 108 110 112 114 116 118 120 122 124 3 ## [35] 5 7 9 11 13 15 17 19 21 23 25 27 14 16 18 21 23 ## [52] 25 27 29 3 2 4 6 8 66","title":"split"},{"location":"intermediate_r_the_apply_family/#strsplit","text":"1 2 3 4 5 # The vector pioneers pioneers <- c ( 'GAUSS:1777' , 'BAYES:1702' , 'PASCAL:1623' , 'PEARSON:1857' ) # Split names from birth year: split_math split_math <- strsplit ( pioneers , ':' )","title":"strsplit"},{"location":"intermediate_r_the_apply_family/#vectorize","text":"Vectorize a scalar function. 1 2 # Scalar function rep ( c ( 1 , 2 ), 2 ) 1 ## [1] 1 2 1 2 1 2 3 # Vector function vrep <- Vectorize ( rep.int ) vrep ( c ( 1 , 2 ), 2 ) 1 2 3 ## [,1] [,2] ## [1,] 1 2 ## [2,] 1 2 1 2 3 # Scalar function f <- function ( x = 1 : 3 , y ) c ( x , y ) f ( 1 : 3 , 1 : 3 ) 1 ## [1] 1 2 3 1 2 3 1 2 3 # Vector function vf <- Vectorize ( f , SIMPLIFY = FALSE ) vf ( 1 : 3 , 1 : 3 ) 1 2 3 4 5 6 7 8 ## [[1]] ## [1] 1 1 ## ## [[2]] ## [1] 2 2 ## ## [[3]] ## [1] 3 3","title":"Vectorize"},{"location":"intermediate_r_the_apply_family/#lapply-list-apply","text":"For lists, vectors, and data frames. 1 2 3 4 5 6 7 8 # Dataset A <- matrix ( 1 : 9 , nrow = 3 , ncol = 3 ) B <- matrix ( 10 : 18 , nrow = 3 , ncol = 3 ) C <- matrix ( 19 : 28 , nrow = 3 , ncol = 3 ) # Create a list of matrices; an array (a 3D matrix) MyList <- list ( A , B , C ) MyList 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 ## [[1]] ## [,1] [,2] [,3] ## [1,] 1 4 7 ## [2,] 2 5 8 ## [3,] 3 6 9 ## ## [[2]] ## [,1] [,2] [,3] ## [1,] 10 13 16 ## [2,] 11 14 17 ## [3,] 12 15 18 ## ## [[3]] ## [,1] [,2] [,3] ## [1,] 19 22 25 ## [2,] 20 23 26 ## [3,] 21 24 27 1 2 # Extract values MyList[[1]] 1 2 3 4 ## [,1] [,2] [,3] ## [1,] 1 4 7 ## [2,] 2 5 8 ## [3,] 3 6 9 1 MyList[[1]][1 , ] 1 ## [1] 1 4 7 1 MyList[[1]][ , 1 ] 1 ## [1] 1 2 3 1 MyList[[1]][1 , 1 ] 1 ## [1] 1 1 2 # Extract the 2nd column from `MyList` with the selection operator `[` with `lapply()` lapply ( MyList , \"[\" , , 2 ) 1 2 3 4 5 6 7 8 ## [[1]] ## [1] 4 5 6 ## ## [[2]] ## [1] 13 14 15 ## ## [[3]] ## [1] 22 23 24 1 2 # Extract the 1st row from `MyList` lapply ( MyList , \"[\" , 1 , ) 1 2 3 4 5 6 7 8 ## [[1]] ## [1] 1 4 7 ## ## [[2]] ## [1] 10 13 16 ## ## [[3]] ## [1] 19 22 25 Lambda & custom function 1 2 # Dataset MyList 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 ## [[1]] ## [,1] [,2] [,3] ## [1,] 1 4 7 ## [2,] 2 5 8 ## [3,] 3 6 9 ## ## [[2]] ## [,1] [,2] [,3] ## [1,] 10 13 16 ## [2,] 11 14 17 ## [3,] 12 15 18 ## ## [[3]] ## [,1] [,2] [,3] ## [1,] 19 22 25 ## [2,] 20 23 26 ## [3,] 21 24 27 1 2 # Built-in function lapply ( MyList , max ) 1 2 3 4 5 6 7 8 ## [[1]] ## [1] 9 ## ## [[2]] ## [1] 18 ## ## [[3]] ## [1] 27 1 2 # Lambda function lapply ( MyList , function ( x ) max ( x ) + 10 ) 1 2 3 4 5 6 7 8 ## [[1]] ## [1] 19 ## ## [[2]] ## [1] 28 ## ## [[3]] ## [1] 37 1 2 3 4 5 # Custom function select_first <- function ( x ) { return ( x[1] ) } lapply ( MyList , select_first ) 1 2 3 4 5 6 7 8 ## [[1]] ## [1] 1 ## ## [[2]] ## [1] 10 ## ## [[3]] ## [1] 19 1 2 3 4 5 # Custom function with two arguments select_el <- function ( x , i ) { x[i] } lapply ( MyList , select_el , i = 2 ) 1 2 3 4 5 6 7 8 ## [[1]] ## [1] 2 ## ## [[2]] ## [1] 11 ## ## [[3]] ## [1] 20 Strings 1 2 # Dataset Y 1 2 3 ## [,1] [,2] ## [1,] \"a\" \"c\" ## [2,] \"b\" \"d\" 1 2 # Change the format lapply ( Y , toupper ) 1 2 3 4 5 6 7 8 9 10 11 ## [[1]] ## [1] \"A\" ## ## [[2]] ## [1] \"B\" ## ## [[3]] ## [1] \"C\" ## ## [[4]] ## [1] \"D\"","title":"lapply: list-apply"},{"location":"intermediate_r_the_apply_family/#sapply-simplify-list-apply","text":"A wrapper that simplifies lapply . 1 2 # Dataset MyList 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 ## [[1]] ## [,1] [,2] [,3] ## [1,] 1 4 7 ## [2,] 2 5 8 ## [3,] 3 6 9 ## ## [[2]] ## [,1] [,2] [,3] ## [1,] 10 13 16 ## [2,] 11 14 17 ## [3,] 12 15 18 ## ## [[3]] ## [,1] [,2] [,3] ## [1,] 19 22 25 ## [2,] 20 23 26 ## [3,] 21 24 27 1 2 # Return a list with `lapply()` lapply ( MyList , \"[\" , 2 , 1 ) 1 2 3 4 5 6 7 8 ## [[1]] ## [1] 2 ## ## [[2]] ## [1] 11 ## ## [[3]] ## [1] 20 1 2 # Return a vector with `sapply()` sapply ( MyList , \"[\" , 2 , 1 ) # same result, but simpler 1 ## [1] 2 11 20 1 2 # Return a list with `sapply()` sapply ( MyList , \"[\" , 2 , 1 , simplify = T ) # by default 1 ## [1] 2 11 20 1 sapply ( MyList , \"[\" , 2 , 1 , simplify = F ) 1 2 3 4 5 6 7 8 ## [[1]] ## [1] 2 ## ## [[2]] ## [1] 11 ## ## [[3]] ## [1] 20 1 2 # Return a vector with `unlist()` unlist ( lapply ( MyList , \"[\" , 2 , 1 )) # similar 1 ## [1] 2 11 20 Lambda & custom function 1 2 # Dataset MyList 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 ## [[1]] ## [,1] [,2] [,3] ## [1,] 1 4 7 ## [2,] 2 5 8 ## [3,] 3 6 9 ## ## [[2]] ## [,1] [,2] [,3] ## [1,] 10 13 16 ## [2,] 11 14 17 ## [3,] 12 15 18 ## ## [[3]] ## [,1] [,2] [,3] ## [1,] 19 22 25 ## [2,] 20 23 26 ## [3,] 21 24 27 1 2 # Built-in function sapply ( MyList , max ) 1 ## [1] 9 18 27 1 2 # Lambda function lapply ( MyList , function ( x ) max ( x ) + 10 ) 1 2 3 4 5 6 7 8 ## [[1]] ## [1] 19 ## ## [[2]] ## [1] 28 ## ## [[3]] ## [1] 37 1 2 3 4 5 # Custom function select_first <- function ( x ) { return ( x[1] ) } sapply ( MyList , select_first ) 1 ## [1] 1 10 19 1 2 3 4 5 # Custom function with two arguments select_el <- function ( x , i ) { x[i] } sapply ( MyList , select_el , i = 2 ) 1 ## [1] 2 11 20 Strings 1 2 # Dataset Y 1 2 3 ## [,1] [,2] ## [1,] \"a\" \"c\" ## [2,] \"b\" \"d\" 1 2 # Change the format sapply ( Y , toupper ) 1 2 ## a b c d ## \"A\" \"B\" \"C\" \"D\"","title":"sapply: simplify-list-apply"},{"location":"intermediate_r_the_apply_family/#vapply","text":"A variant.","title":"vapply"},{"location":"intermediate_r_the_apply_family/#rapply-recursive-list-apply","text":"1 2 # Dataset MyList 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 ## [[1]] ## [,1] [,2] [,3] ## [1,] 1 4 7 ## [2,] 2 5 8 ## [3,] 3 6 9 ## ## [[2]] ## [,1] [,2] [,3] ## [1,] 10 13 16 ## [2,] 11 14 17 ## [3,] 12 15 18 ## ## [[3]] ## [,1] [,2] [,3] ## [1,] 19 22 25 ## [2,] 20 23 26 ## [3,] 21 24 27 1 2 # Build-in function rapply ( MyList , sqrt , classes = \"ANY\" , how = \"replace\" ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 ## [[1]] ## [,1] [,2] [,3] ## [1,] 1.000000 2.000000 2.645751 ## [2,] 1.414214 2.236068 2.828427 ## [3,] 1.732051 2.449490 3.000000 ## ## [[2]] ## [,1] [,2] [,3] ## [1,] 3.162278 3.605551 4.000000 ## [2,] 3.316625 3.741657 4.123106 ## [3,] 3.464102 3.872983 4.242641 ## ## [[3]] ## [,1] [,2] [,3] ## [1,] 4.358899 4.690416 5.000000 ## [2,] 4.472136 4.795832 5.099020 ## [3,] 4.582576 4.898979 5.196152 1 2 # Lambda or custom the function rapply ( MyList , function ( x ) x^2 , classes = \"ANY\" , how = \"replace\" ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 ## [[1]] ## [,1] [,2] [,3] ## [1,] 1 16 49 ## [2,] 4 25 64 ## [3,] 9 36 81 ## ## [[2]] ## [,1] [,2] [,3] ## [1,] 100 169 256 ## [2,] 121 196 289 ## [3,] 144 225 324 ## ## [[3]] ## [,1] [,2] [,3] ## [1,] 361 484 625 ## [2,] 400 529 676 ## [3,] 441 576 729 1 2 3 4 select_first <- function ( x ) { return ( x[1] ) } rapply ( MyList , select_first , classes = \"ANY\" , how = \"replace\" ) 1 2 3 4 5 6 7 8 ## [[1]] ## [1] 1 ## ## [[2]] ## [1] 10 ## ## [[3]] ## [1] 19 1 2 # Change classes = class ( MyList ) # the object 1 ## [1] \"list\" 1 class ( MyList[[1]][1] ) # each entry 1 ## [1] \"integer\" 1 rapply ( MyList , sqrt , classes = \"list\" , how = \"replace\" ) # not work 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 ## [[1]] ## [,1] [,2] [,3] ## [1,] 1 4 7 ## [2,] 2 5 8 ## [3,] 3 6 9 ## ## [[2]] ## [,1] [,2] [,3] ## [1,] 10 13 16 ## [2,] 11 14 17 ## [3,] 12 15 18 ## ## [[3]] ## [,1] [,2] [,3] ## [1,] 19 22 25 ## [2,] 20 23 26 ## [3,] 21 24 27 1 rapply ( MyList , sqrt , classes = \"ANY\" , how = \"replace\" ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 ## [[1]] ## [,1] [,2] [,3] ## [1,] 1.000000 2.000000 2.645751 ## [2,] 1.414214 2.236068 2.828427 ## [3,] 1.732051 2.449490 3.000000 ## ## [[2]] ## [,1] [,2] [,3] ## [1,] 3.162278 3.605551 4.000000 ## [2,] 3.316625 3.741657 4.123106 ## [3,] 3.464102 3.872983 4.242641 ## ## [[3]] ## [,1] [,2] [,3] ## [1,] 4.358899 4.690416 5.000000 ## [2,] 4.472136 4.795832 5.099020 ## [3,] 4.582576 4.898979 5.196152 1 2 # Change how = rapply ( MyList , sqrt , classes = \"ANY\" , how = \"list\" ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 ## [[1]] ## [,1] [,2] [,3] ## [1,] 1.000000 2.000000 2.645751 ## [2,] 1.414214 2.236068 2.828427 ## [3,] 1.732051 2.449490 3.000000 ## ## [[2]] ## [,1] [,2] [,3] ## [1,] 3.162278 3.605551 4.000000 ## [2,] 3.316625 3.741657 4.123106 ## [3,] 3.464102 3.872983 4.242641 ## ## [[3]] ## [,1] [,2] [,3] ## [1,] 4.358899 4.690416 5.000000 ## [2,] 4.472136 4.795832 5.099020 ## [3,] 4.582576 4.898979 5.196152 1 rapply ( MyList , sqrt , classes = \"ANY\" , how = \"unlist\" ) 1 2 3 4 ## [1] 1.000000 1.414214 1.732051 2.000000 2.236068 2.449490 2.645751 ## [8] 2.828427 3.000000 3.162278 3.316625 3.464102 3.605551 3.741657 ## [15] 3.872983 4.000000 4.123106 4.242641 4.358899 4.472136 4.582576 ## [22] 4.690416 4.795832 4.898979 5.000000 5.099020 5.196152 Built-in or custom function(x) x . classes = \"ANY\" for any object classes or a given object class. Useful when the data is mixed and we want to focus on one class only. how = \"replace\" or \"list\" , \"unlist\"","title":"rapply: recursive-list-apply"},{"location":"intermediate_r_the_apply_family/#mapply-multivariate-apply","text":"1 2 3 # Create a 4x4 matrix Q1 <- matrix ( c ( rep ( 1 , 4 ), rep ( 2 , 4 ), rep ( 3 , 4 ), rep ( 4 , 4 )), 4 , 4 ) Q1 1 2 3 4 5 ## [,1] [,2] [,3] [,4] ## [1,] 1 2 3 4 ## [2,] 1 2 3 4 ## [3,] 1 2 3 4 ## [4,] 1 2 3 4 1 2 3 # Or use `mapply()` Q2 <- mapply ( rep , 1 : 4 , 4 ) Q2 1 2 3 4 5 ## [,1] [,2] [,3] [,4] ## [1,] 1 2 3 4 ## [2,] 1 2 3 4 ## [3,] 1 2 3 4 ## [4,] 1 2 3 4 Vectorize arguments to a function, rep , that is not usually accepting vectors as arguments. Applies a function to multiple lists, 1 : 4 , or multiple vector, c() , arguments.","title":"mapply: multivariate-apply"},{"location":"intermediate_r_the_apply_family/#vectorize_1","text":"Vectorize a scaler function. 1 2 # Scalar function rep ( c ( 1 , 2 ), 2 ) 1 ## [1] 1 2 1 2 1 2 3 # Vector function vrep <- Vectorize ( rep.int ) vrep ( c ( 1 , 2 ), 2 ) 1 2 3 ## [,1] [,2] ## [1,] 1 2 ## [2,] 1 2 1 2 3 # Scalar function f <- function ( x = 1 : 3 , y ) c ( x , y ) f ( 1 : 3 , 1 : 3 ) 1 ## [1] 1 2 3 1 2 3 1 2 3 # Vector function vf <- Vectorize ( f , SIMPLIFY = FALSE ) vf ( 1 : 3 , 1 : 3 ) 1 2 3 4 5 6 7 8 ## [[1]] ## [1] 1 1 ## ## [[2]] ## [1] 2 2 ## ## [[3]] ## [1] 3 3","title":"Vectorize"},{"location":"intermediate_r_the_apply_family/#and-more","text":"tapply . eapply .","title":"And more"},{"location":"io_snippets_cleaning/","text":"Foreword Output options: the \u2018tango\u2019 syntax and the \u2018readable\u2019 theme. Snippets and results. Datasets \u00b6 R Dataset Packages ; by default in R. Other dataset can be imported with data(Cars93, package = 'MASS') for example. csv/doc Datasets . Free Datasets from the World Bank, Gapminder, Kaggle, Quandl, Reddit, and many more websites. Datasets to Practice Your Data Mining. Houghton Mifflin Data for linear regressions. Regression Datasets for Generalized Linear Models (linear, logistic, poisson, multinomial, survival). Public Datasets on GitHub . Awesome Public Datasets . Importing Data Into R \u00b6 The packages: utils . readr . data.table . readxl . gdata . XLConnect . haven . foreign . DBI . httr . jsonlite . Importing Data from Flat Files \u00b6 R functions, by default. \u00b6 read.csv ; sep = ',' , dec = '.' . read.delim ; .txt, dec = '.' . read.csv2 ; sep = ';' , dec = ',' . read.delim2 ; .txt, dec = ',' . Needs arguments. read.csv for .csv files 1 2 3 4 5 6 # List the files in your working directory dir () # Import swimming_pools.csv: pools # stringAsFactors = FALSE does not import strings as categorical variables pools <- read.csv ( 'swimming_pools.csv' , stringsAsFactors = FALSE ) stringsAsFactors 1 2 3 4 5 # Import swimming_pools.csv correctly: pools pools <- read.csv ( 'swimming_pools.csv' , stringsAsFactor = FALSE , header = TRUE , sep = ',' ) # Import swimming_pools.csv with factors: pools_factor pools_factor <- read.csv ( 'swimming_pools.csv' , header = TRUE , sep = ',' ) read.delim for .txt files 1 2 3 4 5 # Import hotdogs.txt: hotdogs hotdogs <- read.delim ( 'hotdogs.txt' , header = FALSE ) # Name the columns of hotdogs appropriately names ( hotdogs ) <- c ( 'type' , 'calories' , 'sodium' ) Arguments. 1 2 3 4 5 6 7 8 # Load in the hotdogs data set: hotdogs hotdogs <- read.delim ( 'hotdogs.txt' , header = FALSE , sep = '\\t' , col.names = c ( 'type' , 'calories' , 'sodium' )) # Select the hot dog with the least calories: lily lily <- hotdogs [which.min ( hotdogs $ calories ), ] # Select the observation with the most sodium: tom tom <- hotdogs [which.max ( hotdogs $ sodium ), ] 1 2 3 4 5 6 7 8 # Previous call to import hotdogs.txt hotdogs <- read.delim ( 'hotdogs.txt' , header = FALSE , col.names = c ( 'type' , 'calories' , 'sodium' )) # Print a vector representing the classes of the columns sapply ( hotdogs , class ) # Edit the colClasses argument to import the data correctly: hotdogs2 hotdogs2 <- read.delim ( 'hotdogs.txt' , header = FALSE , col.names = c ( 'type' , 'calories' , 'sodium' ), colClasses = c ( 'factor' , 'NULL' , 'numeric' )) The utils package \u00b6 read.table ; sep = '\\t' , = ',' , = ';' . Read any tabular as a d.f. Needs arguments; lots of argument for precision. Slow. 1 library ( utils ) read.table .txt files 1 2 3 4 5 # Create a path to the hotdogs.txt file path <- file.path ( 'hotdogs' , 'hotdogs.txt' ) # Import the hotdogs.txt file: hotdogs hotdogs <- read.table ( path , header = FALSE , sep = '\\t' , col.names = c ( 'type' , 'calories' , 'sodium' )) (from Importing Data from the Web) \u00b6 1 2 3 4 5 # https URL to the swimming_pools csv file. url_csv <- 'https://s3.amazonaws.com/assets.datacamp.com/course/importing_data_into_r/swimming_pools.csv' # Import the file using read.csv(): pools1 pools1 <- read.csv ( url_csv ) The readr package \u00b6 read_delim ; delim = '\\t' , = ',' . read_csv ; read 100.000, 200.000 read_tsv ; idem. read_csv2 ; read 100,000; 200,000 or European files.. read_tsv2 ; idem. read_lines . read_file . write_csv . write_rds . type_convert . parse_factor . parse_date . parse_number . spec_csv . spec_delim . Fast, few arguments. Detect data type. 1 library ( readr ) read_delim .txt files 1 2 # Import potatoes.txt using read_delim(): potatoes potatoes <- read_delim ( 'potatoes.txt' , delim = '\\t' ) read_csv .csv files 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 # Column names properties <- c ( 'area' , 'temp' , 'size' , 'storage' , 'method' , 'texture' , 'flavor' , 'moistness' ) # Import potatoes.csv with read_csv(): potatoes potatoes <- read_csv ( 'potatoes.csv' , col_names = properties ) # Create a copy of potatoes: potatoes2 potatoes2 <- potatoes # Convert the method column of potatoes2 to a factor potatoes2 $ method = factor ( potatoes2 $ method ) # or potatoes2 $ method = as.factor ( potatoes2 $ method ) col_types , skip and n_max in .tsv files 1 2 3 4 5 6 7 8 9 # Column names properties <- c ( 'area' , 'temp' , 'size' , 'storage' , 'method' , 'texture' , 'flavor' , 'moistness' ) # Import 5 observations from potatoes.txt: potatoes_fragment # read_tsv or tab-separated values potatoes_fragment <- read_tsv ( 'potatoes.txt' , col_names = properties , skip = 7 , n_max = 5 ) # Import all data, but force all columns to be character: potatoes_char potatoes_char <- read_tsv ( 'potatoes.txt' , col_types = 'cccccccc' ) Setting column types 1 2 3 4 cols ( weight = col_integer (), feed = col_character () ) Removing NA 1 na = c ( 'NA' , 'null' ) col_types with collectors .tsv files 1 2 3 4 5 6 7 8 9 10 # Import without col_types hotdogs <- read_tsv ( 'hotdogs.txt' , col_names = c ( 'type' , 'calories' , 'sodium' )) # The collectors you will need to import the data fac <- col_factor ( levels = c ( 'Beef' , 'Meat' , 'Poultry' )) int <- col_integer () # Edit the col_types argument to import the data correctly: hotdogs_factor # Change col_types to the correct vector of collectors; coerce the vector into a list hotdogs_factor <- read_tsv ( 'hotdogs.txt' , col_names = c ( 'type' , 'calories' , 'sodium' ), col_types = list ( fac , int , int )) Skiping columns 1 2 3 4 5 salaries <- read_tsv ( 'Salaries.txt' , col_names = FALSE , col_types = cols ( X2 = col_skip (), X3 = col_skip (), X4 = col_skip () )) Reading an ordinary text file 1 2 3 4 5 6 7 8 9 # vector of character strings. # Import as a character vector, one item per line: tweets tweets <- read_lines ( 'tweets.txt' ) tweets # returns a length 1 vector of the entire file, with line breaks represented as \\n # Import as a length 1 vector: tweets_all tweets_all <- read_file ( 'tweets.txt' ) tweets_all Writing .csv and .tsv files 1 2 3 4 5 # Save cwts as chickwts.csv write_csv ( cwts , \"chickwts.csv\" ) # Append cwts2 to chickwts.csv write_csv ( cwts2 , \"chickwts.csv\" , append = TRUE ) Writing .rds files 1 2 3 4 5 6 7 8 # Save trees as trees.rds write_rds ( trees , 'trees.rds' ) # Import trees.rds: trees2 trees2 <- read_rds ( 'trees.rds' ) # Check whether trees and trees2 are the same identical ( trees , trees2 ) Coercing columns to different data types 1 2 # Convert all columns to double trees2 <- type_convert ( trees , col_types = cols ( Girth = 'd' , Height = 'd' , Volume = 'd' )) Coercing character columns into factors 1 2 3 4 5 # Parse the title column salaries $ title <- parse_factor ( salaries $ title , levels = c ( 'Prof' , 'AsstProf' , 'AssocProf' )) # Parse the gender column salaries $ gender <- parse_factor ( salaries $ gender , levels = c ( 'Male' , 'Female' )) Creating Date objects 1 2 # Change type of date column weather $ date <- parse_date ( weather $ date , format = '%m/%d/%Y' ) Parsing number formats 1 2 # Parse amount column as a number debt $ amount <- parse_number ( debt $ amount ) Viewing metadata before importing spec_csv for .csv and .tsv files. spec_delim for .txt files (among others). 1 2 # Specifications of chickwts spec_csv ( 'chickwts.csv' ) (from Importing Data from the Web) \u00b6 Import Flat files from the web 1 2 3 4 5 6 7 8 9 10 11 # Import the csv file: pools url_csv <- 'http://s3.amazonaws.com/assets.datacamp.com/course/importing_data_into_r/swimming_pools.csv' pools <- read_csv ( url_csv ) pools # Import the txt file: potatoes url_delim <- 'http://s3.amazonaws.com/assets.datacamp.com/course/importing_data_into_r/potatoes.txt' potatoes <- read_tsv ( url_delim ) potatoes Secure importing 1 2 # Import the file using read_csv(): pools2 pools2 <- read_csv ( url_csv ) The data.table package \u00b6 fread == read.table . .txt files only. Fast. 1 library ( data.table ) fread for .txt files 1 2 3 4 5 6 7 8 # Import potatoes.txt with fread(): potatoes potatoes <- fread ( 'potatoes.txt' ) # Print out arranged version of potatoes potatoes [order ( moistness ), ] # Import 20 rows of potatoes.txt with fread(): potatoes_part potatoes_part <- fread ( 'potatoes.txt' , nrows = 20 ) fread : more advanced use 1 2 3 4 5 # Import columns 6, 7 and 8 of potatoes.txt: potatoes potatoes <- fread ( 'potatoes.txt' , select = c ( 6 : 8 )) # Keep only tasty potatoes (flavor > 3): tasty_potatoes tasty_potatoes <- subset ( potatoes , potatoes $ flavor > 3 ) Importing Data from Excel \u00b6 The readxl package \u00b6 excel_sheets ; list. read_excel ; import. .xlsx files only. 1 library ( readxl ) List the sheets of an Excel file 1 2 3 4 5 # Find the names of both spreadsheets: sheets # Before, find out what is in the directory with 'dir()' sheets <- excel_sheets ( 'latitude.xlsx' ) sheets Importing an Excel sheet 1 2 3 4 5 6 7 8 # Read the first sheet of latitude.xlsx: latitude_1 latitude_1 <- read_excel ( 'latitude.xlsx' , sheet = 1 ) # Read the second sheet of latitude.xlsx: latitude_2 latitude_2 <- read_excel ( 'latitude.xlsx' , sheet = 2 ) # Put latitude_1 and latitude_2 in a list: lat_list lat_list <- list ( latitude_1 , latitude_2 ) Reading a workbook 1 2 # Read all Excel sheets with lapply(): lat_list lat_list <- lapply ( excel_sheets ( 'latitude.xlsx' ), read_excel , path = 'latitude.xlsx' ) The col_names argument 1 2 3 4 5 # Import the the first Excel sheet of latitude_nonames.xlsx (R gives names): latitude_3 latitude_3 <- read_excel ( 'latitude_nonames.xlsx' , sheet = 1 , col_names = FALSE ) # Import the the second Excel sheet of latitude_nonames.xlsx (specify col_names): latitude_4 latitude_4 <- read_excel ( 'latitude_nonames.xlsx' , sheet = 1 , col_names = c ( 'country' , 'latitude' )) The skip argument 1 2 # Import the second sheet of latitude.xlsx, skipping the first 21 rows: latitude_sel latitude_sel <- read_excel ( 'latitude.xlsx' , skip = 21 , col_names = FALSE ) (from Importing Data from the Web) \u00b6 Import Excel files from the web 1 2 3 4 5 # Download file behind URL, name it local_latitude.xls download.file ( url_xls , 'local_latitude.xls' ) # Import the local .xls file with readxl: excel_readxl excel_readxl <- read_excel ( 'local_latitude.xls' ) Downloading any file, secure or not 1 2 3 4 5 # https URL to the wine RData file. url_rdata <- 'https://s3.amazonaws.com/assets.datacamp.com/course/importing_data_into_r/wine.RData' # Download the wine file to your working directory download.file ( url_rdata , 'wine_local.RData' ) The XLConnect package \u00b6 loadWorkbook . getSheets . readWorksheet . readWorksheetFromFile readNamedRegion readNamedRegionFromFile .xls & .xlsx files. Like reading a database. 1 2 library ( XLConnectJars ) library ( XLConnect ) Import a workbook 1 2 # Build connection to latitude.xlsx: my_book my_book <- loadWorkbook ( 'latitude.xlsx' ) List and read Excel sheets 1 2 3 4 5 6 7 8 9 10 11 # Build connection to latitude.xlsx my_book <- loadWorkbook ( 'latitude.xlsx' ) # List the sheets in latitude.xlsx getSheets ( my_book ) # Import the second sheet in latitude.xlsx readWorksheet ( my_book , sheet = 2 ) # Import the second column of the first sheet in latitude.xlsx readWorksheet ( my_book , sheet = 2 , startCol = 2 ) Add and populate worksheets 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 # Build connection to latitude.xlsx my_book <- loadWorkbook ( 'latitude.xlsx' ) # Create data frame: summ dims1 <- dim ( readWorksheet ( my_book , 1 )) dims2 <- dim ( readWorksheet ( my_book , 2 )) summ <- data.frame ( sheets = getSheets ( my_book ), nrows = c ( dims1[1] , dims2[1] ), ncols = c ( dims1[2] , dims2[2] )) # Add a worksheet to my_book, named 'data_summary' createSheet ( my_book , name = 'data_summary' ) # Populate 'data_summary' with summ data frame writeWorksheet ( my_book , summ , sheet = 'data_summary' ) # Save workbook as latitude_with_summ.xlsx saveWorkbook ( my_book , 'latitude_with_summ.xlsx' ) One unique function 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 # Read in the data set and assign to the object impact <- readWorksheetFromFile ( 'A Hands-on Introduction to Statistics with R.xls' , sheet = 'impact' , header = TRUE , startCol = 1 , startRow = 1 ) # more arguments # endCol = 1 # endRow = 1 # autofitRow = # autofitCol = # region = # rownames = # colTypes = # forceConversion = # dateTimeFormat = # check.names = # useCachedValues = # keep = # drop = # simplify = # readStrategy = Importing Data from Other Statistical Software \u00b6 The haven package \u00b6 read_sas ; sas7bdat & sas7bcat files. read_stata ; version; dta files. read_dta ; idem. read_spss ; sav & por files (and see below). read_por . read_sav . Simple, few arguments. Create a d.f. 1 library ( haven ) Import SAS data with haven 1 2 # Import sales.sas7bdat: sales sales <- read_sas ( 'sales.sas7bdat' ) Import STATA data with haven 1 2 # Import the data from the URL: sugar sugar <- read_dta ( 'http://assets.datacamp.com/course/importing_data_into_r/trade.dta' ) Import SPSS data with haven 1 2 3 4 5 # Specify the file path using file.path(): path path <- file.path ( 'datasets' , 'person.sav' ) # Import person.sav, which is in the datasets folder: traits traits <- read_sav ( path ) Factorize, round two 1 2 # Import SPSS data from the URL: work work <- read_sav ( 'http://assets.datacamp.com/course/importing_data_into_r/employee.sav' ) The foreign package \u00b6 Cannot import SAS, see the sas7bdat package. read.dta ; dta files. read.spss ; sav & por files. Comprehensive. 1 library ( foreign ) Import STATA data with foreign (1) 1 2 # Import florida.dta and name the resulting data frame florida florida <- read.dta ( 'florida.dta' ) Import STATA data with foreign (2) 1 2 3 4 5 6 7 8 9 10 11 # Specify the file path using file.path(): path path <- file.path ( 'worldbank' , 'edequality.dta' ) # Create and print structure of edu_equal_1 edu_equal_1 <- read.dta ( path ) # Create and print structure of edu_equal_2 edu_equal_2 <- read.dta ( path , convert.factors = FALSE ) # Create and print structure of edu_equal_3 edu_equal_3 <- read.dta ( path , convert.underscore = TRUE ) Import SPSS data with foreign (1) 1 2 # Import international.sav as a data frame: demo demo <- read.spss ( 'international.sav' , to.data.frame = TRUE ) Import SPSS data with foreign (2) 1 2 3 4 5 # Import international.sav as demo_1 demo_1 <- read.spss ( 'international.sav' , to.data.frame = TRUE ) # Import international.sav as demo_2 demo_2 <- read.spss ( 'international.sav' , to.data.frame = TRUE , use.value.labels = FALSE ) Importing Data from Relational Data \u00b6 The DBI package \u00b6 dbConnect . dbReadTable . dbGetQuery . dbFetch . dbDisconnect . 1 library ( DBI ) Step 1: Establish a connection 1 2 3 4 # Connect to the MySQL database: con con <- dbConnect ( RMySQL :: MySQL (), dbname = 'tweater' , host = 'courses.csrrinzqubik.us-east-1.rds.amazonaws.com' , port = 3306 , user = 'student' , password = 'datacamp' ) con Step 2: List the database tables 1 2 3 4 5 6 7 8 9 10 11 12 13 # Connect to the MySQL database: con con <- dbConnect ( RMySQL :: MySQL (), dbname = 'tweater' , host = 'courses.csrrinzqubik.us-east-1.rds.amazonaws.com' , port = 3306 , user = 'student' , password = 'datacamp' ) # Build a vector of table names: tables tables <- dbListTables ( con ) # Display structure of tables str ( tables ) Step 3: Import data from a table 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 # Connect to the MySQL database: con con <- dbConnect ( RMySQL :: MySQL (), dbname = 'tweater' , host = 'courses.csrrinzqubik.us-east-1.rds.amazonaws.com' , port = 3306 , user = 'student' , password = 'datacamp' ) # Import the users table from tweater: users users <- dbReadTable ( con , 'users' ) users # Import and print the tweats table from tweater: tweats tweats <- dbReadTable ( con , 'tweats' ) tweats # Import and print the comments table from tweater: comments comments <- dbReadTable ( con , 'comments' ) comments Your very first SQL query 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 con <- dbConnect ( RMySQL :: MySQL (), dbname = 'tweater' , host = 'courses.csrrinzqubik.us-east-1.rds.amazonaws.com' , port = 3306 , user = 'student' , password = 'datacamp' ) # Import post column of tweats where date is higher than '2015-09-21': latest latest <- dbGetQuery ( con , 'SELECT post FROM tweats WHERE date > \\'2015-09-21\\'' ) latest # Import tweat_id column of comments where user_id is 1: elisabeth elisabeth <- dbGetQuery ( con , 'SELECT tweat_id FROM comments WHERE user_id = 1' ) elisabeth More advanced SQL queries 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 # Connect to the database con <- dbConnect ( RMySQL :: MySQL (), dbname = 'tweater' , host = 'courses.csrrinzqubik.us-east-1.rds.amazonaws.com' , port = 3306 , user = 'student' , password = 'datacamp' ) # Create data frame specific specific <- dbGetQuery ( con , 'SELECT message FROM comments WHERE tweat_id = 77 AND user_id > 4' ) specific # Create data frame short short <- dbGetQuery ( con , 'SELECT id, name FROM users WHERE CHAR_LENGTH(name) < 5' ) short Send - Fetch - Clear 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 # Connect to the database con <- dbConnect ( RMySQL :: MySQL (), dbname = 'tweater' , host = 'courses.csrrinzqubik.us-east-1.rds.amazonaws.com' , port = 3306 , user = 'student' , password = 'datacamp' ) # Send query to the database with dbSendQuery(): res res <- dbSendQuery ( con , 'SELECT * FROM comments WHERE user_id > 4' ) # Display information contained in res dbGetInfo ( res ) # Use dbFetch() twice while ( ! dbHasCompleted ( res )) { chunk <- dbFetch ( res , n = 2 ) chunk2 <- dbFetch ( res ) print ( chunk ) } # Clear res dbClearResult ( res ) Be polite and \u2026 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 # Database specifics dbname <- 'tweater' host <- 'courses.csrrinzqubik.us-east-1.rds.amazonaws.com' port <- 3306 user <- 'student' password <- 'datacamp' # Connect to the database con <- dbConnect ( RMySQL :: MySQL (), dbname = 'tweater' , host = 'courses.csrrinzqubik.us-east-1.rds.amazonaws.com' , port = 3306 , user = 'student' , password = 'datacamp' ) # Create the data frame long_tweats long_tweats <- dbGetQuery ( con , 'SELECT post, date FROM tweats WHERE CHAR_LENGTH(post) > 40' ) # Print long_tweats print ( long_tweats ) # Disconnect from the database dbDisconnect ( con ) Other general packages The RODBC package provides access to databases (including Microsoft Access and Microsoft SQL Server) through an ODBC interface. The RJDBC package provides access to databases through a JDBC interface. Specialized packages ROracle provides an interface for Oracle. RMySQL provides access to MySQL. RpostgreSQL to PostgreSQL. RSQLite to SQLite. And there are manu more packages for NoSQL databases such as MongoDB. Importing Data from Relational Data \u2013 More \u00b6 DBI \u00b6 First, change the working directory with setwd . Install the DBI library. Connect and read preliminary results 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 library ( DBI ) library ( sqliter ) # Assign the sqlite database and full path to a variable dbfile = 'chinook.db' # Instantiate the dbDriver to a convenient object sqlite = dbDriver ( 'SQLite' ) # Assign the connection string to a connection object sqlitedb <- dbConnect ( RSQLite :: SQLite (), dbname = dbfile , host = '' , port = 3306 , user = '' , password = '' ) # Request a list of tables using the connection object dbListTables ( sqlitedb ) Extract some data 1 2 3 4 5 6 7 8 # Assign the results of a SQL query to an object results = dbSendQuery ( sqlitedb , \"SELECT * FROM albums\" ) # Return results from a custom object to a data.frame data = fetch ( results ) # Print data frame to console head ( data ) 1 2 3 4 5 # Clear the results and close the connection dbClearResult ( results ) # Disconnect from the database dbDisconnect ( sqlitedb ) RSQLite \u00b6 First, change the working directory with setwd . Install the RSQLite library. Connect and read preliminary results 1 2 3 4 5 6 7 8 9 10 11 12 13 14 library ( RSQLite ) library ( sqliter ) # Assign the sqlite database and full path to a variable dbfile = 'chinook.db' # Instantiate the dbDriver to a convenient object sqlite = dbDriver ( 'SQLite' ) # Assign the connection string to a connection object mysqldb = dbConnect ( sqlite , dbfile ) # Request a list of tables using the connection object dbListTables ( sqlitedb ) Extract some data 1 2 3 4 5 6 7 8 9 10 11 12 # Assign the results of a SQL query to an object results = dbSendQuery ( sqlitedb , \"SELECT * FROM albums\" ) # Check the object results dbGetInfo ( results ) # Return results from a custom object to a data.frame data = fetch ( results ) # Print data frame to console head ( data ) 1 2 3 4 5 # Clear the results and close the connection dbClearResult ( results ) # Disconnect from the database dbDisconnect ( sqlitedb ) MySQL with DBI or RMySQL \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 library ( DBI ) # Assign the sqlite database and full path to a variable dbfile = 'chinook.db' # Instantiate the dbDriver to a convenient object mysql = dbDriver ( 'MySQL' ) # Assign the connection string to a connection object mysqldb <- dbConnect ( RMySQL :: MySQL (), dbname = dbfile , host = '' , port = 3306 , user = '' , password = '' ) # Request a list of tables using the connection object dbListTables ( mysqldb ) # Request a list of tables using the connection object dbListTables ( mysqldb ) # Disconnect from the database dbDisconnect ( mysqldb ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 library ( RMySQL ) # Assign the sqlite database and full path to a variable dbfile = 'chinook.db' # Instantiate the dbDriver to a convenient object mysql = dbDriver ( 'MySQL' ) # Assign the connection string to a connection object mysqldb = dbConnect ( mysql , dbfile ) # Request a list of tables using the connection object dbListTables ( mysqldb ) # Request a list of tables using the connection object dbListTables ( mysqldb ) # Disconnect from the database dbDisconnect ( mysqldb ) PosgreSQL with DBI or RPostgreSQL \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 library ( DBI ) # Assign the sqlite database and full path to a variable dbfile = 'chinook.db' # Instantiate the dbDriver to a convenient object postgresql = dbDriver ( 'PostgreSQL' ) # Assign the connection string to a connection object postgresqldb <- dbConnect ( RPostgreSQL :: PostgreSQL (), dbname = dbfile , host = '' , port = 3306 , user = '' , password = '' ) # Request a list of tables using the connection object dbListTables ( postgresqldb ) # Request a list of tables using the connection object dbListTables ( postgresqldb ) # Disconnect from the database dbDisconnect ( postgresqldb ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 library ( RPostgreSQL ) # Assign the sqlite database and full path to a variable dbfile = 'chinook.db' # Instantiate the dbDriver to a convenient object postgresql = dbDriver ( 'PostgreSQL' ) # Assign the connection string to a connection object postgresqldb = dbConnect ( postgresql , dbfile ) # Request a list of tables using the connection object dbListTables ( postgresqldb ) # Request a list of tables using the connection object dbListTables ( postgresqldb ) # Disconnect from the database dbDisconnect ( postgresqldb ) Importing Data from the Web \u00b6 The other package above can download files from the web. The next packages are web-oriented. The httr package \u00b6 GET pages and files from the web. Concise. Parse JSON files. Communicate with APIs. 1 library ( httr ) HTTP? httr ! 1 2 3 4 5 6 7 8 9 10 11 # Get the url, save response to resp url <- 'http://docs.datacamp.com/teach/' resp <- GET ( url ) resp # Get the raw content of resp raw_content <- content ( resp , as = 'raw' ) # Print the head of content head ( raw_content ) 1 2 3 4 5 6 7 8 9 10 11 12 13 # Get the url url <- 'https://www.omdbapi.com/?t=Annie+Hall&y=&plot=short&r=json' resp <- GET ( url ) # Print resp resp # Print content of resp as text content ( resp , as = 'text' ) # Print content of resp content ( resp ) The jsonlite package \u00b6 Robust. Improve the imported data. fromJSON . from an R object to toJSON prettify . minify . 1 library ( jsonlite ) From JSON to R 1 2 3 4 5 6 7 8 9 10 11 # Convert wine_json to a list: wine wine_json <- '{' name ':' Chateau Migraine ', ' year ':1997, ' alcohol_pct ':12.4, ' color ':' red ', ' awarded ':false}' wine <- fromJSON ( wine_json ) str ( wine ) # Import Quandl data: quandl_data quandl_url <- 'http://www.quandl.com/api/v1/datasets/IWS/INTERNET_INDIA.json?auth_token=i83asDsiWUUyfoypkgMz' quandl_data <- fromJSON ( quandl_url ) str ( quandl_data ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 # Experiment 1 json1 <- '[1, 2, 3, 4, 5, 6]' fromJSON ( json1 ) # Experiment 2 json2 <- '{' a ': [1, 2, 3], ' b ': [4, 5, 6]}' fromJSON ( json2 ) # Experiment 3 json3 <- '[[1, 2], [3, 4]]' fromJSON ( json3 ) # Experiment 4 json4 <- '[{' a ': 1, ' b ': 2}, {' a ': 3, ' b ': 4}, {' a ': 5, ' b ': 6}]' fromJSON ( json4 ) Ask OMDb 1 2 3 4 5 6 7 8 9 10 11 12 13 14 # Definition of the URLs url_sw4 <- 'http://www.omdbapi.com/?i=tt0076759&r=json' url_sw3 <- 'http://www.omdbapi.com/?i=tt0121766&r=json' # Import two URLs with fromJSON(): sw4 and sw3 sw4 <- fromJSON ( url_sw4 ) sw3 <- fromJSON ( url_sw3 ) # Print out the Title element of both lists sw4 $ Title sw3 $ Title # Is the release year of sw4 later than sw3 sw4 $ Year > sw3 $ Year From R to JSON 1 2 3 4 5 6 7 8 9 10 11 12 13 # URL pointing to the .csv file url_csv <- 'http://s3.amazonaws.com/assets.datacamp.com/course/importing_data_into_r/water.csv' # Import the .csv file located at url_csv water <- read.csv ( url_csv , stringsAsFactors = FALSE ) # Generate a summary of water summary ( water ) # Convert the data file according to the requirements water_json <- toJSON ( water ) water_json Minify and prettify 1 2 3 4 5 6 7 8 9 10 11 # Convert mtcars to a pretty JSON: pretty_json pretty_json <- toJSON ( mtcars , pretty = TRUE ) # Print pretty_json pretty_json # Minify pretty_json: mini_json mini_json <- minify ( pretty_json ) # Print mini_json mini_json Keyboard Inputting \u00b6 Coding 1 2 3 4 5 # create a data frame from scratch age <- c ( 25 , 30 , 56 ) gender <- c ( \"male\" , \"female\" , \"male\" ) weight <- c ( 160 , 110 , 220 ) mydata <- data.frame ( age , gender , weight ) Spreadsheet-like 1 2 3 4 5 # enter data using editor mydata <- data.frame ( age = numeric ( 0 ), gender = character ( 0 ), weight = numeric ( 0 )) mydata <- edit ( mydata ) # note that without the assignment in the line above, the edits are not saved! Exporting Data \u00b6 To a Tab-Delimited Text File \u00b6 1 write.table ( mydata , 'c:/mydata.txt' , sep = \"\\t\" ) To an Excel Spreadsheet \u00b6 1 2 3 library ( xlsx ) write.xlsx ( mydata , \"c:/mydata.xlsx\" ) Worksheet 1 2 3 4 5 6 7 8 9 10 11 12 13 library ( XLConnect ) # xls or xlsx # write a worksheet in steps wb <- loadWorkbook ( 'XLConnectExample1.xls' , create = TRUE ) createSheet ( wb , name = 'chickSheet' ) writeWorksheet ( wb , ChickWeight , sheet = 'chickSheet' , startRow = 3 , startCol = 4 ) saveWorkbook ( wb ) # write a worksheet all in one step ChickWeight <- 1 writeWorksheetToFile ( 'XLConnectExample2.xlsx' , data = ChickWeight , sheet = 'chickSheet' , startRow = 3 , startCol = 4 ) Field 1 2 3 4 5 6 7 8 9 # write a field in steps wb = loadWorkbook ( 'XLConnectExample3.xlsx' , create = TRUE ) createSheet ( wb , name = 'womenData' ) createName ( wb , name = 'womenName' , formula = 'womenData!$C$5' , overwrite = TRUE ) writeNamedRegion ( wb , women , name = \"womenName\" ) saveWorkbook ( wb ) # write a field all in one step writeNamedRegionToFile ( \"XLConnectExample4.xlsx\" , women , name = \"womenName\" , formula = \"womenData!$C$5\" ) I/O 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 # Build connection to latitude.xlsx my_book <- loadWorkbook ( 'latitude.xlsx' ) # Create data frame: summ dims1 <- dim ( readWorksheet ( my_book , 1 )) dims2 <- dim ( readWorksheet ( my_book , 2 )) summ <- data.frame ( sheets = getSheets ( my_book ), nrows = c ( dims1[1] , dims2[1] ), ncols = c ( dims1[2] , dims2[2] )) # Add a worksheet to my_book, named 'data_summary' createSheet ( my_book , name = 'data_summary' ) # Populate 'data_summary' with summ data frame writeWorksheet ( my_book , summ , sheet = 'data_summary' ) # Save workbook as latitude_with_summ.xlsx saveWorkbook ( my_book , 'latitude_with_summ.xlsx' ) To SPSS \u00b6 1 2 3 library ( foreign ) write.foreign ( mydata , \"c:/mydata.txt\" , \"c:/mydata.sps\" , package = \"SPSS\" ) To SAS \u00b6 1 2 3 library ( foreign ) write.foreign ( mydata , \"c:/mydata.txt\" , \"c:/mydata.sas\" , package = \"SAS\" ) To Stata \u00b6 1 2 3 library ( foreign ) write.dta ( mydata , \"c:/mydata.dta\" ) Inspecting Data - Missing Data \u00b6 Inspecting \u00b6 ls(object) . names(object) . str(object) . levels(object$v1) . dim(object) . class ( object ) . print(object) . head(object, 10) . tail(object, 20) . Testing for Missing Values 1 2 y <- c ( 1 , 2 , 3 , NA ) # returns TRUE of x is missing is.na ( y ) # returns a vector (F F F T) Recoding Values to Missing 1 2 3 # recode 99 to missing for variable v1 # select rows where v1 is 99 and recode column v1 mydata $ v1[mydata $ v1 == 99 ] <- NA Excluding Missing Values from Analyses 1 2 3 4 x <- c ( 1 , 2 , NA , 3 ) mean ( x ) # returns NA mean ( x , na.rm = TRUE ) # returns 2 1 2 # list rows of data that have missing values mydata[ ! complete.cases ( mydata ), ] 1 2 # create new dataset without missing data newdata <- na.omit ( mydata ) The dplyr package \u00b6 1 2 3 4 5 library ( dplyr ) tbl_df ( iris ) # almost like head/tail glimpse ( iris ) # almost like str View ( iris ) # open a spreadsheet For thorough cleaning \u00b6 The Amelia II software. The mitools package. Labels & Levels \u00b6 Basic 1 2 3 4 5 # variable v1 is coded 1, 2 or 3 # we want to attach value labels 1=red, 2=blue, 3=green mydata $ v1 <- factor ( mydata $ v1 , levels = c ( 1 , 2 , 3 ), labels = c ( \"red\" , \"blue\" , \"green\" )) 1 2 3 4 5 # variable y is coded 1, 3 or 5 # we want to attach value labels 1=Low, 3=Medium, 5=High mydata $ v1 <- ordered ( mydata $ y , levels = c ( 1 , 3 , 5 ), labels = c ( \"Low\" , \"Medium\" , \"High\" )) Order 1 2 3 4 5 # Create a vector of temperature observations temperature_vector <- c ( 'High' , 'Low' , 'High' , 'Low' , 'Medium' ) # Specify that they are ordinal variables with the given levels factor_temperature_vector <- factor ( temperature_vector , order = TRUE , levels = c ( 'Low' , 'Medium' , 'High' )) Add comments to an object 1 2 3 4 names ( iris ) [5] <- \"This is the label for variable 5\" names ( iris ) [5] # the comment iris[5] # the data 1 2 3 4 5 6 7 8 # labeling the variables library ( Hmisc ) label ( iris $ Species ) <- \"Variable label for variable myvar\" describe ( iris $ Species ) # commented #vs describe ( iris $ Sepal.Length ) # not commented How to work with Quandl in R \u00b6 Importing Quandl Datasets \u00b6 Quandl delivers financial, economic and alternative data to the world\u2019s top hedge funds, asset managers and investment banks in several formats: Excel. R. Python. API. DB. The packages used: Quandl . quantmod for plotting. Quandl - A first date 1 2 3 4 5 # Load in the Quandl package library ( Quandl ) # Assign your first dataset to the variable: mydata <- Quandl ( 'NSE/OIL' ) Identifying a dataset with its ID 1 2 # Assign the Prague Stock Exchange to: PragueStockExchange <- Quandl ( 'PRAGUESE/PX' ) Plotting a stock chart 1 2 3 4 5 6 7 8 # The quantmod package library ( quantmod ) # Load the Facebook data with the help of Quandl Facebook <- Quandl ( 'GOOG/NASDAQ_FB' , type = 'xts' ) # Plot the chart with the help of candleChart() candleChart ( Facebook ) Searching a Quandl dataset in R 1 2 3 4 5 6 7 8 # Look up the first 3 results for 'Bitcoin' within the Quandl database: results <- Quandl.search ( query = 'Bitcoin' , silent = FALSE ) # Print out the results str ( results ) # Assign the data set with code BCHAIN/TOTBC BitCoin <- Quandl ( 'BCHAIN/TOTBC' ) Manipulating Quandl Datasets \u00b6 Manipulating data 1 2 # Assign to the variable Exchange Exchange <- Quandl ( 'BNP/USDEUR' , start_date = '2013-01-01' , end_date = '2013-12-01' ) Transforming your Quandl dataset 1 2 3 4 5 6 # API transformation # The result: GDP_Change <- Quandl ( 'FRED/CANRGDPR' , transformation = 'rdiff' ) head ( GDP_Change ) GDP_Chang <- Quandl ( 'FRED/CANRGDPR' ) head ( GDP_Chang ) The magic of frequency collapsing 1 2 # The result: eiaQuarterly <- Quandl ( 'DOE/RWTC' , collapse = 'quarterly' ) Truncation and sort 1 2 3 4 5 # Assign to TruSo the first 5 observations of the crude oil prices TruSo <- Quandl ( 'DOE/RWTC' , sort = 'asc' , rows = 5 ) # Print the result TruSo A complex example 1 2 # Here you should place the return: Final <- Quandl ( 'DOE/RWTC' , collapse = 'daily' , transformation = 'rdiff' , start_date = '2005-01-01' , end_date = '2010-03-01' , sort = 'asc' ) Cleaning Data in R \u00b6 The packages used: dplyr & tidyr for data wrangling. stringr for regex. lubridate for time and date. Introduction and Exploring Raw Data \u00b6 Here\u2019s what messy data look like 1 2 3 4 5 6 7 8 # View the first 6 rows of data head ( weather ) # View the last 6 rows of data tail ( weather ) # View a condensed summary of the data str ( weather ) Getting a feel for your data 1 2 3 4 5 6 7 8 # Check the class of bmi class ( bmi ) # Check the dimensions of bmi dim ( bmi ) # View the column names of bmi names ( bmi ) Viewing the structure of your data 1 2 3 4 5 6 7 8 9 10 11 # Check the structure of bmi str ( bmi ) # Load dplyr library ( dplyr ) # Check the structure of bmi, the dplyr way glimpse ( bmi ) # View a summary of bmi summary ( bmi ) Looking at your data 1 2 3 4 5 6 7 8 9 10 11 12 13 14 # Print bmi to the console print ( bmi ) # View the first 6 rows head ( bmi , 6 ) # View the first 15 rows head ( bmi , 15 ) # View the last 6 rows tail ( bmi , 6 ) # View the last 10 rows tail ( bmi , 10 ) Visualizing your data 1 2 3 4 5 # Histogram of BMIs from 2008 hist ( bmi $ Y2008 ) # Scatter plot comparing BMIs from 1980 to those from 2008 plot ( bmi $ Y1980 , bmi $ Y2008 ) Tidying Data \u00b6 Gathering columns into key-value pairs 1 2 3 4 5 6 7 8 # Load tidyr library ( tidyr ) # Apply gather() to bmi and save the result as bmi_long bmi_long <- gather ( bmi , year , bmi_val , - Country ) # View the first 20 rows of the result head ( bmi_long , 20 ) Spreading key-value pairs into columns 1 2 3 4 5 # Apply spread() to bmi_long bmi_wide <- spread ( bmi_long , year , bmi_val ) # View the head of bmi_wide head ( bmi_wide ) Separating columns 1 2 3 4 5 # Apply separate() to bmi_cc bmi_cc_clean <- separate ( bmi_cc , col = Country_ISO , into = c ( 'Country' , 'ISO' ), sep = '/' ) # Print the head of the result head ( bmi_cc_clean ) Uniting columns 1 2 3 4 5 # Apply unite() to bmi_cc_clean bmi_cc <- unite ( bmi_cc_clean , Country_ISO , Country , ISO , sep = '-' ) # View the head of the result head ( bmi_cc ) Column headers are values, not variable names 1 2 3 4 5 6 7 8 9 10 11 # View the head of census head ( census ) # Gather the month columns census2 <- gather ( census , month , amount , JAN , FEB , MAR , APR , MAY , JUN , JUL , AUG , SEP , OCT , NOV , DEC ) # Arrange rows by YEAR using dplyr's arrange census2 <- arrange ( census2 , YEAR ) # View first 20 rows of census2 head ( census2 , 20 ) Variables are stored in both rows and columns 1 2 3 4 5 6 7 8 # View first 50 rows of census_long head ( census_long , 50 ) # Spread the type column census_long2 <- spread ( census_long , type , amount ) # View first 20 rows of census_long2 head ( census_long2 , 20 ) Multiple values are stored in one column 1 2 3 4 5 6 7 8 # View the head of census_long3 head ( census_long3 ) # Separate the yr_month column into two census_long4 <- separate ( census_long3 , yr_month , c ( 'year' , 'month' ), '_' ) # View the first 6 rows of the result head ( census_long4 , 6 ) Preparing Data for Analysis \u00b6 Types of variables in R 1 2 3 4 5 6 7 8 9 10 11 12 13 14 # Make this evaluate to character class ( 'true' ) # Make this evaluate to numeric class ( 8484.00 ) # Make this evaluate to integer class ( 99L ) # Make this evaluate to factor class ( factor ( 'factor' )) # Make this evaluate to logical class ( FALSE ) Common type conversions 1 2 3 4 5 6 7 8 9 10 11 12 13 14 # Preview students with str() str ( students ) # Coerce Grades to character students $ Grades <- as.character ( students $ Grades ) # Coerce Medu to factor students $ Medu <- as.factor ( students $ Medu ) # Coerce Fedu to factor students $ Fedu <- as.factor ( students $ Fedu ) # Look at students once more with str() str ( students ) Working with dates 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 # Preview students2 with str() str ( students2 ) # Load the lubridate package library ( lubridate ) # Parse as date ymd ( '2015-Sep-17' ) # Parse as date and time (with no seconds!) ymd_hm ( '2012-July-15, 12.56' ) # Coerce dob to a date (with no time) students2 $ dob <- ymd ( students2 $ dob ) # Coerce nurse_visit to a date and time students2 $ nurse_visit <- ymd_hms ( students2 $ nurse_visit ) # Look at students2 once more with str() str ( students2 ) Trimming and padding strings 1 2 3 4 5 6 7 8 # Load the stringr package library ( stringr ) # Trim all leading and trailing whitespace str_trim ( c ( ' Filip ' , 'Nick ' , ' Jonathan' )) # Pad these strings with leading zeros str_pad ( c ( '23485W' , '8823453Q' , '994Z' ), width = 9 , side = 'left' , pad = '0' ) Upper and lower case 1 2 3 4 5 6 7 8 9 # Print state abbreviations states # Make states all uppercase and save result to states_upper states_upper <- toupper ( states ) states_upper # Make states_upper all lowercase again tolower ( states_upper ) Finding and replacing strings 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 # stringr has been loaded for you # Look at the head of students2 head ( students2 ) # Detect all dates of birth (dob) in 1997 str_detect ( students2 $ dob , '1997' ) # In the sex column, replace 'F' with 'Female'... students2 $ sex <- str_replace ( students2 $ sex , 'F' , 'Female' ) # ...And 'M' with 'Male' students2 $ sex <- str_replace ( students2 $ sex , 'M' , 'Male' ) # View the head of students2 head ( students2 ) Finding missing values 1 2 3 4 5 6 7 8 9 10 11 12 # Call is.na() on the full social_df to spot all NAs is.na ( social_df ) # Use the any() function to ask whether there are any NAs in the data any ( is.na ( social_df )) sum ( is.na ( social_df )) # View a summary() of the dataset summary ( social_df ) # Call table() on the status column table ( social_df $ status ) Dealing with missing values 1 2 3 4 5 6 7 8 9 10 11 # Use str_replace() to replace all missing strings in status with NA social_df $ status <- str_replace ( social_df $ status , '^$' , NA ) # Print social_df to the console social_df # Use complete.cases() to see which rows have no missing values complete.cases ( social_df ) # Use na.omit() to remove all rows with any missing values na.omit ( social_df ) Dealing with outliers and obvious errors 1 2 3 4 5 6 7 8 9 10 11 # Look at a summary() of students3 summary ( students3 ) # View a histogram of the age variable hist ( students3 $ age , breaks = 20 ) # View a histogram of the absences variable hist ( students3 $ absences , breaks = 20 ) # View a histogram of absences, but force zeros to be bucketed to the right of zero hist ( students3 $ absences , breaks = 20 , right = FALSE ) Another look at strange values 1 2 3 4 5 # View a boxplot of age boxplot ( students3 $ age ) # View a boxplot of absences boxplot ( students3 $ absences ) Putting it All Together \u00b6 Get a feel for the data 1 2 3 4 5 6 7 8 # Verify that weather is a data.frame class ( weather ) # Check the dimensions dim ( weather ) # View the column names names ( weather ) Summarize the data 1 2 3 4 5 6 7 8 9 10 11 # View the structure of the data str ( weather ) # Load dplyr package library ( dplyr ) # Look at the structure using dplyr's glimpse() glimpse ( weather ) # View a summary of the data summary ( weather ) Take a closer look 1 2 3 4 5 6 7 8 9 10 11 # View first 6 rows head ( weather , 6 ) # View first 15 rows head ( weather , 15 ) # View the last 6 rows tail ( weather , 6 ) # View the last 10 rows tail ( weather , 10 ) Column names are values 1 2 3 4 5 6 7 8 # Load the tidyr package library ( tidyr ) # Gather the columns weather2 <- gather ( weather , day , value , X1 : X31 , na.rm = TRUE ) # View the head head ( weather2 ) Values are variable names 1 2 3 4 5 6 7 8 # First remove column of row names weather2 <- weather2[ , -1 ] # Spread the data weather3 <- spread ( weather2 , measure , value ) # View the head head ( weather3 ) Clean up dates 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 # Load the stringr and lubridate packages library ( stringr ) library ( lubridate ) # Remove X's from day column # weather3$day <- str_pad(str_replace(weather3$day, 'X', ''), width = 2, side = 'left', pad = '0') weather3 $ day <- str_replace ( weather3 $ day , 'X' , '' ) # Unite the year, month, and day columns weather4 <- unite ( weather3 , date , year , month , day , sep = '-' ) # Convert date column to proper date format using stringr's ymd() weather4 $ date <- ymd ( weather4 $ date ) # Rearrange columns using dplyr's select() weather5 <- select ( weather4 , date , Events , CloudCover : WindDirDegrees ) # View the head head ( weather5 ) A closer look at column types 1 2 3 4 5 6 7 8 # View the structure of weather5 str ( weather5 ) # Examine the first 20 rows of weather5. Are most of the characters numeric? head ( weather5 , 20 ) # See what happens if we try to convert PrecipitationIn to numeric as.numeric ( weather5 $ PrecipitationIn ) Column type conversions 1 2 3 4 5 6 7 8 9 # The dplyr package is already loaded # Replace T with 0 (T = trace) weather5 $ PrecipitationIn <- str_replace ( weather5 $ PrecipitationIn , 'T' , '0' ) # Convert characters to numerics weather6 <- mutate_each ( weather5 , funs ( as.numeric ), CloudCover : WindDirDegrees ) # Look at result str ( weather6 ) Find missing values 1 2 3 4 5 6 7 8 9 10 11 12 # Count missing values sum ( is.na ( weather6 )) # Find missing values summary ( weather6 ) # Find indices of NAs in Max.Gust.SpeedMPH ind <- which ( is.na ( weather6 $ Max.Gust.SpeedMPH )) ind # Look at the full rows for records missing Max.Gust.SpeedMPH weather6[ind , ] An obvious error 1 2 3 4 5 6 7 8 9 10 11 # Review distibutions for all variables summary ( weather6 ) # Find row with Max.Humidity of 1000 ind <- which ( weather6 $ Max.Humidity == 1000 ) # Look at the data for that day weather6[ind , ] # Change 1000 to 100 weather6 $ Max.Humidity[ind] <- 100 Another obvious error 1 2 3 4 5 6 7 8 9 10 11 # Look at summary of Mean.VisibilityMiles summary ( weather6 $ Mean.VisibilityMiles ) # Get index of row with -1 value ind <- which ( weather6 $ Mean.VisibilityMiles == -1 ) # Look at full row weather6[ind , ] # Set Mean.VisibilityMiles to the appropriate value weather6 $ Mean.VisibilityMiles[ind] <- 10 Check other extreme values 1 2 3 4 5 6 7 8 9 10 11 # Review summary of full data once more summary ( weather6 ) # Look at histogram for MeanDew.PointF hist ( weather6 $ MeanDew.PointF ) # Look at histogram for Min.TemperatureF hist ( weather6 $ Min.TemperatureF ) # Compare to histogram for Mean.TemperatureF hist ( weather6 $ Mean.TemperatureF ) Finishing touches 1 2 3 4 5 6 7 8 # Clean up column names names ( weather6 ) <- new_colnames # Replace empty cells in Events column weather6 $ events[weather6 $ events == '' ] <- 'None' # Print the first 6 rows of weather6 head ( weather6 , 6 )","title":"I/O Snippets & Cleaning"},{"location":"io_snippets_cleaning/#datasets","text":"R Dataset Packages ; by default in R. Other dataset can be imported with data(Cars93, package = 'MASS') for example. csv/doc Datasets . Free Datasets from the World Bank, Gapminder, Kaggle, Quandl, Reddit, and many more websites. Datasets to Practice Your Data Mining. Houghton Mifflin Data for linear regressions. Regression Datasets for Generalized Linear Models (linear, logistic, poisson, multinomial, survival). Public Datasets on GitHub . Awesome Public Datasets .","title":"Datasets"},{"location":"io_snippets_cleaning/#importing-data-into-r","text":"The packages: utils . readr . data.table . readxl . gdata . XLConnect . haven . foreign . DBI . httr . jsonlite .","title":"Importing Data Into R"},{"location":"io_snippets_cleaning/#importing-data-from-flat-files","text":"","title":"Importing Data from Flat Files"},{"location":"io_snippets_cleaning/#r-functions-by-default","text":"read.csv ; sep = ',' , dec = '.' . read.delim ; .txt, dec = '.' . read.csv2 ; sep = ';' , dec = ',' . read.delim2 ; .txt, dec = ',' . Needs arguments. read.csv for .csv files 1 2 3 4 5 6 # List the files in your working directory dir () # Import swimming_pools.csv: pools # stringAsFactors = FALSE does not import strings as categorical variables pools <- read.csv ( 'swimming_pools.csv' , stringsAsFactors = FALSE ) stringsAsFactors 1 2 3 4 5 # Import swimming_pools.csv correctly: pools pools <- read.csv ( 'swimming_pools.csv' , stringsAsFactor = FALSE , header = TRUE , sep = ',' ) # Import swimming_pools.csv with factors: pools_factor pools_factor <- read.csv ( 'swimming_pools.csv' , header = TRUE , sep = ',' ) read.delim for .txt files 1 2 3 4 5 # Import hotdogs.txt: hotdogs hotdogs <- read.delim ( 'hotdogs.txt' , header = FALSE ) # Name the columns of hotdogs appropriately names ( hotdogs ) <- c ( 'type' , 'calories' , 'sodium' ) Arguments. 1 2 3 4 5 6 7 8 # Load in the hotdogs data set: hotdogs hotdogs <- read.delim ( 'hotdogs.txt' , header = FALSE , sep = '\\t' , col.names = c ( 'type' , 'calories' , 'sodium' )) # Select the hot dog with the least calories: lily lily <- hotdogs [which.min ( hotdogs $ calories ), ] # Select the observation with the most sodium: tom tom <- hotdogs [which.max ( hotdogs $ sodium ), ] 1 2 3 4 5 6 7 8 # Previous call to import hotdogs.txt hotdogs <- read.delim ( 'hotdogs.txt' , header = FALSE , col.names = c ( 'type' , 'calories' , 'sodium' )) # Print a vector representing the classes of the columns sapply ( hotdogs , class ) # Edit the colClasses argument to import the data correctly: hotdogs2 hotdogs2 <- read.delim ( 'hotdogs.txt' , header = FALSE , col.names = c ( 'type' , 'calories' , 'sodium' ), colClasses = c ( 'factor' , 'NULL' , 'numeric' ))","title":"R functions, by default."},{"location":"io_snippets_cleaning/#the-utils-package","text":"read.table ; sep = '\\t' , = ',' , = ';' . Read any tabular as a d.f. Needs arguments; lots of argument for precision. Slow. 1 library ( utils ) read.table .txt files 1 2 3 4 5 # Create a path to the hotdogs.txt file path <- file.path ( 'hotdogs' , 'hotdogs.txt' ) # Import the hotdogs.txt file: hotdogs hotdogs <- read.table ( path , header = FALSE , sep = '\\t' , col.names = c ( 'type' , 'calories' , 'sodium' ))","title":"The utils package"},{"location":"io_snippets_cleaning/#from-importing-data-from-the-web","text":"1 2 3 4 5 # https URL to the swimming_pools csv file. url_csv <- 'https://s3.amazonaws.com/assets.datacamp.com/course/importing_data_into_r/swimming_pools.csv' # Import the file using read.csv(): pools1 pools1 <- read.csv ( url_csv )","title":"(from Importing Data from the Web)"},{"location":"io_snippets_cleaning/#the-readr-package","text":"read_delim ; delim = '\\t' , = ',' . read_csv ; read 100.000, 200.000 read_tsv ; idem. read_csv2 ; read 100,000; 200,000 or European files.. read_tsv2 ; idem. read_lines . read_file . write_csv . write_rds . type_convert . parse_factor . parse_date . parse_number . spec_csv . spec_delim . Fast, few arguments. Detect data type. 1 library ( readr ) read_delim .txt files 1 2 # Import potatoes.txt using read_delim(): potatoes potatoes <- read_delim ( 'potatoes.txt' , delim = '\\t' ) read_csv .csv files 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 # Column names properties <- c ( 'area' , 'temp' , 'size' , 'storage' , 'method' , 'texture' , 'flavor' , 'moistness' ) # Import potatoes.csv with read_csv(): potatoes potatoes <- read_csv ( 'potatoes.csv' , col_names = properties ) # Create a copy of potatoes: potatoes2 potatoes2 <- potatoes # Convert the method column of potatoes2 to a factor potatoes2 $ method = factor ( potatoes2 $ method ) # or potatoes2 $ method = as.factor ( potatoes2 $ method ) col_types , skip and n_max in .tsv files 1 2 3 4 5 6 7 8 9 # Column names properties <- c ( 'area' , 'temp' , 'size' , 'storage' , 'method' , 'texture' , 'flavor' , 'moistness' ) # Import 5 observations from potatoes.txt: potatoes_fragment # read_tsv or tab-separated values potatoes_fragment <- read_tsv ( 'potatoes.txt' , col_names = properties , skip = 7 , n_max = 5 ) # Import all data, but force all columns to be character: potatoes_char potatoes_char <- read_tsv ( 'potatoes.txt' , col_types = 'cccccccc' ) Setting column types 1 2 3 4 cols ( weight = col_integer (), feed = col_character () ) Removing NA 1 na = c ( 'NA' , 'null' ) col_types with collectors .tsv files 1 2 3 4 5 6 7 8 9 10 # Import without col_types hotdogs <- read_tsv ( 'hotdogs.txt' , col_names = c ( 'type' , 'calories' , 'sodium' )) # The collectors you will need to import the data fac <- col_factor ( levels = c ( 'Beef' , 'Meat' , 'Poultry' )) int <- col_integer () # Edit the col_types argument to import the data correctly: hotdogs_factor # Change col_types to the correct vector of collectors; coerce the vector into a list hotdogs_factor <- read_tsv ( 'hotdogs.txt' , col_names = c ( 'type' , 'calories' , 'sodium' ), col_types = list ( fac , int , int )) Skiping columns 1 2 3 4 5 salaries <- read_tsv ( 'Salaries.txt' , col_names = FALSE , col_types = cols ( X2 = col_skip (), X3 = col_skip (), X4 = col_skip () )) Reading an ordinary text file 1 2 3 4 5 6 7 8 9 # vector of character strings. # Import as a character vector, one item per line: tweets tweets <- read_lines ( 'tweets.txt' ) tweets # returns a length 1 vector of the entire file, with line breaks represented as \\n # Import as a length 1 vector: tweets_all tweets_all <- read_file ( 'tweets.txt' ) tweets_all Writing .csv and .tsv files 1 2 3 4 5 # Save cwts as chickwts.csv write_csv ( cwts , \"chickwts.csv\" ) # Append cwts2 to chickwts.csv write_csv ( cwts2 , \"chickwts.csv\" , append = TRUE ) Writing .rds files 1 2 3 4 5 6 7 8 # Save trees as trees.rds write_rds ( trees , 'trees.rds' ) # Import trees.rds: trees2 trees2 <- read_rds ( 'trees.rds' ) # Check whether trees and trees2 are the same identical ( trees , trees2 ) Coercing columns to different data types 1 2 # Convert all columns to double trees2 <- type_convert ( trees , col_types = cols ( Girth = 'd' , Height = 'd' , Volume = 'd' )) Coercing character columns into factors 1 2 3 4 5 # Parse the title column salaries $ title <- parse_factor ( salaries $ title , levels = c ( 'Prof' , 'AsstProf' , 'AssocProf' )) # Parse the gender column salaries $ gender <- parse_factor ( salaries $ gender , levels = c ( 'Male' , 'Female' )) Creating Date objects 1 2 # Change type of date column weather $ date <- parse_date ( weather $ date , format = '%m/%d/%Y' ) Parsing number formats 1 2 # Parse amount column as a number debt $ amount <- parse_number ( debt $ amount ) Viewing metadata before importing spec_csv for .csv and .tsv files. spec_delim for .txt files (among others). 1 2 # Specifications of chickwts spec_csv ( 'chickwts.csv' )","title":"The readr package"},{"location":"io_snippets_cleaning/#from-importing-data-from-the-web_1","text":"Import Flat files from the web 1 2 3 4 5 6 7 8 9 10 11 # Import the csv file: pools url_csv <- 'http://s3.amazonaws.com/assets.datacamp.com/course/importing_data_into_r/swimming_pools.csv' pools <- read_csv ( url_csv ) pools # Import the txt file: potatoes url_delim <- 'http://s3.amazonaws.com/assets.datacamp.com/course/importing_data_into_r/potatoes.txt' potatoes <- read_tsv ( url_delim ) potatoes Secure importing 1 2 # Import the file using read_csv(): pools2 pools2 <- read_csv ( url_csv )","title":"(from Importing Data from the Web)"},{"location":"io_snippets_cleaning/#the-datatable-package","text":"fread == read.table . .txt files only. Fast. 1 library ( data.table ) fread for .txt files 1 2 3 4 5 6 7 8 # Import potatoes.txt with fread(): potatoes potatoes <- fread ( 'potatoes.txt' ) # Print out arranged version of potatoes potatoes [order ( moistness ), ] # Import 20 rows of potatoes.txt with fread(): potatoes_part potatoes_part <- fread ( 'potatoes.txt' , nrows = 20 ) fread : more advanced use 1 2 3 4 5 # Import columns 6, 7 and 8 of potatoes.txt: potatoes potatoes <- fread ( 'potatoes.txt' , select = c ( 6 : 8 )) # Keep only tasty potatoes (flavor > 3): tasty_potatoes tasty_potatoes <- subset ( potatoes , potatoes $ flavor > 3 )","title":"The data.table package"},{"location":"io_snippets_cleaning/#importing-data-from-excel","text":"","title":"Importing Data from Excel"},{"location":"io_snippets_cleaning/#the-readxl-package","text":"excel_sheets ; list. read_excel ; import. .xlsx files only. 1 library ( readxl ) List the sheets of an Excel file 1 2 3 4 5 # Find the names of both spreadsheets: sheets # Before, find out what is in the directory with 'dir()' sheets <- excel_sheets ( 'latitude.xlsx' ) sheets Importing an Excel sheet 1 2 3 4 5 6 7 8 # Read the first sheet of latitude.xlsx: latitude_1 latitude_1 <- read_excel ( 'latitude.xlsx' , sheet = 1 ) # Read the second sheet of latitude.xlsx: latitude_2 latitude_2 <- read_excel ( 'latitude.xlsx' , sheet = 2 ) # Put latitude_1 and latitude_2 in a list: lat_list lat_list <- list ( latitude_1 , latitude_2 ) Reading a workbook 1 2 # Read all Excel sheets with lapply(): lat_list lat_list <- lapply ( excel_sheets ( 'latitude.xlsx' ), read_excel , path = 'latitude.xlsx' ) The col_names argument 1 2 3 4 5 # Import the the first Excel sheet of latitude_nonames.xlsx (R gives names): latitude_3 latitude_3 <- read_excel ( 'latitude_nonames.xlsx' , sheet = 1 , col_names = FALSE ) # Import the the second Excel sheet of latitude_nonames.xlsx (specify col_names): latitude_4 latitude_4 <- read_excel ( 'latitude_nonames.xlsx' , sheet = 1 , col_names = c ( 'country' , 'latitude' )) The skip argument 1 2 # Import the second sheet of latitude.xlsx, skipping the first 21 rows: latitude_sel latitude_sel <- read_excel ( 'latitude.xlsx' , skip = 21 , col_names = FALSE )","title":"The readxl package"},{"location":"io_snippets_cleaning/#from-importing-data-from-the-web_2","text":"Import Excel files from the web 1 2 3 4 5 # Download file behind URL, name it local_latitude.xls download.file ( url_xls , 'local_latitude.xls' ) # Import the local .xls file with readxl: excel_readxl excel_readxl <- read_excel ( 'local_latitude.xls' ) Downloading any file, secure or not 1 2 3 4 5 # https URL to the wine RData file. url_rdata <- 'https://s3.amazonaws.com/assets.datacamp.com/course/importing_data_into_r/wine.RData' # Download the wine file to your working directory download.file ( url_rdata , 'wine_local.RData' )","title":"(from Importing Data from the Web)"},{"location":"io_snippets_cleaning/#the-xlconnect-package","text":"loadWorkbook . getSheets . readWorksheet . readWorksheetFromFile readNamedRegion readNamedRegionFromFile .xls & .xlsx files. Like reading a database. 1 2 library ( XLConnectJars ) library ( XLConnect ) Import a workbook 1 2 # Build connection to latitude.xlsx: my_book my_book <- loadWorkbook ( 'latitude.xlsx' ) List and read Excel sheets 1 2 3 4 5 6 7 8 9 10 11 # Build connection to latitude.xlsx my_book <- loadWorkbook ( 'latitude.xlsx' ) # List the sheets in latitude.xlsx getSheets ( my_book ) # Import the second sheet in latitude.xlsx readWorksheet ( my_book , sheet = 2 ) # Import the second column of the first sheet in latitude.xlsx readWorksheet ( my_book , sheet = 2 , startCol = 2 ) Add and populate worksheets 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 # Build connection to latitude.xlsx my_book <- loadWorkbook ( 'latitude.xlsx' ) # Create data frame: summ dims1 <- dim ( readWorksheet ( my_book , 1 )) dims2 <- dim ( readWorksheet ( my_book , 2 )) summ <- data.frame ( sheets = getSheets ( my_book ), nrows = c ( dims1[1] , dims2[1] ), ncols = c ( dims1[2] , dims2[2] )) # Add a worksheet to my_book, named 'data_summary' createSheet ( my_book , name = 'data_summary' ) # Populate 'data_summary' with summ data frame writeWorksheet ( my_book , summ , sheet = 'data_summary' ) # Save workbook as latitude_with_summ.xlsx saveWorkbook ( my_book , 'latitude_with_summ.xlsx' ) One unique function 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 # Read in the data set and assign to the object impact <- readWorksheetFromFile ( 'A Hands-on Introduction to Statistics with R.xls' , sheet = 'impact' , header = TRUE , startCol = 1 , startRow = 1 ) # more arguments # endCol = 1 # endRow = 1 # autofitRow = # autofitCol = # region = # rownames = # colTypes = # forceConversion = # dateTimeFormat = # check.names = # useCachedValues = # keep = # drop = # simplify = # readStrategy =","title":"The XLConnect package"},{"location":"io_snippets_cleaning/#importing-data-from-other-statistical-software","text":"","title":"Importing Data from Other Statistical Software"},{"location":"io_snippets_cleaning/#the-haven-package","text":"read_sas ; sas7bdat & sas7bcat files. read_stata ; version; dta files. read_dta ; idem. read_spss ; sav & por files (and see below). read_por . read_sav . Simple, few arguments. Create a d.f. 1 library ( haven ) Import SAS data with haven 1 2 # Import sales.sas7bdat: sales sales <- read_sas ( 'sales.sas7bdat' ) Import STATA data with haven 1 2 # Import the data from the URL: sugar sugar <- read_dta ( 'http://assets.datacamp.com/course/importing_data_into_r/trade.dta' ) Import SPSS data with haven 1 2 3 4 5 # Specify the file path using file.path(): path path <- file.path ( 'datasets' , 'person.sav' ) # Import person.sav, which is in the datasets folder: traits traits <- read_sav ( path ) Factorize, round two 1 2 # Import SPSS data from the URL: work work <- read_sav ( 'http://assets.datacamp.com/course/importing_data_into_r/employee.sav' )","title":"The haven package"},{"location":"io_snippets_cleaning/#theforeign-package","text":"Cannot import SAS, see the sas7bdat package. read.dta ; dta files. read.spss ; sav & por files. Comprehensive. 1 library ( foreign ) Import STATA data with foreign (1) 1 2 # Import florida.dta and name the resulting data frame florida florida <- read.dta ( 'florida.dta' ) Import STATA data with foreign (2) 1 2 3 4 5 6 7 8 9 10 11 # Specify the file path using file.path(): path path <- file.path ( 'worldbank' , 'edequality.dta' ) # Create and print structure of edu_equal_1 edu_equal_1 <- read.dta ( path ) # Create and print structure of edu_equal_2 edu_equal_2 <- read.dta ( path , convert.factors = FALSE ) # Create and print structure of edu_equal_3 edu_equal_3 <- read.dta ( path , convert.underscore = TRUE ) Import SPSS data with foreign (1) 1 2 # Import international.sav as a data frame: demo demo <- read.spss ( 'international.sav' , to.data.frame = TRUE ) Import SPSS data with foreign (2) 1 2 3 4 5 # Import international.sav as demo_1 demo_1 <- read.spss ( 'international.sav' , to.data.frame = TRUE ) # Import international.sav as demo_2 demo_2 <- read.spss ( 'international.sav' , to.data.frame = TRUE , use.value.labels = FALSE )","title":"Theforeign package"},{"location":"io_snippets_cleaning/#importing-data-from-relational-data","text":"","title":"Importing Data from Relational Data"},{"location":"io_snippets_cleaning/#the-dbi-package","text":"dbConnect . dbReadTable . dbGetQuery . dbFetch . dbDisconnect . 1 library ( DBI ) Step 1: Establish a connection 1 2 3 4 # Connect to the MySQL database: con con <- dbConnect ( RMySQL :: MySQL (), dbname = 'tweater' , host = 'courses.csrrinzqubik.us-east-1.rds.amazonaws.com' , port = 3306 , user = 'student' , password = 'datacamp' ) con Step 2: List the database tables 1 2 3 4 5 6 7 8 9 10 11 12 13 # Connect to the MySQL database: con con <- dbConnect ( RMySQL :: MySQL (), dbname = 'tweater' , host = 'courses.csrrinzqubik.us-east-1.rds.amazonaws.com' , port = 3306 , user = 'student' , password = 'datacamp' ) # Build a vector of table names: tables tables <- dbListTables ( con ) # Display structure of tables str ( tables ) Step 3: Import data from a table 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 # Connect to the MySQL database: con con <- dbConnect ( RMySQL :: MySQL (), dbname = 'tweater' , host = 'courses.csrrinzqubik.us-east-1.rds.amazonaws.com' , port = 3306 , user = 'student' , password = 'datacamp' ) # Import the users table from tweater: users users <- dbReadTable ( con , 'users' ) users # Import and print the tweats table from tweater: tweats tweats <- dbReadTable ( con , 'tweats' ) tweats # Import and print the comments table from tweater: comments comments <- dbReadTable ( con , 'comments' ) comments Your very first SQL query 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 con <- dbConnect ( RMySQL :: MySQL (), dbname = 'tweater' , host = 'courses.csrrinzqubik.us-east-1.rds.amazonaws.com' , port = 3306 , user = 'student' , password = 'datacamp' ) # Import post column of tweats where date is higher than '2015-09-21': latest latest <- dbGetQuery ( con , 'SELECT post FROM tweats WHERE date > \\'2015-09-21\\'' ) latest # Import tweat_id column of comments where user_id is 1: elisabeth elisabeth <- dbGetQuery ( con , 'SELECT tweat_id FROM comments WHERE user_id = 1' ) elisabeth More advanced SQL queries 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 # Connect to the database con <- dbConnect ( RMySQL :: MySQL (), dbname = 'tweater' , host = 'courses.csrrinzqubik.us-east-1.rds.amazonaws.com' , port = 3306 , user = 'student' , password = 'datacamp' ) # Create data frame specific specific <- dbGetQuery ( con , 'SELECT message FROM comments WHERE tweat_id = 77 AND user_id > 4' ) specific # Create data frame short short <- dbGetQuery ( con , 'SELECT id, name FROM users WHERE CHAR_LENGTH(name) < 5' ) short Send - Fetch - Clear 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 # Connect to the database con <- dbConnect ( RMySQL :: MySQL (), dbname = 'tweater' , host = 'courses.csrrinzqubik.us-east-1.rds.amazonaws.com' , port = 3306 , user = 'student' , password = 'datacamp' ) # Send query to the database with dbSendQuery(): res res <- dbSendQuery ( con , 'SELECT * FROM comments WHERE user_id > 4' ) # Display information contained in res dbGetInfo ( res ) # Use dbFetch() twice while ( ! dbHasCompleted ( res )) { chunk <- dbFetch ( res , n = 2 ) chunk2 <- dbFetch ( res ) print ( chunk ) } # Clear res dbClearResult ( res ) Be polite and \u2026 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 # Database specifics dbname <- 'tweater' host <- 'courses.csrrinzqubik.us-east-1.rds.amazonaws.com' port <- 3306 user <- 'student' password <- 'datacamp' # Connect to the database con <- dbConnect ( RMySQL :: MySQL (), dbname = 'tweater' , host = 'courses.csrrinzqubik.us-east-1.rds.amazonaws.com' , port = 3306 , user = 'student' , password = 'datacamp' ) # Create the data frame long_tweats long_tweats <- dbGetQuery ( con , 'SELECT post, date FROM tweats WHERE CHAR_LENGTH(post) > 40' ) # Print long_tweats print ( long_tweats ) # Disconnect from the database dbDisconnect ( con ) Other general packages The RODBC package provides access to databases (including Microsoft Access and Microsoft SQL Server) through an ODBC interface. The RJDBC package provides access to databases through a JDBC interface. Specialized packages ROracle provides an interface for Oracle. RMySQL provides access to MySQL. RpostgreSQL to PostgreSQL. RSQLite to SQLite. And there are manu more packages for NoSQL databases such as MongoDB.","title":"The DBI package"},{"location":"io_snippets_cleaning/#importing-data-from-relational-data-more","text":"","title":"Importing Data from Relational Data -- More"},{"location":"io_snippets_cleaning/#dbi","text":"First, change the working directory with setwd . Install the DBI library. Connect and read preliminary results 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 library ( DBI ) library ( sqliter ) # Assign the sqlite database and full path to a variable dbfile = 'chinook.db' # Instantiate the dbDriver to a convenient object sqlite = dbDriver ( 'SQLite' ) # Assign the connection string to a connection object sqlitedb <- dbConnect ( RSQLite :: SQLite (), dbname = dbfile , host = '' , port = 3306 , user = '' , password = '' ) # Request a list of tables using the connection object dbListTables ( sqlitedb ) Extract some data 1 2 3 4 5 6 7 8 # Assign the results of a SQL query to an object results = dbSendQuery ( sqlitedb , \"SELECT * FROM albums\" ) # Return results from a custom object to a data.frame data = fetch ( results ) # Print data frame to console head ( data ) 1 2 3 4 5 # Clear the results and close the connection dbClearResult ( results ) # Disconnect from the database dbDisconnect ( sqlitedb )","title":"DBI"},{"location":"io_snippets_cleaning/#rsqlite","text":"First, change the working directory with setwd . Install the RSQLite library. Connect and read preliminary results 1 2 3 4 5 6 7 8 9 10 11 12 13 14 library ( RSQLite ) library ( sqliter ) # Assign the sqlite database and full path to a variable dbfile = 'chinook.db' # Instantiate the dbDriver to a convenient object sqlite = dbDriver ( 'SQLite' ) # Assign the connection string to a connection object mysqldb = dbConnect ( sqlite , dbfile ) # Request a list of tables using the connection object dbListTables ( sqlitedb ) Extract some data 1 2 3 4 5 6 7 8 9 10 11 12 # Assign the results of a SQL query to an object results = dbSendQuery ( sqlitedb , \"SELECT * FROM albums\" ) # Check the object results dbGetInfo ( results ) # Return results from a custom object to a data.frame data = fetch ( results ) # Print data frame to console head ( data ) 1 2 3 4 5 # Clear the results and close the connection dbClearResult ( results ) # Disconnect from the database dbDisconnect ( sqlitedb )","title":"RSQLite"},{"location":"io_snippets_cleaning/#mysql-with-dbi-or-rmysql","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 library ( DBI ) # Assign the sqlite database and full path to a variable dbfile = 'chinook.db' # Instantiate the dbDriver to a convenient object mysql = dbDriver ( 'MySQL' ) # Assign the connection string to a connection object mysqldb <- dbConnect ( RMySQL :: MySQL (), dbname = dbfile , host = '' , port = 3306 , user = '' , password = '' ) # Request a list of tables using the connection object dbListTables ( mysqldb ) # Request a list of tables using the connection object dbListTables ( mysqldb ) # Disconnect from the database dbDisconnect ( mysqldb ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 library ( RMySQL ) # Assign the sqlite database and full path to a variable dbfile = 'chinook.db' # Instantiate the dbDriver to a convenient object mysql = dbDriver ( 'MySQL' ) # Assign the connection string to a connection object mysqldb = dbConnect ( mysql , dbfile ) # Request a list of tables using the connection object dbListTables ( mysqldb ) # Request a list of tables using the connection object dbListTables ( mysqldb ) # Disconnect from the database dbDisconnect ( mysqldb )","title":"MySQL with DBI or RMySQL"},{"location":"io_snippets_cleaning/#posgresql-with-dbi-or-rpostgresql","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 library ( DBI ) # Assign the sqlite database and full path to a variable dbfile = 'chinook.db' # Instantiate the dbDriver to a convenient object postgresql = dbDriver ( 'PostgreSQL' ) # Assign the connection string to a connection object postgresqldb <- dbConnect ( RPostgreSQL :: PostgreSQL (), dbname = dbfile , host = '' , port = 3306 , user = '' , password = '' ) # Request a list of tables using the connection object dbListTables ( postgresqldb ) # Request a list of tables using the connection object dbListTables ( postgresqldb ) # Disconnect from the database dbDisconnect ( postgresqldb ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 library ( RPostgreSQL ) # Assign the sqlite database and full path to a variable dbfile = 'chinook.db' # Instantiate the dbDriver to a convenient object postgresql = dbDriver ( 'PostgreSQL' ) # Assign the connection string to a connection object postgresqldb = dbConnect ( postgresql , dbfile ) # Request a list of tables using the connection object dbListTables ( postgresqldb ) # Request a list of tables using the connection object dbListTables ( postgresqldb ) # Disconnect from the database dbDisconnect ( postgresqldb )","title":"PosgreSQL with DBI or RPostgreSQL"},{"location":"io_snippets_cleaning/#importing-data-from-the-web","text":"The other package above can download files from the web. The next packages are web-oriented.","title":"Importing Data from the Web"},{"location":"io_snippets_cleaning/#the-httr-package","text":"GET pages and files from the web. Concise. Parse JSON files. Communicate with APIs. 1 library ( httr ) HTTP? httr ! 1 2 3 4 5 6 7 8 9 10 11 # Get the url, save response to resp url <- 'http://docs.datacamp.com/teach/' resp <- GET ( url ) resp # Get the raw content of resp raw_content <- content ( resp , as = 'raw' ) # Print the head of content head ( raw_content ) 1 2 3 4 5 6 7 8 9 10 11 12 13 # Get the url url <- 'https://www.omdbapi.com/?t=Annie+Hall&y=&plot=short&r=json' resp <- GET ( url ) # Print resp resp # Print content of resp as text content ( resp , as = 'text' ) # Print content of resp content ( resp )","title":"The httr package"},{"location":"io_snippets_cleaning/#the-jsonlite-package","text":"Robust. Improve the imported data. fromJSON . from an R object to toJSON prettify . minify . 1 library ( jsonlite ) From JSON to R 1 2 3 4 5 6 7 8 9 10 11 # Convert wine_json to a list: wine wine_json <- '{' name ':' Chateau Migraine ', ' year ':1997, ' alcohol_pct ':12.4, ' color ':' red ', ' awarded ':false}' wine <- fromJSON ( wine_json ) str ( wine ) # Import Quandl data: quandl_data quandl_url <- 'http://www.quandl.com/api/v1/datasets/IWS/INTERNET_INDIA.json?auth_token=i83asDsiWUUyfoypkgMz' quandl_data <- fromJSON ( quandl_url ) str ( quandl_data ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 # Experiment 1 json1 <- '[1, 2, 3, 4, 5, 6]' fromJSON ( json1 ) # Experiment 2 json2 <- '{' a ': [1, 2, 3], ' b ': [4, 5, 6]}' fromJSON ( json2 ) # Experiment 3 json3 <- '[[1, 2], [3, 4]]' fromJSON ( json3 ) # Experiment 4 json4 <- '[{' a ': 1, ' b ': 2}, {' a ': 3, ' b ': 4}, {' a ': 5, ' b ': 6}]' fromJSON ( json4 ) Ask OMDb 1 2 3 4 5 6 7 8 9 10 11 12 13 14 # Definition of the URLs url_sw4 <- 'http://www.omdbapi.com/?i=tt0076759&r=json' url_sw3 <- 'http://www.omdbapi.com/?i=tt0121766&r=json' # Import two URLs with fromJSON(): sw4 and sw3 sw4 <- fromJSON ( url_sw4 ) sw3 <- fromJSON ( url_sw3 ) # Print out the Title element of both lists sw4 $ Title sw3 $ Title # Is the release year of sw4 later than sw3 sw4 $ Year > sw3 $ Year From R to JSON 1 2 3 4 5 6 7 8 9 10 11 12 13 # URL pointing to the .csv file url_csv <- 'http://s3.amazonaws.com/assets.datacamp.com/course/importing_data_into_r/water.csv' # Import the .csv file located at url_csv water <- read.csv ( url_csv , stringsAsFactors = FALSE ) # Generate a summary of water summary ( water ) # Convert the data file according to the requirements water_json <- toJSON ( water ) water_json Minify and prettify 1 2 3 4 5 6 7 8 9 10 11 # Convert mtcars to a pretty JSON: pretty_json pretty_json <- toJSON ( mtcars , pretty = TRUE ) # Print pretty_json pretty_json # Minify pretty_json: mini_json mini_json <- minify ( pretty_json ) # Print mini_json mini_json","title":"The jsonlite package"},{"location":"io_snippets_cleaning/#keyboard-inputting","text":"Coding 1 2 3 4 5 # create a data frame from scratch age <- c ( 25 , 30 , 56 ) gender <- c ( \"male\" , \"female\" , \"male\" ) weight <- c ( 160 , 110 , 220 ) mydata <- data.frame ( age , gender , weight ) Spreadsheet-like 1 2 3 4 5 # enter data using editor mydata <- data.frame ( age = numeric ( 0 ), gender = character ( 0 ), weight = numeric ( 0 )) mydata <- edit ( mydata ) # note that without the assignment in the line above, the edits are not saved!","title":"Keyboard Inputting"},{"location":"io_snippets_cleaning/#exporting-data","text":"","title":"Exporting Data"},{"location":"io_snippets_cleaning/#to-a-tab-delimited-text-file","text":"1 write.table ( mydata , 'c:/mydata.txt' , sep = \"\\t\" )","title":"To a Tab-Delimited Text File"},{"location":"io_snippets_cleaning/#to-an-excel-spreadsheet","text":"1 2 3 library ( xlsx ) write.xlsx ( mydata , \"c:/mydata.xlsx\" ) Worksheet 1 2 3 4 5 6 7 8 9 10 11 12 13 library ( XLConnect ) # xls or xlsx # write a worksheet in steps wb <- loadWorkbook ( 'XLConnectExample1.xls' , create = TRUE ) createSheet ( wb , name = 'chickSheet' ) writeWorksheet ( wb , ChickWeight , sheet = 'chickSheet' , startRow = 3 , startCol = 4 ) saveWorkbook ( wb ) # write a worksheet all in one step ChickWeight <- 1 writeWorksheetToFile ( 'XLConnectExample2.xlsx' , data = ChickWeight , sheet = 'chickSheet' , startRow = 3 , startCol = 4 ) Field 1 2 3 4 5 6 7 8 9 # write a field in steps wb = loadWorkbook ( 'XLConnectExample3.xlsx' , create = TRUE ) createSheet ( wb , name = 'womenData' ) createName ( wb , name = 'womenName' , formula = 'womenData!$C$5' , overwrite = TRUE ) writeNamedRegion ( wb , women , name = \"womenName\" ) saveWorkbook ( wb ) # write a field all in one step writeNamedRegionToFile ( \"XLConnectExample4.xlsx\" , women , name = \"womenName\" , formula = \"womenData!$C$5\" ) I/O 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 # Build connection to latitude.xlsx my_book <- loadWorkbook ( 'latitude.xlsx' ) # Create data frame: summ dims1 <- dim ( readWorksheet ( my_book , 1 )) dims2 <- dim ( readWorksheet ( my_book , 2 )) summ <- data.frame ( sheets = getSheets ( my_book ), nrows = c ( dims1[1] , dims2[1] ), ncols = c ( dims1[2] , dims2[2] )) # Add a worksheet to my_book, named 'data_summary' createSheet ( my_book , name = 'data_summary' ) # Populate 'data_summary' with summ data frame writeWorksheet ( my_book , summ , sheet = 'data_summary' ) # Save workbook as latitude_with_summ.xlsx saveWorkbook ( my_book , 'latitude_with_summ.xlsx' )","title":"To an Excel Spreadsheet"},{"location":"io_snippets_cleaning/#to-spss","text":"1 2 3 library ( foreign ) write.foreign ( mydata , \"c:/mydata.txt\" , \"c:/mydata.sps\" , package = \"SPSS\" )","title":"To SPSS"},{"location":"io_snippets_cleaning/#to-sas","text":"1 2 3 library ( foreign ) write.foreign ( mydata , \"c:/mydata.txt\" , \"c:/mydata.sas\" , package = \"SAS\" )","title":"To SAS"},{"location":"io_snippets_cleaning/#to-stata","text":"1 2 3 library ( foreign ) write.dta ( mydata , \"c:/mydata.dta\" )","title":"To Stata"},{"location":"io_snippets_cleaning/#inspecting-data-missing-data","text":"","title":"Inspecting Data - Missing Data"},{"location":"io_snippets_cleaning/#inspecting","text":"ls(object) . names(object) . str(object) . levels(object$v1) . dim(object) . class ( object ) . print(object) . head(object, 10) . tail(object, 20) . Testing for Missing Values 1 2 y <- c ( 1 , 2 , 3 , NA ) # returns TRUE of x is missing is.na ( y ) # returns a vector (F F F T) Recoding Values to Missing 1 2 3 # recode 99 to missing for variable v1 # select rows where v1 is 99 and recode column v1 mydata $ v1[mydata $ v1 == 99 ] <- NA Excluding Missing Values from Analyses 1 2 3 4 x <- c ( 1 , 2 , NA , 3 ) mean ( x ) # returns NA mean ( x , na.rm = TRUE ) # returns 2 1 2 # list rows of data that have missing values mydata[ ! complete.cases ( mydata ), ] 1 2 # create new dataset without missing data newdata <- na.omit ( mydata )","title":"Inspecting"},{"location":"io_snippets_cleaning/#the-dplyr-package","text":"1 2 3 4 5 library ( dplyr ) tbl_df ( iris ) # almost like head/tail glimpse ( iris ) # almost like str View ( iris ) # open a spreadsheet","title":"The dplyr package"},{"location":"io_snippets_cleaning/#for-thorough-cleaning","text":"The Amelia II software. The mitools package.","title":"For thorough cleaning"},{"location":"io_snippets_cleaning/#labels-levels","text":"Basic 1 2 3 4 5 # variable v1 is coded 1, 2 or 3 # we want to attach value labels 1=red, 2=blue, 3=green mydata $ v1 <- factor ( mydata $ v1 , levels = c ( 1 , 2 , 3 ), labels = c ( \"red\" , \"blue\" , \"green\" )) 1 2 3 4 5 # variable y is coded 1, 3 or 5 # we want to attach value labels 1=Low, 3=Medium, 5=High mydata $ v1 <- ordered ( mydata $ y , levels = c ( 1 , 3 , 5 ), labels = c ( \"Low\" , \"Medium\" , \"High\" )) Order 1 2 3 4 5 # Create a vector of temperature observations temperature_vector <- c ( 'High' , 'Low' , 'High' , 'Low' , 'Medium' ) # Specify that they are ordinal variables with the given levels factor_temperature_vector <- factor ( temperature_vector , order = TRUE , levels = c ( 'Low' , 'Medium' , 'High' )) Add comments to an object 1 2 3 4 names ( iris ) [5] <- \"This is the label for variable 5\" names ( iris ) [5] # the comment iris[5] # the data 1 2 3 4 5 6 7 8 # labeling the variables library ( Hmisc ) label ( iris $ Species ) <- \"Variable label for variable myvar\" describe ( iris $ Species ) # commented #vs describe ( iris $ Sepal.Length ) # not commented","title":"Labels &amp; Levels"},{"location":"io_snippets_cleaning/#how-to-work-with-quandl-in-r","text":"","title":"How to work with Quandl in R"},{"location":"io_snippets_cleaning/#importing-quandl-datasets","text":"Quandl delivers financial, economic and alternative data to the world\u2019s top hedge funds, asset managers and investment banks in several formats: Excel. R. Python. API. DB. The packages used: Quandl . quantmod for plotting. Quandl - A first date 1 2 3 4 5 # Load in the Quandl package library ( Quandl ) # Assign your first dataset to the variable: mydata <- Quandl ( 'NSE/OIL' ) Identifying a dataset with its ID 1 2 # Assign the Prague Stock Exchange to: PragueStockExchange <- Quandl ( 'PRAGUESE/PX' ) Plotting a stock chart 1 2 3 4 5 6 7 8 # The quantmod package library ( quantmod ) # Load the Facebook data with the help of Quandl Facebook <- Quandl ( 'GOOG/NASDAQ_FB' , type = 'xts' ) # Plot the chart with the help of candleChart() candleChart ( Facebook ) Searching a Quandl dataset in R 1 2 3 4 5 6 7 8 # Look up the first 3 results for 'Bitcoin' within the Quandl database: results <- Quandl.search ( query = 'Bitcoin' , silent = FALSE ) # Print out the results str ( results ) # Assign the data set with code BCHAIN/TOTBC BitCoin <- Quandl ( 'BCHAIN/TOTBC' )","title":"Importing Quandl Datasets"},{"location":"io_snippets_cleaning/#manipulating-quandl-datasets","text":"Manipulating data 1 2 # Assign to the variable Exchange Exchange <- Quandl ( 'BNP/USDEUR' , start_date = '2013-01-01' , end_date = '2013-12-01' ) Transforming your Quandl dataset 1 2 3 4 5 6 # API transformation # The result: GDP_Change <- Quandl ( 'FRED/CANRGDPR' , transformation = 'rdiff' ) head ( GDP_Change ) GDP_Chang <- Quandl ( 'FRED/CANRGDPR' ) head ( GDP_Chang ) The magic of frequency collapsing 1 2 # The result: eiaQuarterly <- Quandl ( 'DOE/RWTC' , collapse = 'quarterly' ) Truncation and sort 1 2 3 4 5 # Assign to TruSo the first 5 observations of the crude oil prices TruSo <- Quandl ( 'DOE/RWTC' , sort = 'asc' , rows = 5 ) # Print the result TruSo A complex example 1 2 # Here you should place the return: Final <- Quandl ( 'DOE/RWTC' , collapse = 'daily' , transformation = 'rdiff' , start_date = '2005-01-01' , end_date = '2010-03-01' , sort = 'asc' )","title":"Manipulating Quandl Datasets"},{"location":"io_snippets_cleaning/#cleaning-data-in-r","text":"The packages used: dplyr & tidyr for data wrangling. stringr for regex. lubridate for time and date.","title":"Cleaning Data in R"},{"location":"io_snippets_cleaning/#introduction-and-exploring-raw-data","text":"Here\u2019s what messy data look like 1 2 3 4 5 6 7 8 # View the first 6 rows of data head ( weather ) # View the last 6 rows of data tail ( weather ) # View a condensed summary of the data str ( weather ) Getting a feel for your data 1 2 3 4 5 6 7 8 # Check the class of bmi class ( bmi ) # Check the dimensions of bmi dim ( bmi ) # View the column names of bmi names ( bmi ) Viewing the structure of your data 1 2 3 4 5 6 7 8 9 10 11 # Check the structure of bmi str ( bmi ) # Load dplyr library ( dplyr ) # Check the structure of bmi, the dplyr way glimpse ( bmi ) # View a summary of bmi summary ( bmi ) Looking at your data 1 2 3 4 5 6 7 8 9 10 11 12 13 14 # Print bmi to the console print ( bmi ) # View the first 6 rows head ( bmi , 6 ) # View the first 15 rows head ( bmi , 15 ) # View the last 6 rows tail ( bmi , 6 ) # View the last 10 rows tail ( bmi , 10 ) Visualizing your data 1 2 3 4 5 # Histogram of BMIs from 2008 hist ( bmi $ Y2008 ) # Scatter plot comparing BMIs from 1980 to those from 2008 plot ( bmi $ Y1980 , bmi $ Y2008 )","title":"Introduction and Exploring Raw Data"},{"location":"io_snippets_cleaning/#tidying-data","text":"Gathering columns into key-value pairs 1 2 3 4 5 6 7 8 # Load tidyr library ( tidyr ) # Apply gather() to bmi and save the result as bmi_long bmi_long <- gather ( bmi , year , bmi_val , - Country ) # View the first 20 rows of the result head ( bmi_long , 20 ) Spreading key-value pairs into columns 1 2 3 4 5 # Apply spread() to bmi_long bmi_wide <- spread ( bmi_long , year , bmi_val ) # View the head of bmi_wide head ( bmi_wide ) Separating columns 1 2 3 4 5 # Apply separate() to bmi_cc bmi_cc_clean <- separate ( bmi_cc , col = Country_ISO , into = c ( 'Country' , 'ISO' ), sep = '/' ) # Print the head of the result head ( bmi_cc_clean ) Uniting columns 1 2 3 4 5 # Apply unite() to bmi_cc_clean bmi_cc <- unite ( bmi_cc_clean , Country_ISO , Country , ISO , sep = '-' ) # View the head of the result head ( bmi_cc ) Column headers are values, not variable names 1 2 3 4 5 6 7 8 9 10 11 # View the head of census head ( census ) # Gather the month columns census2 <- gather ( census , month , amount , JAN , FEB , MAR , APR , MAY , JUN , JUL , AUG , SEP , OCT , NOV , DEC ) # Arrange rows by YEAR using dplyr's arrange census2 <- arrange ( census2 , YEAR ) # View first 20 rows of census2 head ( census2 , 20 ) Variables are stored in both rows and columns 1 2 3 4 5 6 7 8 # View first 50 rows of census_long head ( census_long , 50 ) # Spread the type column census_long2 <- spread ( census_long , type , amount ) # View first 20 rows of census_long2 head ( census_long2 , 20 ) Multiple values are stored in one column 1 2 3 4 5 6 7 8 # View the head of census_long3 head ( census_long3 ) # Separate the yr_month column into two census_long4 <- separate ( census_long3 , yr_month , c ( 'year' , 'month' ), '_' ) # View the first 6 rows of the result head ( census_long4 , 6 )","title":"Tidying Data"},{"location":"io_snippets_cleaning/#preparing-data-for-analysis","text":"Types of variables in R 1 2 3 4 5 6 7 8 9 10 11 12 13 14 # Make this evaluate to character class ( 'true' ) # Make this evaluate to numeric class ( 8484.00 ) # Make this evaluate to integer class ( 99L ) # Make this evaluate to factor class ( factor ( 'factor' )) # Make this evaluate to logical class ( FALSE ) Common type conversions 1 2 3 4 5 6 7 8 9 10 11 12 13 14 # Preview students with str() str ( students ) # Coerce Grades to character students $ Grades <- as.character ( students $ Grades ) # Coerce Medu to factor students $ Medu <- as.factor ( students $ Medu ) # Coerce Fedu to factor students $ Fedu <- as.factor ( students $ Fedu ) # Look at students once more with str() str ( students ) Working with dates 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 # Preview students2 with str() str ( students2 ) # Load the lubridate package library ( lubridate ) # Parse as date ymd ( '2015-Sep-17' ) # Parse as date and time (with no seconds!) ymd_hm ( '2012-July-15, 12.56' ) # Coerce dob to a date (with no time) students2 $ dob <- ymd ( students2 $ dob ) # Coerce nurse_visit to a date and time students2 $ nurse_visit <- ymd_hms ( students2 $ nurse_visit ) # Look at students2 once more with str() str ( students2 ) Trimming and padding strings 1 2 3 4 5 6 7 8 # Load the stringr package library ( stringr ) # Trim all leading and trailing whitespace str_trim ( c ( ' Filip ' , 'Nick ' , ' Jonathan' )) # Pad these strings with leading zeros str_pad ( c ( '23485W' , '8823453Q' , '994Z' ), width = 9 , side = 'left' , pad = '0' ) Upper and lower case 1 2 3 4 5 6 7 8 9 # Print state abbreviations states # Make states all uppercase and save result to states_upper states_upper <- toupper ( states ) states_upper # Make states_upper all lowercase again tolower ( states_upper ) Finding and replacing strings 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 # stringr has been loaded for you # Look at the head of students2 head ( students2 ) # Detect all dates of birth (dob) in 1997 str_detect ( students2 $ dob , '1997' ) # In the sex column, replace 'F' with 'Female'... students2 $ sex <- str_replace ( students2 $ sex , 'F' , 'Female' ) # ...And 'M' with 'Male' students2 $ sex <- str_replace ( students2 $ sex , 'M' , 'Male' ) # View the head of students2 head ( students2 ) Finding missing values 1 2 3 4 5 6 7 8 9 10 11 12 # Call is.na() on the full social_df to spot all NAs is.na ( social_df ) # Use the any() function to ask whether there are any NAs in the data any ( is.na ( social_df )) sum ( is.na ( social_df )) # View a summary() of the dataset summary ( social_df ) # Call table() on the status column table ( social_df $ status ) Dealing with missing values 1 2 3 4 5 6 7 8 9 10 11 # Use str_replace() to replace all missing strings in status with NA social_df $ status <- str_replace ( social_df $ status , '^$' , NA ) # Print social_df to the console social_df # Use complete.cases() to see which rows have no missing values complete.cases ( social_df ) # Use na.omit() to remove all rows with any missing values na.omit ( social_df ) Dealing with outliers and obvious errors 1 2 3 4 5 6 7 8 9 10 11 # Look at a summary() of students3 summary ( students3 ) # View a histogram of the age variable hist ( students3 $ age , breaks = 20 ) # View a histogram of the absences variable hist ( students3 $ absences , breaks = 20 ) # View a histogram of absences, but force zeros to be bucketed to the right of zero hist ( students3 $ absences , breaks = 20 , right = FALSE ) Another look at strange values 1 2 3 4 5 # View a boxplot of age boxplot ( students3 $ age ) # View a boxplot of absences boxplot ( students3 $ absences )","title":"Preparing Data for Analysis"},{"location":"io_snippets_cleaning/#putting-it-all-together","text":"Get a feel for the data 1 2 3 4 5 6 7 8 # Verify that weather is a data.frame class ( weather ) # Check the dimensions dim ( weather ) # View the column names names ( weather ) Summarize the data 1 2 3 4 5 6 7 8 9 10 11 # View the structure of the data str ( weather ) # Load dplyr package library ( dplyr ) # Look at the structure using dplyr's glimpse() glimpse ( weather ) # View a summary of the data summary ( weather ) Take a closer look 1 2 3 4 5 6 7 8 9 10 11 # View first 6 rows head ( weather , 6 ) # View first 15 rows head ( weather , 15 ) # View the last 6 rows tail ( weather , 6 ) # View the last 10 rows tail ( weather , 10 ) Column names are values 1 2 3 4 5 6 7 8 # Load the tidyr package library ( tidyr ) # Gather the columns weather2 <- gather ( weather , day , value , X1 : X31 , na.rm = TRUE ) # View the head head ( weather2 ) Values are variable names 1 2 3 4 5 6 7 8 # First remove column of row names weather2 <- weather2[ , -1 ] # Spread the data weather3 <- spread ( weather2 , measure , value ) # View the head head ( weather3 ) Clean up dates 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 # Load the stringr and lubridate packages library ( stringr ) library ( lubridate ) # Remove X's from day column # weather3$day <- str_pad(str_replace(weather3$day, 'X', ''), width = 2, side = 'left', pad = '0') weather3 $ day <- str_replace ( weather3 $ day , 'X' , '' ) # Unite the year, month, and day columns weather4 <- unite ( weather3 , date , year , month , day , sep = '-' ) # Convert date column to proper date format using stringr's ymd() weather4 $ date <- ymd ( weather4 $ date ) # Rearrange columns using dplyr's select() weather5 <- select ( weather4 , date , Events , CloudCover : WindDirDegrees ) # View the head head ( weather5 ) A closer look at column types 1 2 3 4 5 6 7 8 # View the structure of weather5 str ( weather5 ) # Examine the first 20 rows of weather5. Are most of the characters numeric? head ( weather5 , 20 ) # See what happens if we try to convert PrecipitationIn to numeric as.numeric ( weather5 $ PrecipitationIn ) Column type conversions 1 2 3 4 5 6 7 8 9 # The dplyr package is already loaded # Replace T with 0 (T = trace) weather5 $ PrecipitationIn <- str_replace ( weather5 $ PrecipitationIn , 'T' , '0' ) # Convert characters to numerics weather6 <- mutate_each ( weather5 , funs ( as.numeric ), CloudCover : WindDirDegrees ) # Look at result str ( weather6 ) Find missing values 1 2 3 4 5 6 7 8 9 10 11 12 # Count missing values sum ( is.na ( weather6 )) # Find missing values summary ( weather6 ) # Find indices of NAs in Max.Gust.SpeedMPH ind <- which ( is.na ( weather6 $ Max.Gust.SpeedMPH )) ind # Look at the full rows for records missing Max.Gust.SpeedMPH weather6[ind , ] An obvious error 1 2 3 4 5 6 7 8 9 10 11 # Review distibutions for all variables summary ( weather6 ) # Find row with Max.Humidity of 1000 ind <- which ( weather6 $ Max.Humidity == 1000 ) # Look at the data for that day weather6[ind , ] # Change 1000 to 100 weather6 $ Max.Humidity[ind] <- 100 Another obvious error 1 2 3 4 5 6 7 8 9 10 11 # Look at summary of Mean.VisibilityMiles summary ( weather6 $ Mean.VisibilityMiles ) # Get index of row with -1 value ind <- which ( weather6 $ Mean.VisibilityMiles == -1 ) # Look at full row weather6[ind , ] # Set Mean.VisibilityMiles to the appropriate value weather6 $ Mean.VisibilityMiles[ind] <- 10 Check other extreme values 1 2 3 4 5 6 7 8 9 10 11 # Review summary of full data once more summary ( weather6 ) # Look at histogram for MeanDew.PointF hist ( weather6 $ MeanDew.PointF ) # Look at histogram for Min.TemperatureF hist ( weather6 $ Min.TemperatureF ) # Compare to histogram for Mean.TemperatureF hist ( weather6 $ Mean.TemperatureF ) Finishing touches 1 2 3 4 5 6 7 8 # Clean up column names names ( weather6 ) <- new_colnames # Replace empty cells in Events column weather6 $ events[weather6 $ events == '' ] <- 'None' # Print the first 6 rows of weather6 head ( weather6 , 6 )","title":"Putting it All Together"},{"location":"plot_snippets_basics/","text":"Foreword Output options: the \u2018tango\u2019 syntax and the \u2018readable\u2019 theme. Code snippets and results. Some data might necessitate more specialized packages. For explaining data, presenting results, reporting and publishing, we can generate prettier graphics with ggvis or ggplot2 , and interactive packages such as shiny . Plotting Packages \u00b6 Graphics: maps for grids and mapping. diagram for flow charts. plotrix for ternary, polar plots. gplots . pixmap , png , rtiff , ReadImages , EBImage , RImageJ . leaflet . Grid: vcd for mosaic, ternary plots. grImport for vectors. ggplot2 and extensions. lattice and latticeExtra . gridBase . Devices: JavaGD . Cairo . tikzDevice . Interactive: rgl . ggvis . iplots . rggobi . Others: ash for density plots. cluster for dendrograms. copula for multivariate analyses. corrplot for correlations. compositions for geometries, ternary plots. extracat for missing values. soiltexture for ternary plots and more. KernSmooth for histograms-density plots. openair for polar, circular plots. sm for density plots. car for scatter plots. vioplot for boxplots. vcd for mosaic plots and multivariate analyses. hexbin for scatter plots. scatterplot3d for 3D scatter plots. cluster for dendrograms. shiny for interactive plots. ggvis . Data Type & Dataset \u00b6 Data Types \u00b6 continuous vs categorical (or discrete). continuous: float, x-y-z, 3D, map coordinates, trianguar, lat-long, polar, degree-distance, angle-vector. categorical: integer, binary, dichotomic, dummy, factor, ordinal (ordered). Continuous variable characteristics: asymmetry. outliers. multimodality. gaps, missing values. heaping, redundance. rounding, integer. impossibilities, anomalies. errors. \u2026 Categorical variable characteristics: unexpected pattern of results. uneven distribution. extra categories. unbalanced experiments. large numbers of categories. NA, errors, missings\u2026 nominal: no fixed order. ordinal: fixed order (scale of 1 to 5). discrete: counts, integers. dependencies, correlation, associations. causal relationships, outliers, groups, clusters, gaps, barriers, conditional relationship. \u2026 Univariate main plots: histogram. density. qqmath chart. box & whickers chart. bar chart. dot. Bivariate main plots: xy chart. qq chart. Trivariate main plots: cloud. wireframe. countour. level. Multivariate main plots: sploms. parallel charts (coordinate). Specialized plots: frequencies, crosstabs: bar charts, mosaic plots, association plots. correlations: sploms, pairs, correlograms. t-tests, non-parrametric tests of group differences: box plot, density plot. regression: scatter plot. ANOVA: box plots, line plots. Functions \u00b6 Create a new variable 1 2 iris2 <- within ( iris , area <- Petal.Width * Petal.Length ) head ( iris2 , 3 ) 1 2 3 4 ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species area ## 1 5.1 3.5 1.4 0.2 setosa 0.28 ## 2 4.9 3.0 1.4 0.2 setosa 0.28 ## 3 4.7 3.2 1.3 0.2 setosa 0.26 1 2 area <- with ( iris , area <- Petal.Width * Petal.Length ) head ( area , 3 ) 1 ## [1] 0.28 0.28 0.26 Dataset \u00b6 For most examples, we use the mtcars dataset. Prepare the dataset. 1 attach ( mtcars ) Get data attached to a package (an example). 1 data ( gvhd10 , package = 'latticeExtra' ) The Basic Package \u00b6 Basic Plots, Options & Parameters \u00b6 Standardize the parameters (an example) 1 2 # color and tick mark text orientation par ( col = 'black' , las = 1 ) Grid and layout One plot. 1 plot ( hp , mpg , xlab = 'horsepower' , ylab = 'miles per gallon' ) A grid of plots. 1 2 3 4 5 6 7 8 9 par ( mfrow = c ( 2 , 1 )) plot ( mpg , hp , ylab = 'horsepower' , xlab = 'miles per gallon' ) boxplot ( mpg ~ cyl , xlab = 'mile per gallon' , ylab = 'number of cylinders' , horizontal = TRUE ) par ( mfrow = c ( 1 , 2 )) plot ( mpg , hp , ylab = 'horsepower' , xlab = 'miles per gallon' ) boxplot ( mpg ~ cyl , xlab = 'mile per gallon' , ylab = 'number of cylinders' , horizontal = TRUE ) 1 par ( mfrow = c ( 1 , 1 )) Other grids. 1 2 3 4 5 layout ( matrix ( c ( 1 , 1 , 2 , 3 ), 2 , 2 , byrow = TRUE )) plot ( mpg , xlab = 'observations' , ylab = 'miles per gallon' ) plot ( hp , mpg , xlab = 'horsepower' , ylab = 'miles per gallon' ) boxplot ( mpg ~ cyl , ylab = 'mile per gallon' , xlab = 'number of cylinders' ) 1 2 # view matrix ( c ( 1 , 2 , 1 , 3 ), 2 , 2 , byrow = TRUE ) 1 2 3 ## [,1] [,2] ## [1,] 1 2 ## [2,] 1 3 1 2 3 4 5 layout ( matrix ( c ( 1 , 2 , 1 , 3 ), 2 , 2 , byrow = TRUE )) hist ( wt ) hist ( mpg ) hist ( disp ) 1 2 3 4 5 layout ( matrix ( c ( 1 , 1 , 2 , 3 ), 2 , 2 , byrow = TRUE ), widths = c ( 3 , 1 ), heights = c ( 1 , 2 )) hist ( wt ) hist ( mpg ) hist ( disp ) 1 2 nf <- layout ( matrix ( c ( 1 , 1 , 2 , 3 ), 2 , 2 , byrow = TRUE ), widths = lcm ( 12 ), heights = lcm ( 6 )) layout.show ( nf ) 1 2 3 plot ( mpg , xlab = 'observations' , ylab = 'miles per gallon' ) plot ( hp , mpg , xlab = 'horsepower' , ylab = 'miles per gallon' ) boxplot ( mpg ~ cyl , ylab = 'mile per gallon' , xlab = 'number of cylinders' ) Gridview with additional packages. 1 library ( vcd ) 1 mplot ( A , B , C ) See the lattice and latticeExtra packages for built-in facet/gridview. ggplot2 as well. Plot and add ablines 1 2 3 4 5 6 7 plot ( hp , mpg , xlab = 'horsepower' , ylab = 'miles per gallon' ) # abline(h = yvalues, v = xvalues) abline ( lm ( mpg ~ hp )) # main = 'Title' or... title ( 'Title' ) 1 2 3 4 5 plot ( hp , mpg , xlab = 'horsepower' , ylab = 'miles per gallon' ) abline ( h = c ( 20 , 25 )) abline ( v = c ( 50 , 150 )) abline ( v = seq ( 200 , 300 , 50 ), lty = 2 , col = 'blue' ) Add a legend 1 2 3 4 boxplot ( mpg ~ cyl , main = 'Title' , yaxt = 'n' , xlab = 'mile per gallon' , horizontal = TRUE , col = terrain.colors ( 3 )) legend ( 'topright' , inset = 0.05 , title = 'number of cylinders' , c ( '4' , '6' , '8' ), fill = terrain.colors ( 3 ), horiz = TRUE ) Save 1 2 3 4 5 6 7 mygraph <- plot ( hp , mpg , main = 'Title' , xlab = 'horsepower' , ylab = 'miles per gallon' ) pdf ( 'mygraph.pdf' ) png ( 'mygraph.png' ) jpeg ( 'mygraph.jpg' ) bmp ( 'mygraph.bmp' ) postscript ( 'mygraph.ps' ) View in a new window Typing the function will open a new window to render the plot. windows() for Windows. X11() for Linux. quartz() for OS X. 1 2 3 4 # open the new windows windows () plot ( hp , mpg , main = 'Title' , xlab = 'horsepower' , ylab = 'miles per gallon' ) Enrich the plot, add text 1 2 3 4 5 6 7 8 9 10 11 plot ( hp , mpg , main = 'Title' , col.main = 'blue' , sub = 'figure 1' , col.sub = 'blue' , xlab = 'horsepower' , ylab = 'miles per gallon' , col.lab = 'red' , cex.lab = 0.9 , xlim = c ( 50 , 350 ), ylim = c ( 0 , 40 )) text ( 100 , 10 , 'text 1' ) # x and y coordinate mtext ( 'text 2' , 4 , line = 0.5 ) # pos = 1 (bottom), 2 (left), 3 (top), 4 (right); line (margin) With locator() , use the mouse; with 1 for 1 click, 2 for\u2026 Find the coordinates to be entered in the code. For example (after two clicks): 1 2 3 4 5 6 > locator(2) $x [1] 212.5308 293.7854 $y [1] 33.34040 31.87281 1 2 3 4 5 6 plot ( hp , mpg , main = 'Title' , xlab = 'horsepower' , ylab = 'miles per gallon' ) text ( hp , mpg , row.names ( mtcars ), cex = 0.7 , pos = 4 , col = 'red' ) Enrich the plot, add symbols 1 2 3 4 5 6 7 plot ( hp , mpg , main = 'Title' , xlab = 'horsepower' , ylab = 'miles per gallon' ) symbols ( 250 , 20 , squares = 1 , add = TRUE , inches = 0.1 , fg = 'red' ) symbols ( 250 , 25 , circles = 1 , add = TRUE , inches = 0.1 , fg = 'red' ) 1 2 3 4 #rectangles #stars #thermometers #boxplots Combine plots; change pch = & col = 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 par ( mfrow = c ( 2 , 2 )) # 1 plot ( hp , mpg , main = 'P1' , xlab = 'horsepower' , ylab = 'miles per gallon' , pch = 1 , col = 'black' ) # 2 plot ( hp , mpg , main = 'P2' , xlab = 'horsepower' , ylab = 'miles per gallon' , pch = 3 , col = 'blue' , cex = 0.5 ) # 3 plot ( hp , mpg , main = 'P3' , xlab = 'horsepower' , ylab = 'miles per gallon' , pch = 5 , col = 'red' , cex = 2 ) # 4 plot ( hp , mpg , main = 'P4' , xlab = 'horsepower' , ylab = 'miles per gallon' , pch = 7 , col = 'green' ) 1 2 # reverse par ( mfrow = c ( 1 , 1 )) Change col = Change pch = Change lty = 1 2 3 4 5 6 7 8 9 10 11 12 13 par ( fig = c ( 0 , 0.8 , 0 , 0.8 )) plot ( mtcars $ wt , mtcars $ mpg , xlab = 'Car Weight' , ylab = 'miles Per Gallon' ) par ( fig = c ( 0 , 0.8 , 0.55 , 1 ), new = TRUE ) boxplot ( mtcars $ wt , horizontal = TRUE , axes = FALSE ) par ( fig = c ( 0.65 , 1 , 0 , 0.8 ), new = TRUE ) boxplot ( mtcars $ mpg , axes = FALSE ) mtext ( 'Enhanced Scatterplot' , side = 3 , outer = TRUE , line = -3 ) 1 2 # reverse par ( mfrow = c ( 1 , 1 )) Change type = ; without dots 1 2 3 4 5 6 7 8 9 10 11 12 x <- c ( 1 : 5 ); y <- x par ( pch = 22 , col = 'red' ) # plotting symbol and color par ( mfrow = c ( 2 , 4 )) # all plots on one page opts = c ( 'p' , 'l' , 'o' , 'b' , 'c' , 's' , 'S' , 'h' ) for ( i in 1 : length ( opts )) { heading = paste ( 'type =' , opts[i] ) plot ( x , y , type = 'n' , main = heading ) lines ( x , y , type = opts[i] ) } 1 2 # reverse par ( mfrow = c ( 1 , 1 ), col = 'black' ) Change type = ; with dots 1 2 3 4 5 6 7 8 9 10 11 12 x <- c ( 1 : 5 ); y <- x par ( pch = 22 , col = 'blue' ) # plotting symbol and color par ( mfrow = c ( 2 , 4 )) # all plots on one page opts = c ( 'p' , 'l' , 'o' , 'b' , 'c' , 's' , 'S' , 'h' ) for ( i in 1 : length ( opts )) { heading = paste ( 'type =' , opts[i] ) plot ( x , y , main = heading ) lines ( x , y , type = opts[i] ) } 1 2 # reverse par ( mfrow = c ( 1 , 1 ), col = 'black' ) Add or modify the axes 1 2 3 4 5 6 7 8 9 10 plot ( hp , mpg , main = 'Title' , xlab = 'horsepower' , ylab = 'miles per gallon' , xaxt = 'n' , yaxt = 'n' ) axis ( 1 , at = c ( 100 , 200 , 300 ), labels = NULL , pos = 15 , lty = 'dashed' , col = 'green' , las = 2 , tck = -0.05 ) axis ( 4 , at = c ( 20 , 30 ), labels = c ( 'bt' , 'up' ), pos = 125 , lty = 'dashed' , col = 'blue' , las = 2 , tck = -0.05 ) 1 2 # reverse par ( las = 1 ) Add layers to the first plot 1 2 3 4 5 6 7 plot ( mpg , main = 'Title' , xlab = 'horsepower' , ylab = 'miles per gallon' ) # add lines lines ( mpg[1 : 10 ] , type = 'l' , col = 'green' ) Univariate Plots \u00b6 Plot; continuous 1 plot ( mpg , main = 'Title' , xlab = 'observations' , ylab = 'miles per gallon' ) Plot; categorical 1 plot ( cyl , main = 'Title' , xlab = 'observations' , ylab = 'cylinders' ) QQnorm; continuous 1 qqnorm ( mpg , main = 'Title' , xlab = 'observations' , ylab = 'cylinders' ) QQnorm; categorical 1 qqnorm ( cyl , main = 'Title' , xlab = 'observations' , ylab = 'cylinders' ) Stripchart; continuous 1 stripchart ( mpg , main = 'Title' , xlab = 'miles per gallon' ) Stripchart; categorical 1 stripchart ( cyl , main = 'Title' , xlab = 'cylinders' ) Barplot (vertical); continuous 1 barplot ( mpg[1 : 10 ] , main = 'Title' , xlab = 'observations' , ylab = 'miles per gallon' ) Barplot (horizontal); categorical 1 barplot ( cyl[1 : 10 ] , main = 'Title' , horiz = TRUE , xlab = 'cylinders' , ylab = 'observations' ) Barplots options Group with table() . 1 2 counts <- table ( cyl ) counts 1 2 3 ## cyl ## 4 6 8 ## 11 7 14 1 barplot ( counts , main = 'Title' , horiz = TRUE , xlab = 'count' , names.arg = c ( '4 Cyl' , '6 Cyl' , '8 Cyl' )) 1 2 counts <- table ( vs , gear ) counts 1 2 3 4 ## gear ## vs 3 4 5 ## 0 12 2 4 ## 1 3 10 1 1 barplot ( counts , main = 'Title' , xlab = 'gearbox' , col = c ( 'darkblue' , 'red' ), legend = rownames ( counts )) 1 2 counts <- table ( vs , gear ) counts 1 2 3 4 ## gear ## vs 3 4 5 ## 0 12 2 4 ## 1 3 10 1 1 barplot ( counts , main = 'Title' , xlab = 'gearbox' , col = c ( 'darkblue' , 'red' ), legend = rownames ( counts ), beside = TRUE ) Group with aggregate() . 1 aggregate ( mtcars , by = list ( cyl , vs ), FUN = mean , na.rm = TRUE ) 1 2 3 4 5 6 7 8 9 10 11 12 ## Group.1 Group.2 mpg cyl disp hp drat wt qsec ## 1 4 0 26.00000 4 120.30 91.0000 4.430000 2.140000 16.70000 ## 2 6 0 20.56667 6 155.00 131.6667 3.806667 2.755000 16.32667 ## 3 8 0 15.10000 8 353.10 209.2143 3.229286 3.999214 16.77214 ## 4 4 1 26.73000 4 103.62 81.8000 4.035000 2.300300 19.38100 ## 5 6 1 19.12500 6 204.55 115.2500 3.420000 3.388750 19.21500 ## vs am gear carb ## 1 0 1.0000000 5.000000 2.000000 ## 2 0 1.0000000 4.333333 4.666667 ## 3 0 0.1428571 3.285714 3.500000 ## 4 1 0.7000000 4.000000 1.500000 ## 5 1 0.0000000 3.500000 2.500000 1 2 3 4 5 6 par ( las = 2 ) # make label text perpendicular to axis par ( mar = c ( 5 , 8 , 4 , 2 )) # increase y-axis margin. counts <- table ( mtcars $ gear ) barplot ( counts , main = 'Car Distribution' , horiz = TRUE , names.arg = c ( '3 Gears' , '4 Gears' , '5 Gears' ), cex.names = 0.8 ) 1 2 # reverse par ( las = 1 ) Colors. 1 2 3 4 5 6 library ( RColorBrewer ) par ( mfrow = c ( 2 , 1 )) barplot ( iris $ Petal.Length ) barplot ( table ( iris $ Species , iris $ Sepal.Length ), col = brewer.pal ( 3 , 'Set1' )) 1 par ( mfrow = c ( 1 , 1 )) Pie Chart Avoid! Dotchart; continuous 1 dotchart ( mpg , main = 'Title' , xlab = 'miles per gallon' , ylab = 'observations' ) Dotchart; categorical 1 dotchart ( cyl , main = 'Title' , xlab = 'cylinders' , ylab = 'observations' ) Dotchart options 1 dotchart ( mpg , labels = row.names ( mtcars ), cex = 0.7 , main = 'Title' , xlab = 'miles per gallon' ) 1 2 3 4 5 6 7 8 9 10 # sort by mpg x <- mtcars [order ( mpg ), ] # must be factors x $ cyl <- factor ( x $ cyl ) x $ color[x $ cyl == 4 ] <- 'red' x $ color[x $ cyl == 6 ] <- 'blue' x $ color[x $ cyl == 8 ] <- 'darkgreen' dotchart ( x $ mpg , labels = row.names ( x ), cex = 0.7 , groups = x $ cyl , main = 'Title' , xlab = 'miles per gallon' , gcolor = 'black' , color = x $ color ) More with the hmisc package and panel.dotplot() and in the lattice package section. Boxplot; continuous 1 boxplot ( mpg , main = 'Title' , xlab = 'miles per gallon' , ylab = 'observations' ) Stem; continuous 1 stem ( mpg ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 ## ## The decimal point is at the | ## ## 10 | 44 ## 12 | 3 ## 14 | 3702258 ## 16 | 438 ## 18 | 17227 ## 20 | 00445 ## 22 | 88 ## 24 | 4 ## 26 | 03 ## 28 | ## 30 | 44 ## 32 | 49 Histogram; continuous 1 hist ( mpg , main = 'Title' , xlab = 'miles per gallon - bins' , ylab = 'count' ) Histogram; categorical 1 hist ( cyl , main = 'Title' , xlab = 'cylinders - bins' , ylab = 'count' ) Histogram options 1 hist ( mpg , breaks = 12 , col = 'red' ) 1 2 3 4 5 6 7 8 9 x <- mpg h <- hist ( x , breaks = 10 , main = 'Title' , xlab = 'miles per gallon' ) xfit <- seq ( min ( x ), max ( x ), length = 40 ) yfit <- dnorm ( xfit , mean = mean ( x ), sd = sd ( x )) yfit <- yfit * diff ( h $ mids[1 : 2 ] ) * length ( x ) lines ( xfit , yfit , col = 'blue' , lwd = 2 ) Colors. 1 2 3 4 5 6 7 8 9 10 library ( RColorBrewer ) par ( mfrow = c ( 2 , 3 )) hist ( VADeaths , breaks = 10 , col = brewer.pal ( 3 , 'Set3' ), main = '3, Set3' ) hist ( VADeaths , breaks = 4 , col = brewer.pal ( 3 , 'Set2' ), main = '3, Set2' ) hist ( VADeaths , breaks = 8 , col = brewer.pal ( 3 , 'Set1' ), main = '3, Set1' ) hist ( VADeaths , breaks = 2 , col = brewer.pal ( 8 , 'Set3' ), main = '8, Set3' ) hist ( VADeaths , breaks = 10 , col = brewer.pal ( 8 , 'Greys' ), main = '8, Greys' ) hist ( VADeaths , breaks = 10 , col = brewer.pal ( 8 , 'Greens' ), main = '8, Greens' ) 1 par ( mfrow = c ( 1 , 1 )) Density Plot; continuous 1 plot ( density ( mpg ), main = 'Title' ) 1 2 3 plot ( density ( mpg ), main = 'Title' ) polygon ( density ( mpg ), col = 'red' , border = 'blue' ) 1 2 3 4 5 6 d1 <- density ( mtcars $ mpg ) plot ( d1 ) rug ( mtcars $ mpg ) lines ( density ( mtcars $ mpg , d1 $ bw / 2 ), col = 'green' ) lines ( density ( mtcars $ mpg , d1 $ bw / 5 ), col = 'blue' ) Bivariate (Multivariate) Plots \u00b6 Plot, continuous/continuous 1 plot ( mpg , hp , main = 'Title' , xlab = 'miles per gallon' , ylab = 'horsepowers' ) Plot, continuous/categorical 1 plot ( mpg , cyl , main = 'Title' , xlab = 'miles per gallon' , ylab = 'cylinders' ) Plot options 1 2 3 4 plot ( wt , mpg , main = 'Title' , xlab = 'weight' , ylab = 'miles per gallon ' ) abline ( lm ( mpg ~ wt ), col = 'red' ) # regression lines ( lowess ( wt , mpg ), col = 'blue' ) # lowess line SmoothScatter; continuous/continuous 1 smoothScatter ( mpg , hp , main = 'Title' , xlab = 'miles per gallon' , ylab = 'horsepowers' ) Sunflowerplot; categorical/categorical Special symbols at each location: one observation = one dot; more observations = cross, star, etc. 1 sunflowerplot ( gear , cyl , main = 'Title' , xlab = 'gearbox' , ylab = 'cylinders' ) Boxplot 1 boxplot ( mpg ~ cyl , main = 'Title' , xlab = 'cylinders' , ylab = 'miles per gallon' ) Colors. 1 2 3 4 5 6 library ( RColorBrewer ) par ( mfrow = c ( 1 , 2 )) boxplot ( iris $ Sepal.Length , col = 'red' ) boxplot ( iris $ Sepal.Length ~ iris $ Species , col = topo.colors ( 3 )) 1 par ( mfrow = c ( 1 , 1 )) 1 2 3 4 5 6 library ( dplyr ) data ( Pima.tr2 , package = 'MASS' ) PimaV <- select ( Pima.tr2 , glu : age ) boxplot ( scale ( PimaV ), pch = 16 , outcol = 'red' ) Boxplot options 1 2 3 4 5 6 7 four <- subset ( mpg , cyl == 4 ) six <- subset ( mpg , cyl == 6 ) eight <- subset ( mpg , cyl == 8 ) boxplot ( four , six , eight , main = 'Title' , ylab = 'miles per gallon' ) axis ( 1 , at = c ( 1 , 2 , 3 ), labels = c ( '4 Cyl' , '6 Cyl' , '8 Cyl' )) Dotchart 1 2 counts <- table ( gear , cyl ) counts 1 2 3 4 5 ## cyl ## gear 4 6 8 ## 3 1 2 12 ## 4 8 4 0 ## 5 2 1 2 1 dotchart ( counts , main = 'Title' , xlab = 'count' , ylab = 'cylinders/gearbox' ) 1 2 counts <- table ( cyl , gear ) counts 1 2 3 4 5 ## gear ## cyl 3 4 5 ## 4 1 8 2 ## 6 2 4 1 ## 8 12 0 2 1 dotchart ( counts , main = 'Title' , xlab = 'count' , ylab = 'gearbox/cylinders' ) Barplot with its options Vertical or horizontal. The legend as well can be horizontal or vertical. 1 2 counts <- table ( gear , cyl ) counts 1 2 3 4 5 ## cyl ## gear 4 6 8 ## 3 1 2 12 ## 4 8 4 0 ## 5 2 1 2 1 2 3 4 barplot ( counts , main = 'Title' , xlab = 'cylinders' , ylab = 'count' , ylim = c ( 0 , 20 ), col = terrain.colors ( 3 )) legend ( 'topleft' , inset = .04 , title = 'gearbox' , c ( '3' , '4' , '5' ), fill = terrain.colors ( 3 ), horiz = TRUE ) 1 2 counts <- table ( gear , cyl ) counts 1 2 3 4 5 ## cyl ## gear 4 6 8 ## 3 1 2 12 ## 4 8 4 0 ## 5 2 1 2 1 barplot ( counts , main = 'Title' , xlab = 'cylinders' , ylab = 'count' , ylim = c ( 0 , 25 ), col = terrain.colors ( 3 ), legend = rownames ( counts )) 1 2 counts <- table ( gear , cyl ) counts 1 2 3 4 5 ## cyl ## gear 4 6 8 ## 3 1 2 12 ## 4 8 4 0 ## 5 2 1 2 1 barplot ( counts , main = 'Title' , xlab = 'cylinders' , ylab = 'count' , ylim = c ( 0 , 20 ), col = terrain.colors ( 3 ), legend = rownames ( counts ), beside = TRUE ) Spineplot \u2018Count\u2019 = blocks; categorical (with factors). 1 2 3 4 cyl2 <- as.factor ( cyl ) # mandatory for the y gear2 <- as.factor ( gear ) spineplot ( gear2 , cyl2 , main = 'Title' , xlab = 'gearbox' , ylab = 'cylinders' ) Count = blocks; continuous. 1 spineplot ( mpg , cyl2 , main = 'Title' , xlab = 'miles per gallon' , ylab = 'cylinders' ) Mosaicplot Count = blocks. 1 2 counts <- table ( gear , cyl ) counts 1 2 3 4 5 ## cyl ## gear 4 6 8 ## 3 1 2 12 ## 4 8 4 0 ## 5 2 1 2 1 mosaicplot ( counts , main = 'Title' , xlab = 'gearbox' , ylab = 'cylinders' ) Multivariate Plots \u00b6 Pairs 1 pairs ( ~ mpg + disp + hp ) Coplot 1 coplot ( mpg ~ hp | wt ) Correlograms 1 2 3 library ( corrgram ) corrgram ( mtcars , order = TRUE , lower.panel = panel.shade , upper.panel = panel.pie , text.panel = panel.txt , main = 'Car Milage Data in PC2/PC1 Order' ) Plot a dataset with colors 1 2 3 library ( RColorBrewer ) plot ( iris , col = brewer.pal ( 3 , 'Set1' )) Stars The star branches are explanatory; be careful with the interpretation! Well-advised for visual and pattern exploration. 1 mtcars[1 : 4 , c ( 1 , 4 , 6 ) ] 1 2 3 4 5 ## mpg hp wt ## Mazda RX4 21.0 110 2.620 ## Mazda RX4 Wag 21.0 110 2.875 ## Datsun 710 22.8 93 2.320 ## Hornet 4 Drive 21.4 110 3.215 1 stars ( mtcars[1 : 4 , c ( 1 , 4 , 6 ) ] ) Trivariate plots image() . contour() . filled.contour() . persp() . symbols() . Times Series \u00b6 Add packages: zoo and xts . Basics 1 plot ( AirPassengers , type = 'l' ) Change the type = 1 2 3 4 5 6 y1 <- rnorm ( 100 ) par ( mfrow = c ( 2 , 1 )) plot ( y1 , type = 'p' , main = 'p vs l' ) plot ( y1 , type = 'l' ) 1 2 plot ( y1 , type = 'l' , main = 'l vs h' ) plot ( y1 , type = 'h' ) 1 2 plot ( y1 , type = 'l' , lty = 3 , main = 'l 3 vs o' ) plot ( y1 , type = 'o' ) 1 2 plot ( y1 , type = 'b' , main = 'b vs c' ) plot ( y1 , type = 'c' ) 1 2 plot ( y1 , type = 's' , main = 's vs S' ) plot ( y1 , type = 'S' ) 1 2 # reverse par ( mfrow = c ( 1 , 1 )) Add a box 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 y1 <- rnorm ( 100 ) y2 <- rnorm ( 100 ) par ( mfrow = ( c ( 2 , 1 ))) plot ( y1 , type = 'l' , axes = FALSE , xlab = '' , ylab = '' , main = '' ) box ( col = 'gray' ) lines ( x = c ( 20 , 20 , 40 , 40 ), y = c ( -7 , max ( y1 ), max ( y1 ), -7 ), lwd = 3 , col = 'gray' ) plot ( y2 , type = 'l' , axes = FALSE , xlab = '' , ylab = '' , main = '' ) box ( col = 'gray' ) lines ( x = c ( 20 , 20 , 40 , 40 ), y = c ( 7 , min ( y2 ), min ( y2 ), 7 ), lwd = 3 , col = 'gray' ) 1 2 # reverse par ( mfrow = c ( 1 , 1 )) Add lines and text within the plot 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 y1 <- rnorm ( 100 ) # x goes from 0 to 100 # xaxt = 'n' remove the x ticks plot ( y1 , type = 'l' , lwd = 2 , lty = 'longdash' , main = 'Title' , ylab = 'y' , xlab = 'time' , xaxt = 'n' ) abline ( h = 0 , lty = 'longdash' ) abline ( v = 20 , lty = 'longdash' ) abline ( v = 50 , lty = 'longdash' ) abline ( v = 95 , lty = 'longdash' ) text ( 17 , 1.5 , srt = 90 , adj = 0 , labels = 'Tag 1' , cex = 0.8 ) text ( 47 , 1.5 , srt = 90 , adj = 0 , labels = 'Tag a' , cex = 0.8 ) text ( 92 , 1.5 , srt = 90 , adj = 0 , labels = 'Tag alpha' , cex = 0.8 ) A comprehensive example 1 2 # new data head ( Orange ) 1 2 3 4 5 6 7 ## Tree age circumference ## 1 1 118 30 ## 2 1 484 58 ## 3 1 664 87 ## 4 1 1004 115 ## 5 1 1231 120 ## 6 1 1372 142 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 # convert factor to numeric for convenience Orange $ Tree <- as.numeric ( Orange $ Tree ) ntrees <- max ( Orange $ Tree ) # get the range for the x and y axis xrange <- range ( Orange $ age ) yrange <- range ( Orange $ circumference ) # set up the plot plot ( xrange , yrange , type = 'n' , xlab = 'Age (days)' , ylab = 'Circumference (mm)' ) colors <- rainbow ( ntrees ) linetype <- c ( 1 : ntrees ) plotchar <- seq ( 18 , 18 + ntrees , 1 ) # add lines for ( i in 1 : ntrees ) { tree <- subset ( Orange , Tree == i ) lines ( tree $ age , tree $ circumference , type = 'b' , lwd = 1.5 , lty = linetype[i] , col = colors[i] , pch = plotchar[i] ) } # add a title and subtitle title ( 'Tree Growth' , 'example of line plot' ) # add a legend legend ( xrange[1] , yrange[2] , 1 : ntrees , cex = 0.8 , col = colors , pch = plotchar , lty = linetype , title = 'Tree' ) Regressions and Residual Plots \u00b6 1 2 3 4 # first regr <- lm ( mpg ~ hp ) summary ( regr ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 ## ## Call: ## lm(formula = mpg ~ hp) ## ## Residuals: ## Min 1Q Median 3Q Max ## -5.7121 -2.1122 -0.8854 1.5819 8.2360 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 30.09886 1.63392 18.421 &lt; 2e-16 *** ## hp -0.06823 0.01012 -6.742 1.79e-07 *** ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## Residual standard error: 3.863 on 30 degrees of freedom ## Multiple R-squared: 0.6024, Adjusted R-squared: 0.5892 ## F-statistic: 45.46 on 1 and 30 DF, p-value: 1.788e-07 1 2 plot ( mpg ~ hp ) abline ( regr ) 1 2 3 4 par ( mfrow = c ( 2 , 2 )) # then plot ( regr ) 1 2 # reverse par ( mfrow = c ( 1 , 1 )) The lattice and latticeExtra Packages \u00b6 1 library ( lattice ) Coloring \u00b6 1 2 # Show the default settings show.settings () 1 2 3 4 5 6 # Save the default theme mytheme <- trellis.par.get () # Turn the B&W trellis.par.set ( canonical.theme ( color = FALSE )) show.settings () Documentation \u00b6 National Park Service, Advanced Graphics (Lattice) Treillis Plots A note on reordering the levels (factors) \u00b6 1 2 3 4 # start cyl <- mtcars $ cyl cyl <- as.factor ( cyl ) cyl 1 2 ## [1] 6 6 4 6 8 6 8 4 4 6 6 8 8 8 8 8 8 4 4 4 4 8 8 8 8 4 4 4 8 6 8 4 ## Levels: 4 6 8 1 levels ( cyl ) 1 ## [1] \"4\" \"6\" \"8\" 1 2 3 4 5 # option 1 cyl <- factor ( cyl , levels = c ( '8' , '6' , '4' )) # or levels = 3:1 # or levels = letters[3:1] levels ( cyl ) 1 ## [1] \"8\" \"6\" \"4\" 1 2 3 4 5 cyl <- mtcars $ cyl cyl <- as.factor ( cyl ) # option 2 cyl <- reorder ( cyl , new.order = 3 : 1 ) levels ( cyl ) 1 ## [1] \"8\" \"6\" \"4\" 1 2 3 4 library ( lattice ) # normalized x-axis for comparison barchart ( Class ~ Freq | Sex + Age , data = as.data.frame ( Titanic ), groups = Survived , stack = TRUE , layout = c ( 4 , 1 ), auto.key = list ( title = 'Survived' , columns = 2 )) 1 2 # free x-axis barchart ( Class ~ Freq | Sex + Age , data = as.data.frame ( Titanic ), groups = Survived , stack = TRUE , layout = c ( 4 , 1 ), auto.key = list ( title = 'Survived' , columns = 2 ), scales = list ( x = 'free' )) 1 2 3 4 # or bc.titanic <- barchart ( Class ~ Freq | Sex + Age , data = as.data.frame ( Titanic ), groups = Survived , stack = TRUE , layout = c ( 4 , 1 ), auto.key = list ( title = 'Survived' , columns = 2 ), scales = list ( x = 'free' )) bc.titanic 1 2 3 4 5 # add bg grid update ( bc.titanic , panel = function ( ... ) { panel.grid ( h = 0 , v = -1 ) panel.barchart ( ... ) }) 1 2 3 4 # remove lines update ( bc.titanic , panel = function ( ... ) { panel.barchart ( ... , border = 'transparent' ) }) 1 2 # or update ( bc.titanic , border = 'transparent' ) 1 2 Titanic1 <- as.data.frame ( as.table ( Titanic[ , , 'Adult' , ] )) Titanic1 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 ## Class Sex Survived Freq ## 1 1st Male No 118 ## 2 2nd Male No 154 ## 3 3rd Male No 387 ## 4 Crew Male No 670 ## 5 1st Female No 4 ## 6 2nd Female No 13 ## 7 3rd Female No 89 ## 8 Crew Female No 3 ## 9 1st Male Yes 57 ## 10 2nd Male Yes 14 ## 11 3rd Male Yes 75 ## 12 Crew Male Yes 192 ## 13 1st Female Yes 140 ## 14 2nd Female Yes 80 ## 15 3rd Female Yes 76 ## 16 Crew Female Yes 20 1 barchart ( Class ~ Freq | Sex , Titanic1 , groups = Survived , stack = TRUE , auto.key = list ( title = 'Survived' , columns = 2 )) 1 2 3 4 5 Titanic2 <- reshape ( Titanic1 , direction = 'wide' , v.names = 'Freq' , idvar = c ( 'Class' , 'Sex' ), timevar = 'Survived' ) names ( Titanic2 ) <- c ( 'Class' , 'Sex' , 'Dead' , 'Alive' ) barchart ( Class ~ Dead + Alive | Sex , Titanic2 , stack = TRUE , auto.key = list ( columns = 2 )) Uni-, Bi-, Multivariate Plots \u00b6 Barchart Like barplot() . 1 2 # y ~ x barchart ( mpg ~ hp , main = 'Title' , xlab = 'horsepowers' , ylab = 'miles per gallon' ) 1 2 # y ~ x barchart ( mpg ~ hp , main = 'Title' , xlab = 'horsepowers' , ylab = 'miles per gallon' , horizontal = FALSE ) 1 barchart ( VADeaths , groups = FALSE , layout = c ( 1 , 4 ), aspect = 0.7 , reference = FALSE , main = 'Title' , xlab = 'rate per 100' ) 1 2 3 data ( postdoc , package = 'latticeExtra' ) barchart ( prop.table ( postdoc , margin = 1 ), xlab = 'Proportion' , auto.key = list ( adj = 1 )) Change layout = c(x, y, page) 1 barchart ( mpg ~ hp | factor ( cyl ), main = 'Title' , xlab = 'horsepowers' , ylab = 'cylinders - miles per gallon' , layout = c ( 1 , 3 )) 1 barchart ( mpg ~ hp | factor ( cyl ), main = 'Title' , xlab = 'cylinders - horsepowers' , ylab = 'miles per gallon' , layout = c ( 3 , 1 )) Change aspect = 1 1 for square. 1 barchart ( mpg ~ hp | factor ( cyl ), main = 'Title' , xlab = 'horsepowers' , ylab = 'miles per gallon' , layout = c ( 3 , 1 ), aspect = 1 ) Colors 1 barchart ( mpg ~ hp , group = cyl , auto.key = list ( space = 'right' ), main = 'Title' , xlab = 'horsepowers' , ylab = 'miles per gallon' ) shingle() ; control the ranges. equal.count() ; grid. Dotplot Like dotchart() . 1 dotplot ( mpg , main = 'Title' , xlab = 'miles per gallon' ) 1 dotplot ( factor ( cyl ) ~ mpg , main = 'Title' , xlab = 'miles per gallon' , ylab = 'cylinders' ) 1 dotplot ( factor ( cyl ) ~ mpg | factor ( gear ), main = 'Title' , xlab = 'gearbox - miles per gallon' , ylab = 'cylinders' , layout = c ( 3 , 1 )) 1 dotplot ( factor ( cyl ) ~ mpg | factor ( gear ), main = 'Title' , xlab = 'miles per gallon' , ylab = 'gearbox - cylinders' , layout = c ( 1 , 3 ), aspect = 0.3 ) 1 dotplot ( factor ( cyl ) ~ mpg | factor ( gear ), main = 'Title' , xlab = 'miles per gallon' , ylab = 'gearbox - cylinders' , layout = c ( 1 , 3 ), aspect = 0.3 , origin = 0 ) 1 dotplot ( factor ( cyl ) ~ mpg | factor ( gear ), main = 'Title' , xlab = 'miles per gallon' , ylab = 'gearbox - cylinders' , layout = c ( 1 , 3 ), aspect = 0.3 , origin = 0 , type = c ( 'p' , 'h' )) Set auto.key . 1 2 3 4 5 6 7 8 9 # maybe we'll want this later old.pars <- trellis.par.get () #trellis.par.set(superpose.symbol = list(pch = c(1,3), col = 12:14)) trellis.par.set ( superpose.symbol = list ( pch = c ( 1 , 3 ), col = 1 )) # Optionally put things back how they were #trellis.par.set(old.pars) Use auto.key . 1 dotplot ( factor ( cyl ) ~ mpg | factor ( gear ), main = 'Title' , xlab = 'miles per gallon' , ylab = 'gearbox - cylinders' , layout = c ( 1 , 3 ), groups = vs , auto.key = list ( space = 'right' )) 1 trellis.par.set ( old.pars ) 1 2 3 trellis.par.set ( superpose.symbol = list ( pch = c ( 1 , 3 ), col = 1 )) dotplot ( variety ~ yield | site , barley , layout = c ( 1 , 6 ), aspect = c ( 0.7 ), groups = year , auto.key = list ( space = 'right' )) 1 trellis.par.set ( old.pars ) Vertical. 1 dotplot ( mpg ~ factor ( cyl ) | factor ( gear ), main = 'Title' , xlab = 'cylinders' , ylab = 'gearbox - miles per gallon' , layout = c ( 1 , 3 ), aspect = 0.3 ) 1 2 3 4 5 library ( readr ) density <- read_csv ( 'density.csv' ) density $ Density <- as.numeric ( density $ Density ) dotplot ( reorder ( MetropolitanArea , Density ) ~ Density , density , type = c ( 'p' , 'h' ), main = 'Title' , xlab = 'Population Density (pop / sq.mi)' ) 1 dotplot ( reorder ( MetropolitanArea , Density ) ~ Density | Region , density , type = c ( 'p' , 'h' ), strip = FALSE , strip.left = TRUE , layout = c ( 1 , 3 ), scales = list ( y = list ( relation = 'free' )), main = 'Title' , xlab = 'Population Density (pop / sq.mi)' ) Stripplot Like stripchart() . 1 stripplot ( mpg , main = 'Title' , xlab = 'miles per gallon' ) 1 stripplot ( factor ( cyl ) ~ mpg , main = 'Title' , xlab = 'miles per gallon' , ylab = 'cylinders' ) 1 stripplot ( factor ( cyl ) ~ mpg | factor ( gear ), main = 'Title' , xlab = 'gearbox - miles per gallon' , ylab = 'cylinders' , layout = c ( 1 , 3 )) 1 stripplot ( factor ( cyl ) ~ mpg | factor ( gear ), main = 'Title' , xlab = 'gearbox - miles per gallon' , ylab = 'cylinders' , layout = c ( 1 , 3 ), groups = vs , auto.key = list ( space = 'right' )) 1 stripplot ( mpg ~ factor ( cyl ) | factor ( gear ), main = 'Title' , xlab = 'cylinders' , ylab = 'gearbox - miles per gallon' , layout = c ( 1 , 3 )) Histogram Like hist() . 1 histogram ( mpg , main = 'Title' , xlab = 'miles per gallon' ) 1 histogram ( ~ mpg | factor ( cyl ), layout = c ( 1 , 3 ), main = 'Title' , xlab = 'miles per gallon' , ylab = 'density' ) Densityplot Like plot.density() . 1 densityplot ( mpg , main = 'Title' , xlab = 'miles per gallon' , ylab = 'density' ) 1 densityplot ( ~ mpg | factor ( cyl ), layout = c ( 1 , 3 ), main = 'Title' , xlab = 'miles per gallon' , ylab = 'density' ) ECDFplot 1 2 3 library ( latticeExtra ) ecdfplot ( mpg , main = 'Title' , xlab = 'miles per gallon' , ylab = '' ) BWplot Like boxplot . 1 bwplot ( mpg , main = 'Title' , xlab = 'miles per gallon' , ylab = 'density' ) 1 bwplot ( factor ( cyl ) ~ mpg , main = 'Title' , xlab = 'miles per gallon' , ylab = 'cylinders' ) 1 bwplot ( factor ( cyl ) ~ mpg | factor ( gear ), main = 'Title' , xlab = 'miles per gallon' , ylab = 'gearbox - cylinders' , layout = c ( 1 , 3 )) 1 bwplot ( mpg ~ factor ( cyl ) | factor ( gear ), main = 'Title' , xlab = 'gearbox - cylinders' , ylab = 'miles per gallon' , layout = c ( 3 , 1 )) QQmath Like qqnorm() . 1 qqmath ( mpg , main = 'Title' , ylab = 'miles per gallon' ) XYplot Like plot() . 1 xyplot ( mpg ~ disp | factor ( cyl ), main = 'Title' , xlab = 'horsepower' , ylab = 'cylinders - miles per gallon' , layout = c ( 1 , 3 )) 1 xyplot ( mpg ~ disp | factor ( cyl ), main = 'Title' , xlab = 'cylinder - horsepowers' , ylab = 'miles per gallon' , layout = c ( 3 , 1 )) XYplot options 1 xyplot ( mpg ~ disp | factor ( cyl ), main = 'Title' , xlab = 'cylinder - horsepowers' , ylab = 'miles per gallon' , layout = c ( 3 , 1 ), aspect = 1 ) 1 xyplot ( mpg ~ disp | factor ( cyl ), main = 'Title' , xlab = 'cylinder - horsepowers' , ylab = 'miles per gallon' , layout = c ( 3 , 1 ), aspect = 1 , scales = list ( y = list ( at = seq ( 10 , 30 , 10 )))) 1 2 3 4 5 6 7 meanmpg <- mean ( mpg ) xyplot ( mpg ~ disp | factor ( cyl ), main = 'Title' , xlab = 'cylinder - horsepowers' , ylab = 'miles per gallon' , layout = c ( 3 , 1 ), aspect = 1 , panel = function ( ... ) { panel.xyplot ( ... ) panel.abline ( h = meanmpg , lty = 'dashed' ) panel.text ( 450 , meanmpg + 1 , 'avg' , adj = c ( 1 , 0 ), cex = 0.7 ) }) 1 2 3 4 xyplot ( mpg ~ disp | factor ( cyl ), main = 'Title' , xlab = 'cylinder - horsepowers' , ylab = 'miles per gallon' , layout = c ( 3 , 1 ), aspect = 1 , panel = function ( x , y , ... ) { panel.lmline ( x , y ) panel.xyplot ( x , y , ... ) }) panel.points() . panel.lines() . panel.segments() . panel.arrows() . panel.rect() . panel.polygon() . panel.text() . panel.abline() . panel.lmline() . panel.xyplot() . panel.curve() . panel.rug() . panel.grid() . panel.bwplot() . panel.histogram() . panel.loess() . panel.violin() . panel.smoothScatter() . \u2026 par.settings . \u2026 1 2 3 4 5 library ( lattice ) data ( SeatacWeather , package = 'latticeExtra' ) xyplot ( min.temp + max.temp + precip ~ day | month , ylab = 'Temperature and Rainfall' , data = SeatacWeather , layout = c ( 3 , 1 ), type = 'l' , lty = 1 , col = 'black' ) 1 xyplot ( min.temp + max.temp + precip ~ day | month , ylab = 'Temperature and Rainfall' , data = SeatacWeather , layout = c ( 3 , 1 ), type = 'p' , lty = 1 , col = 'black' ) 1 xyplot ( min.temp + max.temp + precip ~ day | month , ylab = 'Temperature and Rainfall' , data = SeatacWeather , layout = c ( 3 , 1 ), type = 'l' , lty = 1 , col = 'black' ) 1 xyplot ( min.temp + max.temp + precip ~ day | month , ylab = 'Temperature and Rainfall' , data = SeatacWeather , layout = c ( 3 , 1 ), type = 'o' , lty = 1 , col = 'black' ) 1 xyplot ( min.temp + max.temp + precip ~ day | month , ylab = 'Temperature and Rainfall' , data = SeatacWeather , layout = c ( 3 , 1 ), type = 'r' , lty = 1 , col = 'black' ) 1 xyplot ( min.temp + max.temp + precip ~ day | month , ylab = 'Temperature and Rainfall' , data = SeatacWeather , layout = c ( 3 , 1 ), type = 'g' , lty = 1 , col = 'black' ) 1 xyplot ( min.temp + max.temp + precip ~ day | month , ylab = 'Temperature and Rainfall' , data = SeatacWeather , layout = c ( 3 , 1 ), type = 's' , lty = 1 , col = 'black' ) 1 xyplot ( min.temp + max.temp + precip ~ day | month , ylab = 'Temperature and Rainfall' , data = SeatacWeather , layout = c ( 3 , 1 ), type = 'S' , lty = 1 , col = 'black' ) 1 xyplot ( min.temp + max.temp + precip ~ day | month , ylab = 'Temperature and Rainfall' , data = SeatacWeather , layout = c ( 3 , 1 ), type = 'h' , lty = 1 , col = 'black' ) 1 xyplot ( min.temp + max.temp + precip ~ day | month , ylab = 'Temperature and Rainfall' , data = SeatacWeather , layout = c ( 3 , 1 ), type = 'a' , lty = 1 , col = 'black' ) 1 xyplot ( min.temp + max.temp + precip ~ day | month , ylab = 'Temperature and Rainfall' , data = SeatacWeather , layout = c ( 3 , 1 ), type = 'smooth' , lty = 1 , col = 'black' ) 1 xyplot ( mpg ~ hp , main = 'Title' , xlab = 'horsepowers' , ylab = 'miles per gallon' ) 1 xyplot ( mpg ~ hp , main = 'Title' , xlab = 'horsepowers' , ylab = 'miles per gallon' , type = 'o' ) 1 xyplot ( mpg ~ hp , main = 'Title' , xlab = 'horsepowers' , ylab = 'miles per gallon' , type = 'o' , pch = 16 , lty = 'dashed' ) 1 xyplot ( mpg ~ hp , main = 'Title' , xlab = 'horsepowers' , ylab = 'miles per gallon' ) 1 2 3 data ( USAge.df , package = 'latticeExtra' ) xyplot ( Population ~ Age | factor ( Year ), USAge.df , groups = Sex , type = c ( 'l' , 'g' ), auto.key = list ( points = FALSE , lines = TRUE , columns = 2 ), aspect = 'xy' , ylab = 'Population (millions)' , subset = Year %in% seq ( 1905 , 1975 , by = 10 )) 1 xyplot ( Population ~ Year | factor ( Age ), USAge.df , groups = Sex , type = 'l' , strip = FALSE , strip.left = TRUE , layout = c ( 1 , 3 ), ylab = 'Population (millions)' , auto.key = list ( lines = TRUE , points = FALSE , columns = 2 ), subset = Age %in% c ( 0 , 10 , 20 )) 1 2 3 4 5 6 7 8 9 data ( USCancerRates , package = 'latticeExtra' ) xyplot ( rate.male ~ rate.female | state , USCancerRates , aspect = 'iso' , pch = '.' , cex = 2 , index.cond = function ( x , y ) { median ( y - x , na.rm = TRUE ) }, scales = list ( log = 2 , at = c ( 75 , 150 , 300 , 600 )), panel = function ( ... ) { panel.grid ( h = -1 , v = -1 ) panel.abline ( 0 , 1 ) panel.xyplot ( ... ) }, xlab = 'a' , ylab = 'b' ) 1 2 3 4 5 data ( biocAccess , package = 'latticeExtra' ) baxy <- xyplot ( log10 ( counts ) ~ hour | month + weekday , biocAccess , type = c ( 'p' , 'a' ), as.table = TRUE , pch = '.' , cex = 2 , col.line = 'black' ) baxy 1 2 library ( latticeExtra ) useOuterStrips ( baxy ) 1 xyplot ( sunspot.year , aspect = 'xy' , strip = FALSE , strip.left = TRUE , cut = list ( number = 4 , overlap = 0.05 )) 1 2 3 4 5 data ( biocAccess , package = 'latticeExtra' ) ssd <- stl ( ts ( biocAccess $ counts[1 : ( 24 * 30 * 2 ) ] , frequency = 24 ), 'periodic' ) xyplot ( ssd , main = 'Title' , xlab = 'Time (Days)' ) Splom 1 splom ( mtcars [c ( 1 , 3 , 6 ) ] , groups = cyl , data = mtcars , panel = panel.superpose , key = list ( title = 'Three Cylinder Options' , columns = 3 , points = list ( text = list ( c ( '4 Cylinder' , '6 Cylinder' , '8 Cylinder' ))))) 1 2 3 trellis.par.set ( superpose.symbol = list ( pch = c ( 1 , 3 , 22 ), col = 1 , alpha = 0.5 )) splom ( ~ data.frame ( mpg , disp , hp , drat , wt , qsec ), data = mtcars , groups = cyl , pscales = 0 , varnames = c ( 'miles\\nper\\ngallon' , 'displacement\\n(cu.in(' , 'horsepower' , 'rear\\naxle\\nratio' , 'weight' , '1/4\\nmile\\ntime' ), auto.key = list ( columns = 3 , title = 'Title' )) 1 trellis.par.set ( old.pars ) 1 splom ( USArrests ) 1 splom ( ~ USArrests [c ( 3 , 1 , 2 , 4 ) ] | state.region , pscales = 0 , type = c ( 'g' , 'p' , 'smooth' )) Parallel plot For multivariate continuous data. 1 parallelplot ( ~ iris[1 : 4 ] ) 1 parallelplot ( ~ iris[1 : 4 ] , horizontal.axis = FALSE ) 1 parallelplot ( ~ iris[1 : 4 ] , scales = list ( x = list ( rot = 90 ))) 1 parallelplot ( ~ iris[1 : 4 ] | Species , iris ) 1 2 parallelplot ( ~ iris[1 : 4 ] , iris , groups = Species , horizontal.axis = FALSE , scales = list ( x = list ( rot = 90 ))) Trivariate plots Like image() , contour() , filled.contour() , persp() , symbols() . levelplot() . contourplot() . cloud() . wireframe() . Additional Packages \u00b6 The sm Package (density) \u00b6 1 library ( sm ) Density plot 1 2 3 4 5 6 7 8 9 10 11 # create value labels cyl.f <- factor ( cyl , levels = c ( 4 , 6 , 8 ), labels = c ( '4 cyl' , '6 cyl' , '8 cyl' )) # plot densities sm.density.compare ( mpg , cyl , xlab = 'miles per gallon' ) title ( main = 'Title' ) # add legend via mouse click colfill <- c ( 2 : ( 2 + length ( levels ( cyl.f )))) legend ( 25 , 0.19 , levels ( cyl.f ), fill = colfill ) The car Package (scatter) \u00b6 1 library ( car ) Scatter plot 1 scatterplot ( mpg ~ wt | cyl , data = mtcars , xlab = 'weight' , ylab = 'miles per gallon' , labels = row.names ( mtcars )) Splom 1 scatterplotMatrix ( ~ mpg + disp + drat + wt | cyl , data = mtcars , main = 'Title' ) scatterplotMatrix == spm . 1 spm ( ~ mpg + disp + drat + wt | cyl , data = mtcars , main = 'Title' ) The vioplot Package (boxplot) \u00b6 1 library ( vioplot ) Violin boxplot 1 2 3 4 5 6 7 x1 <- mpg[mtcars $ cyl == 4 ] x2 <- mpg[mtcars $ cyl == 6 ] x3 <- mpg[mtcars $ cyl == 8 ] vioplot ( x1 , x2 , x3 , names = c ( '4 cyl' , '6 cyl' , '8 cyl' ), col = 'green' ) title ( 'Title' ) The vcd Package (count, correlation, mosaic) \u00b6 1 library ( vcd ) The package provides a variety of methods for visualizing multivariate categorical data. Count 1 2 counts <- table ( gear , cyl ) counts 1 2 3 4 5 ## cyl ## gear 8 6 4 ## 3 12 2 1 ## 4 0 4 8 ## 5 2 1 2 1 mosaic ( counts , shade = TRUE , legend = TRUE ) Correlation 1 2 counts <- table ( gear , cyl ) counts 1 2 3 4 5 ## cyl ## gear 8 6 4 ## 3 12 2 1 ## 4 0 4 8 ## 5 2 1 2 1 assoc ( counts , shade = TRUE ) Mosaic 1 2 3 4 5 6 ucb <- data.frame ( UCBAdmissions ) ucb <- within ( ucb , Accept <- factor ( Admit , levels = c ( 'Rejected' , 'Admitted' ))) library ( vcd ); library ( grid ) doubledecker ( xtabs ( Freq ~ Dept + Gender + Accept , data = ucb ), gp = gpar ( fill = c ( 'grey90' , 'steelblue' ))) 1 2 3 data ( Fertility , package = 'AER' ) doubledecker ( morekids ~ age , data = Fertility , gp = gpar ( fill = c ( 'grey90' , 'green' )), spacing = spacing_equal ( 0 )) 1 doubledecker ( morekids ~ gender1 + gender2 , data = Fertility , gp = gpar ( fill = c ( 'grey90' , 'green' ))) 1 doubledecker ( morekids ~ age + gender1 + gender2 , data = Fertility , gp = gpar ( fill = c ( 'grey90' , 'green' )), spacing = spacing_dimequal ( c ( 0.1 , 0 , 0 , 0 ))) The hexbin Package (scatter) \u00b6 1 library ( hexbin ) Scatter plot 1 2 3 4 5 # new data data ( NHANES ) # compare plot ( Serum.Iron ~ Transferin , NHANES , main = 'Title' , xlab = 'Transferin' , ylab = 'Iron' ) 1 2 # with hexbinplot ( Serum.Iron ~ Transferin , NHANES , main = 'Title' , xlab = 'Transferin' , ylab = 'Iron' ) 1 hexbinplot ( mpg ~ hp , main = 'Title' , xlab = 'horsepowers' , ylab = 'miles per gallon' ) 1 2 3 4 5 x <- rnorm ( 1000 ) y <- rnorm ( 1000 ) bin <- hexbin ( x , y , xbins = 50 ) plot ( bin , main = 'Title' ) 1 2 3 4 x <- rnorm ( 1000 ) y <- rnorm ( 1000 ) plot ( x , y , main = 'Title' , col = rgb ( 0 , 100 , 0 , 50 , maxColorValue = 255 ), pch = 16 ) 1 2 3 4 5 6 7 data ( Diamonds , package = 'Stat2Data' ) a = hexbin ( Diamonds $ PricePerCt , Diamonds $ Carat , xbins = 40 ) library ( RColorBrewer ) plot ( a ) Colors. 1 2 3 rf <- colorRampPalette ( rev ( brewer.pal ( 12 , 'Set3' ))) hexbinplot ( Diamonds $ PricePerCt ~ Diamonds $ Carat , colramp = rf ) Mix lattice and hexbin 1 2 3 data ( gvhd10 , package = 'latticeExtra' ) xyplot ( asinh ( SSC.H ) ~ asinh ( FL2.H ), gvhd10 , aspect = 1 , panel = panel.hexbinplot , .aspect.ratio = 1 , trans = sqrt ) 1 xyplot ( asinh ( SSC.H ) ~ asinh ( FL2.H ) | Days , gvhd10 , aspect = 1 , panel = panel.hexbinplot , .aspect.ratio = 1 , trans = sqrt ) The car Package (scatter) \u00b6 1 library ( car ) Scatter plot 1 2 scatterplotMatrix ( ~ mpg + disp + drat + wt | cyl , data = mtcars , main = 'Three Cylinder Options' ) The scatterplot3d Package \u00b6 1 library ( scatterplot3d ) Scatter plot 1 scatterplot3d ( wt , disp , mpg , main = 'Title' ) 1 scatterplot3d ( wt , disp , mpg , pch = 16 , highlight.3d = TRUE , type = 'h' , main = 'Title' ) 1 2 3 4 5 s3d <- scatterplot3d ( wt , disp , mpg , pch = 16 , highlight.3d = TRUE , type = 'h' , main = ' Title' ) fit <- lm ( mpg ~ wt + disp ) s3d $ plane3d ( fit ) The rgl Package (interactive) \u00b6 1 library ( rgl ) Interactive plot The plot will open a new window. 1 plot3d ( wt , disp , mpg , col = 'red' , size = 3 ) The cluster Package (dendrogram) \u00b6 1 library ( cluster ) Dendrogram Use the iris dataset. 1 2 3 subset <- sample ( 1 : 150 , 20 ) cS <- as.character ( Sp <- iris $ Species[subset] ) cS 1 2 3 4 ## [1] \"setosa\" \"versicolor\" \"setosa\" \"virginica\" \"virginica\" ## [6] \"setosa\" \"setosa\" \"setosa\" \"virginica\" \"setosa\" ## [11] \"versicolor\" \"versicolor\" \"virginica\" \"setosa\" \"versicolor\" ## [16] \"versicolor\" \"setosa\" \"virginica\" \"versicolor\" \"versicolor\" 1 2 3 4 5 6 7 cS[Sp == 'setosa' ] <- 'S' cS[Sp == 'versicolor' ] <- 'V' cS[Sp == 'virginica' ] <- 'g' ai <- agnes ( iris[subset , 1 : 4 ] ) plot ( ai , label = cS ) The extracat Package (splom) \u00b6 1 library ( extracat ) Splom For missing values. Binary matrix with reordering and filtering of rows and columns. The x-axis shows the frequency of NA. The y-axis shows the marginal distribution of NA. 1 2 3 4 # example 1 data ( CHAIN , package = 'mi' ) visna ( CHAIN , sort = 'b' ) 1 summary ( CHAIN ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 ## log_virus age income healthy ## Min. : 0.000 Min. :21.00 Min. : 1.000 Min. :16.67 ## 1st Qu.: 0.000 1st Qu.:37.00 1st Qu.: 2.000 1st Qu.:35.00 ## Median : 0.000 Median :43.00 Median : 3.000 Median :45.37 ## Mean : 4.324 Mean :42.56 Mean : 3.377 Mean :44.40 ## 3rd Qu.: 9.105 3rd Qu.:48.00 3rd Qu.: 5.000 3rd Qu.:54.89 ## Max. :13.442 Max. :70.00 Max. :10.000 Max. :70.11 ## NA's :179 NA's :24 NA's :38 NA's :24 ## mental damage treatment ## Min. :0.0000 Min. :1.000 Min. :0.0000 ## 1st Qu.:0.0000 1st Qu.:3.000 1st Qu.:0.0000 ## Median :0.0000 Median :4.000 Median :1.0000 ## Mean :0.2717 Mean :3.578 Mean :0.8602 ## 3rd Qu.:1.0000 3rd Qu.:5.000 3rd Qu.:2.0000 ## Max. :1.0000 Max. :5.000 Max. :2.0000 ## NA's :24 NA's :63 NA's :24 1 2 3 4 5 6 7 8 9 # example 2 data ( oly12 , package = 'VGAMdata' ) oly12d <- oly12[ , names ( oly12 ) != 'DOB' ] oly12a <- oly12 names ( oly12a ) <- abbreviate ( names ( oly12 ), 3 ) visna ( oly12a , sort = 'b' ) 1 2 3 4 5 6 # example 3 data ( freetrade , package = 'Amelia' ) freetrade <- within ( freetrade , land1 <- reorder ( country , tariff , function ( x ) sum ( is.na ( x )))) fluctile ( xtabs ( is.na ( tariff ) ~ land1 + year , data = freetrade )) 1 ## viewport[base] 1 2 3 4 # example 4 data ( Pima.tr2 , package = 'MASS' ) visna ( Pima.tr2 , sort = 'b' ) The ash Package (density) \u00b6 1 library ( ash ) Density plot 1 plot ( ash1 ( bin1 ( mtcars $ mpg , nbin = 50 )), type = 'l' ) 1 ## [1] \"ash estimate nonzero outside interval ab\" The KernSmooth Package (density) \u00b6 1 library ( KernSmooth ) Density plot 1 2 3 4 5 with ( mtcars , { hist ( mpg , freq = FALSE , main = '' , col = 'bisque2' , ylab = '' ) lines ( density ( mpg ), lwd = 2 ) ks1 <- bkde ( mpg , bandwidth = dpik ( mpg )) lines ( ks1 , col = 'red' , lty = 5 , lwd = 2 )}) The corrplot Package (correlation) \u00b6 1 library ( corrplot ) Splom 1 2 3 4 # Create a correlation matrix for the dataset (9-14 are the '2' variables only) correlations <- cor ( mtcars ) corrplot ( correlations )","title":"Plot Snippets for Exploratory (and some Explanatory) Analyses"},{"location":"plot_snippets_basics/#plotting-packages","text":"Graphics: maps for grids and mapping. diagram for flow charts. plotrix for ternary, polar plots. gplots . pixmap , png , rtiff , ReadImages , EBImage , RImageJ . leaflet . Grid: vcd for mosaic, ternary plots. grImport for vectors. ggplot2 and extensions. lattice and latticeExtra . gridBase . Devices: JavaGD . Cairo . tikzDevice . Interactive: rgl . ggvis . iplots . rggobi . Others: ash for density plots. cluster for dendrograms. copula for multivariate analyses. corrplot for correlations. compositions for geometries, ternary plots. extracat for missing values. soiltexture for ternary plots and more. KernSmooth for histograms-density plots. openair for polar, circular plots. sm for density plots. car for scatter plots. vioplot for boxplots. vcd for mosaic plots and multivariate analyses. hexbin for scatter plots. scatterplot3d for 3D scatter plots. cluster for dendrograms. shiny for interactive plots. ggvis .","title":"Plotting Packages"},{"location":"plot_snippets_basics/#data-type-dataset","text":"","title":"Data Type &amp; Dataset"},{"location":"plot_snippets_basics/#data-types","text":"continuous vs categorical (or discrete). continuous: float, x-y-z, 3D, map coordinates, trianguar, lat-long, polar, degree-distance, angle-vector. categorical: integer, binary, dichotomic, dummy, factor, ordinal (ordered). Continuous variable characteristics: asymmetry. outliers. multimodality. gaps, missing values. heaping, redundance. rounding, integer. impossibilities, anomalies. errors. \u2026 Categorical variable characteristics: unexpected pattern of results. uneven distribution. extra categories. unbalanced experiments. large numbers of categories. NA, errors, missings\u2026 nominal: no fixed order. ordinal: fixed order (scale of 1 to 5). discrete: counts, integers. dependencies, correlation, associations. causal relationships, outliers, groups, clusters, gaps, barriers, conditional relationship. \u2026 Univariate main plots: histogram. density. qqmath chart. box & whickers chart. bar chart. dot. Bivariate main plots: xy chart. qq chart. Trivariate main plots: cloud. wireframe. countour. level. Multivariate main plots: sploms. parallel charts (coordinate). Specialized plots: frequencies, crosstabs: bar charts, mosaic plots, association plots. correlations: sploms, pairs, correlograms. t-tests, non-parrametric tests of group differences: box plot, density plot. regression: scatter plot. ANOVA: box plots, line plots.","title":"Data Types"},{"location":"plot_snippets_basics/#functions","text":"Create a new variable 1 2 iris2 <- within ( iris , area <- Petal.Width * Petal.Length ) head ( iris2 , 3 ) 1 2 3 4 ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species area ## 1 5.1 3.5 1.4 0.2 setosa 0.28 ## 2 4.9 3.0 1.4 0.2 setosa 0.28 ## 3 4.7 3.2 1.3 0.2 setosa 0.26 1 2 area <- with ( iris , area <- Petal.Width * Petal.Length ) head ( area , 3 ) 1 ## [1] 0.28 0.28 0.26","title":"Functions"},{"location":"plot_snippets_basics/#dataset","text":"For most examples, we use the mtcars dataset. Prepare the dataset. 1 attach ( mtcars ) Get data attached to a package (an example). 1 data ( gvhd10 , package = 'latticeExtra' )","title":"Dataset"},{"location":"plot_snippets_basics/#the-basic-package","text":"","title":"The Basic Package"},{"location":"plot_snippets_basics/#basic-plots-options-parameters","text":"Standardize the parameters (an example) 1 2 # color and tick mark text orientation par ( col = 'black' , las = 1 ) Grid and layout One plot. 1 plot ( hp , mpg , xlab = 'horsepower' , ylab = 'miles per gallon' ) A grid of plots. 1 2 3 4 5 6 7 8 9 par ( mfrow = c ( 2 , 1 )) plot ( mpg , hp , ylab = 'horsepower' , xlab = 'miles per gallon' ) boxplot ( mpg ~ cyl , xlab = 'mile per gallon' , ylab = 'number of cylinders' , horizontal = TRUE ) par ( mfrow = c ( 1 , 2 )) plot ( mpg , hp , ylab = 'horsepower' , xlab = 'miles per gallon' ) boxplot ( mpg ~ cyl , xlab = 'mile per gallon' , ylab = 'number of cylinders' , horizontal = TRUE ) 1 par ( mfrow = c ( 1 , 1 )) Other grids. 1 2 3 4 5 layout ( matrix ( c ( 1 , 1 , 2 , 3 ), 2 , 2 , byrow = TRUE )) plot ( mpg , xlab = 'observations' , ylab = 'miles per gallon' ) plot ( hp , mpg , xlab = 'horsepower' , ylab = 'miles per gallon' ) boxplot ( mpg ~ cyl , ylab = 'mile per gallon' , xlab = 'number of cylinders' ) 1 2 # view matrix ( c ( 1 , 2 , 1 , 3 ), 2 , 2 , byrow = TRUE ) 1 2 3 ## [,1] [,2] ## [1,] 1 2 ## [2,] 1 3 1 2 3 4 5 layout ( matrix ( c ( 1 , 2 , 1 , 3 ), 2 , 2 , byrow = TRUE )) hist ( wt ) hist ( mpg ) hist ( disp ) 1 2 3 4 5 layout ( matrix ( c ( 1 , 1 , 2 , 3 ), 2 , 2 , byrow = TRUE ), widths = c ( 3 , 1 ), heights = c ( 1 , 2 )) hist ( wt ) hist ( mpg ) hist ( disp ) 1 2 nf <- layout ( matrix ( c ( 1 , 1 , 2 , 3 ), 2 , 2 , byrow = TRUE ), widths = lcm ( 12 ), heights = lcm ( 6 )) layout.show ( nf ) 1 2 3 plot ( mpg , xlab = 'observations' , ylab = 'miles per gallon' ) plot ( hp , mpg , xlab = 'horsepower' , ylab = 'miles per gallon' ) boxplot ( mpg ~ cyl , ylab = 'mile per gallon' , xlab = 'number of cylinders' ) Gridview with additional packages. 1 library ( vcd ) 1 mplot ( A , B , C ) See the lattice and latticeExtra packages for built-in facet/gridview. ggplot2 as well. Plot and add ablines 1 2 3 4 5 6 7 plot ( hp , mpg , xlab = 'horsepower' , ylab = 'miles per gallon' ) # abline(h = yvalues, v = xvalues) abline ( lm ( mpg ~ hp )) # main = 'Title' or... title ( 'Title' ) 1 2 3 4 5 plot ( hp , mpg , xlab = 'horsepower' , ylab = 'miles per gallon' ) abline ( h = c ( 20 , 25 )) abline ( v = c ( 50 , 150 )) abline ( v = seq ( 200 , 300 , 50 ), lty = 2 , col = 'blue' ) Add a legend 1 2 3 4 boxplot ( mpg ~ cyl , main = 'Title' , yaxt = 'n' , xlab = 'mile per gallon' , horizontal = TRUE , col = terrain.colors ( 3 )) legend ( 'topright' , inset = 0.05 , title = 'number of cylinders' , c ( '4' , '6' , '8' ), fill = terrain.colors ( 3 ), horiz = TRUE ) Save 1 2 3 4 5 6 7 mygraph <- plot ( hp , mpg , main = 'Title' , xlab = 'horsepower' , ylab = 'miles per gallon' ) pdf ( 'mygraph.pdf' ) png ( 'mygraph.png' ) jpeg ( 'mygraph.jpg' ) bmp ( 'mygraph.bmp' ) postscript ( 'mygraph.ps' ) View in a new window Typing the function will open a new window to render the plot. windows() for Windows. X11() for Linux. quartz() for OS X. 1 2 3 4 # open the new windows windows () plot ( hp , mpg , main = 'Title' , xlab = 'horsepower' , ylab = 'miles per gallon' ) Enrich the plot, add text 1 2 3 4 5 6 7 8 9 10 11 plot ( hp , mpg , main = 'Title' , col.main = 'blue' , sub = 'figure 1' , col.sub = 'blue' , xlab = 'horsepower' , ylab = 'miles per gallon' , col.lab = 'red' , cex.lab = 0.9 , xlim = c ( 50 , 350 ), ylim = c ( 0 , 40 )) text ( 100 , 10 , 'text 1' ) # x and y coordinate mtext ( 'text 2' , 4 , line = 0.5 ) # pos = 1 (bottom), 2 (left), 3 (top), 4 (right); line (margin) With locator() , use the mouse; with 1 for 1 click, 2 for\u2026 Find the coordinates to be entered in the code. For example (after two clicks): 1 2 3 4 5 6 > locator(2) $x [1] 212.5308 293.7854 $y [1] 33.34040 31.87281 1 2 3 4 5 6 plot ( hp , mpg , main = 'Title' , xlab = 'horsepower' , ylab = 'miles per gallon' ) text ( hp , mpg , row.names ( mtcars ), cex = 0.7 , pos = 4 , col = 'red' ) Enrich the plot, add symbols 1 2 3 4 5 6 7 plot ( hp , mpg , main = 'Title' , xlab = 'horsepower' , ylab = 'miles per gallon' ) symbols ( 250 , 20 , squares = 1 , add = TRUE , inches = 0.1 , fg = 'red' ) symbols ( 250 , 25 , circles = 1 , add = TRUE , inches = 0.1 , fg = 'red' ) 1 2 3 4 #rectangles #stars #thermometers #boxplots Combine plots; change pch = & col = 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 par ( mfrow = c ( 2 , 2 )) # 1 plot ( hp , mpg , main = 'P1' , xlab = 'horsepower' , ylab = 'miles per gallon' , pch = 1 , col = 'black' ) # 2 plot ( hp , mpg , main = 'P2' , xlab = 'horsepower' , ylab = 'miles per gallon' , pch = 3 , col = 'blue' , cex = 0.5 ) # 3 plot ( hp , mpg , main = 'P3' , xlab = 'horsepower' , ylab = 'miles per gallon' , pch = 5 , col = 'red' , cex = 2 ) # 4 plot ( hp , mpg , main = 'P4' , xlab = 'horsepower' , ylab = 'miles per gallon' , pch = 7 , col = 'green' ) 1 2 # reverse par ( mfrow = c ( 1 , 1 )) Change col = Change pch = Change lty = 1 2 3 4 5 6 7 8 9 10 11 12 13 par ( fig = c ( 0 , 0.8 , 0 , 0.8 )) plot ( mtcars $ wt , mtcars $ mpg , xlab = 'Car Weight' , ylab = 'miles Per Gallon' ) par ( fig = c ( 0 , 0.8 , 0.55 , 1 ), new = TRUE ) boxplot ( mtcars $ wt , horizontal = TRUE , axes = FALSE ) par ( fig = c ( 0.65 , 1 , 0 , 0.8 ), new = TRUE ) boxplot ( mtcars $ mpg , axes = FALSE ) mtext ( 'Enhanced Scatterplot' , side = 3 , outer = TRUE , line = -3 ) 1 2 # reverse par ( mfrow = c ( 1 , 1 )) Change type = ; without dots 1 2 3 4 5 6 7 8 9 10 11 12 x <- c ( 1 : 5 ); y <- x par ( pch = 22 , col = 'red' ) # plotting symbol and color par ( mfrow = c ( 2 , 4 )) # all plots on one page opts = c ( 'p' , 'l' , 'o' , 'b' , 'c' , 's' , 'S' , 'h' ) for ( i in 1 : length ( opts )) { heading = paste ( 'type =' , opts[i] ) plot ( x , y , type = 'n' , main = heading ) lines ( x , y , type = opts[i] ) } 1 2 # reverse par ( mfrow = c ( 1 , 1 ), col = 'black' ) Change type = ; with dots 1 2 3 4 5 6 7 8 9 10 11 12 x <- c ( 1 : 5 ); y <- x par ( pch = 22 , col = 'blue' ) # plotting symbol and color par ( mfrow = c ( 2 , 4 )) # all plots on one page opts = c ( 'p' , 'l' , 'o' , 'b' , 'c' , 's' , 'S' , 'h' ) for ( i in 1 : length ( opts )) { heading = paste ( 'type =' , opts[i] ) plot ( x , y , main = heading ) lines ( x , y , type = opts[i] ) } 1 2 # reverse par ( mfrow = c ( 1 , 1 ), col = 'black' ) Add or modify the axes 1 2 3 4 5 6 7 8 9 10 plot ( hp , mpg , main = 'Title' , xlab = 'horsepower' , ylab = 'miles per gallon' , xaxt = 'n' , yaxt = 'n' ) axis ( 1 , at = c ( 100 , 200 , 300 ), labels = NULL , pos = 15 , lty = 'dashed' , col = 'green' , las = 2 , tck = -0.05 ) axis ( 4 , at = c ( 20 , 30 ), labels = c ( 'bt' , 'up' ), pos = 125 , lty = 'dashed' , col = 'blue' , las = 2 , tck = -0.05 ) 1 2 # reverse par ( las = 1 ) Add layers to the first plot 1 2 3 4 5 6 7 plot ( mpg , main = 'Title' , xlab = 'horsepower' , ylab = 'miles per gallon' ) # add lines lines ( mpg[1 : 10 ] , type = 'l' , col = 'green' )","title":"Basic Plots, Options &amp; Parameters"},{"location":"plot_snippets_basics/#univariate-plots","text":"Plot; continuous 1 plot ( mpg , main = 'Title' , xlab = 'observations' , ylab = 'miles per gallon' ) Plot; categorical 1 plot ( cyl , main = 'Title' , xlab = 'observations' , ylab = 'cylinders' ) QQnorm; continuous 1 qqnorm ( mpg , main = 'Title' , xlab = 'observations' , ylab = 'cylinders' ) QQnorm; categorical 1 qqnorm ( cyl , main = 'Title' , xlab = 'observations' , ylab = 'cylinders' ) Stripchart; continuous 1 stripchart ( mpg , main = 'Title' , xlab = 'miles per gallon' ) Stripchart; categorical 1 stripchart ( cyl , main = 'Title' , xlab = 'cylinders' ) Barplot (vertical); continuous 1 barplot ( mpg[1 : 10 ] , main = 'Title' , xlab = 'observations' , ylab = 'miles per gallon' ) Barplot (horizontal); categorical 1 barplot ( cyl[1 : 10 ] , main = 'Title' , horiz = TRUE , xlab = 'cylinders' , ylab = 'observations' ) Barplots options Group with table() . 1 2 counts <- table ( cyl ) counts 1 2 3 ## cyl ## 4 6 8 ## 11 7 14 1 barplot ( counts , main = 'Title' , horiz = TRUE , xlab = 'count' , names.arg = c ( '4 Cyl' , '6 Cyl' , '8 Cyl' )) 1 2 counts <- table ( vs , gear ) counts 1 2 3 4 ## gear ## vs 3 4 5 ## 0 12 2 4 ## 1 3 10 1 1 barplot ( counts , main = 'Title' , xlab = 'gearbox' , col = c ( 'darkblue' , 'red' ), legend = rownames ( counts )) 1 2 counts <- table ( vs , gear ) counts 1 2 3 4 ## gear ## vs 3 4 5 ## 0 12 2 4 ## 1 3 10 1 1 barplot ( counts , main = 'Title' , xlab = 'gearbox' , col = c ( 'darkblue' , 'red' ), legend = rownames ( counts ), beside = TRUE ) Group with aggregate() . 1 aggregate ( mtcars , by = list ( cyl , vs ), FUN = mean , na.rm = TRUE ) 1 2 3 4 5 6 7 8 9 10 11 12 ## Group.1 Group.2 mpg cyl disp hp drat wt qsec ## 1 4 0 26.00000 4 120.30 91.0000 4.430000 2.140000 16.70000 ## 2 6 0 20.56667 6 155.00 131.6667 3.806667 2.755000 16.32667 ## 3 8 0 15.10000 8 353.10 209.2143 3.229286 3.999214 16.77214 ## 4 4 1 26.73000 4 103.62 81.8000 4.035000 2.300300 19.38100 ## 5 6 1 19.12500 6 204.55 115.2500 3.420000 3.388750 19.21500 ## vs am gear carb ## 1 0 1.0000000 5.000000 2.000000 ## 2 0 1.0000000 4.333333 4.666667 ## 3 0 0.1428571 3.285714 3.500000 ## 4 1 0.7000000 4.000000 1.500000 ## 5 1 0.0000000 3.500000 2.500000 1 2 3 4 5 6 par ( las = 2 ) # make label text perpendicular to axis par ( mar = c ( 5 , 8 , 4 , 2 )) # increase y-axis margin. counts <- table ( mtcars $ gear ) barplot ( counts , main = 'Car Distribution' , horiz = TRUE , names.arg = c ( '3 Gears' , '4 Gears' , '5 Gears' ), cex.names = 0.8 ) 1 2 # reverse par ( las = 1 ) Colors. 1 2 3 4 5 6 library ( RColorBrewer ) par ( mfrow = c ( 2 , 1 )) barplot ( iris $ Petal.Length ) barplot ( table ( iris $ Species , iris $ Sepal.Length ), col = brewer.pal ( 3 , 'Set1' )) 1 par ( mfrow = c ( 1 , 1 )) Pie Chart Avoid! Dotchart; continuous 1 dotchart ( mpg , main = 'Title' , xlab = 'miles per gallon' , ylab = 'observations' ) Dotchart; categorical 1 dotchart ( cyl , main = 'Title' , xlab = 'cylinders' , ylab = 'observations' ) Dotchart options 1 dotchart ( mpg , labels = row.names ( mtcars ), cex = 0.7 , main = 'Title' , xlab = 'miles per gallon' ) 1 2 3 4 5 6 7 8 9 10 # sort by mpg x <- mtcars [order ( mpg ), ] # must be factors x $ cyl <- factor ( x $ cyl ) x $ color[x $ cyl == 4 ] <- 'red' x $ color[x $ cyl == 6 ] <- 'blue' x $ color[x $ cyl == 8 ] <- 'darkgreen' dotchart ( x $ mpg , labels = row.names ( x ), cex = 0.7 , groups = x $ cyl , main = 'Title' , xlab = 'miles per gallon' , gcolor = 'black' , color = x $ color ) More with the hmisc package and panel.dotplot() and in the lattice package section. Boxplot; continuous 1 boxplot ( mpg , main = 'Title' , xlab = 'miles per gallon' , ylab = 'observations' ) Stem; continuous 1 stem ( mpg ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 ## ## The decimal point is at the | ## ## 10 | 44 ## 12 | 3 ## 14 | 3702258 ## 16 | 438 ## 18 | 17227 ## 20 | 00445 ## 22 | 88 ## 24 | 4 ## 26 | 03 ## 28 | ## 30 | 44 ## 32 | 49 Histogram; continuous 1 hist ( mpg , main = 'Title' , xlab = 'miles per gallon - bins' , ylab = 'count' ) Histogram; categorical 1 hist ( cyl , main = 'Title' , xlab = 'cylinders - bins' , ylab = 'count' ) Histogram options 1 hist ( mpg , breaks = 12 , col = 'red' ) 1 2 3 4 5 6 7 8 9 x <- mpg h <- hist ( x , breaks = 10 , main = 'Title' , xlab = 'miles per gallon' ) xfit <- seq ( min ( x ), max ( x ), length = 40 ) yfit <- dnorm ( xfit , mean = mean ( x ), sd = sd ( x )) yfit <- yfit * diff ( h $ mids[1 : 2 ] ) * length ( x ) lines ( xfit , yfit , col = 'blue' , lwd = 2 ) Colors. 1 2 3 4 5 6 7 8 9 10 library ( RColorBrewer ) par ( mfrow = c ( 2 , 3 )) hist ( VADeaths , breaks = 10 , col = brewer.pal ( 3 , 'Set3' ), main = '3, Set3' ) hist ( VADeaths , breaks = 4 , col = brewer.pal ( 3 , 'Set2' ), main = '3, Set2' ) hist ( VADeaths , breaks = 8 , col = brewer.pal ( 3 , 'Set1' ), main = '3, Set1' ) hist ( VADeaths , breaks = 2 , col = brewer.pal ( 8 , 'Set3' ), main = '8, Set3' ) hist ( VADeaths , breaks = 10 , col = brewer.pal ( 8 , 'Greys' ), main = '8, Greys' ) hist ( VADeaths , breaks = 10 , col = brewer.pal ( 8 , 'Greens' ), main = '8, Greens' ) 1 par ( mfrow = c ( 1 , 1 )) Density Plot; continuous 1 plot ( density ( mpg ), main = 'Title' ) 1 2 3 plot ( density ( mpg ), main = 'Title' ) polygon ( density ( mpg ), col = 'red' , border = 'blue' ) 1 2 3 4 5 6 d1 <- density ( mtcars $ mpg ) plot ( d1 ) rug ( mtcars $ mpg ) lines ( density ( mtcars $ mpg , d1 $ bw / 2 ), col = 'green' ) lines ( density ( mtcars $ mpg , d1 $ bw / 5 ), col = 'blue' )","title":"Univariate Plots"},{"location":"plot_snippets_basics/#bivariate-multivariate-plots","text":"Plot, continuous/continuous 1 plot ( mpg , hp , main = 'Title' , xlab = 'miles per gallon' , ylab = 'horsepowers' ) Plot, continuous/categorical 1 plot ( mpg , cyl , main = 'Title' , xlab = 'miles per gallon' , ylab = 'cylinders' ) Plot options 1 2 3 4 plot ( wt , mpg , main = 'Title' , xlab = 'weight' , ylab = 'miles per gallon ' ) abline ( lm ( mpg ~ wt ), col = 'red' ) # regression lines ( lowess ( wt , mpg ), col = 'blue' ) # lowess line SmoothScatter; continuous/continuous 1 smoothScatter ( mpg , hp , main = 'Title' , xlab = 'miles per gallon' , ylab = 'horsepowers' ) Sunflowerplot; categorical/categorical Special symbols at each location: one observation = one dot; more observations = cross, star, etc. 1 sunflowerplot ( gear , cyl , main = 'Title' , xlab = 'gearbox' , ylab = 'cylinders' ) Boxplot 1 boxplot ( mpg ~ cyl , main = 'Title' , xlab = 'cylinders' , ylab = 'miles per gallon' ) Colors. 1 2 3 4 5 6 library ( RColorBrewer ) par ( mfrow = c ( 1 , 2 )) boxplot ( iris $ Sepal.Length , col = 'red' ) boxplot ( iris $ Sepal.Length ~ iris $ Species , col = topo.colors ( 3 )) 1 par ( mfrow = c ( 1 , 1 )) 1 2 3 4 5 6 library ( dplyr ) data ( Pima.tr2 , package = 'MASS' ) PimaV <- select ( Pima.tr2 , glu : age ) boxplot ( scale ( PimaV ), pch = 16 , outcol = 'red' ) Boxplot options 1 2 3 4 5 6 7 four <- subset ( mpg , cyl == 4 ) six <- subset ( mpg , cyl == 6 ) eight <- subset ( mpg , cyl == 8 ) boxplot ( four , six , eight , main = 'Title' , ylab = 'miles per gallon' ) axis ( 1 , at = c ( 1 , 2 , 3 ), labels = c ( '4 Cyl' , '6 Cyl' , '8 Cyl' )) Dotchart 1 2 counts <- table ( gear , cyl ) counts 1 2 3 4 5 ## cyl ## gear 4 6 8 ## 3 1 2 12 ## 4 8 4 0 ## 5 2 1 2 1 dotchart ( counts , main = 'Title' , xlab = 'count' , ylab = 'cylinders/gearbox' ) 1 2 counts <- table ( cyl , gear ) counts 1 2 3 4 5 ## gear ## cyl 3 4 5 ## 4 1 8 2 ## 6 2 4 1 ## 8 12 0 2 1 dotchart ( counts , main = 'Title' , xlab = 'count' , ylab = 'gearbox/cylinders' ) Barplot with its options Vertical or horizontal. The legend as well can be horizontal or vertical. 1 2 counts <- table ( gear , cyl ) counts 1 2 3 4 5 ## cyl ## gear 4 6 8 ## 3 1 2 12 ## 4 8 4 0 ## 5 2 1 2 1 2 3 4 barplot ( counts , main = 'Title' , xlab = 'cylinders' , ylab = 'count' , ylim = c ( 0 , 20 ), col = terrain.colors ( 3 )) legend ( 'topleft' , inset = .04 , title = 'gearbox' , c ( '3' , '4' , '5' ), fill = terrain.colors ( 3 ), horiz = TRUE ) 1 2 counts <- table ( gear , cyl ) counts 1 2 3 4 5 ## cyl ## gear 4 6 8 ## 3 1 2 12 ## 4 8 4 0 ## 5 2 1 2 1 barplot ( counts , main = 'Title' , xlab = 'cylinders' , ylab = 'count' , ylim = c ( 0 , 25 ), col = terrain.colors ( 3 ), legend = rownames ( counts )) 1 2 counts <- table ( gear , cyl ) counts 1 2 3 4 5 ## cyl ## gear 4 6 8 ## 3 1 2 12 ## 4 8 4 0 ## 5 2 1 2 1 barplot ( counts , main = 'Title' , xlab = 'cylinders' , ylab = 'count' , ylim = c ( 0 , 20 ), col = terrain.colors ( 3 ), legend = rownames ( counts ), beside = TRUE ) Spineplot \u2018Count\u2019 = blocks; categorical (with factors). 1 2 3 4 cyl2 <- as.factor ( cyl ) # mandatory for the y gear2 <- as.factor ( gear ) spineplot ( gear2 , cyl2 , main = 'Title' , xlab = 'gearbox' , ylab = 'cylinders' ) Count = blocks; continuous. 1 spineplot ( mpg , cyl2 , main = 'Title' , xlab = 'miles per gallon' , ylab = 'cylinders' ) Mosaicplot Count = blocks. 1 2 counts <- table ( gear , cyl ) counts 1 2 3 4 5 ## cyl ## gear 4 6 8 ## 3 1 2 12 ## 4 8 4 0 ## 5 2 1 2 1 mosaicplot ( counts , main = 'Title' , xlab = 'gearbox' , ylab = 'cylinders' )","title":"Bivariate (Multivariate) Plots"},{"location":"plot_snippets_basics/#multivariate-plots","text":"Pairs 1 pairs ( ~ mpg + disp + hp ) Coplot 1 coplot ( mpg ~ hp | wt ) Correlograms 1 2 3 library ( corrgram ) corrgram ( mtcars , order = TRUE , lower.panel = panel.shade , upper.panel = panel.pie , text.panel = panel.txt , main = 'Car Milage Data in PC2/PC1 Order' ) Plot a dataset with colors 1 2 3 library ( RColorBrewer ) plot ( iris , col = brewer.pal ( 3 , 'Set1' )) Stars The star branches are explanatory; be careful with the interpretation! Well-advised for visual and pattern exploration. 1 mtcars[1 : 4 , c ( 1 , 4 , 6 ) ] 1 2 3 4 5 ## mpg hp wt ## Mazda RX4 21.0 110 2.620 ## Mazda RX4 Wag 21.0 110 2.875 ## Datsun 710 22.8 93 2.320 ## Hornet 4 Drive 21.4 110 3.215 1 stars ( mtcars[1 : 4 , c ( 1 , 4 , 6 ) ] ) Trivariate plots image() . contour() . filled.contour() . persp() . symbols() .","title":"Multivariate Plots"},{"location":"plot_snippets_basics/#times-series","text":"Add packages: zoo and xts . Basics 1 plot ( AirPassengers , type = 'l' ) Change the type = 1 2 3 4 5 6 y1 <- rnorm ( 100 ) par ( mfrow = c ( 2 , 1 )) plot ( y1 , type = 'p' , main = 'p vs l' ) plot ( y1 , type = 'l' ) 1 2 plot ( y1 , type = 'l' , main = 'l vs h' ) plot ( y1 , type = 'h' ) 1 2 plot ( y1 , type = 'l' , lty = 3 , main = 'l 3 vs o' ) plot ( y1 , type = 'o' ) 1 2 plot ( y1 , type = 'b' , main = 'b vs c' ) plot ( y1 , type = 'c' ) 1 2 plot ( y1 , type = 's' , main = 's vs S' ) plot ( y1 , type = 'S' ) 1 2 # reverse par ( mfrow = c ( 1 , 1 )) Add a box 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 y1 <- rnorm ( 100 ) y2 <- rnorm ( 100 ) par ( mfrow = ( c ( 2 , 1 ))) plot ( y1 , type = 'l' , axes = FALSE , xlab = '' , ylab = '' , main = '' ) box ( col = 'gray' ) lines ( x = c ( 20 , 20 , 40 , 40 ), y = c ( -7 , max ( y1 ), max ( y1 ), -7 ), lwd = 3 , col = 'gray' ) plot ( y2 , type = 'l' , axes = FALSE , xlab = '' , ylab = '' , main = '' ) box ( col = 'gray' ) lines ( x = c ( 20 , 20 , 40 , 40 ), y = c ( 7 , min ( y2 ), min ( y2 ), 7 ), lwd = 3 , col = 'gray' ) 1 2 # reverse par ( mfrow = c ( 1 , 1 )) Add lines and text within the plot 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 y1 <- rnorm ( 100 ) # x goes from 0 to 100 # xaxt = 'n' remove the x ticks plot ( y1 , type = 'l' , lwd = 2 , lty = 'longdash' , main = 'Title' , ylab = 'y' , xlab = 'time' , xaxt = 'n' ) abline ( h = 0 , lty = 'longdash' ) abline ( v = 20 , lty = 'longdash' ) abline ( v = 50 , lty = 'longdash' ) abline ( v = 95 , lty = 'longdash' ) text ( 17 , 1.5 , srt = 90 , adj = 0 , labels = 'Tag 1' , cex = 0.8 ) text ( 47 , 1.5 , srt = 90 , adj = 0 , labels = 'Tag a' , cex = 0.8 ) text ( 92 , 1.5 , srt = 90 , adj = 0 , labels = 'Tag alpha' , cex = 0.8 ) A comprehensive example 1 2 # new data head ( Orange ) 1 2 3 4 5 6 7 ## Tree age circumference ## 1 1 118 30 ## 2 1 484 58 ## 3 1 664 87 ## 4 1 1004 115 ## 5 1 1231 120 ## 6 1 1372 142 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 # convert factor to numeric for convenience Orange $ Tree <- as.numeric ( Orange $ Tree ) ntrees <- max ( Orange $ Tree ) # get the range for the x and y axis xrange <- range ( Orange $ age ) yrange <- range ( Orange $ circumference ) # set up the plot plot ( xrange , yrange , type = 'n' , xlab = 'Age (days)' , ylab = 'Circumference (mm)' ) colors <- rainbow ( ntrees ) linetype <- c ( 1 : ntrees ) plotchar <- seq ( 18 , 18 + ntrees , 1 ) # add lines for ( i in 1 : ntrees ) { tree <- subset ( Orange , Tree == i ) lines ( tree $ age , tree $ circumference , type = 'b' , lwd = 1.5 , lty = linetype[i] , col = colors[i] , pch = plotchar[i] ) } # add a title and subtitle title ( 'Tree Growth' , 'example of line plot' ) # add a legend legend ( xrange[1] , yrange[2] , 1 : ntrees , cex = 0.8 , col = colors , pch = plotchar , lty = linetype , title = 'Tree' )","title":"Times Series"},{"location":"plot_snippets_basics/#regressions-and-residual-plots","text":"1 2 3 4 # first regr <- lm ( mpg ~ hp ) summary ( regr ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 ## ## Call: ## lm(formula = mpg ~ hp) ## ## Residuals: ## Min 1Q Median 3Q Max ## -5.7121 -2.1122 -0.8854 1.5819 8.2360 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 30.09886 1.63392 18.421 &lt; 2e-16 *** ## hp -0.06823 0.01012 -6.742 1.79e-07 *** ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## Residual standard error: 3.863 on 30 degrees of freedom ## Multiple R-squared: 0.6024, Adjusted R-squared: 0.5892 ## F-statistic: 45.46 on 1 and 30 DF, p-value: 1.788e-07 1 2 plot ( mpg ~ hp ) abline ( regr ) 1 2 3 4 par ( mfrow = c ( 2 , 2 )) # then plot ( regr ) 1 2 # reverse par ( mfrow = c ( 1 , 1 ))","title":"Regressions and Residual Plots"},{"location":"plot_snippets_basics/#the-lattice-and-latticeextra-packages","text":"1 library ( lattice )","title":"The lattice and latticeExtra Packages"},{"location":"plot_snippets_basics/#coloring","text":"1 2 # Show the default settings show.settings () 1 2 3 4 5 6 # Save the default theme mytheme <- trellis.par.get () # Turn the B&W trellis.par.set ( canonical.theme ( color = FALSE )) show.settings ()","title":"Coloring"},{"location":"plot_snippets_basics/#documentation","text":"National Park Service, Advanced Graphics (Lattice) Treillis Plots","title":"Documentation"},{"location":"plot_snippets_basics/#a-note-on-reordering-the-levels-factors","text":"1 2 3 4 # start cyl <- mtcars $ cyl cyl <- as.factor ( cyl ) cyl 1 2 ## [1] 6 6 4 6 8 6 8 4 4 6 6 8 8 8 8 8 8 4 4 4 4 8 8 8 8 4 4 4 8 6 8 4 ## Levels: 4 6 8 1 levels ( cyl ) 1 ## [1] \"4\" \"6\" \"8\" 1 2 3 4 5 # option 1 cyl <- factor ( cyl , levels = c ( '8' , '6' , '4' )) # or levels = 3:1 # or levels = letters[3:1] levels ( cyl ) 1 ## [1] \"8\" \"6\" \"4\" 1 2 3 4 5 cyl <- mtcars $ cyl cyl <- as.factor ( cyl ) # option 2 cyl <- reorder ( cyl , new.order = 3 : 1 ) levels ( cyl ) 1 ## [1] \"8\" \"6\" \"4\" 1 2 3 4 library ( lattice ) # normalized x-axis for comparison barchart ( Class ~ Freq | Sex + Age , data = as.data.frame ( Titanic ), groups = Survived , stack = TRUE , layout = c ( 4 , 1 ), auto.key = list ( title = 'Survived' , columns = 2 )) 1 2 # free x-axis barchart ( Class ~ Freq | Sex + Age , data = as.data.frame ( Titanic ), groups = Survived , stack = TRUE , layout = c ( 4 , 1 ), auto.key = list ( title = 'Survived' , columns = 2 ), scales = list ( x = 'free' )) 1 2 3 4 # or bc.titanic <- barchart ( Class ~ Freq | Sex + Age , data = as.data.frame ( Titanic ), groups = Survived , stack = TRUE , layout = c ( 4 , 1 ), auto.key = list ( title = 'Survived' , columns = 2 ), scales = list ( x = 'free' )) bc.titanic 1 2 3 4 5 # add bg grid update ( bc.titanic , panel = function ( ... ) { panel.grid ( h = 0 , v = -1 ) panel.barchart ( ... ) }) 1 2 3 4 # remove lines update ( bc.titanic , panel = function ( ... ) { panel.barchart ( ... , border = 'transparent' ) }) 1 2 # or update ( bc.titanic , border = 'transparent' ) 1 2 Titanic1 <- as.data.frame ( as.table ( Titanic[ , , 'Adult' , ] )) Titanic1 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 ## Class Sex Survived Freq ## 1 1st Male No 118 ## 2 2nd Male No 154 ## 3 3rd Male No 387 ## 4 Crew Male No 670 ## 5 1st Female No 4 ## 6 2nd Female No 13 ## 7 3rd Female No 89 ## 8 Crew Female No 3 ## 9 1st Male Yes 57 ## 10 2nd Male Yes 14 ## 11 3rd Male Yes 75 ## 12 Crew Male Yes 192 ## 13 1st Female Yes 140 ## 14 2nd Female Yes 80 ## 15 3rd Female Yes 76 ## 16 Crew Female Yes 20 1 barchart ( Class ~ Freq | Sex , Titanic1 , groups = Survived , stack = TRUE , auto.key = list ( title = 'Survived' , columns = 2 )) 1 2 3 4 5 Titanic2 <- reshape ( Titanic1 , direction = 'wide' , v.names = 'Freq' , idvar = c ( 'Class' , 'Sex' ), timevar = 'Survived' ) names ( Titanic2 ) <- c ( 'Class' , 'Sex' , 'Dead' , 'Alive' ) barchart ( Class ~ Dead + Alive | Sex , Titanic2 , stack = TRUE , auto.key = list ( columns = 2 ))","title":"A note on reordering the levels (factors)"},{"location":"plot_snippets_basics/#uni-bi-multivariate-plots","text":"Barchart Like barplot() . 1 2 # y ~ x barchart ( mpg ~ hp , main = 'Title' , xlab = 'horsepowers' , ylab = 'miles per gallon' ) 1 2 # y ~ x barchart ( mpg ~ hp , main = 'Title' , xlab = 'horsepowers' , ylab = 'miles per gallon' , horizontal = FALSE ) 1 barchart ( VADeaths , groups = FALSE , layout = c ( 1 , 4 ), aspect = 0.7 , reference = FALSE , main = 'Title' , xlab = 'rate per 100' ) 1 2 3 data ( postdoc , package = 'latticeExtra' ) barchart ( prop.table ( postdoc , margin = 1 ), xlab = 'Proportion' , auto.key = list ( adj = 1 )) Change layout = c(x, y, page) 1 barchart ( mpg ~ hp | factor ( cyl ), main = 'Title' , xlab = 'horsepowers' , ylab = 'cylinders - miles per gallon' , layout = c ( 1 , 3 )) 1 barchart ( mpg ~ hp | factor ( cyl ), main = 'Title' , xlab = 'cylinders - horsepowers' , ylab = 'miles per gallon' , layout = c ( 3 , 1 )) Change aspect = 1 1 for square. 1 barchart ( mpg ~ hp | factor ( cyl ), main = 'Title' , xlab = 'horsepowers' , ylab = 'miles per gallon' , layout = c ( 3 , 1 ), aspect = 1 ) Colors 1 barchart ( mpg ~ hp , group = cyl , auto.key = list ( space = 'right' ), main = 'Title' , xlab = 'horsepowers' , ylab = 'miles per gallon' ) shingle() ; control the ranges. equal.count() ; grid. Dotplot Like dotchart() . 1 dotplot ( mpg , main = 'Title' , xlab = 'miles per gallon' ) 1 dotplot ( factor ( cyl ) ~ mpg , main = 'Title' , xlab = 'miles per gallon' , ylab = 'cylinders' ) 1 dotplot ( factor ( cyl ) ~ mpg | factor ( gear ), main = 'Title' , xlab = 'gearbox - miles per gallon' , ylab = 'cylinders' , layout = c ( 3 , 1 )) 1 dotplot ( factor ( cyl ) ~ mpg | factor ( gear ), main = 'Title' , xlab = 'miles per gallon' , ylab = 'gearbox - cylinders' , layout = c ( 1 , 3 ), aspect = 0.3 ) 1 dotplot ( factor ( cyl ) ~ mpg | factor ( gear ), main = 'Title' , xlab = 'miles per gallon' , ylab = 'gearbox - cylinders' , layout = c ( 1 , 3 ), aspect = 0.3 , origin = 0 ) 1 dotplot ( factor ( cyl ) ~ mpg | factor ( gear ), main = 'Title' , xlab = 'miles per gallon' , ylab = 'gearbox - cylinders' , layout = c ( 1 , 3 ), aspect = 0.3 , origin = 0 , type = c ( 'p' , 'h' )) Set auto.key . 1 2 3 4 5 6 7 8 9 # maybe we'll want this later old.pars <- trellis.par.get () #trellis.par.set(superpose.symbol = list(pch = c(1,3), col = 12:14)) trellis.par.set ( superpose.symbol = list ( pch = c ( 1 , 3 ), col = 1 )) # Optionally put things back how they were #trellis.par.set(old.pars) Use auto.key . 1 dotplot ( factor ( cyl ) ~ mpg | factor ( gear ), main = 'Title' , xlab = 'miles per gallon' , ylab = 'gearbox - cylinders' , layout = c ( 1 , 3 ), groups = vs , auto.key = list ( space = 'right' )) 1 trellis.par.set ( old.pars ) 1 2 3 trellis.par.set ( superpose.symbol = list ( pch = c ( 1 , 3 ), col = 1 )) dotplot ( variety ~ yield | site , barley , layout = c ( 1 , 6 ), aspect = c ( 0.7 ), groups = year , auto.key = list ( space = 'right' )) 1 trellis.par.set ( old.pars ) Vertical. 1 dotplot ( mpg ~ factor ( cyl ) | factor ( gear ), main = 'Title' , xlab = 'cylinders' , ylab = 'gearbox - miles per gallon' , layout = c ( 1 , 3 ), aspect = 0.3 ) 1 2 3 4 5 library ( readr ) density <- read_csv ( 'density.csv' ) density $ Density <- as.numeric ( density $ Density ) dotplot ( reorder ( MetropolitanArea , Density ) ~ Density , density , type = c ( 'p' , 'h' ), main = 'Title' , xlab = 'Population Density (pop / sq.mi)' ) 1 dotplot ( reorder ( MetropolitanArea , Density ) ~ Density | Region , density , type = c ( 'p' , 'h' ), strip = FALSE , strip.left = TRUE , layout = c ( 1 , 3 ), scales = list ( y = list ( relation = 'free' )), main = 'Title' , xlab = 'Population Density (pop / sq.mi)' ) Stripplot Like stripchart() . 1 stripplot ( mpg , main = 'Title' , xlab = 'miles per gallon' ) 1 stripplot ( factor ( cyl ) ~ mpg , main = 'Title' , xlab = 'miles per gallon' , ylab = 'cylinders' ) 1 stripplot ( factor ( cyl ) ~ mpg | factor ( gear ), main = 'Title' , xlab = 'gearbox - miles per gallon' , ylab = 'cylinders' , layout = c ( 1 , 3 )) 1 stripplot ( factor ( cyl ) ~ mpg | factor ( gear ), main = 'Title' , xlab = 'gearbox - miles per gallon' , ylab = 'cylinders' , layout = c ( 1 , 3 ), groups = vs , auto.key = list ( space = 'right' )) 1 stripplot ( mpg ~ factor ( cyl ) | factor ( gear ), main = 'Title' , xlab = 'cylinders' , ylab = 'gearbox - miles per gallon' , layout = c ( 1 , 3 )) Histogram Like hist() . 1 histogram ( mpg , main = 'Title' , xlab = 'miles per gallon' ) 1 histogram ( ~ mpg | factor ( cyl ), layout = c ( 1 , 3 ), main = 'Title' , xlab = 'miles per gallon' , ylab = 'density' ) Densityplot Like plot.density() . 1 densityplot ( mpg , main = 'Title' , xlab = 'miles per gallon' , ylab = 'density' ) 1 densityplot ( ~ mpg | factor ( cyl ), layout = c ( 1 , 3 ), main = 'Title' , xlab = 'miles per gallon' , ylab = 'density' ) ECDFplot 1 2 3 library ( latticeExtra ) ecdfplot ( mpg , main = 'Title' , xlab = 'miles per gallon' , ylab = '' ) BWplot Like boxplot . 1 bwplot ( mpg , main = 'Title' , xlab = 'miles per gallon' , ylab = 'density' ) 1 bwplot ( factor ( cyl ) ~ mpg , main = 'Title' , xlab = 'miles per gallon' , ylab = 'cylinders' ) 1 bwplot ( factor ( cyl ) ~ mpg | factor ( gear ), main = 'Title' , xlab = 'miles per gallon' , ylab = 'gearbox - cylinders' , layout = c ( 1 , 3 )) 1 bwplot ( mpg ~ factor ( cyl ) | factor ( gear ), main = 'Title' , xlab = 'gearbox - cylinders' , ylab = 'miles per gallon' , layout = c ( 3 , 1 )) QQmath Like qqnorm() . 1 qqmath ( mpg , main = 'Title' , ylab = 'miles per gallon' ) XYplot Like plot() . 1 xyplot ( mpg ~ disp | factor ( cyl ), main = 'Title' , xlab = 'horsepower' , ylab = 'cylinders - miles per gallon' , layout = c ( 1 , 3 )) 1 xyplot ( mpg ~ disp | factor ( cyl ), main = 'Title' , xlab = 'cylinder - horsepowers' , ylab = 'miles per gallon' , layout = c ( 3 , 1 )) XYplot options 1 xyplot ( mpg ~ disp | factor ( cyl ), main = 'Title' , xlab = 'cylinder - horsepowers' , ylab = 'miles per gallon' , layout = c ( 3 , 1 ), aspect = 1 ) 1 xyplot ( mpg ~ disp | factor ( cyl ), main = 'Title' , xlab = 'cylinder - horsepowers' , ylab = 'miles per gallon' , layout = c ( 3 , 1 ), aspect = 1 , scales = list ( y = list ( at = seq ( 10 , 30 , 10 )))) 1 2 3 4 5 6 7 meanmpg <- mean ( mpg ) xyplot ( mpg ~ disp | factor ( cyl ), main = 'Title' , xlab = 'cylinder - horsepowers' , ylab = 'miles per gallon' , layout = c ( 3 , 1 ), aspect = 1 , panel = function ( ... ) { panel.xyplot ( ... ) panel.abline ( h = meanmpg , lty = 'dashed' ) panel.text ( 450 , meanmpg + 1 , 'avg' , adj = c ( 1 , 0 ), cex = 0.7 ) }) 1 2 3 4 xyplot ( mpg ~ disp | factor ( cyl ), main = 'Title' , xlab = 'cylinder - horsepowers' , ylab = 'miles per gallon' , layout = c ( 3 , 1 ), aspect = 1 , panel = function ( x , y , ... ) { panel.lmline ( x , y ) panel.xyplot ( x , y , ... ) }) panel.points() . panel.lines() . panel.segments() . panel.arrows() . panel.rect() . panel.polygon() . panel.text() . panel.abline() . panel.lmline() . panel.xyplot() . panel.curve() . panel.rug() . panel.grid() . panel.bwplot() . panel.histogram() . panel.loess() . panel.violin() . panel.smoothScatter() . \u2026 par.settings . \u2026 1 2 3 4 5 library ( lattice ) data ( SeatacWeather , package = 'latticeExtra' ) xyplot ( min.temp + max.temp + precip ~ day | month , ylab = 'Temperature and Rainfall' , data = SeatacWeather , layout = c ( 3 , 1 ), type = 'l' , lty = 1 , col = 'black' ) 1 xyplot ( min.temp + max.temp + precip ~ day | month , ylab = 'Temperature and Rainfall' , data = SeatacWeather , layout = c ( 3 , 1 ), type = 'p' , lty = 1 , col = 'black' ) 1 xyplot ( min.temp + max.temp + precip ~ day | month , ylab = 'Temperature and Rainfall' , data = SeatacWeather , layout = c ( 3 , 1 ), type = 'l' , lty = 1 , col = 'black' ) 1 xyplot ( min.temp + max.temp + precip ~ day | month , ylab = 'Temperature and Rainfall' , data = SeatacWeather , layout = c ( 3 , 1 ), type = 'o' , lty = 1 , col = 'black' ) 1 xyplot ( min.temp + max.temp + precip ~ day | month , ylab = 'Temperature and Rainfall' , data = SeatacWeather , layout = c ( 3 , 1 ), type = 'r' , lty = 1 , col = 'black' ) 1 xyplot ( min.temp + max.temp + precip ~ day | month , ylab = 'Temperature and Rainfall' , data = SeatacWeather , layout = c ( 3 , 1 ), type = 'g' , lty = 1 , col = 'black' ) 1 xyplot ( min.temp + max.temp + precip ~ day | month , ylab = 'Temperature and Rainfall' , data = SeatacWeather , layout = c ( 3 , 1 ), type = 's' , lty = 1 , col = 'black' ) 1 xyplot ( min.temp + max.temp + precip ~ day | month , ylab = 'Temperature and Rainfall' , data = SeatacWeather , layout = c ( 3 , 1 ), type = 'S' , lty = 1 , col = 'black' ) 1 xyplot ( min.temp + max.temp + precip ~ day | month , ylab = 'Temperature and Rainfall' , data = SeatacWeather , layout = c ( 3 , 1 ), type = 'h' , lty = 1 , col = 'black' ) 1 xyplot ( min.temp + max.temp + precip ~ day | month , ylab = 'Temperature and Rainfall' , data = SeatacWeather , layout = c ( 3 , 1 ), type = 'a' , lty = 1 , col = 'black' ) 1 xyplot ( min.temp + max.temp + precip ~ day | month , ylab = 'Temperature and Rainfall' , data = SeatacWeather , layout = c ( 3 , 1 ), type = 'smooth' , lty = 1 , col = 'black' ) 1 xyplot ( mpg ~ hp , main = 'Title' , xlab = 'horsepowers' , ylab = 'miles per gallon' ) 1 xyplot ( mpg ~ hp , main = 'Title' , xlab = 'horsepowers' , ylab = 'miles per gallon' , type = 'o' ) 1 xyplot ( mpg ~ hp , main = 'Title' , xlab = 'horsepowers' , ylab = 'miles per gallon' , type = 'o' , pch = 16 , lty = 'dashed' ) 1 xyplot ( mpg ~ hp , main = 'Title' , xlab = 'horsepowers' , ylab = 'miles per gallon' ) 1 2 3 data ( USAge.df , package = 'latticeExtra' ) xyplot ( Population ~ Age | factor ( Year ), USAge.df , groups = Sex , type = c ( 'l' , 'g' ), auto.key = list ( points = FALSE , lines = TRUE , columns = 2 ), aspect = 'xy' , ylab = 'Population (millions)' , subset = Year %in% seq ( 1905 , 1975 , by = 10 )) 1 xyplot ( Population ~ Year | factor ( Age ), USAge.df , groups = Sex , type = 'l' , strip = FALSE , strip.left = TRUE , layout = c ( 1 , 3 ), ylab = 'Population (millions)' , auto.key = list ( lines = TRUE , points = FALSE , columns = 2 ), subset = Age %in% c ( 0 , 10 , 20 )) 1 2 3 4 5 6 7 8 9 data ( USCancerRates , package = 'latticeExtra' ) xyplot ( rate.male ~ rate.female | state , USCancerRates , aspect = 'iso' , pch = '.' , cex = 2 , index.cond = function ( x , y ) { median ( y - x , na.rm = TRUE ) }, scales = list ( log = 2 , at = c ( 75 , 150 , 300 , 600 )), panel = function ( ... ) { panel.grid ( h = -1 , v = -1 ) panel.abline ( 0 , 1 ) panel.xyplot ( ... ) }, xlab = 'a' , ylab = 'b' ) 1 2 3 4 5 data ( biocAccess , package = 'latticeExtra' ) baxy <- xyplot ( log10 ( counts ) ~ hour | month + weekday , biocAccess , type = c ( 'p' , 'a' ), as.table = TRUE , pch = '.' , cex = 2 , col.line = 'black' ) baxy 1 2 library ( latticeExtra ) useOuterStrips ( baxy ) 1 xyplot ( sunspot.year , aspect = 'xy' , strip = FALSE , strip.left = TRUE , cut = list ( number = 4 , overlap = 0.05 )) 1 2 3 4 5 data ( biocAccess , package = 'latticeExtra' ) ssd <- stl ( ts ( biocAccess $ counts[1 : ( 24 * 30 * 2 ) ] , frequency = 24 ), 'periodic' ) xyplot ( ssd , main = 'Title' , xlab = 'Time (Days)' ) Splom 1 splom ( mtcars [c ( 1 , 3 , 6 ) ] , groups = cyl , data = mtcars , panel = panel.superpose , key = list ( title = 'Three Cylinder Options' , columns = 3 , points = list ( text = list ( c ( '4 Cylinder' , '6 Cylinder' , '8 Cylinder' ))))) 1 2 3 trellis.par.set ( superpose.symbol = list ( pch = c ( 1 , 3 , 22 ), col = 1 , alpha = 0.5 )) splom ( ~ data.frame ( mpg , disp , hp , drat , wt , qsec ), data = mtcars , groups = cyl , pscales = 0 , varnames = c ( 'miles\\nper\\ngallon' , 'displacement\\n(cu.in(' , 'horsepower' , 'rear\\naxle\\nratio' , 'weight' , '1/4\\nmile\\ntime' ), auto.key = list ( columns = 3 , title = 'Title' )) 1 trellis.par.set ( old.pars ) 1 splom ( USArrests ) 1 splom ( ~ USArrests [c ( 3 , 1 , 2 , 4 ) ] | state.region , pscales = 0 , type = c ( 'g' , 'p' , 'smooth' )) Parallel plot For multivariate continuous data. 1 parallelplot ( ~ iris[1 : 4 ] ) 1 parallelplot ( ~ iris[1 : 4 ] , horizontal.axis = FALSE ) 1 parallelplot ( ~ iris[1 : 4 ] , scales = list ( x = list ( rot = 90 ))) 1 parallelplot ( ~ iris[1 : 4 ] | Species , iris ) 1 2 parallelplot ( ~ iris[1 : 4 ] , iris , groups = Species , horizontal.axis = FALSE , scales = list ( x = list ( rot = 90 ))) Trivariate plots Like image() , contour() , filled.contour() , persp() , symbols() . levelplot() . contourplot() . cloud() . wireframe() .","title":"Uni-, Bi-, Multivariate Plots"},{"location":"plot_snippets_basics/#additional-packages","text":"","title":"Additional Packages"},{"location":"plot_snippets_basics/#the-sm-package-density","text":"1 library ( sm ) Density plot 1 2 3 4 5 6 7 8 9 10 11 # create value labels cyl.f <- factor ( cyl , levels = c ( 4 , 6 , 8 ), labels = c ( '4 cyl' , '6 cyl' , '8 cyl' )) # plot densities sm.density.compare ( mpg , cyl , xlab = 'miles per gallon' ) title ( main = 'Title' ) # add legend via mouse click colfill <- c ( 2 : ( 2 + length ( levels ( cyl.f )))) legend ( 25 , 0.19 , levels ( cyl.f ), fill = colfill )","title":"The sm Package (density)"},{"location":"plot_snippets_basics/#the-car-package-scatter","text":"1 library ( car ) Scatter plot 1 scatterplot ( mpg ~ wt | cyl , data = mtcars , xlab = 'weight' , ylab = 'miles per gallon' , labels = row.names ( mtcars )) Splom 1 scatterplotMatrix ( ~ mpg + disp + drat + wt | cyl , data = mtcars , main = 'Title' ) scatterplotMatrix == spm . 1 spm ( ~ mpg + disp + drat + wt | cyl , data = mtcars , main = 'Title' )","title":"The car Package (scatter)"},{"location":"plot_snippets_basics/#the-vioplot-package-boxplot","text":"1 library ( vioplot ) Violin boxplot 1 2 3 4 5 6 7 x1 <- mpg[mtcars $ cyl == 4 ] x2 <- mpg[mtcars $ cyl == 6 ] x3 <- mpg[mtcars $ cyl == 8 ] vioplot ( x1 , x2 , x3 , names = c ( '4 cyl' , '6 cyl' , '8 cyl' ), col = 'green' ) title ( 'Title' )","title":"The vioplot Package (boxplot)"},{"location":"plot_snippets_basics/#the-vcd-package-count-correlation-mosaic","text":"1 library ( vcd ) The package provides a variety of methods for visualizing multivariate categorical data. Count 1 2 counts <- table ( gear , cyl ) counts 1 2 3 4 5 ## cyl ## gear 8 6 4 ## 3 12 2 1 ## 4 0 4 8 ## 5 2 1 2 1 mosaic ( counts , shade = TRUE , legend = TRUE ) Correlation 1 2 counts <- table ( gear , cyl ) counts 1 2 3 4 5 ## cyl ## gear 8 6 4 ## 3 12 2 1 ## 4 0 4 8 ## 5 2 1 2 1 assoc ( counts , shade = TRUE ) Mosaic 1 2 3 4 5 6 ucb <- data.frame ( UCBAdmissions ) ucb <- within ( ucb , Accept <- factor ( Admit , levels = c ( 'Rejected' , 'Admitted' ))) library ( vcd ); library ( grid ) doubledecker ( xtabs ( Freq ~ Dept + Gender + Accept , data = ucb ), gp = gpar ( fill = c ( 'grey90' , 'steelblue' ))) 1 2 3 data ( Fertility , package = 'AER' ) doubledecker ( morekids ~ age , data = Fertility , gp = gpar ( fill = c ( 'grey90' , 'green' )), spacing = spacing_equal ( 0 )) 1 doubledecker ( morekids ~ gender1 + gender2 , data = Fertility , gp = gpar ( fill = c ( 'grey90' , 'green' ))) 1 doubledecker ( morekids ~ age + gender1 + gender2 , data = Fertility , gp = gpar ( fill = c ( 'grey90' , 'green' )), spacing = spacing_dimequal ( c ( 0.1 , 0 , 0 , 0 )))","title":"The vcd Package (count, correlation, mosaic)"},{"location":"plot_snippets_basics/#the-hexbin-package-scatter","text":"1 library ( hexbin ) Scatter plot 1 2 3 4 5 # new data data ( NHANES ) # compare plot ( Serum.Iron ~ Transferin , NHANES , main = 'Title' , xlab = 'Transferin' , ylab = 'Iron' ) 1 2 # with hexbinplot ( Serum.Iron ~ Transferin , NHANES , main = 'Title' , xlab = 'Transferin' , ylab = 'Iron' ) 1 hexbinplot ( mpg ~ hp , main = 'Title' , xlab = 'horsepowers' , ylab = 'miles per gallon' ) 1 2 3 4 5 x <- rnorm ( 1000 ) y <- rnorm ( 1000 ) bin <- hexbin ( x , y , xbins = 50 ) plot ( bin , main = 'Title' ) 1 2 3 4 x <- rnorm ( 1000 ) y <- rnorm ( 1000 ) plot ( x , y , main = 'Title' , col = rgb ( 0 , 100 , 0 , 50 , maxColorValue = 255 ), pch = 16 ) 1 2 3 4 5 6 7 data ( Diamonds , package = 'Stat2Data' ) a = hexbin ( Diamonds $ PricePerCt , Diamonds $ Carat , xbins = 40 ) library ( RColorBrewer ) plot ( a ) Colors. 1 2 3 rf <- colorRampPalette ( rev ( brewer.pal ( 12 , 'Set3' ))) hexbinplot ( Diamonds $ PricePerCt ~ Diamonds $ Carat , colramp = rf ) Mix lattice and hexbin 1 2 3 data ( gvhd10 , package = 'latticeExtra' ) xyplot ( asinh ( SSC.H ) ~ asinh ( FL2.H ), gvhd10 , aspect = 1 , panel = panel.hexbinplot , .aspect.ratio = 1 , trans = sqrt ) 1 xyplot ( asinh ( SSC.H ) ~ asinh ( FL2.H ) | Days , gvhd10 , aspect = 1 , panel = panel.hexbinplot , .aspect.ratio = 1 , trans = sqrt )","title":"The hexbin Package (scatter)"},{"location":"plot_snippets_basics/#the-car-package-scatter_1","text":"1 library ( car ) Scatter plot 1 2 scatterplotMatrix ( ~ mpg + disp + drat + wt | cyl , data = mtcars , main = 'Three Cylinder Options' )","title":"The car Package (scatter)"},{"location":"plot_snippets_basics/#the-scatterplot3d-package","text":"1 library ( scatterplot3d ) Scatter plot 1 scatterplot3d ( wt , disp , mpg , main = 'Title' ) 1 scatterplot3d ( wt , disp , mpg , pch = 16 , highlight.3d = TRUE , type = 'h' , main = 'Title' ) 1 2 3 4 5 s3d <- scatterplot3d ( wt , disp , mpg , pch = 16 , highlight.3d = TRUE , type = 'h' , main = ' Title' ) fit <- lm ( mpg ~ wt + disp ) s3d $ plane3d ( fit )","title":"The scatterplot3d Package"},{"location":"plot_snippets_basics/#the-rgl-package-interactive","text":"1 library ( rgl ) Interactive plot The plot will open a new window. 1 plot3d ( wt , disp , mpg , col = 'red' , size = 3 )","title":"The rgl Package (interactive)"},{"location":"plot_snippets_basics/#the-cluster-package-dendrogram","text":"1 library ( cluster ) Dendrogram Use the iris dataset. 1 2 3 subset <- sample ( 1 : 150 , 20 ) cS <- as.character ( Sp <- iris $ Species[subset] ) cS 1 2 3 4 ## [1] \"setosa\" \"versicolor\" \"setosa\" \"virginica\" \"virginica\" ## [6] \"setosa\" \"setosa\" \"setosa\" \"virginica\" \"setosa\" ## [11] \"versicolor\" \"versicolor\" \"virginica\" \"setosa\" \"versicolor\" ## [16] \"versicolor\" \"setosa\" \"virginica\" \"versicolor\" \"versicolor\" 1 2 3 4 5 6 7 cS[Sp == 'setosa' ] <- 'S' cS[Sp == 'versicolor' ] <- 'V' cS[Sp == 'virginica' ] <- 'g' ai <- agnes ( iris[subset , 1 : 4 ] ) plot ( ai , label = cS )","title":"The cluster Package (dendrogram)"},{"location":"plot_snippets_basics/#the-extracat-package-splom","text":"1 library ( extracat ) Splom For missing values. Binary matrix with reordering and filtering of rows and columns. The x-axis shows the frequency of NA. The y-axis shows the marginal distribution of NA. 1 2 3 4 # example 1 data ( CHAIN , package = 'mi' ) visna ( CHAIN , sort = 'b' ) 1 summary ( CHAIN ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 ## log_virus age income healthy ## Min. : 0.000 Min. :21.00 Min. : 1.000 Min. :16.67 ## 1st Qu.: 0.000 1st Qu.:37.00 1st Qu.: 2.000 1st Qu.:35.00 ## Median : 0.000 Median :43.00 Median : 3.000 Median :45.37 ## Mean : 4.324 Mean :42.56 Mean : 3.377 Mean :44.40 ## 3rd Qu.: 9.105 3rd Qu.:48.00 3rd Qu.: 5.000 3rd Qu.:54.89 ## Max. :13.442 Max. :70.00 Max. :10.000 Max. :70.11 ## NA's :179 NA's :24 NA's :38 NA's :24 ## mental damage treatment ## Min. :0.0000 Min. :1.000 Min. :0.0000 ## 1st Qu.:0.0000 1st Qu.:3.000 1st Qu.:0.0000 ## Median :0.0000 Median :4.000 Median :1.0000 ## Mean :0.2717 Mean :3.578 Mean :0.8602 ## 3rd Qu.:1.0000 3rd Qu.:5.000 3rd Qu.:2.0000 ## Max. :1.0000 Max. :5.000 Max. :2.0000 ## NA's :24 NA's :63 NA's :24 1 2 3 4 5 6 7 8 9 # example 2 data ( oly12 , package = 'VGAMdata' ) oly12d <- oly12[ , names ( oly12 ) != 'DOB' ] oly12a <- oly12 names ( oly12a ) <- abbreviate ( names ( oly12 ), 3 ) visna ( oly12a , sort = 'b' ) 1 2 3 4 5 6 # example 3 data ( freetrade , package = 'Amelia' ) freetrade <- within ( freetrade , land1 <- reorder ( country , tariff , function ( x ) sum ( is.na ( x )))) fluctile ( xtabs ( is.na ( tariff ) ~ land1 + year , data = freetrade )) 1 ## viewport[base] 1 2 3 4 # example 4 data ( Pima.tr2 , package = 'MASS' ) visna ( Pima.tr2 , sort = 'b' )","title":"The extracat Package (splom)"},{"location":"plot_snippets_basics/#the-ash-package-density","text":"1 library ( ash ) Density plot 1 plot ( ash1 ( bin1 ( mtcars $ mpg , nbin = 50 )), type = 'l' ) 1 ## [1] \"ash estimate nonzero outside interval ab\"","title":"The ash Package (density)"},{"location":"plot_snippets_basics/#the-kernsmooth-package-density","text":"1 library ( KernSmooth ) Density plot 1 2 3 4 5 with ( mtcars , { hist ( mpg , freq = FALSE , main = '' , col = 'bisque2' , ylab = '' ) lines ( density ( mpg ), lwd = 2 ) ks1 <- bkde ( mpg , bandwidth = dpik ( mpg )) lines ( ks1 , col = 'red' , lty = 5 , lwd = 2 )})","title":"The KernSmooth Package (density)"},{"location":"plot_snippets_basics/#the-corrplot-package-correlation","text":"1 library ( corrplot ) Splom 1 2 3 4 # Create a correlation matrix for the dataset (9-14 are the '2' variables only) correlations <- cor ( mtcars ) corrplot ( correlations )","title":"The corrplot Package (correlation)"},{"location":"plot_snippets_colours/","text":"Foreword Output options: the \u2018tango\u2019 syntax and the \u2018readable\u2019 theme. Snippets and results. Default colours \u00b6 1 2 3 # Set palette ( 'default' ) palette () 1 2 ## [1] \"black\" \"red\" \"green3\" \"blue\" \"cyan\" \"magenta\" \"yellow\" ## [8] \"gray\" 1 2 3 4 5 6 7 # Show par ( mfrow = c ( 1 , 2 )) n <- 8 pie ( rep ( 1 , n ), col = FALSE , main = 'colourless' ) n <- 8 pie ( rep ( 1 , n ), col = palette (), main = 'default' ) 1 par ( mfrow = c ( 1 , 1 )) Basic colours \u00b6 1 2 3 # 8 types times 9 tones n <- 9 rainbow ( n , s = 1 , v = 1 , start = 0 , end = max ( 1 , n - 1 ) / n , alpha = 1 ) 1 2 ## [1] \"#FF0000FF\" \"#FFAA00FF\" \"#AAFF00FF\" \"#00FF00FF\" \"#00FFAAFF\" \"#00AAFFFF\" ## [7] \"#0000FFFF\" \"#AA00FFFF\" \"#FF00AAFF\" 1 2 m <- ( 1 : n ) / n gray ( m ) 1 2 ## [1] \"#1C1C1C\" \"#393939\" \"#555555\" \"#717171\" \"#8E8E8E\" \"#AAAAAA\" \"#C6C6C6\" ## [8] \"#E3E3E3\" \"#FFFFFF\" 1 hsv ( m , s = 1 , v = 1 , alpha = 1 ) 1 2 ## [1] \"#FFAA00FF\" \"#AAFF00FF\" \"#00FF00FF\" \"#00FFAAFF\" \"#00AAFFFF\" \"#0000FFFF\" ## [7] \"#AA00FFFF\" \"#FF00AAFF\" \"#FF0000FF\" 1 blues9 1 2 ## [1] \"#F7FBFF\" \"#DEEBF7\" \"#C6DBEF\" \"#9ECAE1\" \"#6BAED6\" \"#4292C6\" \"#2171B5\" ## [8] \"#08519C\" \"#08306B\" 1 heat.colors ( n , alpha = 1 ) 1 2 ## [1] \"#FF0000FF\" \"#FF2A00FF\" \"#FF5500FF\" \"#FF8000FF\" \"#FFAA00FF\" \"#FFD500FF\" ## [7] \"#FFFF00FF\" \"#FFFF40FF\" \"#FFFFBFFF\" 1 terrain.colors ( n , alpha = 1 ) 1 2 ## [1] \"#00A600FF\" \"#3EBB00FF\" \"#8BD000FF\" \"#E6E600FF\" \"#E8C32EFF\" \"#EBB25EFF\" ## [7] \"#EDB48EFF\" \"#F0C9C0FF\" \"#F2F2F2FF\" 1 cm.colors ( n , alpha = 1 ) 1 2 ## [1] \"#80FFFFFF\" \"#9FFFFFFF\" \"#BFFFFFFF\" \"#DFFFFFFF\" \"#FFFFFFFF\" \"#FFDFFFFF\" ## [7] \"#FFBFFFFF\" \"#FF9FFFFF\" \"#FF80FFFF\" 1 topo.colors ( n , alpha = 1 ) 1 2 ## [1] \"#4C00FFFF\" \"#004CFFFF\" \"#00E5FFFF\" \"#00FF4DFF\" \"#4DFF00FF\" \"#E6FF00FF\" ## [7] \"#FFFF00FF\" \"#FFDE59FF\" \"#FFE0B3FF\" 1 2 3 4 5 6 7 # Show par ( mfrow = c ( 2 , 2 )) pie ( rep ( 1 , n ), col = rainbow ( n , alpha = 1 ), main = 'rainbow' ) pie ( rep ( 1 , n ), col = gray ( m ), main = 'gray' ) pie ( rep ( 1 , n ), col = hsv ( m , alpha = 1 ), main = 'hsv' ) pie ( rep ( 1 , n ), col = blues9 , main = 'blues9' ) 1 2 3 4 pie ( rep ( 1 , n ), col = heat.colors ( n , alpha = 1 ), main = 'heat' ) pie ( rep ( 1 , n ), col = terrain.colors ( n , alpha = 1 ), main = 'terrain' ) pie ( rep ( 1 , n ), col = cm.colors ( n ), main = 'cm.colors' ) pie ( rep ( 1 , n ), col = topo.colors ( n , alpha = 1 ), main = 'topo' ) 1 par ( mfrow = c ( 1 , 1 )) RColorBrewer examples \u00b6 1 2 3 4 library ( RColorBrewer ) # Show all display.brewer.all () 1 2 3 4 # Pick a palette n <- 8 colors <- brewer.pal ( n , \"BuPu\" ) colors 1 2 ## [1] \"#F7FCFD\" \"#E0ECF4\" \"#BFD3E6\" \"#9EBCDA\" \"#8C96C6\" \"#8C6BB1\" \"#88419D\" ## [8] \"#6E016B\" 1 2 3 4 5 6 par ( mfrow = c ( 1 , 2 )) pie ( rep ( 1 , n ), col = colors , main = 'Sequential RdPu' ) # Interpolate these colors pal <- colorRampPalette ( brewer.pal ( n , 'RdPu' )) pal ( 8 ) 1 2 ## [1] \"#FFF7F3\" \"#FDE0DD\" \"#FCC5C0\" \"#FA9FB5\" \"#F768A1\" \"#DD3497\" \"#AE017E\" ## [8] \"#7A0177\" 1 pie ( rep ( 1 , n ), col = pal ( 8 ), main = 'Interpolated RdPu' ) 1 2 3 4 5 # Apply data ( volcano ) par ( mfrow = c ( 2 , 1 )) image ( volcano , col = pal ( 8 )) image ( volcano , col = pal ( 30 )) 1 par ( mfrow = c ( 1 , 1 )) 1 2 3 4 5 6 7 8 9 10 11 # Show samples par ( mfrow = c ( 2 , 2 )) n = 9 pie ( rep ( 1 , n ), col = brewer.pal ( n , 'RdPu' ), main = 'Sequential RdPu' ) n = 9 pie ( rep ( 1 , n ), col = brewer.pal ( n , 'Set1' ), main = 'Qualitative Set1' ) n = 12 pie ( rep ( 1 , n ), col = brewer.pal ( n , 'Paired' ), main = 'Qualitative Paired' ) n = 11 pie ( rep ( 1 , n ), col = brewer.pal ( n , 'RdBu' ), main = 'Divergent RdBu' ) 1 par ( mfrow = c ( 1 , 1 )) 1 2 3 4 # Show n = 8 darkcols <- brewer.pal ( n , 'Dark2' ) pie ( rep ( 1 , n ), col = darkcols , main = 'Dark2' ) Building a palette \u00b6 1 2 # All head ( colors ()) 1 2 ## [1] \"white\" \"aliceblue\" \"antiquewhite\" \"antiquewhite1\" ## [5] \"antiquewhite2\" \"antiquewhite3\" 1 length ( colors ()) # 657 1 ## [1] 657 1 2 3 4 5 6 7 8 # Create mycols <- colors () [c ( 8 , 5 , 30 , 53 , 118 , 72 ) ] # # or # mycols <- c('aquamarine', 'antiquewhite2', 'blue4', 'chocolate1', 'deeppink2', 'cyan4') # Show n = 6 pie ( rep ( 1 , n ), col = mycols , main = 'mycols' ) 1 2 3 4 5 6 7 8 # Generate randomly cl <- colors ( distinct = TRUE ) set.seed ( 15887 ) # to set random generator seed mycols2 <- sample ( cl , 7 ) # Show n = 7 pie ( rep ( 1 , n ), col = mycols2 , main = 'mycols2 (random)' ) Grabing colours \u00b6 Grab Website Colors . RGB Color Codes Chart .","title":"Plot Snippets - Colours"},{"location":"plot_snippets_colours/#default-colours","text":"1 2 3 # Set palette ( 'default' ) palette () 1 2 ## [1] \"black\" \"red\" \"green3\" \"blue\" \"cyan\" \"magenta\" \"yellow\" ## [8] \"gray\" 1 2 3 4 5 6 7 # Show par ( mfrow = c ( 1 , 2 )) n <- 8 pie ( rep ( 1 , n ), col = FALSE , main = 'colourless' ) n <- 8 pie ( rep ( 1 , n ), col = palette (), main = 'default' ) 1 par ( mfrow = c ( 1 , 1 ))","title":"Default colours"},{"location":"plot_snippets_colours/#basic-colours","text":"1 2 3 # 8 types times 9 tones n <- 9 rainbow ( n , s = 1 , v = 1 , start = 0 , end = max ( 1 , n - 1 ) / n , alpha = 1 ) 1 2 ## [1] \"#FF0000FF\" \"#FFAA00FF\" \"#AAFF00FF\" \"#00FF00FF\" \"#00FFAAFF\" \"#00AAFFFF\" ## [7] \"#0000FFFF\" \"#AA00FFFF\" \"#FF00AAFF\" 1 2 m <- ( 1 : n ) / n gray ( m ) 1 2 ## [1] \"#1C1C1C\" \"#393939\" \"#555555\" \"#717171\" \"#8E8E8E\" \"#AAAAAA\" \"#C6C6C6\" ## [8] \"#E3E3E3\" \"#FFFFFF\" 1 hsv ( m , s = 1 , v = 1 , alpha = 1 ) 1 2 ## [1] \"#FFAA00FF\" \"#AAFF00FF\" \"#00FF00FF\" \"#00FFAAFF\" \"#00AAFFFF\" \"#0000FFFF\" ## [7] \"#AA00FFFF\" \"#FF00AAFF\" \"#FF0000FF\" 1 blues9 1 2 ## [1] \"#F7FBFF\" \"#DEEBF7\" \"#C6DBEF\" \"#9ECAE1\" \"#6BAED6\" \"#4292C6\" \"#2171B5\" ## [8] \"#08519C\" \"#08306B\" 1 heat.colors ( n , alpha = 1 ) 1 2 ## [1] \"#FF0000FF\" \"#FF2A00FF\" \"#FF5500FF\" \"#FF8000FF\" \"#FFAA00FF\" \"#FFD500FF\" ## [7] \"#FFFF00FF\" \"#FFFF40FF\" \"#FFFFBFFF\" 1 terrain.colors ( n , alpha = 1 ) 1 2 ## [1] \"#00A600FF\" \"#3EBB00FF\" \"#8BD000FF\" \"#E6E600FF\" \"#E8C32EFF\" \"#EBB25EFF\" ## [7] \"#EDB48EFF\" \"#F0C9C0FF\" \"#F2F2F2FF\" 1 cm.colors ( n , alpha = 1 ) 1 2 ## [1] \"#80FFFFFF\" \"#9FFFFFFF\" \"#BFFFFFFF\" \"#DFFFFFFF\" \"#FFFFFFFF\" \"#FFDFFFFF\" ## [7] \"#FFBFFFFF\" \"#FF9FFFFF\" \"#FF80FFFF\" 1 topo.colors ( n , alpha = 1 ) 1 2 ## [1] \"#4C00FFFF\" \"#004CFFFF\" \"#00E5FFFF\" \"#00FF4DFF\" \"#4DFF00FF\" \"#E6FF00FF\" ## [7] \"#FFFF00FF\" \"#FFDE59FF\" \"#FFE0B3FF\" 1 2 3 4 5 6 7 # Show par ( mfrow = c ( 2 , 2 )) pie ( rep ( 1 , n ), col = rainbow ( n , alpha = 1 ), main = 'rainbow' ) pie ( rep ( 1 , n ), col = gray ( m ), main = 'gray' ) pie ( rep ( 1 , n ), col = hsv ( m , alpha = 1 ), main = 'hsv' ) pie ( rep ( 1 , n ), col = blues9 , main = 'blues9' ) 1 2 3 4 pie ( rep ( 1 , n ), col = heat.colors ( n , alpha = 1 ), main = 'heat' ) pie ( rep ( 1 , n ), col = terrain.colors ( n , alpha = 1 ), main = 'terrain' ) pie ( rep ( 1 , n ), col = cm.colors ( n ), main = 'cm.colors' ) pie ( rep ( 1 , n ), col = topo.colors ( n , alpha = 1 ), main = 'topo' ) 1 par ( mfrow = c ( 1 , 1 ))","title":"Basic colours"},{"location":"plot_snippets_colours/#rcolorbrewer-examples","text":"1 2 3 4 library ( RColorBrewer ) # Show all display.brewer.all () 1 2 3 4 # Pick a palette n <- 8 colors <- brewer.pal ( n , \"BuPu\" ) colors 1 2 ## [1] \"#F7FCFD\" \"#E0ECF4\" \"#BFD3E6\" \"#9EBCDA\" \"#8C96C6\" \"#8C6BB1\" \"#88419D\" ## [8] \"#6E016B\" 1 2 3 4 5 6 par ( mfrow = c ( 1 , 2 )) pie ( rep ( 1 , n ), col = colors , main = 'Sequential RdPu' ) # Interpolate these colors pal <- colorRampPalette ( brewer.pal ( n , 'RdPu' )) pal ( 8 ) 1 2 ## [1] \"#FFF7F3\" \"#FDE0DD\" \"#FCC5C0\" \"#FA9FB5\" \"#F768A1\" \"#DD3497\" \"#AE017E\" ## [8] \"#7A0177\" 1 pie ( rep ( 1 , n ), col = pal ( 8 ), main = 'Interpolated RdPu' ) 1 2 3 4 5 # Apply data ( volcano ) par ( mfrow = c ( 2 , 1 )) image ( volcano , col = pal ( 8 )) image ( volcano , col = pal ( 30 )) 1 par ( mfrow = c ( 1 , 1 )) 1 2 3 4 5 6 7 8 9 10 11 # Show samples par ( mfrow = c ( 2 , 2 )) n = 9 pie ( rep ( 1 , n ), col = brewer.pal ( n , 'RdPu' ), main = 'Sequential RdPu' ) n = 9 pie ( rep ( 1 , n ), col = brewer.pal ( n , 'Set1' ), main = 'Qualitative Set1' ) n = 12 pie ( rep ( 1 , n ), col = brewer.pal ( n , 'Paired' ), main = 'Qualitative Paired' ) n = 11 pie ( rep ( 1 , n ), col = brewer.pal ( n , 'RdBu' ), main = 'Divergent RdBu' ) 1 par ( mfrow = c ( 1 , 1 )) 1 2 3 4 # Show n = 8 darkcols <- brewer.pal ( n , 'Dark2' ) pie ( rep ( 1 , n ), col = darkcols , main = 'Dark2' )","title":"RColorBrewer examples"},{"location":"plot_snippets_colours/#building-a-palette","text":"1 2 # All head ( colors ()) 1 2 ## [1] \"white\" \"aliceblue\" \"antiquewhite\" \"antiquewhite1\" ## [5] \"antiquewhite2\" \"antiquewhite3\" 1 length ( colors ()) # 657 1 ## [1] 657 1 2 3 4 5 6 7 8 # Create mycols <- colors () [c ( 8 , 5 , 30 , 53 , 118 , 72 ) ] # # or # mycols <- c('aquamarine', 'antiquewhite2', 'blue4', 'chocolate1', 'deeppink2', 'cyan4') # Show n = 6 pie ( rep ( 1 , n ), col = mycols , main = 'mycols' ) 1 2 3 4 5 6 7 8 # Generate randomly cl <- colors ( distinct = TRUE ) set.seed ( 15887 ) # to set random generator seed mycols2 <- sample ( cl , 7 ) # Show n = 7 pie ( rep ( 1 , n ), col = mycols2 , main = 'mycols2 (random)' )","title":"Building a palette"},{"location":"plot_snippets_colours/#grabing-colours","text":"Grab Website Colors . RGB Color Codes Chart .","title":"Grabing colours"},{"location":"plot_snippets_ggplot2/","text":"Foreword Output options: the \u2018tango\u2019 syntax and the \u2018readable\u2019 theme. Snippets and results. Documentation \u00b6 ggplot2 . ggthemes . r4stats . Dataset \u00b6 For most examples, we use the mtcars , diamonds , iris , ChickWeight , recess , fish , Vocab , Titanic , mamsleep , barley , adult datasets. The ggplot2 Package \u00b6 1 library ( ggplot2 ) Import additional packages. 1 2 3 4 5 6 7 8 9 library ( digest ) library ( grid ) library ( gtable ) library ( MASS ) library ( plyr ) library ( reshape2 ) library ( scales ) library ( stats ) library ( tidyr ) For this project, import additional packages. 1 2 3 4 5 6 7 library ( ggthemes ) library ( RColorBrewer ) library ( gridExtra ) library ( GGally ) library ( car ) library ( Hmisc ) library ( dplyr ) Suggested additional packages\u2026 SECTION 1 \u00b6 Introduction \u00b6 Exploring ggplot2 , part 1 1 2 3 # basic plot ggplot ( mtcars , aes ( x = cyl , y = mpg )) + geom_point () Exploring ggplot2 , part 2 1 2 3 # cyl is a factor ggplot ( mtcars , aes ( x = factor ( cyl ), y = mpg )) + geom_point () Exploring ggplot2 , part 3 1 2 3 # scatter plot ggplot ( mtcars , aes ( x = wt , y = mpg )) + geom_point () 1 2 3 # add color ggplot ( mtcars , aes ( x = wt , y = mpg , col = disp )) + geom_point () 1 2 3 # change size ggplot ( mtcars , aes ( x = wt , y = mpg , size = disp )) + geom_point () Exploring ggplot2 , part 4 1 2 3 # Add geom_point() with + ggplot ( diamonds , aes ( x = Carat , y = PricePerCt )) + geom_point () 1 2 3 # Add geom_point() and geom_smooth() with + ggplot ( diamonds , aes ( x = Carat , y = PricePerCt )) + geom_point () + geom_smooth () Exploring ggplot2 , part 5 1 2 3 # only the smooth line ggplot ( diamonds , aes ( x = Carat , y = PricePerCt )) + geom_smooth () 1 2 3 # change col ggplot ( diamonds , aes ( x = Carat , y = PricePerCt , col = Clarity )) + geom_point () 1 2 3 # change the alpha ggplot ( diamonds , aes ( x = Carat , y = PricePerCt , col = Clarity )) + geom_point ( alpha = 0.4 ) Exploring ggplot2 , part 6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 # 2 facets for comparison library ( gridExtra ) data ( father.son , package = 'UsingR' ) a <- ggplot ( father.son , aes ( fheight , sheight )) + geom_point () + geom_smooth ( method = 'lm' , colour = 'red' ) + geom_abline ( slope = 1 , intercept = 0 ) b <- ggplot ( father.son , aes ( fheight , sheight )) + geom_point () + geom_smooth ( method = 'lm' , colour = 'red' , se = FALSE ) + stat_smooth () grid.arrange ( a , b , nrow = 1 ) 1 2 3 4 5 6 7 # load more data data ( oly12 , package = 'VGAMdata' ) # 2 facets for comparison ggplot ( oly12 , aes ( Height , Weight )) + geom_point ( size = 1 ) + facet_wrap ( ~ Sex , ncol = 1 ) 1 2 3 4 5 6 7 8 # create a new variable inside de data frame oly12S <- within ( oly12 , oly12 $ Sport <- abbreviate ( oly12 $ Sport , 12 )) # multiple facets or splom ggplot ( oly12S , aes ( Height , Weight )) + geom_point ( size = 1 ) + facet_wrap ( ~ Sport ) + ggtitle ( 'Weight and Height by Sport' ) Understanding the grammar, part 1 1 2 3 4 5 6 # create the object containing the data and aes layers dia_plot <- ggplot ( diamonds , aes ( x = Carat , y = PricePerCt )) # add a geom layer dia_plot + geom_point () 1 2 3 # add the same geom layer, but with aes() inside dia_plot + geom_point ( aes ( col = Clarity )) Understanding the grammar, part 2 1 2 3 4 5 6 7 8 9 10 set.seed ( 1 ) # create the object containing the data and aes layers dia_plot <- ggplot ( diamonds , aes ( x = Carat , y = PricePerCt )) # add geom_point() with alpha set to 0.2 dia_plot <- dia_plot + geom_point ( alpha = 0.2 ) dia_plot 1 2 3 # plot dia_plot with additional geom_smooth() with se set to FALSE dia_plot + geom_smooth ( se = FALSE ) Data \u00b6 Base package and ggplot2 , part 1 - plot 1 2 # basic plot plot ( mtcars $ wt , mtcars $ mpg , col = mtcars $ cyl ) 1 2 3 4 5 # change cyl inside mtcars to a factor mtcars $ cyl <- as.factor ( mtcars $ cyl ) # make the same plot as in the first instruction plot ( mtcars $ wt , mtcars $ mpg , col = mtcars $ cyl ) Base package and ggplot2 , part 2 - lm transfer to other 1 2 3 4 5 6 7 8 9 10 11 12 13 14 # Basic plot mtcars $ cyl <- as.factor ( mtcars $ cyl ) plot ( mtcars $ wt , mtcars $ mpg , col = mtcars $ cyl ) # use lm() to calculate a linear model and save it as carModel carModel <- lm ( mpg ~ wt , data = mtcars ) # Call abline() with carModel as first argument and lty as second abline ( carModel , lty = 2 ) # plot each subset efficiently with lapply lapply ( mtcars $ cyl , function ( x ) { abline ( lm ( mpg ~ wt , mtcars , subset = ( cyl == x )), col = x ) }) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 ## [[1]] ## NULL ## ## [[2]] ## NULL ## ## [[3]] ## NULL ## ## [[4]] ## NULL ## ## [[5]] ## NULL ## ## [[6]] ## NULL ## ## [[7]] ## NULL ## ## [[8]] ## NULL ## ## [[9]] ## NULL ## ## [[10]] ## NULL ## ## [[11]] ## NULL ## ## [[12]] ## NULL ## ## [[13]] ## NULL ## ## [[14]] ## NULL ## ## [[15]] ## NULL ## ## [[16]] ## NULL ## ## [[17]] ## NULL ## ## [[18]] ## NULL ## ## [[19]] ## NULL ## ## [[20]] ## NULL ## ## [[21]] ## NULL ## ## [[22]] ## NULL ## ## [[23]] ## NULL ## ## [[24]] ## NULL ## ## [[25]] ## NULL ## ## [[26]] ## NULL ## ## [[27]] ## NULL ## ## [[28]] ## NULL ## ## [[29]] ## NULL ## ## [[30]] ## NULL ## ## [[31]] ## NULL ## ## [[32]] ## NULL 1 2 # draw the legend of the plot legend ( x = 5 , y = 33 , legend = levels ( mtcars $ cyl ), col = 1 : 3 , pch = 1 , bty = 'n' ) Base package and ggplot2 , part 3 1 2 3 # scatter plot ggplot ( mtcars , aes ( x = wt , y = mpg , col = cyl )) + geom_point () 1 2 3 4 # include the lines of the linear models, per cyl ggplot ( mtcars , aes ( x = wt , y = mpg , col = cyl )) + geom_point () + geom_smooth ( method = 'lm' , se = FALSE ) 1 2 3 4 5 # include a lm for the entire dataset in its whole ggplot ( mtcars , aes ( x = wt , y = mpg , col = cyl )) + geom_point () + geom_smooth ( method = 'lm' , se = FALSE ) + geom_smooth ( aes ( group = 1 ), method = 'lm' , se = FALSE , linetype = 2 ) Variables to visuals, part 1 1 2 3 4 5 6 7 iris.tidy <- iris %>% gather ( key , Value , - Species ) %>% separate ( key , c ( 'Part' , 'Measure' ), '\\\\.' ) # create 2 facets ggplot ( iris.tidy , aes ( x = Species , y = Value , col = Part )) + geom_jitter () + facet_grid ( . ~ Measure ) Variables to visuals, part 2 1 2 3 4 5 6 7 8 9 10 11 12 # Add a new column, Flower, to iris that contains unique ids iris $ Flower <- 1 : nrow ( iris ) iris.wide <- iris %>% gather ( key , value , - Species , - Flower ) %>% separate ( key , c ( 'Part' , 'Measure' ), '\\\\.' ) %>% spread ( Measure , value ) # create 3 facets ggplot ( iris.wide , aes ( x = Length , y = Width , col = Part )) + geom_jitter () + facet_grid ( . ~ Species ) Aesthetics \u00b6 All about aesthetics, part 1 1 2 3 # map cyl to y ggplot ( mtcars , aes ( x = mpg , y = cyl )) + geom_point () 1 2 3 # map cyl to x ggplot ( mtcars , aes ( y = mpg , x = cyl )) + geom_point () 1 2 3 # map cyl to col ggplot ( mtcars , aes ( x = wt , y = mpg , col = cyl )) + geom_point () 1 2 3 # change shape and size of the points ggplot ( mtcars , aes ( x = wt , y = mpg , col = cyl )) + geom_point ( shape = 1 , size = 4 ) All about aesthetics, part 2 1 2 3 # map cyl to fill ggplot ( mtcars , aes ( x = wt , y = mpg , fill = cyl )) + geom_point () 1 2 3 # Change shape, size and alpha of the points in the above plot ggplot ( mtcars , aes ( x = wt , y = mpg , fill = cyl )) + geom_point ( shape = 16 , size = 6 , alpha = 0.6 ) All about aesthetics, part 3 1 2 3 # map cyl to size ggplot ( mtcars , aes ( x = wt , y = mpg , size = cyl )) + geom_point () 1 2 3 # map cyl to alpha ggplot ( mtcars , aes ( x = wt , y = mpg , alpha = cyl )) + geom_point () 1 2 3 # map cyl to shape ggplot ( mtcars , aes ( x = wt , y = mpg , shape = cyl , label = cyl )) + geom_point () 1 2 3 # map cyl to labels ggplot ( mtcars , aes ( x = wt , y = mpg , label = cyl )) + geom_text () All about attributes, part 1 1 2 3 4 5 6 # define a hexadecimal color my_color <- '#123456' # set the color aesthetic ggplot ( mtcars , aes ( x = wt , y = mpg , col = cyl )) + geom_point () 1 2 3 # set the color aesthetic and attribute ggplot ( mtcars , aes ( x = wt , y = mpg , col = cyl )) + geom_point ( col = my_color ) 1 2 3 # set the fill aesthetic and color, size and shape attributes ggplot ( mtcars , aes ( x = wt , y = mpg , fill = cyl )) + geom_point ( size = 10 , shape = 23 , col = my_color ) All about attributes, part 2 1 2 3 # draw points with alpha 0.5 ggplot ( mtcars , aes ( x = wt , y = mpg , fill = cyl )) + geom_point ( alpha = 0.5 ) 1 2 3 # raw points with shape 24 and color yellow ggplot ( mtcars , aes ( x = wt , y = mpg , fill = cyl )) + geom_point ( shape = 24 , col = 'yellow' ) 1 2 3 # draw text with label x, color red and size 10 ggplot ( mtcars , aes ( x = wt , y = mpg , fill = cyl )) + geom_text ( label = 'x' , col = 'red' , size = 10 ) Going all out 1 2 3 # Map mpg onto x, qsec onto y and factor(cyl) onto col ggplot ( mtcars , aes ( x = mpg , y = qsec , col = factor ( cyl ))) + geom_point () 1 2 3 # Add mapping: factor(am) onto shape ggplot ( mtcars , aes ( x = mpg , y = qsec , col = factor ( cyl ), shape = factor ( am ))) + geom_point () 1 2 3 # Add mapping: (hp/wt) onto size ggplot ( mtcars , aes ( x = mpg , y = qsec , col = factor ( cyl ), shape = factor ( am ), size = hp / wt )) + geom_point () 1 2 3 # Add mapping: rownames(mtcars) onto label ggplot ( mtcars , aes ( x = mpg , y = qsec , col = factor ( cyl ), shape = factor ( am ), size = hp / wt )) + geom_text ( aes ( label = rownames ( mtcars ))) Position 1 2 3 4 5 6 # base layers cyl.am <- ggplot ( mtcars , aes ( x = factor ( cyl ), fill = factor ( am ))) # add geom (position = 'stack'' by default) cyl.am + geom_bar ( position = 'stack' ) 1 2 3 # show proportion cyl.am + geom_bar ( position = 'fill' ) 1 2 3 # dodging cyl.am + geom_bar ( position = 'dodge' ) 1 2 3 4 5 6 7 8 # clean up the axes with scale_ functions val = c ( '#E41A1C' , '#377EB8' ) lab = c ( 'Manual' , 'Automatic' ) cyl.am + geom_bar ( position = 'dodge' , ) + scale_x_discrete ( 'Cylinders' ) + scale_y_continuous ( 'Number' ) + scale_fill_manual ( 'Transmission' , values = val , labels = lab ) Setting a dummy aesthetic 1 2 3 4 5 # add a new column called group mtcars $ group <- 0 # create jittered plot of mtcars: mpg onto x, group onto y ggplot ( mtcars , aes ( x = mpg , y = group )) + geom_jitter () 1 2 3 4 # change the y aesthetic limits ggplot ( mtcars , aes ( x = mpg , y = group )) + geom_jitter () + scale_y_continuous ( limits = c ( -2 , 2 )) Overplotting 1 - Point shape and transparency 1 2 3 # basic scatter plot: wt on x-axis and mpg on y-axis; map cyl to col ggplot ( mtcars , aes ( x = wt , y = mpg , col = cyl )) + geom_point ( size = 4 ) 1 2 3 # hollow circles - an improvement ggplot ( mtcars , aes ( x = wt , y = mpg , col = cyl )) + geom_point ( size = 4 , shape = 1 ) 1 2 3 # add transparency - very nice ggplot ( mtcars , aes ( x = wt , y = mpg , col = cyl )) + geom_point ( size = 4 , shape = 1 , alpha = 0.6 ) Overplotting 2 - alpha with large datasets 1 2 3 # scatter plot: carat (x), price (y), clarity (col) ggplot ( diamonds , aes ( x = Carat , y = PricePerCt , col = Clarity )) + geom_point () 1 2 3 # adjust for overplotting ggplot ( diamonds , aes ( x = Carat , y = PricePerCt , col = Clarity )) + geom_point ( alpha = 0.5 ) 1 2 3 # scatter plot: clarity (x), carat (y), price (col) ggplot ( diamonds , aes ( x = Clarity , y = Carat , col = PricePerCt )) + geom_point ( alpha = 0.5 ) 1 2 3 # dot plot with jittering ggplot ( diamonds , aes ( x = Clarity , y = Carat , col = PricePerCt )) + geom_point ( alpha = 0.5 , position = 'jitter' ) Geometries \u00b6 Scatter plots and jittering (1) 1 2 3 # plot the cyl on the x-axis and wt on the y-axis ggplot ( mtcars , aes ( x = cyl , y = wt )) + geom_point () 1 2 3 # Use geom_jitter() instead of geom_point() ggplot ( mtcars , aes ( x = cyl , y = wt )) + geom_jitter () 1 2 3 4 5 6 # Define the position object using position_jitter(): posn.j posn.j <- position_jitter ( 0.1 ) # Use posn.j in geom_point() ggplot ( mtcars , aes ( x = cyl , y = wt )) + geom_point ( position = posn.j ) Scatter plots and jittering (2) 1 2 3 # scatter plot of vocabulary (y) against education (x). Use geom_point() ggplot ( Vocab , aes ( x = education , y = vocabulary )) + geom_point () 1 2 3 # use geom_jitter() instead of geom_point() ggplot ( Vocab , aes ( x = education , y = vocabulary )) + geom_jitter () 1 2 3 # set alpha to a very low 0.2 ggplot ( Vocab , aes ( x = education , y = vocabulary )) + geom_jitter ( alpha = 0.2 ) 1 2 3 # set the shape to 1 ggplot ( Vocab , aes ( x = education , y = vocabulary )) + geom_jitter ( alpha = 0.2 , shape = 1 ) Histograms 1 2 3 # univariate histogram ggplot ( mtcars , aes ( x = mpg )) + geom_histogram () 1 2 3 # change the bin width to 1 ggplot ( mtcars , aes ( x = mpg )) + geom_histogram ( binwidth = 1 ) 1 2 3 # change the y aesthetic to density ggplot ( mtcars , aes ( x = mpg )) + geom_histogram ( aes ( y = ..density.. ), binwidth = 1 ) 1 2 3 4 5 6 # custom color code myBlue <- '#377EB8' # Change the fill color to myBlue ggplot ( mtcars , aes ( x = mpg )) + geom_histogram ( aes ( y = ..density.. ), binwidth = 1 , fill = myBlue ) Position 1 2 3 4 5 mtcars $ am <- as.factor ( mtcars $ am ) # bar plot of cyl, filled according to am ggplot ( mtcars , aes ( x = cyl , fill = am )) + geom_bar () 1 2 3 # change the position argument to stack ggplot ( mtcars , aes ( x = cyl , fill = am )) + geom_bar ( position = 'stack' ) 1 2 3 # change the position argument to fill ggplot ( mtcars , aes ( x = cyl , fill = am )) + geom_bar ( position = 'fill' ) 1 2 3 # change the position argument to dodge ggplot ( mtcars , aes ( x = cyl , fill = am )) + geom_bar ( position = 'dodge' ) Overlapping bar plots 1 2 3 # bar plot of cyl, filled according to am ggplot ( mtcars , aes ( x = cyl , fill = am )) + geom_bar () 1 2 3 # change the position argument to 'dodge' ggplot ( mtcars , aes ( x = cyl , fill = am )) + geom_bar ( position = 'dodge' ) 1 2 3 4 5 6 # define posn_d with position_dodge() posn_d <- position_dodge ( 0.2 ) # change the position argument to posn_d ggplot ( mtcars , aes ( x = cyl , fill = am )) + geom_bar ( position = posn_d ) 1 2 3 # use posn_d as position and adjust alpha to 0.6 ggplot ( mtcars , aes ( x = cyl , fill = am )) + geom_bar ( position = posn_d , alpha = 0.6 ) Overlapping histograms 1 2 3 # histogram, add coloring defined by cyl ggplot ( mtcars , aes ( mpg , fill = cyl )) + geom_histogram ( binwidth = 1 ) 1 2 3 # change position to identity ggplot ( mtcars , aes ( mpg , fill = cyl )) + geom_histogram ( binwidth = 1 , position = 'identity' ) 1 2 3 # change geom to freqpoly (position is identity by default) ggplot ( mtcars , aes ( mpg , col = cyl )) + geom_freqpoly ( binwidth = 1 ) Facets or splom histograms 1 2 3 4 5 6 7 8 9 10 11 # load the package library ( reshape2 ) # load new data data ( uniranks , package = 'GDAdata' ) # name the variables names ( uniranks ) [c ( 5 , 6 , 8 , 8 , 10 , 11 , 13 ) ] <- c ( 'AvTeach' , 'NSSTeach' , 'SpendperSt' , 'StudentStaffR' , 'Careers' , 'VAddScore' , 'NSSFeedb' ) # reshape the data frame ur2 <- melt ( uniranks[ , c ( 3 , 5 : 13 ) ] , id.vars = 'UniGroup' , variable.name = 'uniV' , value.name = 'uniX' ) 1 2 3 4 5 6 # Splom ggplot ( ur2 , aes ( uniX )) + geom_histogram () + xlab ( '' ) + ylab ( '' ) + facet_grid ( UniGroup ~ uniV , scales = 'free_x' ) 1 2 3 4 5 6 7 8 9 10 11 12 library ( ggplot2 ) library ( gridExtra ) data ( Pima.tr2 , package = 'MASS' ) h1 <- ggplot ( Pima.tr2 , aes ( glu )) + geom_histogram () h2 <- ggplot ( Pima.tr2 , aes ( bp )) + geom_histogram () h3 <- ggplot ( Pima.tr2 , aes ( skin )) + geom_histogram () h4 <- ggplot ( Pima.tr2 , aes ( bmi )) + geom_histogram () h5 <- ggplot ( Pima.tr2 , aes ( ped )) + geom_histogram () h6 <- ggplot ( Pima.tr2 , aes ( age )) + geom_histogram () grid.arrange ( h1 , h2 , h3 , h4 , h5 , h6 , nrow = 2 ) Bar plots with color ramp, part 1 1 2 3 4 # Example of how to use a brewed color palette ggplot ( mtcars , aes ( x = cyl , fill = am )) + geom_bar () + scale_fill_brewer ( palette = 'Set1' ) 1 2 3 4 5 6 Vocab $ education <- as.factor ( Vocab $ education ) Vocab $ vocabulary <- as.factor ( Vocab $ vocabulary ) # Plot education on x and vocabulary on fill # Use the default brewed color palette ggplot ( Vocab , aes ( x = education , fill = vocabulary )) + geom_bar ( position = 'fill' ) + scale_fill_brewer ( palette = 'Set3' ) Bar plots with color ramp, part 2 1 2 3 4 5 6 7 8 9 10 # Definition of a set of blue colors blues <- brewer.pal ( 9 , 'Blues' ) # Make a color range using colorRampPalette() and the set of blues blue_range <- colorRampPalette ( blues ) # Use blue_range to adjust the color of the bars, use scale_fill_manual() ggplot ( Vocab , aes ( x = education , fill = vocabulary )) + geom_bar ( position = 'fill' ) + scale_fill_manual ( values = blue_range ( 11 )) Overlapping histograms (2) 1 2 # histogram ggplot ( mtcars , aes ( mpg )) + geom_histogram ( binwidth = 1 ) 1 2 3 # expand the histogram to fill using am ggplot ( mtcars , aes ( mpg , fill = am )) + geom_histogram ( binwidth = 1 ) 1 2 3 # change the position argument to 'dodge' ggplot ( mtcars , aes ( mpg , fill = am )) + geom_histogram ( position = 'dodge' , binwidth = 1 ) 1 2 3 # change the position argument to 'fill' ggplot ( mtcars , aes ( mpg , fill = am )) + geom_histogram ( position = 'fill' , binwidth = 1 ) 1 2 3 # change the position argument to 'identity' and set alpha to 0.4 ggplot ( mtcars , aes ( mpg , fill = am )) + geom_histogram ( position = 'identity' , binwidth = 1 , alpha = 0.4 ) 1 2 3 # change fill to cyl ggplot ( mtcars , aes ( mpg , fill = cyl )) + geom_histogram ( position = 'identity' , binwidth = 1 , alpha = 0.4 ) Line plots 1 2 3 # plot unemploy as a function of date using a line plot ggplot ( economics , aes ( x = date , y = unemploy )) + geom_line () 1 2 3 # adjust plot to represent the fraction of total population that is unemployed ggplot ( economics , aes ( x = date , y = unemploy / pop )) + geom_line () Periods of recession 1 2 3 4 # draw the recess periods ggplot ( economics , aes ( x = date , y = unemploy / pop )) + geom_line () + geom_rect ( data = recess , inherit.aes = FALSE , aes ( xmin = begin , xmax = end , ymin = - Inf , ymax = + Inf ), fill = 'red' , alpha = 0.2 ) Multiple time series, part 1 1 2 # use gather to go from fish to fish.tidy. fish.tidy <- gather ( fish , Species , Capture , - Year ) Multiple time series, part 2 1 2 3 # plot ggplot ( fish.tidy , aes ( x = Year , y = Capture , col = Species )) + geom_line () qplot and wrap-up \u00b6 Using qplot 1 2 # the old way plot ( mpg ~ wt , data = mtcars ) 1 2 3 # using ggplot ggplot ( mtcars , aes ( x = wt , y = mpg )) + geom_point ( shape = 1 ) 1 2 # Using qplot qplot ( wt , mpg , data = mtcars ) Using aesthetics 1 2 # Categorical: cyl qplot ( wt , mpg , data = mtcars , size = cyl ) 1 2 # gear qplot ( wt , mpg , data = mtcars , size = gear ) 1 2 # Continuous: hp qplot ( wt , mpg , data = mtcars , col = hp ) 1 2 # qsec qplot ( wt , mpg , data = mtcars , size = qsec ) Choosing geoms, part 1 1 2 # qplot() with x only qplot ( factor ( cyl ), data = mtcars ) 1 2 # qplot() with x and y qplot ( factor ( cyl ), factor ( vs ), data = mtcars ) 1 2 # qplot() with geom set to jitter manually qplot ( factor ( cyl ), factor ( vs ), data = mtcars , geom = 'jitter' ) Choosing geoms, part 2 - dotplot 1 2 3 # make a dot plot with ggplot ggplot ( mtcars , aes ( cyl , wt , fill = am )) + geom_dotplot ( stackdir = 'center' , binaxis = 'y' ) 1 2 # qplot with geom 'dotplot', binaxis = 'y' and stackdir = 'center' qplot ( as.numeric ( cyl ), wt , data = mtcars , fill = am , geom = 'dotplot' , stackdir = 'center' , binaxis = 'y' ) Chicken weight 1 2 3 # base ggplot ( ChickWeight , aes ( x = Time , y = weight )) + geom_line ( aes ( group = Chick )) 1 2 3 # color ggplot ( ChickWeight , aes ( x = Time , y = weight , col = Diet )) + geom_line ( aes ( group = Chick )) 1 2 3 4 # lines ggplot ( ChickWeight , aes ( x = Time , y = weight , col = Diet )) + geom_line ( aes ( group = Chick ), alpha = 0.3 ) + geom_smooth ( lwd = 2 , se = FALSE ) Titanic 1 2 3 # Use ggplot() for the first instruction ggplot ( titanic , aes ( x = factor ( Pclass ), fill = factor ( Sex ))) + geom_bar ( position = 'dodge' ) 1 2 3 4 # Use ggplot() for the second instruction ggplot ( titanic , aes ( x = factor ( Pclass ), fill = factor ( Sex ))) + geom_bar ( position = 'dodge' ) + facet_grid ( '. ~ Survived' ) 1 2 3 4 5 6 7 # position jitter posn.j <- position_jitter ( 0.5 , 0 ) # Use ggplot() for the last instruction ggplot ( titanic , aes ( x = factor ( Pclass ), y = Age , col = factor ( Sex ))) + geom_jitter ( size = 3 , alpha = 0.5 , position = posn.j ) + facet_grid ( '. ~ Survived' ) SECTION 2 \u00b6 Statistics \u00b6 Smoothing 1 2 3 4 # scatter plot with LOESS smooth with a CI ribbon ggplot ( mtcars , aes ( x = wt , y = mpg )) + geom_point () + geom_smooth () 1 2 3 4 # scatter plot with LOESS smooth without CI ggplot ( mtcars , aes ( x = wt , y = mpg )) + geom_point () + geom_smooth ( se = FALSE ) 1 2 3 4 # scatter plot with an OLS linear model ggplot ( mtcars , aes ( x = wt , y = mpg )) + geom_point () + geom_smooth ( method = 'lm' ) 1 2 3 # scatter plot with an OLS linear model without points ggplot ( mtcars , aes ( x = wt , y = mpg )) + geom_smooth ( method = 'lm' , se = FALSE ) Grouping variables 1 2 3 4 # cyl as a factor variable ggplot ( mtcars , aes ( x = wt , y = mpg , col = factor ( cyl ))) + geom_point () + stat_smooth ( method = 'lm' , se = FALSE ) 1 2 3 4 # set the group aesthetic ggplot ( mtcars , aes ( x = wt , y = mpg , col = factor ( cyl ), group = 1 )) + geom_point () + stat_smooth ( method = 'lm' , se = F ) 1 2 3 4 5 # add a second smooth layer in which the group aesthetic is set ggplot ( mtcars , aes ( x = wt , y = mpg , col = factor ( cyl ))) + geom_point () + stat_smooth ( method = 'lm' , se = FALSE ) + stat_smooth ( method = 'lm' , se = FALSE , aes ( group = 1 )) Modifying stat_smooth 1 2 3 4 # change the LOESS span ggplot ( mtcars , aes ( x = wt , y = mpg )) + geom_point () + geom_smooth ( se = FALSE , span = 0.7 , method = 'auto' ) 1 # method = 'auto' is by default 1 2 3 4 5 # set the model to the default LOESS and use a span of 0.7 ggplot ( mtcars , aes ( x = wt , y = mpg , col = factor ( cyl ))) + geom_point () + stat_smooth ( method = 'lm' , se = FALSE ) + stat_smooth ( method = 'auto' , se = FALSE , aes ( group = 1 ), col = 'black' , span = 0.7 ) 1 2 3 4 5 # set col to 'All', inside the aes layer ggplot ( mtcars , aes ( x = wt , y = mpg , col = factor ( cyl ))) + geom_point () + stat_smooth ( method = 'lm' , se = FALSE ) + stat_smooth ( method = 'auto' , se = FALSE , aes ( group = 1 , col = 'All cyl' ), span = 0.7 ) 1 2 3 4 5 6 7 8 # add `scale_color_manual` to change the colors myColors <- c ( brewer.pal ( 3 , 'Dark2' ), 'black' ) ggplot ( mtcars , aes ( x = wt , y = mpg , col = factor ( cyl ))) + geom_point () + stat_smooth ( method = 'lm' , se = FALSE ) + stat_smooth ( method = 'auto' , se = FALSE , aes ( group = 1 , col = 'All cyl' ), span = 0.7 ) + scale_color_manual ( 'Cylinders' , values = myColors ) Modifying stat_smooth (2) 1 2 3 4 # jittered scatter plot, add a linear model (lm) smooth ggplot ( Vocab , aes ( x = education , y = vocabulary )) + geom_jitter ( alpha = 0.2 ) + stat_smooth ( method = 'lm' , se = FALSE ) 1 2 3 # only lm, colored by year ggplot ( Vocab , aes ( x = education , y = vocabulary , col = factor ( year ))) + stat_smooth ( method = 'lm' , se = FALSE ) 1 2 3 4 # set a color brewer palette ggplot ( Vocab , aes ( x = education , y = vocabulary , col = factor ( year ))) + stat_smooth ( method = 'lm' , se = FALSE ) + scale_color_brewer ( 'Accent' ) 1 2 3 4 5 # change col and group, specify alpha, size and geom, and add scale_color_gradient ggplot ( Vocab , aes ( x = education , y = vocabulary , col = year , group = factor ( year ))) + stat_smooth ( method = 'lm' , se = FALSE , alpha = 0.6 , size = 2 , geom = 'path' ) + scale_color_brewer ( 'Blues' ) + scale_color_gradientn ( colors = brewer.pal ( 9 , 'YlOrRd' )) Quantiles 1 2 3 # use stat_quantile instead of stat_smooth ggplot ( Vocab , aes ( x = education , y = vocabulary , col = year , group = factor ( year ))) + stat_quantile ( alpha = 0.6 , size = 2 ) + scale_color_gradientn ( colors = brewer.pal ( 9 , 'YlOrRd' )) 1 2 3 4 # set quantile to 0.5 ggplot ( Vocab , aes ( x = education , y = vocabulary , col = year , group = factor ( year ))) + stat_quantile ( alpha = 0.6 , size = 2 , quantiles = c ( 0.5 )) + scale_color_gradientn ( colors = brewer.pal ( 9 , 'YlOrRd' )) Sum 1 2 3 4 5 6 7 # plot with linear and loess model p <- ggplot ( Vocab , aes ( x = education , y = vocabulary )) + stat_smooth ( method = 'loess' , aes ( col = 'red' ), se = F ) + stat_smooth ( method = 'lm' , aes ( col = 'blue' ), se = F ) + scale_color_discrete ( 'Model' , labels = c ( 'red' = 'LOESS' , 'blue' = 'lm' )) p 1 2 3 # add stat_sum (by overall proportion) p + stat_sum () 1 #aes(group = 1) 1 2 3 4 # set size range p + stat_sum () + scale_size ( range = c ( 1 , 10 )) 1 2 3 # proportional within years of education; set group aesthetic p + stat_sum ( aes ( group = education )) 1 2 3 # set the n p + stat_sum ( aes ( group = education , size = ..n.. )) Preparations 1 2 3 4 5 6 7 8 9 10 11 # convert cyl and am to factors mtcars $ cyl <- as.factor ( mtcars $ cyl ) mtcars $ am <- as.factor ( mtcars $ am ) # define positions posn.d <- position_dodge ( width = 0.1 ) posn.jd <- position_jitterdodge ( jitter.width = 0.1 , dodge.width = 0.2 ) posn.j <- position_jitter ( width = 0.2 ) # base layers wt.cyl.am <- ggplot ( mtcars , aes ( x = cyl , y = wt , col = am , group = am , fill = am )) Plotting variations 1 2 # base layer wt.cyl.am <- ggplot ( mtcars , aes ( x = cyl , y = wt , col = am , fill = am , group = am )) 1 2 3 # jittered, dodged scatter plot with transparent points wt.cyl.am + geom_point ( position = posn.jd , alpha = 0.6 ) 1 2 3 # mean and sd wt.cyl.am + geom_point ( position = posn.jd , alpha = 0.6 ) + stat_summary ( fun.data = mean_sdl , fun.args = list ( mult = 1 ), position = posn.d ) 1 2 3 4 # mean and 95% CI wt.cyl.am + geom_point ( position = posn.jd , alpha = 0.6 ) + stat_summary ( fun.data = mean_cl_normal , position = posn.d ) 1 2 3 4 5 # mean and SD with T-tipped error bars wt.cyl.am + geom_point ( position = posn.jd , alpha = 0.6 ) + stat_summary ( geom = 'point' , fun.y = mean , position = posn.d ) + stat_summary ( geom = 'errorbar' , fun.data = mean_sdl , fun.args = list ( mult = 1 ), width = 0.1 , position = posn.d ) Coordinates and Facets \u00b6 Zooming In 1 2 3 4 # basic p <- ggplot ( mtcars , aes ( x = wt , y = hp , col = am )) + geom_point () + geom_smooth () 1 2 3 # add scale_x_continuous p + scale_x_continuous ( limits = c ( 3 , 6 ), expand = c ( 0 , 0 )) 1 2 3 # zoom in p + coord_cartesian ( xlim = c ( 3 , 6 )) Aspect Ratio 1 2 3 4 # scatter plot base.plot <- ggplot ( iris , aes ( y = Sepal.Width , x = Sepal.Length , col = Species )) + geom_jitter () + geom_smooth ( method = 'lm' , se = FALSE ) 1 2 3 4 # default aspect ratio # fix aspect ratio (1:1) base.plot + coord_equal () 1 2 base.plot + coord_fixed () Pie Charts 1 2 3 4 5 # stacked bar plot thin.bar <- ggplot ( mtcars , aes ( x = 1 , fill = cyl )) + geom_bar () thin.bar 1 2 3 # convert thin.bar to pie chart thin.bar + coord_polar ( theta = 'y' ) 1 2 3 4 5 # create stacked bar plot wide.bar <- ggplot ( mtcars , aes ( x = 1 , fill = cyl )) + geom_bar ( width = 1 ) wide.bar 1 2 # Convert wide.bar to pie chart wide.bar + coord_polar ( theta = 'y' ) Facets: the basics 1 2 # scatter plot p <- ggplot ( mtcars , aes ( x = wt , y = mpg )) + geom_point () 1 2 3 4 # separate rows according am # facet_grid(rows ~ cols) p + facet_grid ( am ~ .) 1 2 3 # separate columns according to cyl # facet_grid(rows ~ cols) p + facet_grid ( . ~ cyl ) 1 2 3 4 # separate by both columns and rows # facet_grid(rows ~ cols) p + facet_grid ( am ~ cyl ) Many variables 1 2 3 4 5 # create the `cyl_am` col and `myCol` vector mtcars $ cyl_am <- paste ( mtcars $ cyl , mtcars $ am , sep = '_' ) myCol <- rbind ( brewer.pal ( 9 , 'Blues' ) [c ( 3 , 6 , 8 ) ] , brewer.pal ( 9 , 'Reds' ) [c ( 3 , 6 , 8 ) ] ) 1 2 3 4 # scatter plot, add color scale ggplot ( mtcars , aes ( x = wt , y = mpg , col = cyl_am )) + geom_point () + scale_color_manual ( values = myCol ) 1 2 3 4 5 # facet according on rows and columns ggplot ( mtcars , aes ( x = wt , y = mpg , col = cyl_am )) + geom_point () + scale_color_manual ( values = myCol ) + facet_grid ( gear ~ vs ) 1 2 3 4 5 # add more variables ggplot ( mtcars , aes ( x = wt , y = mpg , col = cyl_am , size = disp )) + geom_point () + scale_color_manual ( values = myCol ) + facet_grid ( gear ~ vs ) Dropping levels 1 2 3 # scatter plot ggplot ( mamsleep , aes ( x = time , y = name , col = sleep )) + geom_point () 1 2 3 4 # facet rows according to `vore` ggplot ( mamsleep , aes ( x = time , y = name , col = sleep )) + geom_point () + facet_grid ( vore ~ .) 1 2 3 4 # specify scale and space arguments to free up rows ggplot ( mamsleep , aes ( x = time , y = name , col = sleep )) + geom_point () + facet_grid ( vore ~ ., scale = 'free_y' , space = 'free_y' ) Themes \u00b6 Rectangles 1 2 3 4 5 6 7 8 9 10 11 12 13 14 # separate columns according to cyl # facet_grid(rows ~ cols) mtcars $ cyl <- c ( 6 , 6 , 4 , 6 , 8 , 6 , 8 , 4 , 4 , 6 , 6 , 8 , 8 , 8 , 8 , 8 , 8 , 4 , 4 , 4 , 4 , 8 , 8 , 8 , 8 , 4 , 4 , 4 , 8 , 6 , 8 , 4 ) mtcars $ Cylinders <- factor ( mtcars $ cyl ) z <- ggplot ( mtcars , aes ( x = wt , y = mpg , col = Cylinders )) + geom_point ( size = 2 , alpha = 0.7 ) + facet_grid ( . ~ cyl ) + labs ( x = 'Weight (lb/1000)' , y = 'Miles/(US) gallon' ) + geom_smooth ( method = 'lm' , se = FALSE ) + theme_base () + scale_colour_economist () z 1 2 3 4 5 # change the plot background color to myPink (#FEE0D2) myPink <- '#FEE0D2' z + theme ( plot.background = element_rect ( fill = myPink )) 1 2 3 # adjust the border to be a black line of size 3 z + theme ( plot.background = element_rect ( fill = myPink , color = 'black' , size = 3 )) 1 2 3 # adjust the border to be a black line of size 3 z + theme ( plot.background = element_rect ( color = 'black' , size = 3 )) 1 2 3 # set panel.background, legend.key, legend.background and strip.background to element_blank() z + theme ( plot.background = element_rect ( fill = myPink , color = 'black' , size = 3 ), panel.background = element_blank (), legend.key = element_blank (), legend.background = element_blank (), strip.background = element_blank ()) Lines 1 2 3 # Extend z with theme() and three arguments z + theme ( panel.grid = element_blank (), axis.line = element_line ( color = 'black' ), axis.ticks = element_line ( color = 'black' )) Text 1 2 3 4 5 # extend z with theme() function and four arguments myRed <- '#99000D' z + theme ( strip.text = element_text ( size = 16 , color = myRed ), axis.title.x = element_text ( color = myRed , hjust = 0 , face = 'italic' ), axis.title.y = element_text ( color = myRed , hjust = 0 , face = 'italic' ), axis.text = element_text ( color = 'black' )) Legends 1 2 3 # move legend by position z + theme ( legend.position = c ( 0.85 , 0.85 )) 1 2 3 # change direction z + theme ( legend.direction = 'horizontal' ) 1 2 3 # change location by name z + theme ( legend.position = 'bottom' ) 1 2 3 # remove legend entirely z + theme ( legend.position = 'none' ) Positions 1 2 3 # increase spacing between facets z + theme ( panel.margin.x = unit ( 2 , 'cm' )) 1 2 3 # add code to remove any excess plot margin space z + theme ( panel.margin.x = unit ( 2 , 'cm' ), plot.margin = unit ( c ( 0 , 0 , 0 , 0 ), 'cm' )) Update Themestheme update 1 2 # theme layer saved as an object, theme_pink theme_pink <- theme ( panel.background = element_blank (), legend.key = element_blank (), legend.background = element_blank (), strip.background = element_blank (), plot.background = element_rect ( fill = myPink , color = 'black' , size = 3 ), panel.grid = element_blank (), axis.line = element_line ( color = 'black' ), axis.ticks = element_line ( color = 'black' ), strip.text = element_text ( size = 16 , color = myRed ), axis.title.y = element_text ( color = myRed , hjust = 0 , face = 'italic' ), axis.title.x = element_text ( color = myRed , hjust = 0 , face = 'italic' ), axis.text = element_text ( color = 'black' ), legend.position = 'none' ) 1 2 3 4 5 z2 <- z # apply theme_pink to z2 z2 + theme_pink 1 2 # change code so that old theme is saved as old old <- theme_update ( panel.background = element_blank (), legend.key = element_blank (), legend.background = element_blank (), strip.background = element_blank (), plot.background = element_rect ( fill = myPink , color = 'black' , size = 3 ), panel.grid = element_blank (), axis.line = element_line ( color = 'black' ), axis.ticks = element_line ( color = 'black' ), strip.text = element_text ( size = 16 , color = myRed ), axis.title.y = element_text ( color = myRed , hjust = 0 , face = 'italic' ), axis.title.x = element_text ( color = myRed , hjust = 0 , face = 'italic' ), axis.text = element_text ( color = 'black' ), legend.position = 'none' ) 1 2 3 4 5 # display the plot z2 theme_set ( theme_pink ) z2 + theme_pink 1 2 3 4 # restore the old plot theme_set ( old ) z2 Exploring ggthemes 1 2 3 4 5 6 7 # apply theme_tufte # set the theme with theme_set theme_set ( theme_tufte ()) # or apply it in the ggplot command z2 + theme_tufte () 1 2 3 4 5 6 7 8 9 # apply theme_tufte, modified # set the theme with theme_set theme_set ( theme_tufte () + theme ( legend.position = c ( 0.9 , 0.9 ), axis.title = element_text ( face = 'italic' , size = 12 ), legend.title = element_text ( face = 'italic' , size = 12 ))) # or apply it in the ggplot command z2 + theme_tufte () + theme ( legend.position = c ( 0.9 , 0.9 ), axis.title = element_text ( face = 'italic' , size = 12 ), legend.title = element_text ( face = 'italic' , size = 12 )) 1 2 3 4 5 6 7 # apply theme_igray # set the theme with `theme_set` theme_set ( theme_igray ()) # or apply it in the ggplot command z2 + theme_igray () 1 2 3 4 5 6 7 8 9 10 11 # apply `theme_igray`, modified # set the theme with `theme_set` theme_set ( theme_igray () + theme ( legend.position = c ( 0.9 , 0.9 ), legend.key = element_blank (), legend.background = element_rect ( fill = 'grey90' ))) z2 + # Or apply it in the ggplot command theme_igray () + theme ( legend.position = c ( 0.9 , 0.9 ), legend.key = element_blank (), legend.background = element_rect ( fill = 'grey90' )) Best Practices \u00b6 Bar Plots (1) 1 2 # base layers m <- ggplot ( mtcars , aes ( x = cyl , y = wt )) 1 2 3 # dynamite plot m + stat_summary ( fun.y = mean , geom = 'bar' , fill = 'skyblue' ) + stat_summary ( fun.data = mean_sdl , fun.args = list ( mult = 1 ), geom = 'errorbar' , width = 0.1 ) Bar Plots (2) 1 2 # base layers m <- ggplot ( mtcars , aes ( x = cyl , y = wt , col = am , fill = am )) 1 2 3 # dynamite plot m + stat_summary ( fun.y = mean , geom = 'bar' ) + stat_summary ( fun.data = mean_sdl , fun.args = list ( mult = 1 ), geom = 'errorbar' , width = 0.1 ) 1 2 3 # set position dodge in each `stat` function m + stat_summary ( fun.y = mean , geom = 'bar' , position = 'dodge' ) + stat_summary ( fun.data = mean_sdl , fun.args = list ( mult = 1 ), geom = 'errorbar' , width = 0.1 , position = 'dodge' ) 1 2 # set your dodge `posn` manually posn.d <- position_dodge ( 0.9 ) 1 2 3 # redraw dynamite plot m + stat_summary ( fun.y = mean , geom = 'bar' , position = posn.d ) + stat_summary ( fun.data = mean_sdl , fun.args = list ( mult = 1 ), geom = 'errorbar' , width = 0.1 , position = posn.d ) Bar Plots (3) 1 2 3 # base layers mtcars.cyl <- mtcars %>% group_by ( cyl ) %>% summarise ( wt.avg = mean ( wt )) mtcars.cyl 1 2 3 4 5 6 ## # A tibble: 3 \u00d7 2 ## cyl wt.avg ## &lt;dbl&gt; &lt;dbl&gt; ## 1 4 2.285727 ## 2 6 3.117143 ## 3 8 3.999214 1 2 m <- ggplot ( mtcars.cyl , aes ( x = cyl , y = wt.avg )) m 1 2 3 # draw bar plot m + geom_bar ( stat = 'identity' , fill = 'skyblue' ) Pie Charts (1) 1 2 # bar chart to pie chart ggplot ( mtcars , aes ( x = cyl , fill = am )) + geom_bar ( position = 'fill' ) 1 ggplot ( mtcars , aes ( x = cyl , fill = am )) + geom_bar ( position = 'fill' ) + facet_grid ( . ~ cyl ) 1 ggplot ( mtcars , aes ( x = factor ( 1 ), fill = am )) + geom_bar ( position = 'fill' ) + facet_grid ( . ~ cyl ) 1 ggplot ( mtcars , aes ( x = factor ( 1 ), fill = am )) + geom_bar ( position = 'fill' ) + facet_grid ( . ~ cyl ) + coord_polar ( theta = 'y' ) 1 ggplot ( mtcars , aes ( x = factor ( 1 ), fill = am )) + geom_bar ( position = 'fill' , width = 1 ) + facet_grid ( . ~ cyl ) + coord_polar ( theta = 'y' ) Parallel coordinate plot 1 2 3 4 # parallel coordinates plot using `GGally` # all columns except `am` (`am` column is the 9th) group_by_am <- 9 my_names_am <- ( 1 : 11 ) [ - group_by_am] 1 2 # parallel plot; each variable plotted as a z-score transformation ggparcoord ( mtcars , columns = my_names_am , groupColumn = group_by_am , alpha = 0.8 ) 1 2 # scaled between 0-1 and most discriminating variable first ggparcoord ( mtcars , columns = my_names_am , groupColumn = group_by_am , alpha = 0.8 , scale = 'uniminmax' , order = 'anyClass' ) 1 ggparcoord ( iris , columns = 1 : 4 , groupColumn = 'Species' ) # xlab, ylab, scale_x_discrete, them 1 ggparcoord ( iris , columns = 1 : 4 , groupColumn = 'Species' , scale = 'uniminmax' ) 1 ggparcoord ( iris , columns = 1 : 4 , groupColumn = 'Species' , scale = 'globalminmax' ) 1 ggparcoord ( iris , columns = 1 : 4 , groupColumn = 'Species' , mapping = aes ( size = 1 )) 1 ggparcoord ( iris , columns = 1 : 4 , groupColumn = 'Species' , alphaLines = 0.3 ) 1 ggparcoord ( iris , columns = 1 : 4 , groupColumn = 'Species' , scale = 'center' ) 1 ggparcoord ( iris , columns = 1 : 4 , groupColumn = 'Species' , scaleSummary = 'median' , missing = 'exclude' ) 1 ggparcoord ( iris , columns = 1 : 4 , groupColumn = 'Species' , order = 'allClass' ) # or custom filter 1 ggparcoord ( iris , columns = 1 : 4 , groupColumn = 'Species' , scale = 'std' ) Splom 1 2 3 4 5 6 library ( dplyr ) data ( Pima.tr2 , package = 'MASS' ) PimaV <- select ( Pima.tr2 , glu : age ) ggpairs ( PimaV , diag = list ( continuous = 'density' ), axisLabels = 'show' ) Heat Maps 1 2 # create color palette myColors <- brewer.pal ( 9 , 'Reds' ) 1 2 3 # heat map ggplot ( barley , aes ( x = year , y = variety , fill = yield )) + geom_tile () 1 2 3 4 # add facet_wrap(~ variable); not like facet_grid(. ~ variable) ggplot ( barley , aes ( x = year , y = variety , fill = yield )) + geom_tile () + facet_wrap ( ~ site , ncol = 1 ) 1 2 3 # ggplot ( barley , aes ( x = year , y = variety , fill = yield )) + geom_tile () + facet_wrap ( ~ site , ncol = 1 ) + scale_fill_gradientn ( colors = myColors ) Heat Maps Alternatives (1) 1 2 3 # line plots ggplot ( barley , aes ( x = year , y = yield , col = variety , group = variety )) + geom_line () + facet_wrap ( facets = ~ site , nrow = 1 ) Heat Maps Alternatives (2) 1 2 3 4 # overlapping ribbon plot ggplot ( barley , aes ( x = year , y = yield , col = site , group = site , fill = site )) + geom_line () + stat_summary ( fun.y = mean , geom = 'line' ) + stat_summary ( fun.data = mean_sdl , fun.args = list ( mult = 1 ), geom = 'ribbon' , col = NA , alpha = 0.1 ) Case Study \u00b6 Sort and order 1 2 3 4 5 6 7 8 # reorder data ( Cars93 , package = 'MASS' ) Cars93 <- within ( Cars93 , TypeWt <- reorder ( Type , Weight , mean )) Cars93 <- within ( Cars93 , Type1 <- factor ( Type , levels = c ( 'Small' , 'Sporty' , 'Compact' , 'Midsize' , 'Large' , 'Van' ))) with ( Cars93 , table ( TypeWt , Type1 )) 1 2 3 4 5 6 7 8 ## Type1 ## TypeWt Small Sporty Compact Midsize Large Van ## Small 21 0 0 0 0 0 ## Sporty 0 14 0 0 0 0 ## Compact 0 0 16 0 0 0 ## Midsize 0 0 0 22 0 0 ## Large 0 0 0 0 11 0 ## Van 0 0 0 0 0 9 1 2 3 4 ggplot ( Cars93 , aes ( TypeWt , 100 / MPG.city )) + geom_boxplot () + ylab ( 'Gallons per 100 miles' ) + xlab ( 'Car type' ) 1 2 3 4 5 6 7 8 Cars93 <- within ( Cars93 , { levels ( Type1 ) <- c ( 'Small' , 'Large' , 'Midsize' , 'Small' , 'Sporty' , 'Large' ) }) ggplot ( Cars93 , aes ( TypeWt , 100 / MPG.city )) + geom_boxplot () + ylab ( 'Gallons per 100 miles' ) + xlab ( 'Car type' ) Ensemble plots 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 library ( gridExtra ) data ( Fertility , package = 'AER' ) p0 <- ggplot ( Fertility ) + geom_histogram ( binwidth = 1 ) + ylab ( '' ) p1 <- p0 + aes ( x = age ) p2 <- p0 + aes ( x = work ) + xlab ( 'Weeks worked in 1979' ) k <- ggplot ( Fertility ) + geom_bar () + ylab ( '' ) + ylim ( 0 , 250000 ) p3 <- k + aes ( x = morekids ) + xlab ( 'has more children' ) p4 <- k + aes ( x = gender1 ) + xlab ( 'first child' ) p5 <- k + aes ( x = gender2 ) + xlab ( 'second child' ) p6 <- k + aes ( x = afam ) + xlab ( 'African-American' ) p7 <- k + aes ( x = hispanic ) + xlab ( 'Hispanic' ) p8 <- k + aes ( x = other ) + xlab ( 'other race' ) grid.arrange ( arrangeGrob ( p1 , p2 , ncol = 2 , widths = c ( 3 , 3 )), arrangeGrob ( p3 , p4 , p5 , p6 , p7 , p8 , ncol = 6 ), nrow = 2 , heights = c ( 1.25 , 1 )) Exploring Data 1 2 3 # histogram ggplot ( adult , aes ( x = SRAGE_P )) + geom_histogram () 1 2 3 # histogram ggplot ( adult , aes ( x = BMI_P )) + geom_histogram () 1 2 3 # color, default binwidth ggplot ( adult , aes ( x = SRAGE_P , fill = factor ( RBMI ))) + geom_histogram ( binwidth = 1 ) Data Cleaning 1 2 3 4 5 6 7 8 9 10 11 # remove individual aboves 84 adult <- adult[adult $ SRAGE_P <= 84 , ] # remove individuals with a BMI below 16 and above or equal to 52 adult <- adult[adult $ BMI_P >= 16 & adult $ BMI_P < 52 , ] # relabel race adult $ RACEHPR2 <- factor ( adult $ RACEHPR2 , labels = c ( 'Latino' , 'Asian' , 'African American' , 'White' )) # relabel the BMI categories variable adult $ RBMI <- factor ( adult $ RBMI , labels = c ( 'Under-weight' , 'Normal-weight' , 'Over-weight' , 'Obese' )) Multiple Histograms 1 2 # color palette BMI_fill BMI_fill <- scale_fill_brewer ( 'BMI Category' , palette = 'Reds' ) 1 2 3 4 5 # histogram, add BMI_fill and customizations ggplot ( adult , aes ( x = SRAGE_P , fill = factor ( RBMI ))) + geom_histogram ( binwidth = 1 ) + BMI_fill + facet_grid ( RBMI ~ .) + theme_classic () Alternatives 1 2 3 4 # count histogram ggplot ( adult , aes ( x = SRAGE_P , fill = factor ( RBMI ))) + geom_histogram ( binwidth = 1 ) + BMI_fill 1 2 3 4 # density histogram ggplot ( adult , aes ( x = SRAGE_P , fill = factor ( RBMI ))) + geom_histogram ( aes ( y = ..density.. ), binwidth = 1 ) + BMI_fill 1 2 3 4 # faceted count histogram ggplot ( adult , aes ( x = SRAGE_P , fill = factor ( RBMI ))) + geom_histogram ( binwidth = 1 ) + BMI_fill + facet_grid ( RBMI ~ .) 1 2 3 4 # faceted density histogram ggplot ( adult , aes ( x = SRAGE_P , fill = factor ( RBMI ))) + geom_histogram ( aes ( y = ..density.. ), binwidth = 1 ) + BMI_fill + facet_grid ( RBMI ~ .) 1 2 3 4 # density histogram with `position = 'fill'` ggplot ( adult , aes ( x = SRAGE_P , fill = factor ( RBMI ))) + geom_histogram ( aes ( y = ..density.. ), binwidth = 1 , position = 'fill' ) + BMI_fill 1 2 3 4 # accurate histogram ggplot ( adult , aes ( x = SRAGE_P , fill = factor ( RBMI ))) + geom_histogram ( aes ( y = ..count.. / sum ( ..count.. )), binwidth = 1 , position = 'fill' ) + BMI_fill Do Things Manually 1 2 3 4 5 # an attempt to facet the accurate frequency histogram from before (failed) ggplot ( adult , aes ( x = SRAGE_P , fill = factor ( RBMI ))) + geom_histogram ( aes ( y = ..count.. / sum ( ..count.. )), binwidth = 1 , position = 'fill' ) + BMI_fill + facet_grid ( RBMI ~ .) 1 2 3 4 5 6 7 8 9 10 11 # create DF with `table()` DF <- table ( adult $ RBMI , adult $ SRAGE_P ) # use apply on DF to get frequency of each group DF_freq <- apply ( DF , 2 , function ( x ) x / sum ( x )) # melt on DF to create DF_melted DF_melted <- melt ( DF_freq ) # change names of DF_melted names ( DF_melted ) <- c ( 'FILL' , 'X' , 'value' ) 1 2 3 4 5 # add code to make this a faceted plot ggplot ( DF_melted , aes ( x = X , y = value , fill = FILL )) + geom_bar ( stat = 'identity' , position = 'stack' ) + BMI_fill + facet_grid ( FILL ~ .) Merimeko/Mosaic Plot 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 # The initial contingency table DF <- as.data.frame.matrix ( table ( adult $ SRAGE_P , adult $ RBMI )) # Add the columns groupsSum, xmax and xmin. Remove groupSum again. DF $ groupSum <- rowSums ( DF ) DF $ xmax <- cumsum ( DF $ groupSum ) DF $ xmin <- DF $ xmax - DF $ groupSum # The groupSum column needs to be removed, don't remove this line DF $ groupSum <- NULL # Copy row names to variable X DF $ X <- row.names ( DF ) # Melt the dataset DF_melted <- melt ( DF , id.vars = c ( 'X' , 'xmin' , 'xmax' ), variable.name = 'FILL' ) # dplyr call to calculate ymin and ymax - don't change DF_melted <- DF_melted %>% group_by ( X ) %>% mutate ( ymax = cumsum ( value / sum ( value )), ymin = ymax - value / sum ( value )) # Plot rectangles - don't change. ggplot ( DF_melted , aes ( ymin = ymin , ymax = ymax , xmin = xmin , xmax = xmax , fill = FILL )) + geom_rect ( colour = 'white' ) + scale_x_continuous ( expand = c ( 0 , 0 )) + scale_y_continuous ( expand = c ( 0 , 0 )) + BMI_fill + theme_tufte () Adding statistics 1 2 3 4 5 6 7 8 9 10 11 # perform chi.sq test (`RBMI` and `SRAGE_P`) results <- chisq.test ( table ( adult $ RBMI , adult $ SRAGE_P )) # melt results$residuals and store as resid resid <- melt ( results $ residuals ) # change names of resid names ( resid ) <- c ( 'FILL' , 'X' , 'residual' ) # merge the two datasets DF_all <- merge ( DF_melted , resid ) 1 2 3 4 5 6 7 # update plot command ggplot ( DF_all , aes ( ymin = ymin , ymax = ymax , xmin = xmin , xmax = xmax , fill = residual )) + geom_rect () + scale_fill_gradient2 () + scale_x_continuous ( expand = c ( 0 , 0 )) + scale_y_continuous ( expand = c ( 0 , 0 )) + theme_tufte () Adding text 1 2 3 4 5 6 # position for labels on x axis DF_all $ xtext <- DF_all $ xmin + ( DF_all $ xmax - DF_all $ xmin ) / 2 # position for labels on y axis index <- DF_all $ xmax == max ( DF_all $ xmax ) DF_all $ ytext <- DF_all $ ymin[index] + ( DF_all $ ymax[index] - DF_all $ ymin[index] ) / 2 1 2 3 4 5 6 7 8 9 10 # plot ggplot ( DF_all , aes ( ymin = ymin , ymax = ymax , xmin = xmin , xmax = xmax , fill = residual )) + geom_rect ( col = 'white' ) + # geom_text for ages (i.e. the x axis) geom_text ( aes ( x = xtext , label = X ), y = 1 , size = 3 , angle = 90 , hjust = 1 , show.legend = FALSE ) + # geom_text for BMI (i.e. the fill axis) geom_text ( aes ( x = max ( xmax ), y = ytext , label = FILL ), size = 3 , hjust = 1 , show.legend = FALSE ) + scale_fill_gradient2 () + theme_tufte () + theme ( legend.position = 'bottom' ) Generalizations 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 # script generalized into a function mosaicGG <- function ( data , X , FILL ) { # Proportions in raw data DF <- as.data.frame.matrix ( table ( data[[X]] , data[[FILL]] )) DF $ groupSum <- rowSums ( DF ) DF $ xmax <- cumsum ( DF $ groupSum ) DF $ xmin <- DF $ xmax - DF $ groupSum DF $ X <- row.names ( DF ) DF $ groupSum <- NULL DF_melted <- melt ( DF , id = c ( 'X' , 'xmin' , 'xmax' ), variable.name = 'FILL' ) DF_melted <- DF_melted %>% group_by ( X ) %>% mutate ( ymax = cumsum ( value / sum ( value )), ymin = ymax - value / sum ( value )) # Chi-sq test results <- chisq.test ( table ( data[[FILL]] , data[[X]] )) # fill and then x resid <- melt ( results $ residuals ) names ( resid ) <- c ( 'FILL' , 'X' , 'residual' ) # Merge data DF_all <- merge ( DF_melted , resid ) # Positions for labels DF_all $ xtext <- DF_all $ xmin + ( DF_all $ xmax - DF_all $ xmin ) / 2 index <- DF_all $ xmax == max ( DF_all $ xmax ) DF_all $ ytext <- DF_all $ ymin[index] + ( DF_all $ ymax[index] - DF_all $ ymin[index] ) / 2 # plot g <- ggplot ( DF_all , aes ( ymin = ymin , ymax = ymax , xmin = xmin , xmax = xmax , fill = residual )) + geom_rect ( col = 'white' ) + geom_text ( aes ( x = xtext , label = X ), y = 1 , size = 3 , angle = 90 , hjust = 1 , show.legend = FALSE ) + geom_text ( aes ( x = max ( xmax ), y = ytext , label = FILL ), size = 3 , hjust = 1 , show.legend = FALSE ) + scale_fill_gradient2 ( 'Residuals' ) + scale_x_continuous ( 'Individuals' , expand = c ( 0 , 0 )) + scale_y_continuous ( 'Proportion' , expand = c ( 0 , 0 )) + theme_tufte () + theme ( legend.position = 'bottom' ) print ( g ) } 1 2 # BMI described by age (in x) mosaicGG ( adult , 'SRAGE_P' , 'RBMI' ) 1 2 # poverty described by age (in x) mosaicGG ( adult , 'SRAGE_P' , 'POVLL' ) 1 2 # `am` described by `cyl` (in x) mosaicGG ( mtcars , 'cyl' , 'am' ) 1 2 # `Vocab` described by education mosaicGG ( Vocab , 'education' , 'vocabulary' ) SECTION 3 \u00b6 SECTION 4 - Cheat List \u00b6 ggplot(data, aes(x = , y = ), col = , fill = , size = , labels = , alpha = , shape = , line = , position = \u2018jitter\u2019) 1 2 3 4 5 6 7 8 9 10 11 12 13 + geom_point() + geom_point(aes(), col = , position = posn.j) + geom_jitter() + facet_grid(. ~ x) # y ~ x + scale_x_continous('Sepal Length', limits = c(2, 8), breaks = seq(2, 8, 3)) + scale_color_discrete('Species', labels = c('a', 'b', 'c')) + labs(x = , y = , col = ) posn.j <- position_jitter(width = 0.1) Data diamonds , prices of 50,000 round cut diamonds. economics , economics_long, US economic time series. faithfuld , 2d density estimate of Old Faithful data. luv_colours , colors(). midwest , midwest demographics. mpg , fuel economy data from 1999 and 2008 for 38 popular models of car. msleep , an updated and expanded version of the mammals sleep dataset. presidential , terms of 11 presidents from Eisenhower to Obama. seals , vector field of seal movements. txhousing , Housing sales in TX. Aesthetics x-axis. y-asix. color. fill. size (points, lines). labels. alpha. shape (points). linetype (lines). aes , Define aesth.etic mappings. aes_ (aes_q, aes_string), Define aesthetic mappings from strings, or quoted calls and formulas. aes_all , Given a character vector, create a set of identity mappings. aes_auto , Automatic aesthetic mapping. aes_colour_fill_alpha (color, colour, fill), Colour related aesthetics: colour, fill and alpha. aes_group_order (group), Aesthetics: group. aes_linetype_size_shape (linetype, shape, size), Differentiation related aesthetics: linetype, size, shape. aes_position (x, xend, xmax, xmin, y, yend, ymax, ymin), Position related aesthetics: x, y, xmin, xmax, ymin, ymax, xend, yend. Position position_dodge , Adjust position by dodging overlaps to the side. position_fill (position_stack), Stack overlapping objects on top of one another. position_identity , Don\u2019t adjust position position_nudge , Nudge points. position_jitter , Jitter points to avoid overplotting. position_jitterdodge , Adjust position by simultaneously dodging and jittering. Scales expand_limits , Expand the plot limits with data. guides , Set guides for each scale. guide_legend , Legend guide. guide_colourbar (guide_colorbar), Continuous colour bar guide. lims (xlim, ylim), Convenience functions to set the axis limits. scale_alpha (scale_alpha_continuous, scale_alpha_discrete), Alpha scales. scale_colour_brewer (scale_color_brewer, scale_color_distiller, scale_colour_distiller, scale_fill_brewer, scale_fill_distiller), Sequential, diverging and qualitative colour scales from colorbrewer.org scale_colour_gradient (scale_color_continuous, scale_color_gradient, scale_color_gradient2, scale_color_gradientn, scale_colour_continuous, scale_colour_date, scale_colour_datetime, scale_colour_gradient2, scale_colour_gradientn, scale_fill_continuous, scale_fill_date, scale_fill_datetime, scale_fill_gradient, scale_fill_gradient2, scale_fill_gradientn). scale_colour_grey (scale_color_grey, scale_fill_grey), Sequential grey colour scale. scale_colour_hue (scale_color_discrete, scale_color_hue, scale_colour_discrete, scale_fill_discrete, scale_fill_hue), Qualitative colour scale with evenly spaced hues. scale_identity (scale_alpha_identity, scale_color_identity, scale_colour_identity, scale_fill_identity, scale_linetype_identity, scale_shape_identity, scale_size_identity), Use values without scaling. scale_manual (scale_alpha_manual, scale_color_manual, scale_colour_manual, scale_fill_manual, scale_linetype_manual, scale_shape_manual, scale_size_manual), Create your own discrete scale. scale_linetype (scale_linetype_continuous, scale_linetype_discrete), Scale for line patterns. scale_shape (scale_shape_continuous, scale_shape_discrete), Scale for shapes, aka glyphs. scale_size (scale_radius, scale_size_area, scale_size_continuous, scale_size_date, scale_size_datetime, scale_size_discrete), Scale size (area or radius). scale_x_discrete (scale_y_discrete), Discrete position. labs (ggtitle, xlab, ylab), Change axis labels and legend titles. update_labels , Update axis/legend labels. Geometries point. line. histogram. bar. boxplot. geom_abline (geom_hline, geom_vline), Lines: horizontal, vertical, and specified by slope and intercept. geom_bar (stat_count), Bars, rectangles with bases on x-axis geom_bin2d (stat_bin2d, stat_bin_2d), Add heatmap of 2d bin counts. geom_blank , Blank, draws nothing. geom_boxplot (stat_boxplot), Box and whiskers plot. geom_contour (stat_contour), Display contours of a 3d surface in 2d. geom_count (stat_sum), Count the number of observations at each location. geom_crossbar (geom_errorbar, geom_linerange, geom_pointrange), Vertical intervals: lines, crossbars & errorbars. geom_density (stat_density), Display a smooth density estimate. geom_density_2d (geom_density2d, stat_density2d, stat_density_2d), Contours from a 2d density estimate. geom_dotplot , Dot plot geom_errorbarh , Horizontal error bars. geom_freqpoly (geom_histogram, stat_bin), Histograms and frequency polygons. geom_hex (stat_bin_hex, stat_binhex), Hexagon binning. geom_jitter , Points, jittered to reduce overplotting. geom_label (geom_text), Textual annotations. geom_map , Polygons from a reference map. geom_path (geom_line, geom_step), Connect observations. geom_point , Points, as for a scatterplot. geom_polygon , Polygon, a filled path. geom_quantile (stat_quantile), Add quantile lines from a quantile regression. geom_raster (geom_rect, geom_tile), Draw rectangles. geom_ribbon (geom_area), Ribbons and area plots. geom_rug , Marginal rug plots. geom_segment (geom_curve), Line segments and curves. geom_smooth (stat_smooth), Add a smoothed conditional mean. geom_violin (stat_ydensity), Violin plot. Facets columns. rows. facet_grid , Lay out panels in a grid. facet_null , Facet specification: a single panel. facet_wrap , Wrap a 1d ribbon of panels into 2d. labeller , Generic labeller function for facets. label_bquote , Backquoted labeller. Annotation annotate , Create an annotation layer. annotation_custom , Annotation: Custom grob. annotation_logticks , Annotation: log tick marks. annotation_map , Annotation: maps. annotation_raster , Annotation: High-performance rectangular tiling. borders , Create a layer of map borders. Fortify fortify , Fortify a model with data. fortify-multcomp (fortify.cld, fortify.confint.glht, fortify.glht, fortify.summary.glht), Fortify methods for objects produced by. fortify.lm , Supplement the data fitted to a linear model with model fit statistics. fortify.map , Fortify method for map objects. fortify.sp (fortify.Line, fortify.Lines, fortify.Polygon, fortify.Polygons, fortify.SpatialLinesDataFrame, fortify.SpatialPolygons, fortify.SpatialPolygonsDataFrame), Fortify method for classes from the sp package. map_data , Create a data frame of map data. Statistics binning. smoothing. descriptive. inferential. stat_ecdf , Empirical Cumulative Density Function. stat_ellipse , Plot data ellipses. stat_function , Superimpose a function. stat_identity , Identity statistic. stat_qq (geom_qq), Calculation for quantile-quantile plot. stat_summary_2d (stat_summary2d, stat_summary_hex), Bin and summarise in 2d (rectangle & hexagons) stat_unique , Remove duplicates. Coordinates. cartesian. fixes. polar. limites. coord_cartesian , Cartesian coordinates. coord_fixed (coord_equal), Cartesian coordinates with fixed relationship between x and y scales. coord_flip , Flipped cartesian coordinates. coord_map (coord_quickmap), Map projections. coord_polar , Polar coordinates. coord_trans , Transformed cartesian coordinate system. Themes theme_bw theme_grey theme_classic theme_minimal ggthemes","title":"Plot Snippets - ggplot2"},{"location":"plot_snippets_ggplot2/#documentation","text":"ggplot2 . ggthemes . r4stats .","title":"Documentation"},{"location":"plot_snippets_ggplot2/#dataset","text":"For most examples, we use the mtcars , diamonds , iris , ChickWeight , recess , fish , Vocab , Titanic , mamsleep , barley , adult datasets.","title":"Dataset"},{"location":"plot_snippets_ggplot2/#the-ggplot2-package","text":"1 library ( ggplot2 ) Import additional packages. 1 2 3 4 5 6 7 8 9 library ( digest ) library ( grid ) library ( gtable ) library ( MASS ) library ( plyr ) library ( reshape2 ) library ( scales ) library ( stats ) library ( tidyr ) For this project, import additional packages. 1 2 3 4 5 6 7 library ( ggthemes ) library ( RColorBrewer ) library ( gridExtra ) library ( GGally ) library ( car ) library ( Hmisc ) library ( dplyr ) Suggested additional packages\u2026","title":"The ggplot2 Package"},{"location":"plot_snippets_ggplot2/#section-1","text":"","title":"SECTION 1"},{"location":"plot_snippets_ggplot2/#introduction","text":"Exploring ggplot2 , part 1 1 2 3 # basic plot ggplot ( mtcars , aes ( x = cyl , y = mpg )) + geom_point () Exploring ggplot2 , part 2 1 2 3 # cyl is a factor ggplot ( mtcars , aes ( x = factor ( cyl ), y = mpg )) + geom_point () Exploring ggplot2 , part 3 1 2 3 # scatter plot ggplot ( mtcars , aes ( x = wt , y = mpg )) + geom_point () 1 2 3 # add color ggplot ( mtcars , aes ( x = wt , y = mpg , col = disp )) + geom_point () 1 2 3 # change size ggplot ( mtcars , aes ( x = wt , y = mpg , size = disp )) + geom_point () Exploring ggplot2 , part 4 1 2 3 # Add geom_point() with + ggplot ( diamonds , aes ( x = Carat , y = PricePerCt )) + geom_point () 1 2 3 # Add geom_point() and geom_smooth() with + ggplot ( diamonds , aes ( x = Carat , y = PricePerCt )) + geom_point () + geom_smooth () Exploring ggplot2 , part 5 1 2 3 # only the smooth line ggplot ( diamonds , aes ( x = Carat , y = PricePerCt )) + geom_smooth () 1 2 3 # change col ggplot ( diamonds , aes ( x = Carat , y = PricePerCt , col = Clarity )) + geom_point () 1 2 3 # change the alpha ggplot ( diamonds , aes ( x = Carat , y = PricePerCt , col = Clarity )) + geom_point ( alpha = 0.4 ) Exploring ggplot2 , part 6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 # 2 facets for comparison library ( gridExtra ) data ( father.son , package = 'UsingR' ) a <- ggplot ( father.son , aes ( fheight , sheight )) + geom_point () + geom_smooth ( method = 'lm' , colour = 'red' ) + geom_abline ( slope = 1 , intercept = 0 ) b <- ggplot ( father.son , aes ( fheight , sheight )) + geom_point () + geom_smooth ( method = 'lm' , colour = 'red' , se = FALSE ) + stat_smooth () grid.arrange ( a , b , nrow = 1 ) 1 2 3 4 5 6 7 # load more data data ( oly12 , package = 'VGAMdata' ) # 2 facets for comparison ggplot ( oly12 , aes ( Height , Weight )) + geom_point ( size = 1 ) + facet_wrap ( ~ Sex , ncol = 1 ) 1 2 3 4 5 6 7 8 # create a new variable inside de data frame oly12S <- within ( oly12 , oly12 $ Sport <- abbreviate ( oly12 $ Sport , 12 )) # multiple facets or splom ggplot ( oly12S , aes ( Height , Weight )) + geom_point ( size = 1 ) + facet_wrap ( ~ Sport ) + ggtitle ( 'Weight and Height by Sport' ) Understanding the grammar, part 1 1 2 3 4 5 6 # create the object containing the data and aes layers dia_plot <- ggplot ( diamonds , aes ( x = Carat , y = PricePerCt )) # add a geom layer dia_plot + geom_point () 1 2 3 # add the same geom layer, but with aes() inside dia_plot + geom_point ( aes ( col = Clarity )) Understanding the grammar, part 2 1 2 3 4 5 6 7 8 9 10 set.seed ( 1 ) # create the object containing the data and aes layers dia_plot <- ggplot ( diamonds , aes ( x = Carat , y = PricePerCt )) # add geom_point() with alpha set to 0.2 dia_plot <- dia_plot + geom_point ( alpha = 0.2 ) dia_plot 1 2 3 # plot dia_plot with additional geom_smooth() with se set to FALSE dia_plot + geom_smooth ( se = FALSE )","title":"Introduction"},{"location":"plot_snippets_ggplot2/#data","text":"Base package and ggplot2 , part 1 - plot 1 2 # basic plot plot ( mtcars $ wt , mtcars $ mpg , col = mtcars $ cyl ) 1 2 3 4 5 # change cyl inside mtcars to a factor mtcars $ cyl <- as.factor ( mtcars $ cyl ) # make the same plot as in the first instruction plot ( mtcars $ wt , mtcars $ mpg , col = mtcars $ cyl ) Base package and ggplot2 , part 2 - lm transfer to other 1 2 3 4 5 6 7 8 9 10 11 12 13 14 # Basic plot mtcars $ cyl <- as.factor ( mtcars $ cyl ) plot ( mtcars $ wt , mtcars $ mpg , col = mtcars $ cyl ) # use lm() to calculate a linear model and save it as carModel carModel <- lm ( mpg ~ wt , data = mtcars ) # Call abline() with carModel as first argument and lty as second abline ( carModel , lty = 2 ) # plot each subset efficiently with lapply lapply ( mtcars $ cyl , function ( x ) { abline ( lm ( mpg ~ wt , mtcars , subset = ( cyl == x )), col = x ) }) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 ## [[1]] ## NULL ## ## [[2]] ## NULL ## ## [[3]] ## NULL ## ## [[4]] ## NULL ## ## [[5]] ## NULL ## ## [[6]] ## NULL ## ## [[7]] ## NULL ## ## [[8]] ## NULL ## ## [[9]] ## NULL ## ## [[10]] ## NULL ## ## [[11]] ## NULL ## ## [[12]] ## NULL ## ## [[13]] ## NULL ## ## [[14]] ## NULL ## ## [[15]] ## NULL ## ## [[16]] ## NULL ## ## [[17]] ## NULL ## ## [[18]] ## NULL ## ## [[19]] ## NULL ## ## [[20]] ## NULL ## ## [[21]] ## NULL ## ## [[22]] ## NULL ## ## [[23]] ## NULL ## ## [[24]] ## NULL ## ## [[25]] ## NULL ## ## [[26]] ## NULL ## ## [[27]] ## NULL ## ## [[28]] ## NULL ## ## [[29]] ## NULL ## ## [[30]] ## NULL ## ## [[31]] ## NULL ## ## [[32]] ## NULL 1 2 # draw the legend of the plot legend ( x = 5 , y = 33 , legend = levels ( mtcars $ cyl ), col = 1 : 3 , pch = 1 , bty = 'n' ) Base package and ggplot2 , part 3 1 2 3 # scatter plot ggplot ( mtcars , aes ( x = wt , y = mpg , col = cyl )) + geom_point () 1 2 3 4 # include the lines of the linear models, per cyl ggplot ( mtcars , aes ( x = wt , y = mpg , col = cyl )) + geom_point () + geom_smooth ( method = 'lm' , se = FALSE ) 1 2 3 4 5 # include a lm for the entire dataset in its whole ggplot ( mtcars , aes ( x = wt , y = mpg , col = cyl )) + geom_point () + geom_smooth ( method = 'lm' , se = FALSE ) + geom_smooth ( aes ( group = 1 ), method = 'lm' , se = FALSE , linetype = 2 ) Variables to visuals, part 1 1 2 3 4 5 6 7 iris.tidy <- iris %>% gather ( key , Value , - Species ) %>% separate ( key , c ( 'Part' , 'Measure' ), '\\\\.' ) # create 2 facets ggplot ( iris.tidy , aes ( x = Species , y = Value , col = Part )) + geom_jitter () + facet_grid ( . ~ Measure ) Variables to visuals, part 2 1 2 3 4 5 6 7 8 9 10 11 12 # Add a new column, Flower, to iris that contains unique ids iris $ Flower <- 1 : nrow ( iris ) iris.wide <- iris %>% gather ( key , value , - Species , - Flower ) %>% separate ( key , c ( 'Part' , 'Measure' ), '\\\\.' ) %>% spread ( Measure , value ) # create 3 facets ggplot ( iris.wide , aes ( x = Length , y = Width , col = Part )) + geom_jitter () + facet_grid ( . ~ Species )","title":"Data"},{"location":"plot_snippets_ggplot2/#aesthetics","text":"All about aesthetics, part 1 1 2 3 # map cyl to y ggplot ( mtcars , aes ( x = mpg , y = cyl )) + geom_point () 1 2 3 # map cyl to x ggplot ( mtcars , aes ( y = mpg , x = cyl )) + geom_point () 1 2 3 # map cyl to col ggplot ( mtcars , aes ( x = wt , y = mpg , col = cyl )) + geom_point () 1 2 3 # change shape and size of the points ggplot ( mtcars , aes ( x = wt , y = mpg , col = cyl )) + geom_point ( shape = 1 , size = 4 ) All about aesthetics, part 2 1 2 3 # map cyl to fill ggplot ( mtcars , aes ( x = wt , y = mpg , fill = cyl )) + geom_point () 1 2 3 # Change shape, size and alpha of the points in the above plot ggplot ( mtcars , aes ( x = wt , y = mpg , fill = cyl )) + geom_point ( shape = 16 , size = 6 , alpha = 0.6 ) All about aesthetics, part 3 1 2 3 # map cyl to size ggplot ( mtcars , aes ( x = wt , y = mpg , size = cyl )) + geom_point () 1 2 3 # map cyl to alpha ggplot ( mtcars , aes ( x = wt , y = mpg , alpha = cyl )) + geom_point () 1 2 3 # map cyl to shape ggplot ( mtcars , aes ( x = wt , y = mpg , shape = cyl , label = cyl )) + geom_point () 1 2 3 # map cyl to labels ggplot ( mtcars , aes ( x = wt , y = mpg , label = cyl )) + geom_text () All about attributes, part 1 1 2 3 4 5 6 # define a hexadecimal color my_color <- '#123456' # set the color aesthetic ggplot ( mtcars , aes ( x = wt , y = mpg , col = cyl )) + geom_point () 1 2 3 # set the color aesthetic and attribute ggplot ( mtcars , aes ( x = wt , y = mpg , col = cyl )) + geom_point ( col = my_color ) 1 2 3 # set the fill aesthetic and color, size and shape attributes ggplot ( mtcars , aes ( x = wt , y = mpg , fill = cyl )) + geom_point ( size = 10 , shape = 23 , col = my_color ) All about attributes, part 2 1 2 3 # draw points with alpha 0.5 ggplot ( mtcars , aes ( x = wt , y = mpg , fill = cyl )) + geom_point ( alpha = 0.5 ) 1 2 3 # raw points with shape 24 and color yellow ggplot ( mtcars , aes ( x = wt , y = mpg , fill = cyl )) + geom_point ( shape = 24 , col = 'yellow' ) 1 2 3 # draw text with label x, color red and size 10 ggplot ( mtcars , aes ( x = wt , y = mpg , fill = cyl )) + geom_text ( label = 'x' , col = 'red' , size = 10 ) Going all out 1 2 3 # Map mpg onto x, qsec onto y and factor(cyl) onto col ggplot ( mtcars , aes ( x = mpg , y = qsec , col = factor ( cyl ))) + geom_point () 1 2 3 # Add mapping: factor(am) onto shape ggplot ( mtcars , aes ( x = mpg , y = qsec , col = factor ( cyl ), shape = factor ( am ))) + geom_point () 1 2 3 # Add mapping: (hp/wt) onto size ggplot ( mtcars , aes ( x = mpg , y = qsec , col = factor ( cyl ), shape = factor ( am ), size = hp / wt )) + geom_point () 1 2 3 # Add mapping: rownames(mtcars) onto label ggplot ( mtcars , aes ( x = mpg , y = qsec , col = factor ( cyl ), shape = factor ( am ), size = hp / wt )) + geom_text ( aes ( label = rownames ( mtcars ))) Position 1 2 3 4 5 6 # base layers cyl.am <- ggplot ( mtcars , aes ( x = factor ( cyl ), fill = factor ( am ))) # add geom (position = 'stack'' by default) cyl.am + geom_bar ( position = 'stack' ) 1 2 3 # show proportion cyl.am + geom_bar ( position = 'fill' ) 1 2 3 # dodging cyl.am + geom_bar ( position = 'dodge' ) 1 2 3 4 5 6 7 8 # clean up the axes with scale_ functions val = c ( '#E41A1C' , '#377EB8' ) lab = c ( 'Manual' , 'Automatic' ) cyl.am + geom_bar ( position = 'dodge' , ) + scale_x_discrete ( 'Cylinders' ) + scale_y_continuous ( 'Number' ) + scale_fill_manual ( 'Transmission' , values = val , labels = lab ) Setting a dummy aesthetic 1 2 3 4 5 # add a new column called group mtcars $ group <- 0 # create jittered plot of mtcars: mpg onto x, group onto y ggplot ( mtcars , aes ( x = mpg , y = group )) + geom_jitter () 1 2 3 4 # change the y aesthetic limits ggplot ( mtcars , aes ( x = mpg , y = group )) + geom_jitter () + scale_y_continuous ( limits = c ( -2 , 2 )) Overplotting 1 - Point shape and transparency 1 2 3 # basic scatter plot: wt on x-axis and mpg on y-axis; map cyl to col ggplot ( mtcars , aes ( x = wt , y = mpg , col = cyl )) + geom_point ( size = 4 ) 1 2 3 # hollow circles - an improvement ggplot ( mtcars , aes ( x = wt , y = mpg , col = cyl )) + geom_point ( size = 4 , shape = 1 ) 1 2 3 # add transparency - very nice ggplot ( mtcars , aes ( x = wt , y = mpg , col = cyl )) + geom_point ( size = 4 , shape = 1 , alpha = 0.6 ) Overplotting 2 - alpha with large datasets 1 2 3 # scatter plot: carat (x), price (y), clarity (col) ggplot ( diamonds , aes ( x = Carat , y = PricePerCt , col = Clarity )) + geom_point () 1 2 3 # adjust for overplotting ggplot ( diamonds , aes ( x = Carat , y = PricePerCt , col = Clarity )) + geom_point ( alpha = 0.5 ) 1 2 3 # scatter plot: clarity (x), carat (y), price (col) ggplot ( diamonds , aes ( x = Clarity , y = Carat , col = PricePerCt )) + geom_point ( alpha = 0.5 ) 1 2 3 # dot plot with jittering ggplot ( diamonds , aes ( x = Clarity , y = Carat , col = PricePerCt )) + geom_point ( alpha = 0.5 , position = 'jitter' )","title":"Aesthetics"},{"location":"plot_snippets_ggplot2/#geometries","text":"Scatter plots and jittering (1) 1 2 3 # plot the cyl on the x-axis and wt on the y-axis ggplot ( mtcars , aes ( x = cyl , y = wt )) + geom_point () 1 2 3 # Use geom_jitter() instead of geom_point() ggplot ( mtcars , aes ( x = cyl , y = wt )) + geom_jitter () 1 2 3 4 5 6 # Define the position object using position_jitter(): posn.j posn.j <- position_jitter ( 0.1 ) # Use posn.j in geom_point() ggplot ( mtcars , aes ( x = cyl , y = wt )) + geom_point ( position = posn.j ) Scatter plots and jittering (2) 1 2 3 # scatter plot of vocabulary (y) against education (x). Use geom_point() ggplot ( Vocab , aes ( x = education , y = vocabulary )) + geom_point () 1 2 3 # use geom_jitter() instead of geom_point() ggplot ( Vocab , aes ( x = education , y = vocabulary )) + geom_jitter () 1 2 3 # set alpha to a very low 0.2 ggplot ( Vocab , aes ( x = education , y = vocabulary )) + geom_jitter ( alpha = 0.2 ) 1 2 3 # set the shape to 1 ggplot ( Vocab , aes ( x = education , y = vocabulary )) + geom_jitter ( alpha = 0.2 , shape = 1 ) Histograms 1 2 3 # univariate histogram ggplot ( mtcars , aes ( x = mpg )) + geom_histogram () 1 2 3 # change the bin width to 1 ggplot ( mtcars , aes ( x = mpg )) + geom_histogram ( binwidth = 1 ) 1 2 3 # change the y aesthetic to density ggplot ( mtcars , aes ( x = mpg )) + geom_histogram ( aes ( y = ..density.. ), binwidth = 1 ) 1 2 3 4 5 6 # custom color code myBlue <- '#377EB8' # Change the fill color to myBlue ggplot ( mtcars , aes ( x = mpg )) + geom_histogram ( aes ( y = ..density.. ), binwidth = 1 , fill = myBlue ) Position 1 2 3 4 5 mtcars $ am <- as.factor ( mtcars $ am ) # bar plot of cyl, filled according to am ggplot ( mtcars , aes ( x = cyl , fill = am )) + geom_bar () 1 2 3 # change the position argument to stack ggplot ( mtcars , aes ( x = cyl , fill = am )) + geom_bar ( position = 'stack' ) 1 2 3 # change the position argument to fill ggplot ( mtcars , aes ( x = cyl , fill = am )) + geom_bar ( position = 'fill' ) 1 2 3 # change the position argument to dodge ggplot ( mtcars , aes ( x = cyl , fill = am )) + geom_bar ( position = 'dodge' ) Overlapping bar plots 1 2 3 # bar plot of cyl, filled according to am ggplot ( mtcars , aes ( x = cyl , fill = am )) + geom_bar () 1 2 3 # change the position argument to 'dodge' ggplot ( mtcars , aes ( x = cyl , fill = am )) + geom_bar ( position = 'dodge' ) 1 2 3 4 5 6 # define posn_d with position_dodge() posn_d <- position_dodge ( 0.2 ) # change the position argument to posn_d ggplot ( mtcars , aes ( x = cyl , fill = am )) + geom_bar ( position = posn_d ) 1 2 3 # use posn_d as position and adjust alpha to 0.6 ggplot ( mtcars , aes ( x = cyl , fill = am )) + geom_bar ( position = posn_d , alpha = 0.6 ) Overlapping histograms 1 2 3 # histogram, add coloring defined by cyl ggplot ( mtcars , aes ( mpg , fill = cyl )) + geom_histogram ( binwidth = 1 ) 1 2 3 # change position to identity ggplot ( mtcars , aes ( mpg , fill = cyl )) + geom_histogram ( binwidth = 1 , position = 'identity' ) 1 2 3 # change geom to freqpoly (position is identity by default) ggplot ( mtcars , aes ( mpg , col = cyl )) + geom_freqpoly ( binwidth = 1 ) Facets or splom histograms 1 2 3 4 5 6 7 8 9 10 11 # load the package library ( reshape2 ) # load new data data ( uniranks , package = 'GDAdata' ) # name the variables names ( uniranks ) [c ( 5 , 6 , 8 , 8 , 10 , 11 , 13 ) ] <- c ( 'AvTeach' , 'NSSTeach' , 'SpendperSt' , 'StudentStaffR' , 'Careers' , 'VAddScore' , 'NSSFeedb' ) # reshape the data frame ur2 <- melt ( uniranks[ , c ( 3 , 5 : 13 ) ] , id.vars = 'UniGroup' , variable.name = 'uniV' , value.name = 'uniX' ) 1 2 3 4 5 6 # Splom ggplot ( ur2 , aes ( uniX )) + geom_histogram () + xlab ( '' ) + ylab ( '' ) + facet_grid ( UniGroup ~ uniV , scales = 'free_x' ) 1 2 3 4 5 6 7 8 9 10 11 12 library ( ggplot2 ) library ( gridExtra ) data ( Pima.tr2 , package = 'MASS' ) h1 <- ggplot ( Pima.tr2 , aes ( glu )) + geom_histogram () h2 <- ggplot ( Pima.tr2 , aes ( bp )) + geom_histogram () h3 <- ggplot ( Pima.tr2 , aes ( skin )) + geom_histogram () h4 <- ggplot ( Pima.tr2 , aes ( bmi )) + geom_histogram () h5 <- ggplot ( Pima.tr2 , aes ( ped )) + geom_histogram () h6 <- ggplot ( Pima.tr2 , aes ( age )) + geom_histogram () grid.arrange ( h1 , h2 , h3 , h4 , h5 , h6 , nrow = 2 ) Bar plots with color ramp, part 1 1 2 3 4 # Example of how to use a brewed color palette ggplot ( mtcars , aes ( x = cyl , fill = am )) + geom_bar () + scale_fill_brewer ( palette = 'Set1' ) 1 2 3 4 5 6 Vocab $ education <- as.factor ( Vocab $ education ) Vocab $ vocabulary <- as.factor ( Vocab $ vocabulary ) # Plot education on x and vocabulary on fill # Use the default brewed color palette ggplot ( Vocab , aes ( x = education , fill = vocabulary )) + geom_bar ( position = 'fill' ) + scale_fill_brewer ( palette = 'Set3' ) Bar plots with color ramp, part 2 1 2 3 4 5 6 7 8 9 10 # Definition of a set of blue colors blues <- brewer.pal ( 9 , 'Blues' ) # Make a color range using colorRampPalette() and the set of blues blue_range <- colorRampPalette ( blues ) # Use blue_range to adjust the color of the bars, use scale_fill_manual() ggplot ( Vocab , aes ( x = education , fill = vocabulary )) + geom_bar ( position = 'fill' ) + scale_fill_manual ( values = blue_range ( 11 )) Overlapping histograms (2) 1 2 # histogram ggplot ( mtcars , aes ( mpg )) + geom_histogram ( binwidth = 1 ) 1 2 3 # expand the histogram to fill using am ggplot ( mtcars , aes ( mpg , fill = am )) + geom_histogram ( binwidth = 1 ) 1 2 3 # change the position argument to 'dodge' ggplot ( mtcars , aes ( mpg , fill = am )) + geom_histogram ( position = 'dodge' , binwidth = 1 ) 1 2 3 # change the position argument to 'fill' ggplot ( mtcars , aes ( mpg , fill = am )) + geom_histogram ( position = 'fill' , binwidth = 1 ) 1 2 3 # change the position argument to 'identity' and set alpha to 0.4 ggplot ( mtcars , aes ( mpg , fill = am )) + geom_histogram ( position = 'identity' , binwidth = 1 , alpha = 0.4 ) 1 2 3 # change fill to cyl ggplot ( mtcars , aes ( mpg , fill = cyl )) + geom_histogram ( position = 'identity' , binwidth = 1 , alpha = 0.4 ) Line plots 1 2 3 # plot unemploy as a function of date using a line plot ggplot ( economics , aes ( x = date , y = unemploy )) + geom_line () 1 2 3 # adjust plot to represent the fraction of total population that is unemployed ggplot ( economics , aes ( x = date , y = unemploy / pop )) + geom_line () Periods of recession 1 2 3 4 # draw the recess periods ggplot ( economics , aes ( x = date , y = unemploy / pop )) + geom_line () + geom_rect ( data = recess , inherit.aes = FALSE , aes ( xmin = begin , xmax = end , ymin = - Inf , ymax = + Inf ), fill = 'red' , alpha = 0.2 ) Multiple time series, part 1 1 2 # use gather to go from fish to fish.tidy. fish.tidy <- gather ( fish , Species , Capture , - Year ) Multiple time series, part 2 1 2 3 # plot ggplot ( fish.tidy , aes ( x = Year , y = Capture , col = Species )) + geom_line ()","title":"Geometries"},{"location":"plot_snippets_ggplot2/#qplot-and-wrap-up","text":"Using qplot 1 2 # the old way plot ( mpg ~ wt , data = mtcars ) 1 2 3 # using ggplot ggplot ( mtcars , aes ( x = wt , y = mpg )) + geom_point ( shape = 1 ) 1 2 # Using qplot qplot ( wt , mpg , data = mtcars ) Using aesthetics 1 2 # Categorical: cyl qplot ( wt , mpg , data = mtcars , size = cyl ) 1 2 # gear qplot ( wt , mpg , data = mtcars , size = gear ) 1 2 # Continuous: hp qplot ( wt , mpg , data = mtcars , col = hp ) 1 2 # qsec qplot ( wt , mpg , data = mtcars , size = qsec ) Choosing geoms, part 1 1 2 # qplot() with x only qplot ( factor ( cyl ), data = mtcars ) 1 2 # qplot() with x and y qplot ( factor ( cyl ), factor ( vs ), data = mtcars ) 1 2 # qplot() with geom set to jitter manually qplot ( factor ( cyl ), factor ( vs ), data = mtcars , geom = 'jitter' ) Choosing geoms, part 2 - dotplot 1 2 3 # make a dot plot with ggplot ggplot ( mtcars , aes ( cyl , wt , fill = am )) + geom_dotplot ( stackdir = 'center' , binaxis = 'y' ) 1 2 # qplot with geom 'dotplot', binaxis = 'y' and stackdir = 'center' qplot ( as.numeric ( cyl ), wt , data = mtcars , fill = am , geom = 'dotplot' , stackdir = 'center' , binaxis = 'y' ) Chicken weight 1 2 3 # base ggplot ( ChickWeight , aes ( x = Time , y = weight )) + geom_line ( aes ( group = Chick )) 1 2 3 # color ggplot ( ChickWeight , aes ( x = Time , y = weight , col = Diet )) + geom_line ( aes ( group = Chick )) 1 2 3 4 # lines ggplot ( ChickWeight , aes ( x = Time , y = weight , col = Diet )) + geom_line ( aes ( group = Chick ), alpha = 0.3 ) + geom_smooth ( lwd = 2 , se = FALSE ) Titanic 1 2 3 # Use ggplot() for the first instruction ggplot ( titanic , aes ( x = factor ( Pclass ), fill = factor ( Sex ))) + geom_bar ( position = 'dodge' ) 1 2 3 4 # Use ggplot() for the second instruction ggplot ( titanic , aes ( x = factor ( Pclass ), fill = factor ( Sex ))) + geom_bar ( position = 'dodge' ) + facet_grid ( '. ~ Survived' ) 1 2 3 4 5 6 7 # position jitter posn.j <- position_jitter ( 0.5 , 0 ) # Use ggplot() for the last instruction ggplot ( titanic , aes ( x = factor ( Pclass ), y = Age , col = factor ( Sex ))) + geom_jitter ( size = 3 , alpha = 0.5 , position = posn.j ) + facet_grid ( '. ~ Survived' )","title":"qplot and wrap-up"},{"location":"plot_snippets_ggplot2/#section-2","text":"","title":"SECTION 2"},{"location":"plot_snippets_ggplot2/#statistics","text":"Smoothing 1 2 3 4 # scatter plot with LOESS smooth with a CI ribbon ggplot ( mtcars , aes ( x = wt , y = mpg )) + geom_point () + geom_smooth () 1 2 3 4 # scatter plot with LOESS smooth without CI ggplot ( mtcars , aes ( x = wt , y = mpg )) + geom_point () + geom_smooth ( se = FALSE ) 1 2 3 4 # scatter plot with an OLS linear model ggplot ( mtcars , aes ( x = wt , y = mpg )) + geom_point () + geom_smooth ( method = 'lm' ) 1 2 3 # scatter plot with an OLS linear model without points ggplot ( mtcars , aes ( x = wt , y = mpg )) + geom_smooth ( method = 'lm' , se = FALSE ) Grouping variables 1 2 3 4 # cyl as a factor variable ggplot ( mtcars , aes ( x = wt , y = mpg , col = factor ( cyl ))) + geom_point () + stat_smooth ( method = 'lm' , se = FALSE ) 1 2 3 4 # set the group aesthetic ggplot ( mtcars , aes ( x = wt , y = mpg , col = factor ( cyl ), group = 1 )) + geom_point () + stat_smooth ( method = 'lm' , se = F ) 1 2 3 4 5 # add a second smooth layer in which the group aesthetic is set ggplot ( mtcars , aes ( x = wt , y = mpg , col = factor ( cyl ))) + geom_point () + stat_smooth ( method = 'lm' , se = FALSE ) + stat_smooth ( method = 'lm' , se = FALSE , aes ( group = 1 )) Modifying stat_smooth 1 2 3 4 # change the LOESS span ggplot ( mtcars , aes ( x = wt , y = mpg )) + geom_point () + geom_smooth ( se = FALSE , span = 0.7 , method = 'auto' ) 1 # method = 'auto' is by default 1 2 3 4 5 # set the model to the default LOESS and use a span of 0.7 ggplot ( mtcars , aes ( x = wt , y = mpg , col = factor ( cyl ))) + geom_point () + stat_smooth ( method = 'lm' , se = FALSE ) + stat_smooth ( method = 'auto' , se = FALSE , aes ( group = 1 ), col = 'black' , span = 0.7 ) 1 2 3 4 5 # set col to 'All', inside the aes layer ggplot ( mtcars , aes ( x = wt , y = mpg , col = factor ( cyl ))) + geom_point () + stat_smooth ( method = 'lm' , se = FALSE ) + stat_smooth ( method = 'auto' , se = FALSE , aes ( group = 1 , col = 'All cyl' ), span = 0.7 ) 1 2 3 4 5 6 7 8 # add `scale_color_manual` to change the colors myColors <- c ( brewer.pal ( 3 , 'Dark2' ), 'black' ) ggplot ( mtcars , aes ( x = wt , y = mpg , col = factor ( cyl ))) + geom_point () + stat_smooth ( method = 'lm' , se = FALSE ) + stat_smooth ( method = 'auto' , se = FALSE , aes ( group = 1 , col = 'All cyl' ), span = 0.7 ) + scale_color_manual ( 'Cylinders' , values = myColors ) Modifying stat_smooth (2) 1 2 3 4 # jittered scatter plot, add a linear model (lm) smooth ggplot ( Vocab , aes ( x = education , y = vocabulary )) + geom_jitter ( alpha = 0.2 ) + stat_smooth ( method = 'lm' , se = FALSE ) 1 2 3 # only lm, colored by year ggplot ( Vocab , aes ( x = education , y = vocabulary , col = factor ( year ))) + stat_smooth ( method = 'lm' , se = FALSE ) 1 2 3 4 # set a color brewer palette ggplot ( Vocab , aes ( x = education , y = vocabulary , col = factor ( year ))) + stat_smooth ( method = 'lm' , se = FALSE ) + scale_color_brewer ( 'Accent' ) 1 2 3 4 5 # change col and group, specify alpha, size and geom, and add scale_color_gradient ggplot ( Vocab , aes ( x = education , y = vocabulary , col = year , group = factor ( year ))) + stat_smooth ( method = 'lm' , se = FALSE , alpha = 0.6 , size = 2 , geom = 'path' ) + scale_color_brewer ( 'Blues' ) + scale_color_gradientn ( colors = brewer.pal ( 9 , 'YlOrRd' )) Quantiles 1 2 3 # use stat_quantile instead of stat_smooth ggplot ( Vocab , aes ( x = education , y = vocabulary , col = year , group = factor ( year ))) + stat_quantile ( alpha = 0.6 , size = 2 ) + scale_color_gradientn ( colors = brewer.pal ( 9 , 'YlOrRd' )) 1 2 3 4 # set quantile to 0.5 ggplot ( Vocab , aes ( x = education , y = vocabulary , col = year , group = factor ( year ))) + stat_quantile ( alpha = 0.6 , size = 2 , quantiles = c ( 0.5 )) + scale_color_gradientn ( colors = brewer.pal ( 9 , 'YlOrRd' )) Sum 1 2 3 4 5 6 7 # plot with linear and loess model p <- ggplot ( Vocab , aes ( x = education , y = vocabulary )) + stat_smooth ( method = 'loess' , aes ( col = 'red' ), se = F ) + stat_smooth ( method = 'lm' , aes ( col = 'blue' ), se = F ) + scale_color_discrete ( 'Model' , labels = c ( 'red' = 'LOESS' , 'blue' = 'lm' )) p 1 2 3 # add stat_sum (by overall proportion) p + stat_sum () 1 #aes(group = 1) 1 2 3 4 # set size range p + stat_sum () + scale_size ( range = c ( 1 , 10 )) 1 2 3 # proportional within years of education; set group aesthetic p + stat_sum ( aes ( group = education )) 1 2 3 # set the n p + stat_sum ( aes ( group = education , size = ..n.. )) Preparations 1 2 3 4 5 6 7 8 9 10 11 # convert cyl and am to factors mtcars $ cyl <- as.factor ( mtcars $ cyl ) mtcars $ am <- as.factor ( mtcars $ am ) # define positions posn.d <- position_dodge ( width = 0.1 ) posn.jd <- position_jitterdodge ( jitter.width = 0.1 , dodge.width = 0.2 ) posn.j <- position_jitter ( width = 0.2 ) # base layers wt.cyl.am <- ggplot ( mtcars , aes ( x = cyl , y = wt , col = am , group = am , fill = am )) Plotting variations 1 2 # base layer wt.cyl.am <- ggplot ( mtcars , aes ( x = cyl , y = wt , col = am , fill = am , group = am )) 1 2 3 # jittered, dodged scatter plot with transparent points wt.cyl.am + geom_point ( position = posn.jd , alpha = 0.6 ) 1 2 3 # mean and sd wt.cyl.am + geom_point ( position = posn.jd , alpha = 0.6 ) + stat_summary ( fun.data = mean_sdl , fun.args = list ( mult = 1 ), position = posn.d ) 1 2 3 4 # mean and 95% CI wt.cyl.am + geom_point ( position = posn.jd , alpha = 0.6 ) + stat_summary ( fun.data = mean_cl_normal , position = posn.d ) 1 2 3 4 5 # mean and SD with T-tipped error bars wt.cyl.am + geom_point ( position = posn.jd , alpha = 0.6 ) + stat_summary ( geom = 'point' , fun.y = mean , position = posn.d ) + stat_summary ( geom = 'errorbar' , fun.data = mean_sdl , fun.args = list ( mult = 1 ), width = 0.1 , position = posn.d )","title":"Statistics"},{"location":"plot_snippets_ggplot2/#coordinates-and-facets","text":"Zooming In 1 2 3 4 # basic p <- ggplot ( mtcars , aes ( x = wt , y = hp , col = am )) + geom_point () + geom_smooth () 1 2 3 # add scale_x_continuous p + scale_x_continuous ( limits = c ( 3 , 6 ), expand = c ( 0 , 0 )) 1 2 3 # zoom in p + coord_cartesian ( xlim = c ( 3 , 6 )) Aspect Ratio 1 2 3 4 # scatter plot base.plot <- ggplot ( iris , aes ( y = Sepal.Width , x = Sepal.Length , col = Species )) + geom_jitter () + geom_smooth ( method = 'lm' , se = FALSE ) 1 2 3 4 # default aspect ratio # fix aspect ratio (1:1) base.plot + coord_equal () 1 2 base.plot + coord_fixed () Pie Charts 1 2 3 4 5 # stacked bar plot thin.bar <- ggplot ( mtcars , aes ( x = 1 , fill = cyl )) + geom_bar () thin.bar 1 2 3 # convert thin.bar to pie chart thin.bar + coord_polar ( theta = 'y' ) 1 2 3 4 5 # create stacked bar plot wide.bar <- ggplot ( mtcars , aes ( x = 1 , fill = cyl )) + geom_bar ( width = 1 ) wide.bar 1 2 # Convert wide.bar to pie chart wide.bar + coord_polar ( theta = 'y' ) Facets: the basics 1 2 # scatter plot p <- ggplot ( mtcars , aes ( x = wt , y = mpg )) + geom_point () 1 2 3 4 # separate rows according am # facet_grid(rows ~ cols) p + facet_grid ( am ~ .) 1 2 3 # separate columns according to cyl # facet_grid(rows ~ cols) p + facet_grid ( . ~ cyl ) 1 2 3 4 # separate by both columns and rows # facet_grid(rows ~ cols) p + facet_grid ( am ~ cyl ) Many variables 1 2 3 4 5 # create the `cyl_am` col and `myCol` vector mtcars $ cyl_am <- paste ( mtcars $ cyl , mtcars $ am , sep = '_' ) myCol <- rbind ( brewer.pal ( 9 , 'Blues' ) [c ( 3 , 6 , 8 ) ] , brewer.pal ( 9 , 'Reds' ) [c ( 3 , 6 , 8 ) ] ) 1 2 3 4 # scatter plot, add color scale ggplot ( mtcars , aes ( x = wt , y = mpg , col = cyl_am )) + geom_point () + scale_color_manual ( values = myCol ) 1 2 3 4 5 # facet according on rows and columns ggplot ( mtcars , aes ( x = wt , y = mpg , col = cyl_am )) + geom_point () + scale_color_manual ( values = myCol ) + facet_grid ( gear ~ vs ) 1 2 3 4 5 # add more variables ggplot ( mtcars , aes ( x = wt , y = mpg , col = cyl_am , size = disp )) + geom_point () + scale_color_manual ( values = myCol ) + facet_grid ( gear ~ vs ) Dropping levels 1 2 3 # scatter plot ggplot ( mamsleep , aes ( x = time , y = name , col = sleep )) + geom_point () 1 2 3 4 # facet rows according to `vore` ggplot ( mamsleep , aes ( x = time , y = name , col = sleep )) + geom_point () + facet_grid ( vore ~ .) 1 2 3 4 # specify scale and space arguments to free up rows ggplot ( mamsleep , aes ( x = time , y = name , col = sleep )) + geom_point () + facet_grid ( vore ~ ., scale = 'free_y' , space = 'free_y' )","title":"Coordinates and Facets"},{"location":"plot_snippets_ggplot2/#themes","text":"Rectangles 1 2 3 4 5 6 7 8 9 10 11 12 13 14 # separate columns according to cyl # facet_grid(rows ~ cols) mtcars $ cyl <- c ( 6 , 6 , 4 , 6 , 8 , 6 , 8 , 4 , 4 , 6 , 6 , 8 , 8 , 8 , 8 , 8 , 8 , 4 , 4 , 4 , 4 , 8 , 8 , 8 , 8 , 4 , 4 , 4 , 8 , 6 , 8 , 4 ) mtcars $ Cylinders <- factor ( mtcars $ cyl ) z <- ggplot ( mtcars , aes ( x = wt , y = mpg , col = Cylinders )) + geom_point ( size = 2 , alpha = 0.7 ) + facet_grid ( . ~ cyl ) + labs ( x = 'Weight (lb/1000)' , y = 'Miles/(US) gallon' ) + geom_smooth ( method = 'lm' , se = FALSE ) + theme_base () + scale_colour_economist () z 1 2 3 4 5 # change the plot background color to myPink (#FEE0D2) myPink <- '#FEE0D2' z + theme ( plot.background = element_rect ( fill = myPink )) 1 2 3 # adjust the border to be a black line of size 3 z + theme ( plot.background = element_rect ( fill = myPink , color = 'black' , size = 3 )) 1 2 3 # adjust the border to be a black line of size 3 z + theme ( plot.background = element_rect ( color = 'black' , size = 3 )) 1 2 3 # set panel.background, legend.key, legend.background and strip.background to element_blank() z + theme ( plot.background = element_rect ( fill = myPink , color = 'black' , size = 3 ), panel.background = element_blank (), legend.key = element_blank (), legend.background = element_blank (), strip.background = element_blank ()) Lines 1 2 3 # Extend z with theme() and three arguments z + theme ( panel.grid = element_blank (), axis.line = element_line ( color = 'black' ), axis.ticks = element_line ( color = 'black' )) Text 1 2 3 4 5 # extend z with theme() function and four arguments myRed <- '#99000D' z + theme ( strip.text = element_text ( size = 16 , color = myRed ), axis.title.x = element_text ( color = myRed , hjust = 0 , face = 'italic' ), axis.title.y = element_text ( color = myRed , hjust = 0 , face = 'italic' ), axis.text = element_text ( color = 'black' )) Legends 1 2 3 # move legend by position z + theme ( legend.position = c ( 0.85 , 0.85 )) 1 2 3 # change direction z + theme ( legend.direction = 'horizontal' ) 1 2 3 # change location by name z + theme ( legend.position = 'bottom' ) 1 2 3 # remove legend entirely z + theme ( legend.position = 'none' ) Positions 1 2 3 # increase spacing between facets z + theme ( panel.margin.x = unit ( 2 , 'cm' )) 1 2 3 # add code to remove any excess plot margin space z + theme ( panel.margin.x = unit ( 2 , 'cm' ), plot.margin = unit ( c ( 0 , 0 , 0 , 0 ), 'cm' )) Update Themestheme update 1 2 # theme layer saved as an object, theme_pink theme_pink <- theme ( panel.background = element_blank (), legend.key = element_blank (), legend.background = element_blank (), strip.background = element_blank (), plot.background = element_rect ( fill = myPink , color = 'black' , size = 3 ), panel.grid = element_blank (), axis.line = element_line ( color = 'black' ), axis.ticks = element_line ( color = 'black' ), strip.text = element_text ( size = 16 , color = myRed ), axis.title.y = element_text ( color = myRed , hjust = 0 , face = 'italic' ), axis.title.x = element_text ( color = myRed , hjust = 0 , face = 'italic' ), axis.text = element_text ( color = 'black' ), legend.position = 'none' ) 1 2 3 4 5 z2 <- z # apply theme_pink to z2 z2 + theme_pink 1 2 # change code so that old theme is saved as old old <- theme_update ( panel.background = element_blank (), legend.key = element_blank (), legend.background = element_blank (), strip.background = element_blank (), plot.background = element_rect ( fill = myPink , color = 'black' , size = 3 ), panel.grid = element_blank (), axis.line = element_line ( color = 'black' ), axis.ticks = element_line ( color = 'black' ), strip.text = element_text ( size = 16 , color = myRed ), axis.title.y = element_text ( color = myRed , hjust = 0 , face = 'italic' ), axis.title.x = element_text ( color = myRed , hjust = 0 , face = 'italic' ), axis.text = element_text ( color = 'black' ), legend.position = 'none' ) 1 2 3 4 5 # display the plot z2 theme_set ( theme_pink ) z2 + theme_pink 1 2 3 4 # restore the old plot theme_set ( old ) z2 Exploring ggthemes 1 2 3 4 5 6 7 # apply theme_tufte # set the theme with theme_set theme_set ( theme_tufte ()) # or apply it in the ggplot command z2 + theme_tufte () 1 2 3 4 5 6 7 8 9 # apply theme_tufte, modified # set the theme with theme_set theme_set ( theme_tufte () + theme ( legend.position = c ( 0.9 , 0.9 ), axis.title = element_text ( face = 'italic' , size = 12 ), legend.title = element_text ( face = 'italic' , size = 12 ))) # or apply it in the ggplot command z2 + theme_tufte () + theme ( legend.position = c ( 0.9 , 0.9 ), axis.title = element_text ( face = 'italic' , size = 12 ), legend.title = element_text ( face = 'italic' , size = 12 )) 1 2 3 4 5 6 7 # apply theme_igray # set the theme with `theme_set` theme_set ( theme_igray ()) # or apply it in the ggplot command z2 + theme_igray () 1 2 3 4 5 6 7 8 9 10 11 # apply `theme_igray`, modified # set the theme with `theme_set` theme_set ( theme_igray () + theme ( legend.position = c ( 0.9 , 0.9 ), legend.key = element_blank (), legend.background = element_rect ( fill = 'grey90' ))) z2 + # Or apply it in the ggplot command theme_igray () + theme ( legend.position = c ( 0.9 , 0.9 ), legend.key = element_blank (), legend.background = element_rect ( fill = 'grey90' ))","title":"Themes"},{"location":"plot_snippets_ggplot2/#best-practices","text":"Bar Plots (1) 1 2 # base layers m <- ggplot ( mtcars , aes ( x = cyl , y = wt )) 1 2 3 # dynamite plot m + stat_summary ( fun.y = mean , geom = 'bar' , fill = 'skyblue' ) + stat_summary ( fun.data = mean_sdl , fun.args = list ( mult = 1 ), geom = 'errorbar' , width = 0.1 ) Bar Plots (2) 1 2 # base layers m <- ggplot ( mtcars , aes ( x = cyl , y = wt , col = am , fill = am )) 1 2 3 # dynamite plot m + stat_summary ( fun.y = mean , geom = 'bar' ) + stat_summary ( fun.data = mean_sdl , fun.args = list ( mult = 1 ), geom = 'errorbar' , width = 0.1 ) 1 2 3 # set position dodge in each `stat` function m + stat_summary ( fun.y = mean , geom = 'bar' , position = 'dodge' ) + stat_summary ( fun.data = mean_sdl , fun.args = list ( mult = 1 ), geom = 'errorbar' , width = 0.1 , position = 'dodge' ) 1 2 # set your dodge `posn` manually posn.d <- position_dodge ( 0.9 ) 1 2 3 # redraw dynamite plot m + stat_summary ( fun.y = mean , geom = 'bar' , position = posn.d ) + stat_summary ( fun.data = mean_sdl , fun.args = list ( mult = 1 ), geom = 'errorbar' , width = 0.1 , position = posn.d ) Bar Plots (3) 1 2 3 # base layers mtcars.cyl <- mtcars %>% group_by ( cyl ) %>% summarise ( wt.avg = mean ( wt )) mtcars.cyl 1 2 3 4 5 6 ## # A tibble: 3 \u00d7 2 ## cyl wt.avg ## &lt;dbl&gt; &lt;dbl&gt; ## 1 4 2.285727 ## 2 6 3.117143 ## 3 8 3.999214 1 2 m <- ggplot ( mtcars.cyl , aes ( x = cyl , y = wt.avg )) m 1 2 3 # draw bar plot m + geom_bar ( stat = 'identity' , fill = 'skyblue' ) Pie Charts (1) 1 2 # bar chart to pie chart ggplot ( mtcars , aes ( x = cyl , fill = am )) + geom_bar ( position = 'fill' ) 1 ggplot ( mtcars , aes ( x = cyl , fill = am )) + geom_bar ( position = 'fill' ) + facet_grid ( . ~ cyl ) 1 ggplot ( mtcars , aes ( x = factor ( 1 ), fill = am )) + geom_bar ( position = 'fill' ) + facet_grid ( . ~ cyl ) 1 ggplot ( mtcars , aes ( x = factor ( 1 ), fill = am )) + geom_bar ( position = 'fill' ) + facet_grid ( . ~ cyl ) + coord_polar ( theta = 'y' ) 1 ggplot ( mtcars , aes ( x = factor ( 1 ), fill = am )) + geom_bar ( position = 'fill' , width = 1 ) + facet_grid ( . ~ cyl ) + coord_polar ( theta = 'y' ) Parallel coordinate plot 1 2 3 4 # parallel coordinates plot using `GGally` # all columns except `am` (`am` column is the 9th) group_by_am <- 9 my_names_am <- ( 1 : 11 ) [ - group_by_am] 1 2 # parallel plot; each variable plotted as a z-score transformation ggparcoord ( mtcars , columns = my_names_am , groupColumn = group_by_am , alpha = 0.8 ) 1 2 # scaled between 0-1 and most discriminating variable first ggparcoord ( mtcars , columns = my_names_am , groupColumn = group_by_am , alpha = 0.8 , scale = 'uniminmax' , order = 'anyClass' ) 1 ggparcoord ( iris , columns = 1 : 4 , groupColumn = 'Species' ) # xlab, ylab, scale_x_discrete, them 1 ggparcoord ( iris , columns = 1 : 4 , groupColumn = 'Species' , scale = 'uniminmax' ) 1 ggparcoord ( iris , columns = 1 : 4 , groupColumn = 'Species' , scale = 'globalminmax' ) 1 ggparcoord ( iris , columns = 1 : 4 , groupColumn = 'Species' , mapping = aes ( size = 1 )) 1 ggparcoord ( iris , columns = 1 : 4 , groupColumn = 'Species' , alphaLines = 0.3 ) 1 ggparcoord ( iris , columns = 1 : 4 , groupColumn = 'Species' , scale = 'center' ) 1 ggparcoord ( iris , columns = 1 : 4 , groupColumn = 'Species' , scaleSummary = 'median' , missing = 'exclude' ) 1 ggparcoord ( iris , columns = 1 : 4 , groupColumn = 'Species' , order = 'allClass' ) # or custom filter 1 ggparcoord ( iris , columns = 1 : 4 , groupColumn = 'Species' , scale = 'std' ) Splom 1 2 3 4 5 6 library ( dplyr ) data ( Pima.tr2 , package = 'MASS' ) PimaV <- select ( Pima.tr2 , glu : age ) ggpairs ( PimaV , diag = list ( continuous = 'density' ), axisLabels = 'show' ) Heat Maps 1 2 # create color palette myColors <- brewer.pal ( 9 , 'Reds' ) 1 2 3 # heat map ggplot ( barley , aes ( x = year , y = variety , fill = yield )) + geom_tile () 1 2 3 4 # add facet_wrap(~ variable); not like facet_grid(. ~ variable) ggplot ( barley , aes ( x = year , y = variety , fill = yield )) + geom_tile () + facet_wrap ( ~ site , ncol = 1 ) 1 2 3 # ggplot ( barley , aes ( x = year , y = variety , fill = yield )) + geom_tile () + facet_wrap ( ~ site , ncol = 1 ) + scale_fill_gradientn ( colors = myColors ) Heat Maps Alternatives (1) 1 2 3 # line plots ggplot ( barley , aes ( x = year , y = yield , col = variety , group = variety )) + geom_line () + facet_wrap ( facets = ~ site , nrow = 1 ) Heat Maps Alternatives (2) 1 2 3 4 # overlapping ribbon plot ggplot ( barley , aes ( x = year , y = yield , col = site , group = site , fill = site )) + geom_line () + stat_summary ( fun.y = mean , geom = 'line' ) + stat_summary ( fun.data = mean_sdl , fun.args = list ( mult = 1 ), geom = 'ribbon' , col = NA , alpha = 0.1 )","title":"Best Practices"},{"location":"plot_snippets_ggplot2/#case-study","text":"Sort and order 1 2 3 4 5 6 7 8 # reorder data ( Cars93 , package = 'MASS' ) Cars93 <- within ( Cars93 , TypeWt <- reorder ( Type , Weight , mean )) Cars93 <- within ( Cars93 , Type1 <- factor ( Type , levels = c ( 'Small' , 'Sporty' , 'Compact' , 'Midsize' , 'Large' , 'Van' ))) with ( Cars93 , table ( TypeWt , Type1 )) 1 2 3 4 5 6 7 8 ## Type1 ## TypeWt Small Sporty Compact Midsize Large Van ## Small 21 0 0 0 0 0 ## Sporty 0 14 0 0 0 0 ## Compact 0 0 16 0 0 0 ## Midsize 0 0 0 22 0 0 ## Large 0 0 0 0 11 0 ## Van 0 0 0 0 0 9 1 2 3 4 ggplot ( Cars93 , aes ( TypeWt , 100 / MPG.city )) + geom_boxplot () + ylab ( 'Gallons per 100 miles' ) + xlab ( 'Car type' ) 1 2 3 4 5 6 7 8 Cars93 <- within ( Cars93 , { levels ( Type1 ) <- c ( 'Small' , 'Large' , 'Midsize' , 'Small' , 'Sporty' , 'Large' ) }) ggplot ( Cars93 , aes ( TypeWt , 100 / MPG.city )) + geom_boxplot () + ylab ( 'Gallons per 100 miles' ) + xlab ( 'Car type' ) Ensemble plots 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 library ( gridExtra ) data ( Fertility , package = 'AER' ) p0 <- ggplot ( Fertility ) + geom_histogram ( binwidth = 1 ) + ylab ( '' ) p1 <- p0 + aes ( x = age ) p2 <- p0 + aes ( x = work ) + xlab ( 'Weeks worked in 1979' ) k <- ggplot ( Fertility ) + geom_bar () + ylab ( '' ) + ylim ( 0 , 250000 ) p3 <- k + aes ( x = morekids ) + xlab ( 'has more children' ) p4 <- k + aes ( x = gender1 ) + xlab ( 'first child' ) p5 <- k + aes ( x = gender2 ) + xlab ( 'second child' ) p6 <- k + aes ( x = afam ) + xlab ( 'African-American' ) p7 <- k + aes ( x = hispanic ) + xlab ( 'Hispanic' ) p8 <- k + aes ( x = other ) + xlab ( 'other race' ) grid.arrange ( arrangeGrob ( p1 , p2 , ncol = 2 , widths = c ( 3 , 3 )), arrangeGrob ( p3 , p4 , p5 , p6 , p7 , p8 , ncol = 6 ), nrow = 2 , heights = c ( 1.25 , 1 )) Exploring Data 1 2 3 # histogram ggplot ( adult , aes ( x = SRAGE_P )) + geom_histogram () 1 2 3 # histogram ggplot ( adult , aes ( x = BMI_P )) + geom_histogram () 1 2 3 # color, default binwidth ggplot ( adult , aes ( x = SRAGE_P , fill = factor ( RBMI ))) + geom_histogram ( binwidth = 1 ) Data Cleaning 1 2 3 4 5 6 7 8 9 10 11 # remove individual aboves 84 adult <- adult[adult $ SRAGE_P <= 84 , ] # remove individuals with a BMI below 16 and above or equal to 52 adult <- adult[adult $ BMI_P >= 16 & adult $ BMI_P < 52 , ] # relabel race adult $ RACEHPR2 <- factor ( adult $ RACEHPR2 , labels = c ( 'Latino' , 'Asian' , 'African American' , 'White' )) # relabel the BMI categories variable adult $ RBMI <- factor ( adult $ RBMI , labels = c ( 'Under-weight' , 'Normal-weight' , 'Over-weight' , 'Obese' )) Multiple Histograms 1 2 # color palette BMI_fill BMI_fill <- scale_fill_brewer ( 'BMI Category' , palette = 'Reds' ) 1 2 3 4 5 # histogram, add BMI_fill and customizations ggplot ( adult , aes ( x = SRAGE_P , fill = factor ( RBMI ))) + geom_histogram ( binwidth = 1 ) + BMI_fill + facet_grid ( RBMI ~ .) + theme_classic () Alternatives 1 2 3 4 # count histogram ggplot ( adult , aes ( x = SRAGE_P , fill = factor ( RBMI ))) + geom_histogram ( binwidth = 1 ) + BMI_fill 1 2 3 4 # density histogram ggplot ( adult , aes ( x = SRAGE_P , fill = factor ( RBMI ))) + geom_histogram ( aes ( y = ..density.. ), binwidth = 1 ) + BMI_fill 1 2 3 4 # faceted count histogram ggplot ( adult , aes ( x = SRAGE_P , fill = factor ( RBMI ))) + geom_histogram ( binwidth = 1 ) + BMI_fill + facet_grid ( RBMI ~ .) 1 2 3 4 # faceted density histogram ggplot ( adult , aes ( x = SRAGE_P , fill = factor ( RBMI ))) + geom_histogram ( aes ( y = ..density.. ), binwidth = 1 ) + BMI_fill + facet_grid ( RBMI ~ .) 1 2 3 4 # density histogram with `position = 'fill'` ggplot ( adult , aes ( x = SRAGE_P , fill = factor ( RBMI ))) + geom_histogram ( aes ( y = ..density.. ), binwidth = 1 , position = 'fill' ) + BMI_fill 1 2 3 4 # accurate histogram ggplot ( adult , aes ( x = SRAGE_P , fill = factor ( RBMI ))) + geom_histogram ( aes ( y = ..count.. / sum ( ..count.. )), binwidth = 1 , position = 'fill' ) + BMI_fill Do Things Manually 1 2 3 4 5 # an attempt to facet the accurate frequency histogram from before (failed) ggplot ( adult , aes ( x = SRAGE_P , fill = factor ( RBMI ))) + geom_histogram ( aes ( y = ..count.. / sum ( ..count.. )), binwidth = 1 , position = 'fill' ) + BMI_fill + facet_grid ( RBMI ~ .) 1 2 3 4 5 6 7 8 9 10 11 # create DF with `table()` DF <- table ( adult $ RBMI , adult $ SRAGE_P ) # use apply on DF to get frequency of each group DF_freq <- apply ( DF , 2 , function ( x ) x / sum ( x )) # melt on DF to create DF_melted DF_melted <- melt ( DF_freq ) # change names of DF_melted names ( DF_melted ) <- c ( 'FILL' , 'X' , 'value' ) 1 2 3 4 5 # add code to make this a faceted plot ggplot ( DF_melted , aes ( x = X , y = value , fill = FILL )) + geom_bar ( stat = 'identity' , position = 'stack' ) + BMI_fill + facet_grid ( FILL ~ .) Merimeko/Mosaic Plot 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 # The initial contingency table DF <- as.data.frame.matrix ( table ( adult $ SRAGE_P , adult $ RBMI )) # Add the columns groupsSum, xmax and xmin. Remove groupSum again. DF $ groupSum <- rowSums ( DF ) DF $ xmax <- cumsum ( DF $ groupSum ) DF $ xmin <- DF $ xmax - DF $ groupSum # The groupSum column needs to be removed, don't remove this line DF $ groupSum <- NULL # Copy row names to variable X DF $ X <- row.names ( DF ) # Melt the dataset DF_melted <- melt ( DF , id.vars = c ( 'X' , 'xmin' , 'xmax' ), variable.name = 'FILL' ) # dplyr call to calculate ymin and ymax - don't change DF_melted <- DF_melted %>% group_by ( X ) %>% mutate ( ymax = cumsum ( value / sum ( value )), ymin = ymax - value / sum ( value )) # Plot rectangles - don't change. ggplot ( DF_melted , aes ( ymin = ymin , ymax = ymax , xmin = xmin , xmax = xmax , fill = FILL )) + geom_rect ( colour = 'white' ) + scale_x_continuous ( expand = c ( 0 , 0 )) + scale_y_continuous ( expand = c ( 0 , 0 )) + BMI_fill + theme_tufte () Adding statistics 1 2 3 4 5 6 7 8 9 10 11 # perform chi.sq test (`RBMI` and `SRAGE_P`) results <- chisq.test ( table ( adult $ RBMI , adult $ SRAGE_P )) # melt results$residuals and store as resid resid <- melt ( results $ residuals ) # change names of resid names ( resid ) <- c ( 'FILL' , 'X' , 'residual' ) # merge the two datasets DF_all <- merge ( DF_melted , resid ) 1 2 3 4 5 6 7 # update plot command ggplot ( DF_all , aes ( ymin = ymin , ymax = ymax , xmin = xmin , xmax = xmax , fill = residual )) + geom_rect () + scale_fill_gradient2 () + scale_x_continuous ( expand = c ( 0 , 0 )) + scale_y_continuous ( expand = c ( 0 , 0 )) + theme_tufte () Adding text 1 2 3 4 5 6 # position for labels on x axis DF_all $ xtext <- DF_all $ xmin + ( DF_all $ xmax - DF_all $ xmin ) / 2 # position for labels on y axis index <- DF_all $ xmax == max ( DF_all $ xmax ) DF_all $ ytext <- DF_all $ ymin[index] + ( DF_all $ ymax[index] - DF_all $ ymin[index] ) / 2 1 2 3 4 5 6 7 8 9 10 # plot ggplot ( DF_all , aes ( ymin = ymin , ymax = ymax , xmin = xmin , xmax = xmax , fill = residual )) + geom_rect ( col = 'white' ) + # geom_text for ages (i.e. the x axis) geom_text ( aes ( x = xtext , label = X ), y = 1 , size = 3 , angle = 90 , hjust = 1 , show.legend = FALSE ) + # geom_text for BMI (i.e. the fill axis) geom_text ( aes ( x = max ( xmax ), y = ytext , label = FILL ), size = 3 , hjust = 1 , show.legend = FALSE ) + scale_fill_gradient2 () + theme_tufte () + theme ( legend.position = 'bottom' ) Generalizations 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 # script generalized into a function mosaicGG <- function ( data , X , FILL ) { # Proportions in raw data DF <- as.data.frame.matrix ( table ( data[[X]] , data[[FILL]] )) DF $ groupSum <- rowSums ( DF ) DF $ xmax <- cumsum ( DF $ groupSum ) DF $ xmin <- DF $ xmax - DF $ groupSum DF $ X <- row.names ( DF ) DF $ groupSum <- NULL DF_melted <- melt ( DF , id = c ( 'X' , 'xmin' , 'xmax' ), variable.name = 'FILL' ) DF_melted <- DF_melted %>% group_by ( X ) %>% mutate ( ymax = cumsum ( value / sum ( value )), ymin = ymax - value / sum ( value )) # Chi-sq test results <- chisq.test ( table ( data[[FILL]] , data[[X]] )) # fill and then x resid <- melt ( results $ residuals ) names ( resid ) <- c ( 'FILL' , 'X' , 'residual' ) # Merge data DF_all <- merge ( DF_melted , resid ) # Positions for labels DF_all $ xtext <- DF_all $ xmin + ( DF_all $ xmax - DF_all $ xmin ) / 2 index <- DF_all $ xmax == max ( DF_all $ xmax ) DF_all $ ytext <- DF_all $ ymin[index] + ( DF_all $ ymax[index] - DF_all $ ymin[index] ) / 2 # plot g <- ggplot ( DF_all , aes ( ymin = ymin , ymax = ymax , xmin = xmin , xmax = xmax , fill = residual )) + geom_rect ( col = 'white' ) + geom_text ( aes ( x = xtext , label = X ), y = 1 , size = 3 , angle = 90 , hjust = 1 , show.legend = FALSE ) + geom_text ( aes ( x = max ( xmax ), y = ytext , label = FILL ), size = 3 , hjust = 1 , show.legend = FALSE ) + scale_fill_gradient2 ( 'Residuals' ) + scale_x_continuous ( 'Individuals' , expand = c ( 0 , 0 )) + scale_y_continuous ( 'Proportion' , expand = c ( 0 , 0 )) + theme_tufte () + theme ( legend.position = 'bottom' ) print ( g ) } 1 2 # BMI described by age (in x) mosaicGG ( adult , 'SRAGE_P' , 'RBMI' ) 1 2 # poverty described by age (in x) mosaicGG ( adult , 'SRAGE_P' , 'POVLL' ) 1 2 # `am` described by `cyl` (in x) mosaicGG ( mtcars , 'cyl' , 'am' ) 1 2 # `Vocab` described by education mosaicGG ( Vocab , 'education' , 'vocabulary' )","title":"Case Study"},{"location":"plot_snippets_ggplot2/#section-3","text":"","title":"SECTION 3"},{"location":"plot_snippets_ggplot2/#section-4-cheat-list","text":"ggplot(data, aes(x = , y = ), col = , fill = , size = , labels = , alpha = , shape = , line = , position = \u2018jitter\u2019) 1 2 3 4 5 6 7 8 9 10 11 12 13 + geom_point() + geom_point(aes(), col = , position = posn.j) + geom_jitter() + facet_grid(. ~ x) # y ~ x + scale_x_continous('Sepal Length', limits = c(2, 8), breaks = seq(2, 8, 3)) + scale_color_discrete('Species', labels = c('a', 'b', 'c')) + labs(x = , y = , col = ) posn.j <- position_jitter(width = 0.1) Data diamonds , prices of 50,000 round cut diamonds. economics , economics_long, US economic time series. faithfuld , 2d density estimate of Old Faithful data. luv_colours , colors(). midwest , midwest demographics. mpg , fuel economy data from 1999 and 2008 for 38 popular models of car. msleep , an updated and expanded version of the mammals sleep dataset. presidential , terms of 11 presidents from Eisenhower to Obama. seals , vector field of seal movements. txhousing , Housing sales in TX. Aesthetics x-axis. y-asix. color. fill. size (points, lines). labels. alpha. shape (points). linetype (lines). aes , Define aesth.etic mappings. aes_ (aes_q, aes_string), Define aesthetic mappings from strings, or quoted calls and formulas. aes_all , Given a character vector, create a set of identity mappings. aes_auto , Automatic aesthetic mapping. aes_colour_fill_alpha (color, colour, fill), Colour related aesthetics: colour, fill and alpha. aes_group_order (group), Aesthetics: group. aes_linetype_size_shape (linetype, shape, size), Differentiation related aesthetics: linetype, size, shape. aes_position (x, xend, xmax, xmin, y, yend, ymax, ymin), Position related aesthetics: x, y, xmin, xmax, ymin, ymax, xend, yend. Position position_dodge , Adjust position by dodging overlaps to the side. position_fill (position_stack), Stack overlapping objects on top of one another. position_identity , Don\u2019t adjust position position_nudge , Nudge points. position_jitter , Jitter points to avoid overplotting. position_jitterdodge , Adjust position by simultaneously dodging and jittering. Scales expand_limits , Expand the plot limits with data. guides , Set guides for each scale. guide_legend , Legend guide. guide_colourbar (guide_colorbar), Continuous colour bar guide. lims (xlim, ylim), Convenience functions to set the axis limits. scale_alpha (scale_alpha_continuous, scale_alpha_discrete), Alpha scales. scale_colour_brewer (scale_color_brewer, scale_color_distiller, scale_colour_distiller, scale_fill_brewer, scale_fill_distiller), Sequential, diverging and qualitative colour scales from colorbrewer.org scale_colour_gradient (scale_color_continuous, scale_color_gradient, scale_color_gradient2, scale_color_gradientn, scale_colour_continuous, scale_colour_date, scale_colour_datetime, scale_colour_gradient2, scale_colour_gradientn, scale_fill_continuous, scale_fill_date, scale_fill_datetime, scale_fill_gradient, scale_fill_gradient2, scale_fill_gradientn). scale_colour_grey (scale_color_grey, scale_fill_grey), Sequential grey colour scale. scale_colour_hue (scale_color_discrete, scale_color_hue, scale_colour_discrete, scale_fill_discrete, scale_fill_hue), Qualitative colour scale with evenly spaced hues. scale_identity (scale_alpha_identity, scale_color_identity, scale_colour_identity, scale_fill_identity, scale_linetype_identity, scale_shape_identity, scale_size_identity), Use values without scaling. scale_manual (scale_alpha_manual, scale_color_manual, scale_colour_manual, scale_fill_manual, scale_linetype_manual, scale_shape_manual, scale_size_manual), Create your own discrete scale. scale_linetype (scale_linetype_continuous, scale_linetype_discrete), Scale for line patterns. scale_shape (scale_shape_continuous, scale_shape_discrete), Scale for shapes, aka glyphs. scale_size (scale_radius, scale_size_area, scale_size_continuous, scale_size_date, scale_size_datetime, scale_size_discrete), Scale size (area or radius). scale_x_discrete (scale_y_discrete), Discrete position. labs (ggtitle, xlab, ylab), Change axis labels and legend titles. update_labels , Update axis/legend labels. Geometries point. line. histogram. bar. boxplot. geom_abline (geom_hline, geom_vline), Lines: horizontal, vertical, and specified by slope and intercept. geom_bar (stat_count), Bars, rectangles with bases on x-axis geom_bin2d (stat_bin2d, stat_bin_2d), Add heatmap of 2d bin counts. geom_blank , Blank, draws nothing. geom_boxplot (stat_boxplot), Box and whiskers plot. geom_contour (stat_contour), Display contours of a 3d surface in 2d. geom_count (stat_sum), Count the number of observations at each location. geom_crossbar (geom_errorbar, geom_linerange, geom_pointrange), Vertical intervals: lines, crossbars & errorbars. geom_density (stat_density), Display a smooth density estimate. geom_density_2d (geom_density2d, stat_density2d, stat_density_2d), Contours from a 2d density estimate. geom_dotplot , Dot plot geom_errorbarh , Horizontal error bars. geom_freqpoly (geom_histogram, stat_bin), Histograms and frequency polygons. geom_hex (stat_bin_hex, stat_binhex), Hexagon binning. geom_jitter , Points, jittered to reduce overplotting. geom_label (geom_text), Textual annotations. geom_map , Polygons from a reference map. geom_path (geom_line, geom_step), Connect observations. geom_point , Points, as for a scatterplot. geom_polygon , Polygon, a filled path. geom_quantile (stat_quantile), Add quantile lines from a quantile regression. geom_raster (geom_rect, geom_tile), Draw rectangles. geom_ribbon (geom_area), Ribbons and area plots. geom_rug , Marginal rug plots. geom_segment (geom_curve), Line segments and curves. geom_smooth (stat_smooth), Add a smoothed conditional mean. geom_violin (stat_ydensity), Violin plot. Facets columns. rows. facet_grid , Lay out panels in a grid. facet_null , Facet specification: a single panel. facet_wrap , Wrap a 1d ribbon of panels into 2d. labeller , Generic labeller function for facets. label_bquote , Backquoted labeller. Annotation annotate , Create an annotation layer. annotation_custom , Annotation: Custom grob. annotation_logticks , Annotation: log tick marks. annotation_map , Annotation: maps. annotation_raster , Annotation: High-performance rectangular tiling. borders , Create a layer of map borders. Fortify fortify , Fortify a model with data. fortify-multcomp (fortify.cld, fortify.confint.glht, fortify.glht, fortify.summary.glht), Fortify methods for objects produced by. fortify.lm , Supplement the data fitted to a linear model with model fit statistics. fortify.map , Fortify method for map objects. fortify.sp (fortify.Line, fortify.Lines, fortify.Polygon, fortify.Polygons, fortify.SpatialLinesDataFrame, fortify.SpatialPolygons, fortify.SpatialPolygonsDataFrame), Fortify method for classes from the sp package. map_data , Create a data frame of map data. Statistics binning. smoothing. descriptive. inferential. stat_ecdf , Empirical Cumulative Density Function. stat_ellipse , Plot data ellipses. stat_function , Superimpose a function. stat_identity , Identity statistic. stat_qq (geom_qq), Calculation for quantile-quantile plot. stat_summary_2d (stat_summary2d, stat_summary_hex), Bin and summarise in 2d (rectangle & hexagons) stat_unique , Remove duplicates. Coordinates. cartesian. fixes. polar. limites. coord_cartesian , Cartesian coordinates. coord_fixed (coord_equal), Cartesian coordinates with fixed relationship between x and y scales. coord_flip , Flipped cartesian coordinates. coord_map (coord_quickmap), Map projections. coord_polar , Polar coordinates. coord_trans , Transformed cartesian coordinate system. Themes theme_bw theme_grey theme_classic theme_minimal ggthemes","title":"SECTION 4 - Cheat List"},{"location":"plot_snippets_ggvis/","text":"Foreword Output options: the \u2018tango\u2019 syntax and the \u2018readable\u2019 theme. Snippets and results. ggvis generates html outputs. Graphics presented here are images: .png files; .gif files when specified. Documentation \u00b6 ggvis Overview . ggvis Cookbook . Dataset \u00b6 For most examples, we use the mtcars , pressure , faithful datasets. The ggvis Package \u00b6 1 library ( ggvis ) The Grammar of Graphics \u00b6 Start to explore 1 2 3 4 # change the code below to plot the disp variable of mtcars on the x axis mtcars %>% ggvis ( ~ disp , ~ mpg ) %>% layer_points () ggvis and its capabilities 1 2 3 4 # Change the code below to make a graph with red points mtcars %>% ggvis ( ~ wt , ~ mpg , fill := \"red\" ) %>% layer_points () 1 2 3 4 # Change the code below draw smooths instead of points mtcars %>% ggvis ( ~ wt , ~ mpg ) %>% layer_smooths () 1 2 3 4 5 # Change the code below to make a graph containing both points and a smoothed summary line mtcars %>% ggvis ( ~ wt , ~ mpg ) %>% layer_points () %>% layer_smooths () ggvis grammar 1 2 3 4 # Make a scatterplot of the pressure dataset pressure %>% ggvis ( ~ temperature , ~ pressure ) %>% layer_points 1 2 3 4 # Adapt the code you wrote for the first challenge: show bars instead of points pressure %>% ggvis ( ~ temperature , ~ pressure ) %>% layer_bars 1 2 3 4 # Adapt the code you wrote for the first challenge: show lines instead of points pressure %>% ggvis ( ~ temperature , ~ pressure ) %>% layer_lines 1 2 3 4 # Adapt the code you wrote for the first challenge: map the fill property to the temperature variable pressure %>% ggvis ( ~ temperature , ~ pressure , fill = ~ temperature ) %>% layer_points 1 2 3 4 # Extend the code you wrote for the previous challenge: map the size property to the pressure variable pressure %>% ggvis ( ~ temperature , ~ pressure , fill = ~ temperature , size = ~ pressure ) %>% layer_points 4 essential components of a graph 1 2 3 4 5 6 faithful %>% ggvis ( ~ waiting , ~ eruptions , fill := \"red\" ) %>% layer_points () %>% add_axis ( \"y\" , title = \"Duration of eruption (m)\" , values = c ( 2 , 3 , 4 , 5 ), subdivide = 9 ) %>% add_axis ( \"x\" , title = \"Time since previous eruption (m)\" ) Lines and Syntax \u00b6 Three operators: %>% , = and := 1 layer_points ( ggvis ( faithful , ~ waiting , ~ eruptions )) 1 2 3 4 # Rewrite the code with the pipe operator faithful %>% ggvis ( ~ waiting , ~ eruptions ) %>% layer_points () 1 2 3 pressure %>% ggvis ( ~ temperature , ~ pressure ) %>% layer_points () 1 2 3 4 # Modify this graph to map the size property to the pressure variable pressure %>% ggvis ( ~ temperature , ~ pressure , size = ~ pressure ) %>% layer_points () 1 2 3 pressure %>% ggvis ( ~ temperature , ~ pressure ) %>% layer_points () 1 2 3 4 # Modify this graph by setting the size property pressure %>% ggvis ( ~ temperature , ~ pressure , size := 100 ) %>% layer_points () 1 2 3 pressure %>% ggvis ( ~ temperature , ~ pressure , fill = \"red\" ) %>% layer_points () 1 2 3 4 # Fix this code to set the fill property to red pressure %>% ggvis ( ~ temperature , ~ pressure , fill := \"red\" ) %>% layer_points () Referring to different objects 1 2 3 4 5 6 red <- \"green\" pressure $ red <- pressure $ temperature pressure %>% ggvis ( ~ temperature , ~ pressure , fill := red ) %>% layer_points () Properties for points 1 2 3 4 # Change the code to set the fills using pressure$black pressure %>% ggvis ( ~ temperature , ~ pressure , fill := ~ 'black' ) %>% layer_points () 1 2 3 4 # Plot the faithful data as described in the second instruction faithful %>% ggvis ( ~ waiting , ~ eruptions , size = ~ eruptions , opacity := 0.5 , fill := \"blue\" , stroke := \"black\" ) %>% layer_points () 1 2 3 4 # Plot the faithful data as described in the third instruction faithful %>% ggvis ( ~ waiting , ~ eruptions , size := 100 , fill := \"red\" , fillOpacity = ~ eruptions , stroke := \"red\" , shape := \"cross\" ) %>% layer_points () Properties for lines 1 2 3 4 # Change the code below to use the lines mark pressure %>% ggvis ( ~ temperature , ~ pressure ) %>% layer_lines () 1 2 3 4 # Set the properties described in the second instruction in the graph below pressure %>% ggvis ( ~ temperature , ~ pressure , stroke := \"red\" , strokeWidth := 2 , strokeDash := 6 ) %>% layer_lines () Display model fits layer_lines() will always connect the points in your plot from the leftmost point to the rightmost point. This can be undesirable if you are trying to plot a specific shape. layer_paths() : this mark connects the points in the order that they appear in the data set. So the paths mark will connect the point that corresponds to the first row of the data to the point that corresponds to the second row of data, and so on - no matter where those points appear in the graph. 1 2 3 4 5 6 7 8 9 # change the third line of code to plot a map of Texas library ( maps ) library ( ggplot2 ) texas <- ggplot2 :: map_data ( \"state\" , region = \"texas\" ) texas %>% ggvis ( ~ long , ~ lat ) %>% layer_paths () 1 2 3 4 # Same plot, but set the fill property of the texas map to dark orange texas %>% ggvis ( ~ long , ~ lat , fill := \"darkorange\" ) %>% layer_paths () compute_smooth() to simplify model fits compute_model_prediction() is a useful function to use with line graphs. It takes a data frame as input and returns a new data frame as output. The new data frame will contain the x and y values of a line fitted to the data in the original data frame. Generate the x and y coordinates for a LOESS smooth line. 1 2 3 faithful %>% compute_model_prediction ( eruptions ~ waiting , model = \"lm\" ) %>% head ( 10 ) 1 2 3 4 5 6 7 8 9 10 11 ## pred_ resp_ ## 1 43.00000 1.377986 ## 2 43.67089 1.428724 ## 3 44.34177 1.479461 ## 4 45.01266 1.530199 ## 5 45.68354 1.580937 ## 6 46.35443 1.631674 ## 7 47.02532 1.682412 ## 8 47.69620 1.733150 ## 9 48.36709 1.783888 ## 10 49.03797 1.834625 1 2 3 4 # Compute the x and y coordinates for a loess smooth line that predicts mpg with the wt mtcars %>% compute_smooth ( mpg ~ wt ) %>% head ( 10 ) 1 2 3 4 5 6 7 8 9 10 11 ## pred_ resp_ ## 1 1.513000 32.08897 ## 2 1.562506 31.68786 ## 3 1.612013 31.28163 ## 4 1.661519 30.87037 ## 5 1.711025 30.45419 ## 6 1.760532 30.03318 ## 7 1.810038 29.60745 ## 8 1.859544 29.17711 ## 9 1.909051 28.74224 ## 10 1.958557 28.30017 1 #model = \"loess\" is set by default Transformations \u00b6 Histograms (1) 1 2 3 4 # Build a histogram of the waiting variable of the faithful data set. faithful %>% ggvis ( ~ waiting ) %>% layer_histograms () 1 2 3 4 # Build the same histogram, but with a binwidth (width argument) of 5 units faithful %>% ggvis ( ~ waiting ) %>% layer_histograms ( width = 5 ) Histograms (2) 1 2 3 faithful %>% ggvis ( ~ waiting ) %>% layer_histograms ( width = 5 ) 1 2 3 # Transform the code: just compute the bins instead of plotting a histogram faithful %>% compute_bin ( ~ waiting , width = 5 ) 1 2 3 4 5 6 7 8 9 10 11 12 ## count_ x_ xmin_ xmax_ width_ ## 1 13 45 42.5 47.5 5 ## 2 24 50 47.5 52.5 5 ## 3 29 55 52.5 57.5 5 ## 4 21 60 57.5 62.5 5 ## 5 13 65 62.5 67.5 5 ## 6 13 70 67.5 72.5 5 ## 7 42 75 72.5 77.5 5 ## 8 58 80 77.5 82.5 5 ## 9 38 85 82.5 87.5 5 ## 10 17 90 87.5 92.5 5 ## 11 4 95 92.5 97.5 5 1 2 3 4 5 # Combine the solution to the first challenge with layer_rects() to build a histogram faithful %>% compute_bin ( ~ waiting , width = 5 ) %>% ggvis ( x = ~ xmin_ , x2 = ~ xmax_ , y = 0 , y2 = ~ count_ ) %>% layer_rects () Density plots 1 2 3 4 5 # Combine compute_density() with layer_lines() to make a density plot of the waiting variable. faithful %>% compute_density ( ~ waiting ) %>% ggvis ( ~ pred_ , ~ resp_ ) %>% layer_lines () 1 2 3 4 # Build a density plot directly using layer_densities. Use the correct variables and properties. faithful %>% ggvis ( ~ waiting ) %>% layer_densities ( fill := \"green\" ) Shortcuts 1 2 3 4 # Complete the code to plot a bar graph of the cyl factor. mtcars %>% ggvis ( ~ factor ( cyl )) %>% layer_bars () 1 2 3 # Adapt the solution to the first challenge to just calculate the count values. No plotting! mtcars %>% compute_count ( ~ factor ( cyl )) 1 2 3 4 ## count_ x_ ## 1 11 4 ## 2 7 6 ## 3 14 8 ggvis and group_by 1 2 3 4 mtcars %>% group_by ( am ) %>% ggvis ( ~ mpg , ~ wt , stroke = ~ factor ( am )) %>% layer_smooths () 1 2 3 4 5 # Change the code to plot a unique smooth line for each value of the cyl variable. mtcars %>% group_by ( cyl ) %>% ggvis ( ~ mpg , ~ wt , stroke = ~ factor ( cyl )) %>% layer_smooths () 1 2 3 4 5 # Adapt the graph to contain a separate density for each value of cyl. mtcars %>% group_by ( cyl ) %>% ggvis ( ~ mpg ) %>% layer_densities () 1 2 3 4 5 # Copy and alter the solution to the second challenge to map the fill property to a categorical version of cyl. mtcars %>% group_by ( cyl ) %>% ggvis ( ~ mpg ) %>% layer_densities ( fill = ~ factor ( cyl )) group_by() versus interaction() 1 2 3 4 mtcars %>% group_by ( cyl ) %>% ggvis ( ~ mpg ) %>% layer_densities ( fill = ~ factor ( cyl )) 1 2 3 4 5 6 7 # Alter the graph: separate density for each unique combination of 'cyl' and 'am'. mtcars %>% group_by ( cyl , am ) %>% ggvis ( ~ mpg , fill = ~ factor ( cyl )) %>% layer_densities () #factor(cyl),factor(am) 1 2 3 4 mtcars %>% group_by ( cyl , am ) %>% ggvis ( ~ mpg , fill = ~ factor ( cyl )) %>% layer_densities () 1 2 3 4 5 # Update the graph to map `fill` to the unique combinations of the grouping variables. mtcars %>% group_by ( cyl , am ) %>% ggvis ( ~ mpg , fill = ~ interaction ( cyl , am )) %>% layer_densities () Chaining is a virtue 1 2 3 4 mtcars %>% group_by ( cyl , am ) %>% ggvis ( ~ mpg , fill = ~ interaction ( cyl , am )) %>% layer_densities () This call is exactly equivalent to the following piece of code that is very hard to read: 1 layer_densities ( ggvis ( group_by ( mtcars , cyl , am ), ~ mpg , fill = ~ interaction ( cyl , am ))) Interactivity and Layers \u00b6 The basics of interactive plots 1 2 3 4 5 # Run this code and inspect the output. Follow the link in the instructions for the interactive version faithful %>% ggvis ( ~ waiting , ~ eruptions , fillOpacity := 0.5 , shape := input_select ( label = \"Choose shape:\" , choices = c ( \"circle\" , \"square\" , \"cross\" , \"diamond\" , \"triangle-up\" , \"triangle-down\" ))) %>% layer_points () .gif file: 1 2 3 4 # Copy the first code chunk and alter the code to make the fill property interactive using a select box faithful %>% ggvis ( ~ waiting , ~ eruptions , fillOpacity := 0.5 , shape := input_select ( label = \"Choose shape:\" , choices = c ( \"circle\" , \"square\" , \"cross\" , \"diamond\" , \"triangle-up\" , \"triangle-down\" )), fill := input_select ( label = \"Choose color:\" , choices = c ( \"black\" , \"red\" , \"blue\" , \"green\" ))) %>% layer_points () .gif file: 1 2 3 4 # Add radio buttons to control the fill of the plot mtcars %>% ggvis ( ~ mpg , ~ wt , fill := input_radiobuttons ( label = \"Choose color:\" , choices = c ( \"black\" , \"red\" , \"blue\" , \"green\" ))) %>% layer_points () .gif file: Input widgets in more detail 1 2 3 mtcars %>% ggvis ( ~ mpg , ~ wt , fill := input_radiobuttons ( label = \"Choose color:\" , choices = c ( \"black\" , \"red\" , \"blue\" , \"green\" ))) %>% layer_points () .gif file: 1 2 3 4 # Change the radiobuttons widget to a text widget mtcars %>% ggvis ( ~ mpg , ~ wt , fill := input_text ( label = \"Choose color:\" , value = c ( \"black\" ))) %>% layer_points () .gif file: 1 2 3 mtcars %>% ggvis ( ~ mpg , ~ wt ) %>% layer_points () 1 2 3 4 # Map the fill property to a select box that returns variable names mtcars %>% ggvis ( ~ mpg , ~ wt , fill = input_select ( label = \"Choose fill variable:\" , choices = names ( mtcars ), map = as.name )) %>% layer_points () .gif file: Input widgets in more detail (2) 1 2 3 4 # Map the fill property to a select box that returns variable names mtcars %>% ggvis ( ~ mpg , ~ wt , fill = input_select ( label = \"Choose fill variable:\" , choices = names ( mtcars ), map = as.name )) %>% layer_points () .gif file: Control parameters and values 1 2 3 4 # Map the bindwidth to a numeric field (\"Choose a binwidth:\") mtcars %>% ggvis ( ~ mpg ) %>% layer_histograms ( width = input_numeric ( value = 1 , label = \"Choose a binwidth:\" )) .gif file: 1 2 3 4 # Map the binwidth to a slider bar (\"Choose a binwidth:\") with the correct specifications mtcars %>% ggvis ( ~ mpg ) %>% layer_histograms ( width = input_slider ( min = 1 , max = 20 , label = \"Choose a binwidth:\" )) .gif file: Multi-layered plots and their properties 1 2 3 4 # Add a layer of points to the graph below. pressure %>% ggvis ( ~ temperature , ~ pressure , stroke := \"skyblue\" ) %>% layer_lines () %>% layer_points () 1 2 3 4 5 # Copy and adapt the solution to the first instruction below so that only the lines layer uses a skyblue stroke. pressure %>% ggvis ( ~ temperature , ~ pressure ) %>% layer_lines ( stroke := \"skyblue\" ) %>% layer_points () 1 2 3 4 5 # Rewrite the code below so that only the points layer uses the shape property. pressure %>% ggvis ( ~ temperature , ~ pressure ) %>% layer_lines ( stroke := \"skyblue\" ) %>% layer_points ( shape := \"triangle-up\" ) 1 2 3 4 5 # Refactor the code for the graph below to make it as concise as possible pressure %>% ggvis ( ~ temperature , ~ pressure , stroke := \"skyblue\" , strokeOpacity := 0.5 , strokeWidth := 5 ) %>% layer_lines () %>% layer_points ( fill = ~ temperature , shape := \"triangle-up\" , size := 300 ) Multi-layered plots and their properties (2) 1 2 3 4 5 # Rewrite the code below so that only the points layer uses the shape property. pressure %>% ggvis ( ~ temperature , ~ pressure ) %>% layer_lines ( stroke := \"skyblue\" ) %>% layer_points ( shape := \"triangle-up\" ) 1 2 3 4 5 6 # Refactor the code for the graph below to make it as concise as possible pressure %>% ggvis ( ~ temperature , ~ pressure , stroke := \"skyblue\" , strokeOpacity := 0.5 , strokeWidth := 5 ) %>% layer_lines () %>% layer_points ( fill = ~ temperature , shape := \"triangle-up\" , size := 300 ) There is no limit on the number of layers! 1 2 3 4 5 6 7 # Create a graph containing a scatterplot, a linear model and a smooth line. pressure %>% ggvis ( ~ temperature , ~ pressure ) %>% layer_points () %>% layer_lines ( stroke := \"black\" , opacity := 0.5 ) %>% layer_model_predictions ( model = \"lm\" , stroke := \"navy\" ) %>% layer_smooths ( stroke := \"skyblue\" ) Taking local and global to the next level 1 2 3 4 5 pressure %>% ggvis ( ~ temperature , ~ pressure , stroke := \"darkred\" ) %>% layer_lines ( stroke := \"orange\" , strokeDash := 5 , strokeWidth := 5 ) %>% layer_points ( shape := \"circle\" , size := 100 , fill := \"lightgreen\" ) %>% layer_smooths () Customizing Axes, Legends, and Scales \u00b6 Axes 1 2 3 4 5 6 # add the title of the x axis: \"Time since previous eruption (m)\" faithful %>% ggvis ( ~ waiting , ~ eruptions ) %>% layer_points () %>% add_axis ( \"y\" , title = \"Duration of eruption (m)\" ) %>% add_axis ( \"x\" , title = \"Time since previous eruption (m)\" ) 1 2 3 4 5 6 # Add to the code to place a labelled tick mark at 50, 60, 70, 80, 90 on the x axis. faithful %>% ggvis ( ~ waiting , ~ eruptions ) %>% layer_points () %>% add_axis ( \"y\" , title = \"Duration of eruption (m)\" , values = c ( 2 , 3 , 4 , 5 ), subdivide = 9 ) %>% add_axis ( \"x\" , title = \"Time since previous eruption (m)\" , values = c ( 50 , 60 , 70 , 80 , 90 ), subdivide = 9 ) 1 2 3 4 5 # Add to the code below to change the location of the y axis faithful %>% ggvis ( ~ waiting , ~ eruptions ) %>% layer_points () %>% add_axis ( \"x\" , orient = \"top\" ) %>% add_axis ( \"y\" , orient = \"right\" ) Legends 1 2 3 4 5 # Add a legend to the plot below: use the correct title and orientation faithful %>% ggvis ( ~ waiting , ~ eruptions , opacity := 0.6 , fill = ~ factor ( round ( eruptions ))) %>% layer_points () %>% add_legend ( \"fill\" , title = \"~ duration (m)\" , orient = \"left\" ) 1 2 3 4 5 6 7 8 #add_legend(vis, scales = NULL, orient = \"right\", title = NULL, format = NULL, values = NULL, properties = NULL) # Use add_legend() to combine the legends in the plot below. Adjust its properties as instructed. faithful %>% ggvis ( ~ waiting , ~ eruptions , opacity := 0.6 , fill = ~ factor ( round ( eruptions )), shape = ~ factor ( round ( eruptions )), size = ~ round ( eruptions )) %>% layer_points () %>% add_legend ( c ( \"fill\" , \"shape\" , \"size\" ), title = \"~ duration (m)\" , values = c ( 2 , 3 , 4 , 5 )) Legends (2) 1 2 3 4 5 6 # Fix the legend faithful %>% ggvis ( ~ waiting , ~ eruptions , opacity := 0.6 , fill = ~ factor ( round ( eruptions )), shape = ~ factor ( round ( eruptions )), size = ~ round ( eruptions )) %>% layer_points () %>% add_legend ( c ( \"fill\" , \"shape\" , \"size\" ), title = \"~ duration (m)\" ) Scale types 1 2 3 4 5 # Add to the code below to make the stroke color range from \"darkred\" to \"orange\". mtcars %>% ggvis ( ~ wt , ~ mpg , fill = ~ disp , stroke = ~ disp , strokeWidth := 2 ) %>% layer_points () %>% scale_numeric ( \"fill\" , range = c ( \"red\" , \"yellow\" )) %>% scale_numeric ( \"stroke\" , range = c ( \"darkred\" , \"orange\" )) 1 2 3 # Change the graph below to make the fill colors range from green to beige. mtcars %>% ggvis ( ~ wt , ~ mpg , fill = ~ hp ) %>% layer_points () %>% scale_numeric ( \"fill\" , range = c ( \"green\" , \"beige\" )) 1 2 3 # Create a scale that will map `factor(cyl)` to a new range of colors: purple, blue, and green. mtcars %>% ggvis ( ~ wt , ~ mpg , fill = ~ factor ( cyl )) %>% layer_points () %>% scale_nominal ( \"fill\" , range = c ( \"purple\" , \"blue\" , \"green\" )) Adjust any visual property 1 2 3 4 5 # Add a scale that limits the range of opacity from 0.2 to 1. mtcars %>% ggvis ( x = ~ wt , y = ~ mpg , fill = ~ factor ( cyl ), opacity = ~ hp ) %>% layer_points () %>% scale_numeric ( \"opacity\" , range = c ( 0.2 , 1 )) 1 2 3 4 5 6 # Add a second scale that will expand the x axis to cover data values from 0 to 6. mtcars %>% ggvis ( ~ wt , ~ mpg , fill = ~ disp ) %>% layer_points () %>% scale_numeric ( \"y\" , domain = c ( 0 , NA )) %>% scale_numeric ( \"x\" , domain = c ( 0 , 6 )) Adjust any visual property (2) 1 2 3 4 5 6 # Add a second scale to set domain for x mtcars %>% ggvis ( ~ wt , ~ mpg , fill = ~ disp ) %>% layer_points () %>% scale_numeric ( \"y\" , domain = c ( 0 , NA )) %>% scale_numeric ( \"x\" , domain = c ( 0 , 6 )) \u201c = \u201d versus \u201c := \u201c 1 2 3 4 5 6 # Set the fill value to the color variable instead of mapping it, and see what happens mtcars $ color <- c ( \"red\" , \"teal\" , \"#cccccc\" , \"tan\" ) mtcars %>% ggvis ( x = ~ wt , y = ~ mpg , fill = ~ color ) %>% layer_points () 1 2 3 mtcars %>% ggvis ( x = ~ wt , y = ~ mpg , fill := ~ color ) %>% layer_points ()","title":"Plot Snippets - ggvis"},{"location":"plot_snippets_ggvis/#documentation","text":"ggvis Overview . ggvis Cookbook .","title":"Documentation"},{"location":"plot_snippets_ggvis/#dataset","text":"For most examples, we use the mtcars , pressure , faithful datasets.","title":"Dataset"},{"location":"plot_snippets_ggvis/#the-ggvis-package","text":"1 library ( ggvis )","title":"The ggvis Package"},{"location":"plot_snippets_ggvis/#the-grammar-of-graphics","text":"Start to explore 1 2 3 4 # change the code below to plot the disp variable of mtcars on the x axis mtcars %>% ggvis ( ~ disp , ~ mpg ) %>% layer_points () ggvis and its capabilities 1 2 3 4 # Change the code below to make a graph with red points mtcars %>% ggvis ( ~ wt , ~ mpg , fill := \"red\" ) %>% layer_points () 1 2 3 4 # Change the code below draw smooths instead of points mtcars %>% ggvis ( ~ wt , ~ mpg ) %>% layer_smooths () 1 2 3 4 5 # Change the code below to make a graph containing both points and a smoothed summary line mtcars %>% ggvis ( ~ wt , ~ mpg ) %>% layer_points () %>% layer_smooths () ggvis grammar 1 2 3 4 # Make a scatterplot of the pressure dataset pressure %>% ggvis ( ~ temperature , ~ pressure ) %>% layer_points 1 2 3 4 # Adapt the code you wrote for the first challenge: show bars instead of points pressure %>% ggvis ( ~ temperature , ~ pressure ) %>% layer_bars 1 2 3 4 # Adapt the code you wrote for the first challenge: show lines instead of points pressure %>% ggvis ( ~ temperature , ~ pressure ) %>% layer_lines 1 2 3 4 # Adapt the code you wrote for the first challenge: map the fill property to the temperature variable pressure %>% ggvis ( ~ temperature , ~ pressure , fill = ~ temperature ) %>% layer_points 1 2 3 4 # Extend the code you wrote for the previous challenge: map the size property to the pressure variable pressure %>% ggvis ( ~ temperature , ~ pressure , fill = ~ temperature , size = ~ pressure ) %>% layer_points 4 essential components of a graph 1 2 3 4 5 6 faithful %>% ggvis ( ~ waiting , ~ eruptions , fill := \"red\" ) %>% layer_points () %>% add_axis ( \"y\" , title = \"Duration of eruption (m)\" , values = c ( 2 , 3 , 4 , 5 ), subdivide = 9 ) %>% add_axis ( \"x\" , title = \"Time since previous eruption (m)\" )","title":"The Grammar of Graphics"},{"location":"plot_snippets_ggvis/#lines-and-syntax","text":"Three operators: %>% , = and := 1 layer_points ( ggvis ( faithful , ~ waiting , ~ eruptions )) 1 2 3 4 # Rewrite the code with the pipe operator faithful %>% ggvis ( ~ waiting , ~ eruptions ) %>% layer_points () 1 2 3 pressure %>% ggvis ( ~ temperature , ~ pressure ) %>% layer_points () 1 2 3 4 # Modify this graph to map the size property to the pressure variable pressure %>% ggvis ( ~ temperature , ~ pressure , size = ~ pressure ) %>% layer_points () 1 2 3 pressure %>% ggvis ( ~ temperature , ~ pressure ) %>% layer_points () 1 2 3 4 # Modify this graph by setting the size property pressure %>% ggvis ( ~ temperature , ~ pressure , size := 100 ) %>% layer_points () 1 2 3 pressure %>% ggvis ( ~ temperature , ~ pressure , fill = \"red\" ) %>% layer_points () 1 2 3 4 # Fix this code to set the fill property to red pressure %>% ggvis ( ~ temperature , ~ pressure , fill := \"red\" ) %>% layer_points () Referring to different objects 1 2 3 4 5 6 red <- \"green\" pressure $ red <- pressure $ temperature pressure %>% ggvis ( ~ temperature , ~ pressure , fill := red ) %>% layer_points () Properties for points 1 2 3 4 # Change the code to set the fills using pressure$black pressure %>% ggvis ( ~ temperature , ~ pressure , fill := ~ 'black' ) %>% layer_points () 1 2 3 4 # Plot the faithful data as described in the second instruction faithful %>% ggvis ( ~ waiting , ~ eruptions , size = ~ eruptions , opacity := 0.5 , fill := \"blue\" , stroke := \"black\" ) %>% layer_points () 1 2 3 4 # Plot the faithful data as described in the third instruction faithful %>% ggvis ( ~ waiting , ~ eruptions , size := 100 , fill := \"red\" , fillOpacity = ~ eruptions , stroke := \"red\" , shape := \"cross\" ) %>% layer_points () Properties for lines 1 2 3 4 # Change the code below to use the lines mark pressure %>% ggvis ( ~ temperature , ~ pressure ) %>% layer_lines () 1 2 3 4 # Set the properties described in the second instruction in the graph below pressure %>% ggvis ( ~ temperature , ~ pressure , stroke := \"red\" , strokeWidth := 2 , strokeDash := 6 ) %>% layer_lines () Display model fits layer_lines() will always connect the points in your plot from the leftmost point to the rightmost point. This can be undesirable if you are trying to plot a specific shape. layer_paths() : this mark connects the points in the order that they appear in the data set. So the paths mark will connect the point that corresponds to the first row of the data to the point that corresponds to the second row of data, and so on - no matter where those points appear in the graph. 1 2 3 4 5 6 7 8 9 # change the third line of code to plot a map of Texas library ( maps ) library ( ggplot2 ) texas <- ggplot2 :: map_data ( \"state\" , region = \"texas\" ) texas %>% ggvis ( ~ long , ~ lat ) %>% layer_paths () 1 2 3 4 # Same plot, but set the fill property of the texas map to dark orange texas %>% ggvis ( ~ long , ~ lat , fill := \"darkorange\" ) %>% layer_paths () compute_smooth() to simplify model fits compute_model_prediction() is a useful function to use with line graphs. It takes a data frame as input and returns a new data frame as output. The new data frame will contain the x and y values of a line fitted to the data in the original data frame. Generate the x and y coordinates for a LOESS smooth line. 1 2 3 faithful %>% compute_model_prediction ( eruptions ~ waiting , model = \"lm\" ) %>% head ( 10 ) 1 2 3 4 5 6 7 8 9 10 11 ## pred_ resp_ ## 1 43.00000 1.377986 ## 2 43.67089 1.428724 ## 3 44.34177 1.479461 ## 4 45.01266 1.530199 ## 5 45.68354 1.580937 ## 6 46.35443 1.631674 ## 7 47.02532 1.682412 ## 8 47.69620 1.733150 ## 9 48.36709 1.783888 ## 10 49.03797 1.834625 1 2 3 4 # Compute the x and y coordinates for a loess smooth line that predicts mpg with the wt mtcars %>% compute_smooth ( mpg ~ wt ) %>% head ( 10 ) 1 2 3 4 5 6 7 8 9 10 11 ## pred_ resp_ ## 1 1.513000 32.08897 ## 2 1.562506 31.68786 ## 3 1.612013 31.28163 ## 4 1.661519 30.87037 ## 5 1.711025 30.45419 ## 6 1.760532 30.03318 ## 7 1.810038 29.60745 ## 8 1.859544 29.17711 ## 9 1.909051 28.74224 ## 10 1.958557 28.30017 1 #model = \"loess\" is set by default","title":"Lines and Syntax"},{"location":"plot_snippets_ggvis/#transformations","text":"Histograms (1) 1 2 3 4 # Build a histogram of the waiting variable of the faithful data set. faithful %>% ggvis ( ~ waiting ) %>% layer_histograms () 1 2 3 4 # Build the same histogram, but with a binwidth (width argument) of 5 units faithful %>% ggvis ( ~ waiting ) %>% layer_histograms ( width = 5 ) Histograms (2) 1 2 3 faithful %>% ggvis ( ~ waiting ) %>% layer_histograms ( width = 5 ) 1 2 3 # Transform the code: just compute the bins instead of plotting a histogram faithful %>% compute_bin ( ~ waiting , width = 5 ) 1 2 3 4 5 6 7 8 9 10 11 12 ## count_ x_ xmin_ xmax_ width_ ## 1 13 45 42.5 47.5 5 ## 2 24 50 47.5 52.5 5 ## 3 29 55 52.5 57.5 5 ## 4 21 60 57.5 62.5 5 ## 5 13 65 62.5 67.5 5 ## 6 13 70 67.5 72.5 5 ## 7 42 75 72.5 77.5 5 ## 8 58 80 77.5 82.5 5 ## 9 38 85 82.5 87.5 5 ## 10 17 90 87.5 92.5 5 ## 11 4 95 92.5 97.5 5 1 2 3 4 5 # Combine the solution to the first challenge with layer_rects() to build a histogram faithful %>% compute_bin ( ~ waiting , width = 5 ) %>% ggvis ( x = ~ xmin_ , x2 = ~ xmax_ , y = 0 , y2 = ~ count_ ) %>% layer_rects () Density plots 1 2 3 4 5 # Combine compute_density() with layer_lines() to make a density plot of the waiting variable. faithful %>% compute_density ( ~ waiting ) %>% ggvis ( ~ pred_ , ~ resp_ ) %>% layer_lines () 1 2 3 4 # Build a density plot directly using layer_densities. Use the correct variables and properties. faithful %>% ggvis ( ~ waiting ) %>% layer_densities ( fill := \"green\" ) Shortcuts 1 2 3 4 # Complete the code to plot a bar graph of the cyl factor. mtcars %>% ggvis ( ~ factor ( cyl )) %>% layer_bars () 1 2 3 # Adapt the solution to the first challenge to just calculate the count values. No plotting! mtcars %>% compute_count ( ~ factor ( cyl )) 1 2 3 4 ## count_ x_ ## 1 11 4 ## 2 7 6 ## 3 14 8 ggvis and group_by 1 2 3 4 mtcars %>% group_by ( am ) %>% ggvis ( ~ mpg , ~ wt , stroke = ~ factor ( am )) %>% layer_smooths () 1 2 3 4 5 # Change the code to plot a unique smooth line for each value of the cyl variable. mtcars %>% group_by ( cyl ) %>% ggvis ( ~ mpg , ~ wt , stroke = ~ factor ( cyl )) %>% layer_smooths () 1 2 3 4 5 # Adapt the graph to contain a separate density for each value of cyl. mtcars %>% group_by ( cyl ) %>% ggvis ( ~ mpg ) %>% layer_densities () 1 2 3 4 5 # Copy and alter the solution to the second challenge to map the fill property to a categorical version of cyl. mtcars %>% group_by ( cyl ) %>% ggvis ( ~ mpg ) %>% layer_densities ( fill = ~ factor ( cyl )) group_by() versus interaction() 1 2 3 4 mtcars %>% group_by ( cyl ) %>% ggvis ( ~ mpg ) %>% layer_densities ( fill = ~ factor ( cyl )) 1 2 3 4 5 6 7 # Alter the graph: separate density for each unique combination of 'cyl' and 'am'. mtcars %>% group_by ( cyl , am ) %>% ggvis ( ~ mpg , fill = ~ factor ( cyl )) %>% layer_densities () #factor(cyl),factor(am) 1 2 3 4 mtcars %>% group_by ( cyl , am ) %>% ggvis ( ~ mpg , fill = ~ factor ( cyl )) %>% layer_densities () 1 2 3 4 5 # Update the graph to map `fill` to the unique combinations of the grouping variables. mtcars %>% group_by ( cyl , am ) %>% ggvis ( ~ mpg , fill = ~ interaction ( cyl , am )) %>% layer_densities () Chaining is a virtue 1 2 3 4 mtcars %>% group_by ( cyl , am ) %>% ggvis ( ~ mpg , fill = ~ interaction ( cyl , am )) %>% layer_densities () This call is exactly equivalent to the following piece of code that is very hard to read: 1 layer_densities ( ggvis ( group_by ( mtcars , cyl , am ), ~ mpg , fill = ~ interaction ( cyl , am )))","title":"Transformations"},{"location":"plot_snippets_ggvis/#interactivity-and-layers","text":"The basics of interactive plots 1 2 3 4 5 # Run this code and inspect the output. Follow the link in the instructions for the interactive version faithful %>% ggvis ( ~ waiting , ~ eruptions , fillOpacity := 0.5 , shape := input_select ( label = \"Choose shape:\" , choices = c ( \"circle\" , \"square\" , \"cross\" , \"diamond\" , \"triangle-up\" , \"triangle-down\" ))) %>% layer_points () .gif file: 1 2 3 4 # Copy the first code chunk and alter the code to make the fill property interactive using a select box faithful %>% ggvis ( ~ waiting , ~ eruptions , fillOpacity := 0.5 , shape := input_select ( label = \"Choose shape:\" , choices = c ( \"circle\" , \"square\" , \"cross\" , \"diamond\" , \"triangle-up\" , \"triangle-down\" )), fill := input_select ( label = \"Choose color:\" , choices = c ( \"black\" , \"red\" , \"blue\" , \"green\" ))) %>% layer_points () .gif file: 1 2 3 4 # Add radio buttons to control the fill of the plot mtcars %>% ggvis ( ~ mpg , ~ wt , fill := input_radiobuttons ( label = \"Choose color:\" , choices = c ( \"black\" , \"red\" , \"blue\" , \"green\" ))) %>% layer_points () .gif file: Input widgets in more detail 1 2 3 mtcars %>% ggvis ( ~ mpg , ~ wt , fill := input_radiobuttons ( label = \"Choose color:\" , choices = c ( \"black\" , \"red\" , \"blue\" , \"green\" ))) %>% layer_points () .gif file: 1 2 3 4 # Change the radiobuttons widget to a text widget mtcars %>% ggvis ( ~ mpg , ~ wt , fill := input_text ( label = \"Choose color:\" , value = c ( \"black\" ))) %>% layer_points () .gif file: 1 2 3 mtcars %>% ggvis ( ~ mpg , ~ wt ) %>% layer_points () 1 2 3 4 # Map the fill property to a select box that returns variable names mtcars %>% ggvis ( ~ mpg , ~ wt , fill = input_select ( label = \"Choose fill variable:\" , choices = names ( mtcars ), map = as.name )) %>% layer_points () .gif file: Input widgets in more detail (2) 1 2 3 4 # Map the fill property to a select box that returns variable names mtcars %>% ggvis ( ~ mpg , ~ wt , fill = input_select ( label = \"Choose fill variable:\" , choices = names ( mtcars ), map = as.name )) %>% layer_points () .gif file: Control parameters and values 1 2 3 4 # Map the bindwidth to a numeric field (\"Choose a binwidth:\") mtcars %>% ggvis ( ~ mpg ) %>% layer_histograms ( width = input_numeric ( value = 1 , label = \"Choose a binwidth:\" )) .gif file: 1 2 3 4 # Map the binwidth to a slider bar (\"Choose a binwidth:\") with the correct specifications mtcars %>% ggvis ( ~ mpg ) %>% layer_histograms ( width = input_slider ( min = 1 , max = 20 , label = \"Choose a binwidth:\" )) .gif file: Multi-layered plots and their properties 1 2 3 4 # Add a layer of points to the graph below. pressure %>% ggvis ( ~ temperature , ~ pressure , stroke := \"skyblue\" ) %>% layer_lines () %>% layer_points () 1 2 3 4 5 # Copy and adapt the solution to the first instruction below so that only the lines layer uses a skyblue stroke. pressure %>% ggvis ( ~ temperature , ~ pressure ) %>% layer_lines ( stroke := \"skyblue\" ) %>% layer_points () 1 2 3 4 5 # Rewrite the code below so that only the points layer uses the shape property. pressure %>% ggvis ( ~ temperature , ~ pressure ) %>% layer_lines ( stroke := \"skyblue\" ) %>% layer_points ( shape := \"triangle-up\" ) 1 2 3 4 5 # Refactor the code for the graph below to make it as concise as possible pressure %>% ggvis ( ~ temperature , ~ pressure , stroke := \"skyblue\" , strokeOpacity := 0.5 , strokeWidth := 5 ) %>% layer_lines () %>% layer_points ( fill = ~ temperature , shape := \"triangle-up\" , size := 300 ) Multi-layered plots and their properties (2) 1 2 3 4 5 # Rewrite the code below so that only the points layer uses the shape property. pressure %>% ggvis ( ~ temperature , ~ pressure ) %>% layer_lines ( stroke := \"skyblue\" ) %>% layer_points ( shape := \"triangle-up\" ) 1 2 3 4 5 6 # Refactor the code for the graph below to make it as concise as possible pressure %>% ggvis ( ~ temperature , ~ pressure , stroke := \"skyblue\" , strokeOpacity := 0.5 , strokeWidth := 5 ) %>% layer_lines () %>% layer_points ( fill = ~ temperature , shape := \"triangle-up\" , size := 300 ) There is no limit on the number of layers! 1 2 3 4 5 6 7 # Create a graph containing a scatterplot, a linear model and a smooth line. pressure %>% ggvis ( ~ temperature , ~ pressure ) %>% layer_points () %>% layer_lines ( stroke := \"black\" , opacity := 0.5 ) %>% layer_model_predictions ( model = \"lm\" , stroke := \"navy\" ) %>% layer_smooths ( stroke := \"skyblue\" ) Taking local and global to the next level 1 2 3 4 5 pressure %>% ggvis ( ~ temperature , ~ pressure , stroke := \"darkred\" ) %>% layer_lines ( stroke := \"orange\" , strokeDash := 5 , strokeWidth := 5 ) %>% layer_points ( shape := \"circle\" , size := 100 , fill := \"lightgreen\" ) %>% layer_smooths ()","title":"Interactivity and Layers"},{"location":"plot_snippets_ggvis/#customizing-axes-legends-and-scales","text":"Axes 1 2 3 4 5 6 # add the title of the x axis: \"Time since previous eruption (m)\" faithful %>% ggvis ( ~ waiting , ~ eruptions ) %>% layer_points () %>% add_axis ( \"y\" , title = \"Duration of eruption (m)\" ) %>% add_axis ( \"x\" , title = \"Time since previous eruption (m)\" ) 1 2 3 4 5 6 # Add to the code to place a labelled tick mark at 50, 60, 70, 80, 90 on the x axis. faithful %>% ggvis ( ~ waiting , ~ eruptions ) %>% layer_points () %>% add_axis ( \"y\" , title = \"Duration of eruption (m)\" , values = c ( 2 , 3 , 4 , 5 ), subdivide = 9 ) %>% add_axis ( \"x\" , title = \"Time since previous eruption (m)\" , values = c ( 50 , 60 , 70 , 80 , 90 ), subdivide = 9 ) 1 2 3 4 5 # Add to the code below to change the location of the y axis faithful %>% ggvis ( ~ waiting , ~ eruptions ) %>% layer_points () %>% add_axis ( \"x\" , orient = \"top\" ) %>% add_axis ( \"y\" , orient = \"right\" ) Legends 1 2 3 4 5 # Add a legend to the plot below: use the correct title and orientation faithful %>% ggvis ( ~ waiting , ~ eruptions , opacity := 0.6 , fill = ~ factor ( round ( eruptions ))) %>% layer_points () %>% add_legend ( \"fill\" , title = \"~ duration (m)\" , orient = \"left\" ) 1 2 3 4 5 6 7 8 #add_legend(vis, scales = NULL, orient = \"right\", title = NULL, format = NULL, values = NULL, properties = NULL) # Use add_legend() to combine the legends in the plot below. Adjust its properties as instructed. faithful %>% ggvis ( ~ waiting , ~ eruptions , opacity := 0.6 , fill = ~ factor ( round ( eruptions )), shape = ~ factor ( round ( eruptions )), size = ~ round ( eruptions )) %>% layer_points () %>% add_legend ( c ( \"fill\" , \"shape\" , \"size\" ), title = \"~ duration (m)\" , values = c ( 2 , 3 , 4 , 5 )) Legends (2) 1 2 3 4 5 6 # Fix the legend faithful %>% ggvis ( ~ waiting , ~ eruptions , opacity := 0.6 , fill = ~ factor ( round ( eruptions )), shape = ~ factor ( round ( eruptions )), size = ~ round ( eruptions )) %>% layer_points () %>% add_legend ( c ( \"fill\" , \"shape\" , \"size\" ), title = \"~ duration (m)\" ) Scale types 1 2 3 4 5 # Add to the code below to make the stroke color range from \"darkred\" to \"orange\". mtcars %>% ggvis ( ~ wt , ~ mpg , fill = ~ disp , stroke = ~ disp , strokeWidth := 2 ) %>% layer_points () %>% scale_numeric ( \"fill\" , range = c ( \"red\" , \"yellow\" )) %>% scale_numeric ( \"stroke\" , range = c ( \"darkred\" , \"orange\" )) 1 2 3 # Change the graph below to make the fill colors range from green to beige. mtcars %>% ggvis ( ~ wt , ~ mpg , fill = ~ hp ) %>% layer_points () %>% scale_numeric ( \"fill\" , range = c ( \"green\" , \"beige\" )) 1 2 3 # Create a scale that will map `factor(cyl)` to a new range of colors: purple, blue, and green. mtcars %>% ggvis ( ~ wt , ~ mpg , fill = ~ factor ( cyl )) %>% layer_points () %>% scale_nominal ( \"fill\" , range = c ( \"purple\" , \"blue\" , \"green\" )) Adjust any visual property 1 2 3 4 5 # Add a scale that limits the range of opacity from 0.2 to 1. mtcars %>% ggvis ( x = ~ wt , y = ~ mpg , fill = ~ factor ( cyl ), opacity = ~ hp ) %>% layer_points () %>% scale_numeric ( \"opacity\" , range = c ( 0.2 , 1 )) 1 2 3 4 5 6 # Add a second scale that will expand the x axis to cover data values from 0 to 6. mtcars %>% ggvis ( ~ wt , ~ mpg , fill = ~ disp ) %>% layer_points () %>% scale_numeric ( \"y\" , domain = c ( 0 , NA )) %>% scale_numeric ( \"x\" , domain = c ( 0 , 6 )) Adjust any visual property (2) 1 2 3 4 5 6 # Add a second scale to set domain for x mtcars %>% ggvis ( ~ wt , ~ mpg , fill = ~ disp ) %>% layer_points () %>% scale_numeric ( \"y\" , domain = c ( 0 , NA )) %>% scale_numeric ( \"x\" , domain = c ( 0 , 6 )) \u201c = \u201d versus \u201c := \u201c 1 2 3 4 5 6 # Set the fill value to the color variable instead of mapping it, and see what happens mtcars $ color <- c ( \"red\" , \"teal\" , \"#cccccc\" , \"tan\" ) mtcars %>% ggvis ( x = ~ wt , y = ~ mpg , fill = ~ color ) %>% layer_points () 1 2 3 mtcars %>% ggvis ( x = ~ wt , y = ~ mpg , fill := ~ color ) %>% layer_points ()","title":"Customizing Axes, Legends, and Scales"},{"location":"reading_data_into_r_with_readr/","text":"Foreword Output options: the \u2018tango\u2019 syntax and the \u2018readable\u2019 theme. Snippets and results. Importing data with readr \u00b6 Reading a .csv file Your first task will be to master the use of the read_csv() function. There are many arguments available, but the only required argument is file , a path to a CSV file on your computer (or the web). One big advantage that read_csv() has over read.csv() is that it doesn\u2019t convert strings into factors by default. read_csv() recognizes 8 different data types (integer, logical, etc.) and leaves anything else as characters. That means you don\u2019t have to set stringsAsFactors = FALSE every time you import a CSV file with character strings! 1 2 3 4 #install.packages('readr') library ( readr ) getwd () 1 ## [1] \"D:/.../Rprojects/Data Wrangling\" 1 setwd ( \"D:/.../Rprojects/Data Wrangling\" ) Import .csv (only \u2018,\u2019). 1 2 3 4 5 # Import chickwts.csv: cwts cwts <- read_csv ( 'chickwts.csv' ) # View the head of cwts head ( cwts ) 1 2 3 4 5 6 7 8 9 ## # A tibble: 6 \u00d7 2 ## weight feed ## &lt;int&gt; &lt;chr&gt; ## 1 179 horsebean ## 2 160 horsebean ## 3 136 horsebean ## 4 227 horsebean ## 5 217 horsebean ## 6 168 horsebean Reading a (.txt) .tsv file Skipping columns with col_skip() . Code only: Setting the column type. 1 2 3 4 cols ( weight = col_integer (), feed = col_character () ) Setting the column names. 1 col_names = c ( 'name' , 'state' , 'phone' ) Removing NA. 1 na = c ( 'NA' , 'null' ) In practice. 1 2 3 4 5 6 7 8 9 # Import data salaries <- read_tsv ( 'Salaries.txt' , col_names = FALSE , col_types = cols ( X2 = col_skip (), X3 = col_skip (), X4 = col_skip () )) # View first six rows of salaries head ( salaries ) 1 2 3 4 5 6 7 8 9 ## # A tibble: 6 \u00d7 3 ## X1 X5 X6 ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 Prof Male 139750 ## 2 Prof Male 173200 ## 3 AsstProf Male 79750 ## 4 Prof Male 115000 ## 5 Prof Male 141500 ## 6 AssocProf Male 97000 Reading a European .csv In most of Europe, commas (rather than periods) are used as decimal points. 1 2 3 4 5 # Import data with read_csv2(): trees trees <- read_csv2 ( 'trees.csv' ) # View dimensions and head of trees dim ( trees ) 1 ## [1] 9 3 1 head ( trees ) 1 2 3 4 5 6 7 8 9 ## # A tibble: 6 \u00d7 3 ## Girth Height Volume ## &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 83 70 103 ## 2 86 65 103 ## 3 88 63 102 ## 4 105 72 164 ## 5 107 81 188 ## 6 108 83 197 Read a fixed-width file Files containing columns of data that are separated by whitespace and all line up on one side. Code only: 1 2 # Import names.txt: names names <- read_table ( 'names.txt' , col_names = c ( 'name' , 'state' , 'phone' ), na = c ( 'NA' , 'null' )) Reading a text file Import ordinary text files. 1 2 3 4 # vector of character strings. # Import as a character vector, one item per line: tweets tweets <- read_lines ( 'tweets.txt' ) tweets 1 2 3 4 5 6 ## [1] \"carrots can be eat by most people\" ## [2] \"On predisents day we honor the big US man himself: Aberham Liclon. Tall, skinny, dry, and cruncy - he was america's carrot\" ## [3] \"knock knoc who is there? yup: carosot ( joke )\" ## [4] \"it is 2016 time for a carot emoji please!\" ## [5] \"when life give you lemnos , have a carrot\" ## [6] \"If you squent your eyes real hard a football look like a dry brown carrot Honestly\" 1 2 3 4 # returns a length 1 vector of the entire file, with line breaks represented as \\n # Import as a length 1 vector: tweets_all tweets_all <- read_file ( 'tweets.txt' ) tweets_all 1 ## [1] \"carrots can be eat by most people\\r\\nOn predisents day we honor the big US man himself: Aberham Liclon. Tall, skinny, dry, and cruncy - he was america's carrot\\r\\nknock knoc who is there? yup: carosot ( joke )\\r\\nit is 2016 time for a carot emoji please!\\r\\nwhen life give you lemnos , have a carrot\\r\\nIf you squent your eyes real hard a football look like a dry brown carrot Honestly\" Writing .csv and .tsv files Code only: 1 2 3 4 5 # Save cwts as chickwts.csv write_csv ( cwts , \"chickwts.csv\" ) # Append cwts2 to chickwts.csv write_csv ( cwts2 , \"chickwts.csv\" , append = TRUE ) Writing .rds files If the R object you\u2019re working with has metadata associated with it, saving to a CSV will cause that information to be lost. Exports an entire R object (metadata and all). Code only: 1 2 3 4 5 6 7 8 # Save trees as trees.rds write_rds ( trees , 'trees.rds' ) # Import trees.rds: trees2 trees2 <- read_rds ( 'trees.rds' ) # Check whether trees and trees2 are the same identical ( trees , trees2 ) Parsing Data with readr \u00b6 Coercing columns to different data types readr functions are quite good at guessing the correct data type for each column in a dataset. Of course, they aren\u2019t perfect, so sometimes you will need to change the type of a column after importing. Code only: 1 2 # Convert all columns to double trees2 <- type_convert ( trees , col_types = cols ( Girth = 'd' , Height = 'd' , Volume = 'd' )) Coercing character columns into factors readr import functions is that they don\u2019t automatically convert strings into factors like read.csv does. Code only: 1 2 3 4 5 # Parse the title column salaries $ title <- parse_factor ( salaries $ title , levels = c ( 'Prof' , 'AsstProf' , 'AssocProf' )) # Parse the gender column salaries $ gender <- parse_factor ( salaries $ gender , levels = c ( 'Male' , 'Female' )) Creating Date objects The readr import functions can automatically recognize dates in standard ISO 8601 format (YYYY-MM-DD) and parse columns accordingly. If you want to import a dataset with dates in other formats, you can use parse_date . Code only: 1 2 # Change type of date column weather $ date <- parse_date ( weather $ date , format = '%m/%d/%Y' ) Parsing number formats The readr importing functions can sometimes run into trouble parsing a column as numbers when it contains non-numeric symbols in addition to numerals. Code only: 1 2 # Parse amount column as a number debt $ amount <- parse_number ( debt $ amount ) Viewing metadata before importing In some cases, it may be easier to get an idea of how readr plans to parse a dataset before you actually import it. When you see the planned column specification, you might decide to change the type of one or more columns, for example. spec_csv for .csv and .tsv files. spec_delim for .txt files (among others). 1 2 # Specifications of chickwts spec_csv ( 'chickwts.csv' ) 1 2 3 4 ## cols( ## weight = col_integer(), ## feed = col_character() ## )","title":"Reading Data into R with readr"},{"location":"reading_data_into_r_with_readr/#importing-data-with-readr","text":"Reading a .csv file Your first task will be to master the use of the read_csv() function. There are many arguments available, but the only required argument is file , a path to a CSV file on your computer (or the web). One big advantage that read_csv() has over read.csv() is that it doesn\u2019t convert strings into factors by default. read_csv() recognizes 8 different data types (integer, logical, etc.) and leaves anything else as characters. That means you don\u2019t have to set stringsAsFactors = FALSE every time you import a CSV file with character strings! 1 2 3 4 #install.packages('readr') library ( readr ) getwd () 1 ## [1] \"D:/.../Rprojects/Data Wrangling\" 1 setwd ( \"D:/.../Rprojects/Data Wrangling\" ) Import .csv (only \u2018,\u2019). 1 2 3 4 5 # Import chickwts.csv: cwts cwts <- read_csv ( 'chickwts.csv' ) # View the head of cwts head ( cwts ) 1 2 3 4 5 6 7 8 9 ## # A tibble: 6 \u00d7 2 ## weight feed ## &lt;int&gt; &lt;chr&gt; ## 1 179 horsebean ## 2 160 horsebean ## 3 136 horsebean ## 4 227 horsebean ## 5 217 horsebean ## 6 168 horsebean Reading a (.txt) .tsv file Skipping columns with col_skip() . Code only: Setting the column type. 1 2 3 4 cols ( weight = col_integer (), feed = col_character () ) Setting the column names. 1 col_names = c ( 'name' , 'state' , 'phone' ) Removing NA. 1 na = c ( 'NA' , 'null' ) In practice. 1 2 3 4 5 6 7 8 9 # Import data salaries <- read_tsv ( 'Salaries.txt' , col_names = FALSE , col_types = cols ( X2 = col_skip (), X3 = col_skip (), X4 = col_skip () )) # View first six rows of salaries head ( salaries ) 1 2 3 4 5 6 7 8 9 ## # A tibble: 6 \u00d7 3 ## X1 X5 X6 ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 Prof Male 139750 ## 2 Prof Male 173200 ## 3 AsstProf Male 79750 ## 4 Prof Male 115000 ## 5 Prof Male 141500 ## 6 AssocProf Male 97000 Reading a European .csv In most of Europe, commas (rather than periods) are used as decimal points. 1 2 3 4 5 # Import data with read_csv2(): trees trees <- read_csv2 ( 'trees.csv' ) # View dimensions and head of trees dim ( trees ) 1 ## [1] 9 3 1 head ( trees ) 1 2 3 4 5 6 7 8 9 ## # A tibble: 6 \u00d7 3 ## Girth Height Volume ## &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 83 70 103 ## 2 86 65 103 ## 3 88 63 102 ## 4 105 72 164 ## 5 107 81 188 ## 6 108 83 197 Read a fixed-width file Files containing columns of data that are separated by whitespace and all line up on one side. Code only: 1 2 # Import names.txt: names names <- read_table ( 'names.txt' , col_names = c ( 'name' , 'state' , 'phone' ), na = c ( 'NA' , 'null' )) Reading a text file Import ordinary text files. 1 2 3 4 # vector of character strings. # Import as a character vector, one item per line: tweets tweets <- read_lines ( 'tweets.txt' ) tweets 1 2 3 4 5 6 ## [1] \"carrots can be eat by most people\" ## [2] \"On predisents day we honor the big US man himself: Aberham Liclon. Tall, skinny, dry, and cruncy - he was america's carrot\" ## [3] \"knock knoc who is there? yup: carosot ( joke )\" ## [4] \"it is 2016 time for a carot emoji please!\" ## [5] \"when life give you lemnos , have a carrot\" ## [6] \"If you squent your eyes real hard a football look like a dry brown carrot Honestly\" 1 2 3 4 # returns a length 1 vector of the entire file, with line breaks represented as \\n # Import as a length 1 vector: tweets_all tweets_all <- read_file ( 'tweets.txt' ) tweets_all 1 ## [1] \"carrots can be eat by most people\\r\\nOn predisents day we honor the big US man himself: Aberham Liclon. Tall, skinny, dry, and cruncy - he was america's carrot\\r\\nknock knoc who is there? yup: carosot ( joke )\\r\\nit is 2016 time for a carot emoji please!\\r\\nwhen life give you lemnos , have a carrot\\r\\nIf you squent your eyes real hard a football look like a dry brown carrot Honestly\" Writing .csv and .tsv files Code only: 1 2 3 4 5 # Save cwts as chickwts.csv write_csv ( cwts , \"chickwts.csv\" ) # Append cwts2 to chickwts.csv write_csv ( cwts2 , \"chickwts.csv\" , append = TRUE ) Writing .rds files If the R object you\u2019re working with has metadata associated with it, saving to a CSV will cause that information to be lost. Exports an entire R object (metadata and all). Code only: 1 2 3 4 5 6 7 8 # Save trees as trees.rds write_rds ( trees , 'trees.rds' ) # Import trees.rds: trees2 trees2 <- read_rds ( 'trees.rds' ) # Check whether trees and trees2 are the same identical ( trees , trees2 )","title":"Importing data with readr"},{"location":"reading_data_into_r_with_readr/#parsing-data-with-readr","text":"Coercing columns to different data types readr functions are quite good at guessing the correct data type for each column in a dataset. Of course, they aren\u2019t perfect, so sometimes you will need to change the type of a column after importing. Code only: 1 2 # Convert all columns to double trees2 <- type_convert ( trees , col_types = cols ( Girth = 'd' , Height = 'd' , Volume = 'd' )) Coercing character columns into factors readr import functions is that they don\u2019t automatically convert strings into factors like read.csv does. Code only: 1 2 3 4 5 # Parse the title column salaries $ title <- parse_factor ( salaries $ title , levels = c ( 'Prof' , 'AsstProf' , 'AssocProf' )) # Parse the gender column salaries $ gender <- parse_factor ( salaries $ gender , levels = c ( 'Male' , 'Female' )) Creating Date objects The readr import functions can automatically recognize dates in standard ISO 8601 format (YYYY-MM-DD) and parse columns accordingly. If you want to import a dataset with dates in other formats, you can use parse_date . Code only: 1 2 # Change type of date column weather $ date <- parse_date ( weather $ date , format = '%m/%d/%Y' ) Parsing number formats The readr importing functions can sometimes run into trouble parsing a column as numbers when it contains non-numeric symbols in addition to numerals. Code only: 1 2 # Parse amount column as a number debt $ amount <- parse_number ( debt $ amount ) Viewing metadata before importing In some cases, it may be easier to get an idea of how readr plans to parse a dataset before you actually import it. When you see the planned column specification, you might decide to change the type of one or more columns, for example. spec_csv for .csv and .tsv files. spec_delim for .txt files (among others). 1 2 # Specifications of chickwts spec_csv ( 'chickwts.csv' ) 1 2 3 4 ## cols( ## weight = col_integer(), ## feed = col_character() ## )","title":"Parsing Data with readr"},{"location":"specializations_cs/","text":"Caret \u00b6 caret . PDF. Data mining \u00b6 Data Mining . PDF only. association rules, sequential patterns, classification & prediction, regression, clustering, outliers, time series, text mining, socila networks, graph mining, spatial data, statistics, graphics, data manipulation, data access, big data, parallel computing, reports, weka, editors, guis data.table \u00b6 data.table Intro . PDF only (explanatory article). data.table . PDF. dplyr & tidyr \u00b6 Grammar of dplyr . PDF only (explanatory slides). dplyr . PDF. dplyr & tidyr . PDF. Machine Learning \u00b6 Machine Learning . PDF only. Supervised Learning; Unsupervised Learning; Deep Learning; Machine Learning Tips and Tricks; Probabilities and Statistics; Linear Algebra and Calculus. Big Data Machine Learning . PDF only. linear regression, logistic regression, regularization (ridge, lasso), neural network, support vector machine, nayesian network and na\u00efve bayes, k-nearest neighbors, decision tree, tree ensembles (bagging or random forest, boosting) Machine Learning Modelling in R . PDF. Machine Learning with R . PDF. quanteda (for NLP). PDF. Keras . PDF. Probabilities \u00b6 Probabilities . PDF only. Quandl \u00b6 Quandl . PDF. Regular Expressions (regex) \u00b6 Basic Regular Expressions . PDF. sjmisc \u00b6 sjmisc . PDF. Spark \u00b6 sparklyr . PDF. Strings \u00b6 stringr . PDF. Survival Analysis & Regression \u00b6 survminer . PDF only. curve, ggplot2, cox model Regressions . PDF only. linear model, variable selection, diagnostics, graphics, tests, variable transformation, ridge, segmented, gls, glm, nls, gnls, loess, splines, robust, structural equation, simultaneous equation, pls, principal components, quantile, linear and nonlinear mixed effects, generalized additive, survival analysis, classification & regression trees, beta Time Series \u00b6 lubridate . PDF only. Time Series . PDF only. input, decomposition, tests, stochastic, graphics, miscellaneous xts . PDF only. Tydyverse (including Purrr) \u00b6 Purrr . PDF. Tidyverse . PDF.","title":"Specializations Cheat Sheets"},{"location":"specializations_cs/#caret","text":"caret . PDF.","title":"Caret"},{"location":"specializations_cs/#data-mining","text":"Data Mining . PDF only. association rules, sequential patterns, classification & prediction, regression, clustering, outliers, time series, text mining, socila networks, graph mining, spatial data, statistics, graphics, data manipulation, data access, big data, parallel computing, reports, weka, editors, guis","title":"Data mining"},{"location":"specializations_cs/#datatable","text":"data.table Intro . PDF only (explanatory article). data.table . PDF.","title":"data.table"},{"location":"specializations_cs/#dplyr-tidyr","text":"Grammar of dplyr . PDF only (explanatory slides). dplyr . PDF. dplyr & tidyr . PDF.","title":"dplyr &amp; tidyr"},{"location":"specializations_cs/#machine-learning","text":"Machine Learning . PDF only. Supervised Learning; Unsupervised Learning; Deep Learning; Machine Learning Tips and Tricks; Probabilities and Statistics; Linear Algebra and Calculus. Big Data Machine Learning . PDF only. linear regression, logistic regression, regularization (ridge, lasso), neural network, support vector machine, nayesian network and na\u00efve bayes, k-nearest neighbors, decision tree, tree ensembles (bagging or random forest, boosting) Machine Learning Modelling in R . PDF. Machine Learning with R . PDF. quanteda (for NLP). PDF. Keras . PDF.","title":"Machine Learning"},{"location":"specializations_cs/#probabilities","text":"Probabilities . PDF only.","title":"Probabilities"},{"location":"specializations_cs/#quandl","text":"Quandl . PDF.","title":"Quandl"},{"location":"specializations_cs/#regular-expressions-regex","text":"Basic Regular Expressions . PDF.","title":"Regular Expressions (regex)"},{"location":"specializations_cs/#sjmisc","text":"sjmisc . PDF.","title":"sjmisc"},{"location":"specializations_cs/#spark","text":"sparklyr . PDF.","title":"Spark"},{"location":"specializations_cs/#strings","text":"stringr . PDF.","title":"Strings"},{"location":"specializations_cs/#survival-analysis-regression","text":"survminer . PDF only. curve, ggplot2, cox model Regressions . PDF only. linear model, variable selection, diagnostics, graphics, tests, variable transformation, ridge, segmented, gls, glm, nls, gnls, loess, splines, robust, structural equation, simultaneous equation, pls, principal components, quantile, linear and nonlinear mixed effects, generalized additive, survival analysis, classification & regression trees, beta","title":"Survival Analysis &amp; Regression"},{"location":"specializations_cs/#time-series","text":"lubridate . PDF only. Time Series . PDF only. input, decomposition, tests, stochastic, graphics, miscellaneous xts . PDF only.","title":"Time Series"},{"location":"specializations_cs/#tydyverse-including-purrr","text":"Purrr . PDF. Tidyverse . PDF.","title":"Tydyverse (including Purrr)"},{"location":"tables/","text":"Foreword Output options: \u2018pygments\u2019 syntax, the \u2018readable\u2019 theme. Snippets and results. Markdown tables \u00b6 Example 1 \u00b6 1 2 3 4 5 6 7 | Right | Left | Default | Center | | ------: |:- ---- | --------- |:- -----: | | 12 | 12 | 12 | 12 | | 123 | 123 | 123 | 123 | | 1 | 1 | 1 | 1 | : Demonstration of pipe table syntax . Demonstration of pipe table syntax. Right Left Default Center 12 12 12 12 123 123 123 123 1 1 1 1 Example 2 \u00b6 1 2 3 4 5 6 7 8 9 10 11 : Sample grid table. +---------------+---------------+--------------------+ | Fruit | Price | Advantages | +===============+===============+====================+ | Bananas | $1.34 | - built-in wrapper | | | | - bright color | +---------------+---------------+--------------------+ | Oranges | $2.10 | - cures scurvy | | | | - tasty | +---------------+---------------+--------------------+ Sample grid table. Fruit Price Advantages Bananas $1.34 built-in wrapper bright color Oranges $2.10 cures scurvy tasty The xtable package \u00b6 The output is in HTML. Example 1 \u00b6 1 2 3 4 5 6 7 library ( xtable ) # given the data in the first row print ( xtable ( output , caption = 'A test table' , align = c ( 'l' , 'c' , 'r' )), type = 'html' ) A test table 1st header 2nd header 1st row Content A Content B 2nd row Content C Content D The knitr::kable function \u00b6 The output is in Markdown. Example 1 \u00b6 1 2 3 4 5 6 library ( knitr ) # given the data in the first row kable ( output , caption = 'A test table' , align = c ( 'c' , 'r' )) A test table 1st header 2nd header 1st row Content A Content B 2nd row Content C Content D We can also write knitr::kable() without calling library(knitr) . The htmlTable package \u00b6 htmlTable on GitHub. Example 1 \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 output <- matrix ( paste ( 'Content' , LETTERS [1 : 16 ] ), ncol = 4 , byrow = TRUE ) library ( htmlTable ) htmlTable ( output , header = paste ( c ( '1st' , '2nd' , '3rd' , '4th' ), 'header' ), rnames = paste ( c ( '1st' , '2nd' , '3rd' , '4th' ), 'row' ), rgroup = c ( 'Group A' , 'Group B' ), n.rgroup = c ( 2 , 2 ), cgroup = c ( 'Cgroup 1' , 'Cgroup 2&dagger;' ), n.cgroup = c ( 2 , 2 ), caption = 'Basic table with both column spanners (groups) and row groups' , tfoot = '&dagger; A table footer commment' ) Basic table with both column spanners (groups) and row groups Cgroup 1 Cgroup 2\u2020 1st header 2nd header 3rd header 4th header Group A 1st row Content A Content B Content C Content D 2nd row Content E Content F Content G Content H Group B 3rd row Content I Content J Content K Content L 4th row Content M Content N Content O Content P \u2020 A table footer commment Example 2 \u00b6 1 2 3 4 5 6 7 8 9 10 library ( htmlTable ) # given the data in the first row htmlTable ( txtRound ( mx , 1 ), cgroup = cgroup , n.cgroup = n.cgroup , rgroup = c ( 'First period' , 'Second period' , 'Third period' ), n.rgroup = rep ( 5 , 3 ), tfoot = txtMergeLines ( '&Delta;<sub>int</sub> correspnds to the change since start' , '&Delta;<sub>std</sub> corresponds to the change compared to national average' )) Sweden Norrbotten county Stockholm county Uppsala county Men Women Men Women Men Women Men Women Age \u0394 int Age \u0394 int Age \u0394 int \u0394 std Age \u0394 int \u0394 std Age \u0394 int \u0394 std Age \u0394 int \u0394 std Age \u0394 int \u0394 std Age \u0394 int \u0394 std First period 1999 38.9 0.0 41.5 0.0 39.7 0.0 0.8 41.9 0.0 0.4 37.3 0.0 -1.6 40.1 0.0 -1.4 37.2 0.0 -1.7 39.3 0.0 -2.2 2000 39.0 0.1 41.6 0.1 40.0 0.3 1.0 42.2 0.3 0.6 37.4 0.1 -1.6 40.1 0.0 -1.5 37.5 0.3 -1.5 39.4 0.1 -2.2 2001 39.2 0.3 41.7 0.2 40.2 0.5 1.0 42.5 0.6 0.8 37.5 0.2 -1.7 40.1 0.0 -1.6 37.6 0.4 -1.6 39.6 0.3 -2.1 2002 39.3 0.4 41.8 0.3 40.5 0.8 1.2 42.8 0.9 1.0 37.6 0.3 -1.7 40.2 0.1 -1.6 37.8 0.6 -1.5 39.7 0.4 -2.1 2003 39.4 0.5 41.9 0.4 40.7 1.0 1.3 43.0 1.1 1.1 37.7 0.4 -1.7 40.2 0.1 -1.7 38.0 0.8 -1.4 39.8 0.5 -2.1 Second period 2004 39.6 0.7 42.0 0.5 40.9 1.2 1.3 43.1 1.2 1.1 37.8 0.5 -1.8 40.3 0.2 -1.7 38.1 0.9 -1.5 40.0 0.7 -2.0 2005 39.7 0.8 42.0 0.5 41.1 1.4 1.4 43.4 1.5 1.4 37.9 0.6 -1.8 40.3 0.2 -1.7 38.3 1.1 -1.4 40.1 0.8 -1.9 2006 39.8 0.9 42.1 0.6 41.3 1.6 1.5 43.5 1.6 1.4 37.9 0.6 -1.9 40.2 0.1 -1.9 38.5 1.3 -1.3 40.4 1.1 -1.7 2007 39.8 0.9 42.1 0.6 41.5 1.8 1.7 43.8 1.9 1.7 37.8 0.5 -2.0 40.1 0.0 -2.0 38.6 1.4 -1.2 40.5 1.2 -1.6 2008 39.9 1.0 42.1 0.6 41.7 2.0 1.8 44.0 2.1 1.9 37.8 0.5 -2.1 40.1 0.0 -2.0 38.7 1.5 -1.2 40.5 1.2 -1.6 Third period 2009 39.9 1.0 42.1 0.6 41.9 2.2 2.0 44.2 2.3 2.1 37.8 0.5 -2.1 40.0 -0.1 -2.1 38.8 1.6 -1.1 40.6 1.3 -1.5 2010 40.0 1.1 42.1 0.6 42.1 2.4 2.1 44.4 2.5 2.3 37.8 0.5 -2.2 40.0 -0.1 -2.1 38.9 1.7 -1.1 40.6 1.3 -1.5 2011 40.1 1.2 42.2 0.7 42.3 2.6 2.2 44.5 2.6 2.3 37.9 0.6 -2.2 39.9 -0.2 -2.3 39.0 1.8 -1.1 40.7 1.4 -1.5 2012 40.2 1.3 42.2 0.7 42.4 2.7 2.2 44.6 2.7 2.4 37.9 0.6 -2.3 39.9 -0.2 -2.3 39.1 1.9 -1.1 40.8 1.5 -1.4 2013 40.2 1.3 42.2 0.7 42.4 2.7 2.2 44.7 2.8 2.5 38.0 0.7 -2.2 39.9 -0.2 -2.3 39.2 2.0 -1.0 40.9 1.6 -1.3 \u0394 int correspnds to the change since start \u0394 std corresponds to the change compared to national average Example 3 \u00b6 1 2 3 4 5 6 7 8 9 10 11 library ( htmlTable ) # given the data in the first row htmlTable ( txtRound ( mx , 1 ), align = 'rrrr|r' , cgroup = cgroup , n.cgroup = n.cgroup , rgroup = c ( 'First period' , 'Second period' , 'Third period' ), n.rgroup = rep ( 5 , 3 ), tfoot = txtMergeLines ( '&Delta;<sub>int</sub> correspnds to the change since start' , '&Delta;<sub>std</sub> corresponds to the change compared to national average' )) Sweden Norrbotten county Stockholm county Uppsala county Men Women Men Women Men Women Men Women Age \u0394 int Age \u0394 int Age \u0394 int \u0394 std Age \u0394 int \u0394 std Age \u0394 int \u0394 std Age \u0394 int \u0394 std Age \u0394 int \u0394 std Age \u0394 int \u0394 std First period 1999 38.9 0.0 41.5 0.0 39.7 0.0 0.8 41.9 0.0 0.4 37.3 0.0 -1.6 40.1 0.0 -1.4 37.2 0.0 -1.7 39.3 0.0 -2.2 2000 39.0 0.1 41.6 0.1 40.0 0.3 1.0 42.2 0.3 0.6 37.4 0.1 -1.6 40.1 0.0 -1.5 37.5 0.3 -1.5 39.4 0.1 -2.2 2001 39.2 0.3 41.7 0.2 40.2 0.5 1.0 42.5 0.6 0.8 37.5 0.2 -1.7 40.1 0.0 -1.6 37.6 0.4 -1.6 39.6 0.3 -2.1 2002 39.3 0.4 41.8 0.3 40.5 0.8 1.2 42.8 0.9 1.0 37.6 0.3 -1.7 40.2 0.1 -1.6 37.8 0.6 -1.5 39.7 0.4 -2.1 2003 39.4 0.5 41.9 0.4 40.7 1.0 1.3 43.0 1.1 1.1 37.7 0.4 -1.7 40.2 0.1 -1.7 38.0 0.8 -1.4 39.8 0.5 -2.1 Second period 2004 39.6 0.7 42.0 0.5 40.9 1.2 1.3 43.1 1.2 1.1 37.8 0.5 -1.8 40.3 0.2 -1.7 38.1 0.9 -1.5 40.0 0.7 -2.0 2005 39.7 0.8 42.0 0.5 41.1 1.4 1.4 43.4 1.5 1.4 37.9 0.6 -1.8 40.3 0.2 -1.7 38.3 1.1 -1.4 40.1 0.8 -1.9 2006 39.8 0.9 42.1 0.6 41.3 1.6 1.5 43.5 1.6 1.4 37.9 0.6 -1.9 40.2 0.1 -1.9 38.5 1.3 -1.3 40.4 1.1 -1.7 2007 39.8 0.9 42.1 0.6 41.5 1.8 1.7 43.8 1.9 1.7 37.8 0.5 -2.0 40.1 0.0 -2.0 38.6 1.4 -1.2 40.5 1.2 -1.6 2008 39.9 1.0 42.1 0.6 41.7 2.0 1.8 44.0 2.1 1.9 37.8 0.5 -2.1 40.1 0.0 -2.0 38.7 1.5 -1.2 40.5 1.2 -1.6 Third period 2009 39.9 1.0 42.1 0.6 41.9 2.2 2.0 44.2 2.3 2.1 37.8 0.5 -2.1 40.0 -0.1 -2.1 38.8 1.6 -1.1 40.6 1.3 -1.5 2010 40.0 1.1 42.1 0.6 42.1 2.4 2.1 44.4 2.5 2.3 37.8 0.5 -2.2 40.0 -0.1 -2.1 38.9 1.7 -1.1 40.6 1.3 -1.5 2011 40.1 1.2 42.2 0.7 42.3 2.6 2.2 44.5 2.6 2.3 37.9 0.6 -2.2 39.9 -0.2 -2.3 39.0 1.8 -1.1 40.7 1.4 -1.5 2012 40.2 1.3 42.2 0.7 42.4 2.7 2.2 44.6 2.7 2.4 37.9 0.6 -2.3 39.9 -0.2 -2.3 39.1 1.9 -1.1 40.8 1.5 -1.4 2013 40.2 1.3 42.2 0.7 42.4 2.7 2.2 44.7 2.8 2.5 38.0 0.7 -2.2 39.9 -0.2 -2.3 39.2 2.0 -1.0 40.9 1.6 -1.3 \u0394 int correspnds to the change since start \u0394 std corresponds to the change compared to national average Example 4 \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 library ( htmlTable ) # given the data in the first row htmlTable ( txtRound ( mx , 1 ), col.columns = c ( rep ( '#E6E6F0' , 4 ), rep ( 'none' , ncol ( mx ) - 4 )), align = 'rrrr|r' , cgroup = cgroup , n.cgroup = n.cgroup , rgroup = c ( 'First period' , 'Second period' , 'Third period' ), n.rgroup = rep ( 5 , 3 ), tfoot = txtMergeLines ( '&Delta;<sub>int</sub> correspnds to the change since start' , '&Delta;<sub>std</sub> corresponds to the change compared to national average' )) Sweden Norrbotten county Stockholm county Uppsala county Men Women Men Women Men Women Men Women Age \u0394 int Age \u0394 int Age \u0394 int \u0394 std Age \u0394 int \u0394 std Age \u0394 int \u0394 std Age \u0394 int \u0394 std Age \u0394 int \u0394 std Age \u0394 int \u0394 std First period 1999 38.9 0.0 41.5 0.0 39.7 0.0 0.8 41.9 0.0 0.4 37.3 0.0 -1.6 40.1 0.0 -1.4 37.2 0.0 -1.7 39.3 0.0 -2.2 2000 39.0 0.1 41.6 0.1 40.0 0.3 1.0 42.2 0.3 0.6 37.4 0.1 -1.6 40.1 0.0 -1.5 37.5 0.3 -1.5 39.4 0.1 -2.2 2001 39.2 0.3 41.7 0.2 40.2 0.5 1.0 42.5 0.6 0.8 37.5 0.2 -1.7 40.1 0.0 -1.6 37.6 0.4 -1.6 39.6 0.3 -2.1 2002 39.3 0.4 41.8 0.3 40.5 0.8 1.2 42.8 0.9 1.0 37.6 0.3 -1.7 40.2 0.1 -1.6 37.8 0.6 -1.5 39.7 0.4 -2.1 2003 39.4 0.5 41.9 0.4 40.7 1.0 1.3 43.0 1.1 1.1 37.7 0.4 -1.7 40.2 0.1 -1.7 38.0 0.8 -1.4 39.8 0.5 -2.1 Second period 2004 39.6 0.7 42.0 0.5 40.9 1.2 1.3 43.1 1.2 1.1 37.8 0.5 -1.8 40.3 0.2 -1.7 38.1 0.9 -1.5 40.0 0.7 -2.0 2005 39.7 0.8 42.0 0.5 41.1 1.4 1.4 43.4 1.5 1.4 37.9 0.6 -1.8 40.3 0.2 -1.7 38.3 1.1 -1.4 40.1 0.8 -1.9 2006 39.8 0.9 42.1 0.6 41.3 1.6 1.5 43.5 1.6 1.4 37.9 0.6 -1.9 40.2 0.1 -1.9 38.5 1.3 -1.3 40.4 1.1 -1.7 2007 39.8 0.9 42.1 0.6 41.5 1.8 1.7 43.8 1.9 1.7 37.8 0.5 -2.0 40.1 0.0 -2.0 38.6 1.4 -1.2 40.5 1.2 -1.6 2008 39.9 1.0 42.1 0.6 41.7 2.0 1.8 44.0 2.1 1.9 37.8 0.5 -2.1 40.1 0.0 -2.0 38.7 1.5 -1.2 40.5 1.2 -1.6 Third period 2009 39.9 1.0 42.1 0.6 41.9 2.2 2.0 44.2 2.3 2.1 37.8 0.5 -2.1 40.0 -0.1 -2.1 38.8 1.6 -1.1 40.6 1.3 -1.5 2010 40.0 1.1 42.1 0.6 42.1 2.4 2.1 44.4 2.5 2.3 37.8 0.5 -2.2 40.0 -0.1 -2.1 38.9 1.7 -1.1 40.6 1.3 -1.5 2011 40.1 1.2 42.2 0.7 42.3 2.6 2.2 44.5 2.6 2.3 37.9 0.6 -2.2 39.9 -0.2 -2.3 39.0 1.8 -1.1 40.7 1.4 -1.5 2012 40.2 1.3 42.2 0.7 42.4 2.7 2.2 44.6 2.7 2.4 37.9 0.6 -2.3 39.9 -0.2 -2.3 39.1 1.9 -1.1 40.8 1.5 -1.4 2013 40.2 1.3 42.2 0.7 42.4 2.7 2.2 44.7 2.8 2.5 38.0 0.7 -2.2 39.9 -0.2 -2.3 39.2 2.0 -1.0 40.9 1.6 -1.3 \u0394 int correspnds to the change since start \u0394 std corresponds to the change compared to national average Example 5 \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 library ( htmlTable ) # given the data in the first row htmlTable ( txtRound ( mx , 1 ), col.rgroup = c ( 'none' , '#FFFFCC' ), col.columns = c ( rep ( '#EFEFF0' , 4 ), rep ( 'none' , ncol ( mx ) - 4 )), align = 'rrrr|r' , cgroup = cgroup , n.cgroup = n.cgroup , # I use the &nbsp; - the no breaking space as I don't want to have a # row break in the row group. This adds a little space in the table # when used together with the cspan.rgroup=1. rgroup = c ( '1st&nbsp;period' , '2nd&nbsp;period' , '3rd&nbsp;period' ), n.rgroup = rep ( 5 , 3 ), tfoot = txtMergeLines ( '&Delta;<sub>int</sub> correspnds to the change since start' , '&Delta;<sub>std</sub> corresponds to the change compared to national average' ), cspan.rgroup = 1 ) Sweden Norrbotten county Stockholm county Uppsala county Men Women Men Women Men Women Men Women Age \u0394 int Age \u0394 int Age \u0394 int \u0394 std Age \u0394 int \u0394 std Age \u0394 int \u0394 std Age \u0394 int \u0394 std Age \u0394 int \u0394 std Age \u0394 int \u0394 std 1st period 1999 38.9 0.0 41.5 0.0 39.7 0.0 0.8 41.9 0.0 0.4 37.3 0.0 -1.6 40.1 0.0 -1.4 37.2 0.0 -1.7 39.3 0.0 -2.2 2000 39.0 0.1 41.6 0.1 40.0 0.3 1.0 42.2 0.3 0.6 37.4 0.1 -1.6 40.1 0.0 -1.5 37.5 0.3 -1.5 39.4 0.1 -2.2 2001 39.2 0.3 41.7 0.2 40.2 0.5 1.0 42.5 0.6 0.8 37.5 0.2 -1.7 40.1 0.0 -1.6 37.6 0.4 -1.6 39.6 0.3 -2.1 2002 39.3 0.4 41.8 0.3 40.5 0.8 1.2 42.8 0.9 1.0 37.6 0.3 -1.7 40.2 0.1 -1.6 37.8 0.6 -1.5 39.7 0.4 -2.1 2003 39.4 0.5 41.9 0.4 40.7 1.0 1.3 43.0 1.1 1.1 37.7 0.4 -1.7 40.2 0.1 -1.7 38.0 0.8 -1.4 39.8 0.5 -2.1 2nd period 2004 39.6 0.7 42.0 0.5 40.9 1.2 1.3 43.1 1.2 1.1 37.8 0.5 -1.8 40.3 0.2 -1.7 38.1 0.9 -1.5 40.0 0.7 -2.0 2005 39.7 0.8 42.0 0.5 41.1 1.4 1.4 43.4 1.5 1.4 37.9 0.6 -1.8 40.3 0.2 -1.7 38.3 1.1 -1.4 40.1 0.8 -1.9 2006 39.8 0.9 42.1 0.6 41.3 1.6 1.5 43.5 1.6 1.4 37.9 0.6 -1.9 40.2 0.1 -1.9 38.5 1.3 -1.3 40.4 1.1 -1.7 2007 39.8 0.9 42.1 0.6 41.5 1.8 1.7 43.8 1.9 1.7 37.8 0.5 -2.0 40.1 0.0 -2.0 38.6 1.4 -1.2 40.5 1.2 -1.6 2008 39.9 1.0 42.1 0.6 41.7 2.0 1.8 44.0 2.1 1.9 37.8 0.5 -2.1 40.1 0.0 -2.0 38.7 1.5 -1.2 40.5 1.2 -1.6 3rd period 2009 39.9 1.0 42.1 0.6 41.9 2.2 2.0 44.2 2.3 2.1 37.8 0.5 -2.1 40.0 -0.1 -2.1 38.8 1.6 -1.1 40.6 1.3 -1.5 2010 40.0 1.1 42.1 0.6 42.1 2.4 2.1 44.4 2.5 2.3 37.8 0.5 -2.2 40.0 -0.1 -2.1 38.9 1.7 -1.1 40.6 1.3 -1.5 2011 40.1 1.2 42.2 0.7 42.3 2.6 2.2 44.5 2.6 2.3 37.9 0.6 -2.2 39.9 -0.2 -2.3 39.0 1.8 -1.1 40.7 1.4 -1.5 2012 40.2 1.3 42.2 0.7 42.4 2.7 2.2 44.6 2.7 2.4 37.9 0.6 -2.3 39.9 -0.2 -2.3 39.1 1.9 -1.1 40.8 1.5 -1.4 2013 40.2 1.3 42.2 0.7 42.4 2.7 2.2 44.7 2.8 2.5 38.0 0.7 -2.2 39.9 -0.2 -2.3 39.2 2.0 -1.0 40.9 1.6 -1.3 \u0394 int correspnds to the change since start \u0394 std corresponds to the change compared to national average Example 6 \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 library ( htmlTable ) # given the data in the first row htmlTable ( out_mx , caption = 'Average age in Sweden counties over a period of 15 years. The Norbotten county is typically known for having a negative migration pattern compared to Stockholm, while Uppsala has a proportionally large population of students.' , pos.rowlabel = 'bottom' , rowlabel = 'Year' , col.rgroup = c ( 'none' , '#FFFFCC' ), col.columns = c ( rep ( '#EFEFF0' , 4 ), rep ( 'none' , ncol ( mx ) - 4 )), align = 'rrrr|r' , cgroup = cgroup , n.cgroup = n.cgroup , rgroup = c ( '1st&nbsp;period' , '2nd&nbsp;period' , '3rd&nbsp;period' ), n.rgroup = rep ( 5 , 3 ), tfoot = txtMergeLines ( '&Delta;<sub>int</sub> correspnds to the change since start' , '&Delta;<sub>std</sub> corresponds to the change compared to national average' ), cspan.rgroup = 1 ) Average age in Sweden counties over a period of 15 years. The Norbotten county is typically known for having a negative migration pattern compared to Stockholm, while Uppsala has a proportionally large population of students. Sweden Norrbotten county Stockholm county Uppsala county Men Women Men Women Men Women Men Women Year Age \u0394 int Age \u0394 int Age \u0394 int \u0394 std Age \u0394 int \u0394 std Age \u0394 int \u0394 std Age \u0394 int \u0394 std Age \u0394 int \u0394 std Age \u0394 int \u0394 std 1st period 1999 38.9 0.0 41.5 0.0 39.7 0.0 0.8 41.9 0.0 0.4 37.3 0.0 -1.6 40.1 0.0 -1.4 37.2 0.0 -1.7 39.3 0.0 -2.2 2000 39.0 0.1 41.6 0.1 40.0 0.3 1.0 42.2 0.3 0.6 37.4 0.1 -1.6 40.1 0.0 -1.5 37.5 0.3 -1.5 39.4 0.1 -2.2 2001 39.2 0.3 41.7 0.2 40.2 0.5 1.0 42.5 0.6 0.8 37.5 0.2 -1.7 40.1 0.0 -1.6 37.6 0.4 -1.6 39.6 0.3 -2.1 2002 39.3 0.4 41.8 0.3 40.5 0.8 1.2 42.8 0.9 1.0 37.6 0.3 -1.7 40.2 0.1 -1.6 37.8 0.6 -1.5 39.7 0.4 -2.1 2003 39.4 0.5 41.9 0.4 40.7 1.0 1.3 43.0 1.1 1.1 37.7 0.4 -1.7 40.2 0.1 -1.7 38.0 0.8 -1.4 39.8 0.5 -2.1 2nd period 2004 39.6 0.7 42.0 0.5 40.9 1.2 1.3 43.1 1.2 1.1 37.8 0.5 -1.8 40.3 0.2 -1.7 38.1 0.9 -1.5 40.0 0.7 -2.0 2005 39.7 0.8 42.0 0.5 41.1 1.4 1.4 43.4 1.5 1.4 37.9 0.6 -1.8 40.3 0.2 -1.7 38.3 1.1 -1.4 40.1 0.8 -1.9 2006 39.8 0.9 42.1 0.6 41.3 1.6 1.5 43.5 1.6 1.4 37.9 0.6 -1.9 40.2 0.1 -1.9 38.5 1.3 -1.3 40.4 1.1 -1.7 2007 39.8 0.9 42.1 0.6 41.5 1.8 1.7 43.8 1.9 1.7 37.8 0.5 -2.0 40.1 0.0 -2.0 38.6 1.4 -1.2 40.5 1.2 -1.6 2008 39.9 1.0 42.1 0.6 41.7 2.0 1.8 44.0 2.1 1.9 37.8 0.5 -2.1 40.1 0.0 -2.0 38.7 1.5 -1.2 40.5 1.2 -1.6 3rd period 2009 39.9 1.0 42.1 0.6 41.9 2.2 2.0 44.2 2.3 2.1 37.8 0.5 -2.1 40.0 -0.1 -2.1 38.8 1.6 -1.1 40.6 1.3 -1.5 2010 40.0 1.1 42.1 0.6 42.1 2.4 2.1 44.4 2.5 2.3 37.8 0.5 -2.2 40.0 -0.1 -2.1 38.9 1.7 -1.1 40.6 1.3 -1.5 2011 40.1 1.2 42.2 0.7 42.3 2.6 2.2 44.5 2.6 2.3 37.9 0.6 -2.2 39.9 -0.2 -2.3 39.0 1.8 -1.1 40.7 1.4 -1.5 2012 40.2 1.3 42.2 0.7 42.4 2.7 2.2 44.6 2.7 2.4 37.9 0.6 -2.3 39.9 -0.2 -2.3 39.1 1.9 -1.1 40.8 1.5 -1.4 2013 40.2 1.3 42.2 0.7 42.4 2.7 2.2 44.7 2.8 2.5 38.0 0.7 -2.2 39.9 -0.2 -2.3 39.2 2.0 -1.0 40.9 1.6 -1.3 \u0394 int correspnds to the change since start \u0394 std corresponds to the change compared to national average Example 7 \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 library ( htmlTable ) # given the data in the first row htmlTable ( out_mx , caption = 'Average age in Sweden counties over a period of 15 years. The Norbotten county is typically known for having a negative migration pattern compared to Stockholm, while Uppsala has a proportionally large population of students.' , pos.rowlabel = 'bottom' , rowlabel = 'Year' , col.rgroup = c ( 'none' , '#FFFFCC' ), col.columns = c ( rep ( '#EFEFF0' , 4 ), rep ( 'none' , ncol ( mx ) - 4 )), align = 'rrrr|r' , cgroup = cgroup , n.cgroup = n.cgroup , rgroup = c ( '1st&nbsp;period' , '2nd&nbsp;period' , '3rd&nbsp;period' ), n.rgroup = rep ( 5 , 3 ), tfoot = txtMergeLines ( '&Delta;<sub>int</sub> correspnds to the change since start' , '&Delta;<sub>std</sub> corresponds to the change compared to national average' ), cspan.rgroup = 1 ) Average age in Sweden counties over a period of 15 years. The Norbotten county is typically known for having a negative migration pattern compared to Stockholm, while Uppsala has a proportionally large population of students. Sweden Norrbotten county Stockholm county Uppsala county Men Women Men Women Men Women Men Women Year Age \u0394 int Age \u0394 int Age \u0394 int \u0394 std Age \u0394 int \u0394 std Age \u0394 int \u0394 std Age \u0394 int \u0394 std Age \u0394 int \u0394 std Age \u0394 int \u0394 std 1st period 1999 38.9 0.0 41.5 0.0 39.7 0.0 0.8 41.9 0.0 0.4 37.3 0.0 -1.6 40.1 0.0 -1.4 37.2 0.0 -1.7 39.3 0.0 -2.2 2000 39.0 0.1 41.6 0.1 40.0 0.3 1.0 42.2 0.3 0.6 37.4 0.1 -1.6 40.1 0.0 -1.5 37.5 0.3 -1.5 39.4 0.1 -2.2 2001 39.2 0.3 41.7 0.2 40.2 0.5 1.0 42.5 0.6 0.8 37.5 0.2 -1.7 40.1 0.0 -1.6 37.6 0.4 -1.6 39.6 0.3 -2.1 2002 39.3 0.4 41.8 0.3 40.5 0.8 1.2 42.8 0.9 1.0 37.6 0.3 -1.7 40.2 0.1 -1.6 37.8 0.6 -1.5 39.7 0.4 -2.1 2003 39.4 0.5 41.9 0.4 40.7 1.0 1.3 43.0 1.1 1.1 37.7 0.4 -1.7 40.2 0.1 -1.7 38.0 0.8 -1.4 39.8 0.5 -2.1 2nd period 2004 39.6 0.7 42.0 0.5 40.9 1.2 1.3 43.1 1.2 1.1 37.8 0.5 -1.8 40.3 0.2 -1.7 38.1 0.9 -1.5 40.0 0.7 -2.0 2005 39.7 0.8 42.0 0.5 41.1 1.4 1.4 43.4 1.5 1.4 37.9 0.6 -1.8 40.3 0.2 -1.7 38.3 1.1 -1.4 40.1 0.8 -1.9 2006 39.8 0.9 42.1 0.6 41.3 1.6 1.5 43.5 1.6 1.4 37.9 0.6 -1.9 40.2 0.1 -1.9 38.5 1.3 -1.3 40.4 1.1 -1.7 2007 39.8 0.9 42.1 0.6 41.5 1.8 1.7 43.8 1.9 1.7 37.8 0.5 -2.0 40.1 0.0 -2.0 38.6 1.4 -1.2 40.5 1.2 -1.6 2008 39.9 1.0 42.1 0.6 41.7 2.0 1.8 44.0 2.1 1.9 37.8 0.5 -2.1 40.1 0.0 -2.0 38.7 1.5 -1.2 40.5 1.2 -1.6 3rd period 2009 39.9 1.0 42.1 0.6 41.9 2.2 2.0 44.2 2.3 2.1 37.8 0.5 -2.1 40.0 -0.1 -2.1 38.8 1.6 -1.1 40.6 1.3 -1.5 2010 40.0 1.1 42.1 0.6 42.1 2.4 2.1 44.4 2.5 2.3 37.8 0.5 -2.2 40.0 -0.1 -2.1 38.9 1.7 -1.1 40.6 1.3 -1.5 2011 40.1 1.2 42.2 0.7 42.3 2.6 2.2 44.5 2.6 2.3 37.9 0.6 -2.2 39.9 -0.2 -2.3 39.0 1.8 -1.1 40.7 1.4 -1.5 2012 40.2 1.3 42.2 0.7 42.4 2.7 2.2 44.6 2.7 2.4 37.9 0.6 -2.3 39.9 -0.2 -2.3 39.1 1.9 -1.1 40.8 1.5 -1.4 2013 40.2 1.3 42.2 0.7 42.4 2.7 2.2 44.7 2.8 2.5 38.0 0.7 -2.2 39.9 -0.2 -2.3 39.2 2.0 -1.0 40.9 1.6 -1.3 \u0394 int correspnds to the change since start \u0394 std corresponds to the change compared to national average The ztable package \u00b6 The package can also export to \\LaTeX \\LaTeX . Example 1 \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 library ( ztable ) options ( ztable.type = 'html' ) # given the data in the first row zt <- ztable ( out_mx , caption = 'Average age in Sweden counties over a period of 15 years. The Norbotten county is typically known for having a negative migration pattern compared to Stockholm, while Uppsala has a proportionally large population of students.' , zebra.type = 1 , zebra = 'peach' , align = paste ( rep ( 'r' , ncol ( out_mx ) + 1 ), collapse = '' )) # zt <- addcgroup(zt, # cgroup = cgroup, # n.cgroup = n.cgroup) # Causes an error: # Error in if (result <= length(vlines)) { : zt <- addrgroup ( zt , rgroup = c ( '1st&nbsp;period' , '2nd&nbsp;period' , '3rd&nbsp;period' ), n.rgroup = rep ( 5 , 3 )) print ( zt )","title":"Tables"},{"location":"tables/#markdown-tables","text":"","title":"Markdown tables"},{"location":"tables/#example-1","text":"1 2 3 4 5 6 7 | Right | Left | Default | Center | | ------: |:- ---- | --------- |:- -----: | | 12 | 12 | 12 | 12 | | 123 | 123 | 123 | 123 | | 1 | 1 | 1 | 1 | : Demonstration of pipe table syntax . Demonstration of pipe table syntax. Right Left Default Center 12 12 12 12 123 123 123 123 1 1 1 1","title":"Example 1"},{"location":"tables/#example-2","text":"1 2 3 4 5 6 7 8 9 10 11 : Sample grid table. +---------------+---------------+--------------------+ | Fruit | Price | Advantages | +===============+===============+====================+ | Bananas | $1.34 | - built-in wrapper | | | | - bright color | +---------------+---------------+--------------------+ | Oranges | $2.10 | - cures scurvy | | | | - tasty | +---------------+---------------+--------------------+ Sample grid table. Fruit Price Advantages Bananas $1.34 built-in wrapper bright color Oranges $2.10 cures scurvy tasty","title":"Example 2"},{"location":"tables/#the-xtable-package","text":"The output is in HTML.","title":"The xtable package"},{"location":"tables/#example-1_1","text":"1 2 3 4 5 6 7 library ( xtable ) # given the data in the first row print ( xtable ( output , caption = 'A test table' , align = c ( 'l' , 'c' , 'r' )), type = 'html' ) A test table 1st header 2nd header 1st row Content A Content B 2nd row Content C Content D","title":"Example 1"},{"location":"tables/#the-knitrkable-function","text":"The output is in Markdown.","title":"The knitr::kable function"},{"location":"tables/#example-1_2","text":"1 2 3 4 5 6 library ( knitr ) # given the data in the first row kable ( output , caption = 'A test table' , align = c ( 'c' , 'r' )) A test table 1st header 2nd header 1st row Content A Content B 2nd row Content C Content D We can also write knitr::kable() without calling library(knitr) .","title":"Example 1"},{"location":"tables/#the-htmltable-package","text":"htmlTable on GitHub.","title":"The htmlTable package"},{"location":"tables/#example-1_3","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 output <- matrix ( paste ( 'Content' , LETTERS [1 : 16 ] ), ncol = 4 , byrow = TRUE ) library ( htmlTable ) htmlTable ( output , header = paste ( c ( '1st' , '2nd' , '3rd' , '4th' ), 'header' ), rnames = paste ( c ( '1st' , '2nd' , '3rd' , '4th' ), 'row' ), rgroup = c ( 'Group A' , 'Group B' ), n.rgroup = c ( 2 , 2 ), cgroup = c ( 'Cgroup 1' , 'Cgroup 2&dagger;' ), n.cgroup = c ( 2 , 2 ), caption = 'Basic table with both column spanners (groups) and row groups' , tfoot = '&dagger; A table footer commment' ) Basic table with both column spanners (groups) and row groups Cgroup 1 Cgroup 2\u2020 1st header 2nd header 3rd header 4th header Group A 1st row Content A Content B Content C Content D 2nd row Content E Content F Content G Content H Group B 3rd row Content I Content J Content K Content L 4th row Content M Content N Content O Content P \u2020 A table footer commment","title":"Example 1"},{"location":"tables/#example-2_1","text":"1 2 3 4 5 6 7 8 9 10 library ( htmlTable ) # given the data in the first row htmlTable ( txtRound ( mx , 1 ), cgroup = cgroup , n.cgroup = n.cgroup , rgroup = c ( 'First period' , 'Second period' , 'Third period' ), n.rgroup = rep ( 5 , 3 ), tfoot = txtMergeLines ( '&Delta;<sub>int</sub> correspnds to the change since start' , '&Delta;<sub>std</sub> corresponds to the change compared to national average' )) Sweden Norrbotten county Stockholm county Uppsala county Men Women Men Women Men Women Men Women Age \u0394 int Age \u0394 int Age \u0394 int \u0394 std Age \u0394 int \u0394 std Age \u0394 int \u0394 std Age \u0394 int \u0394 std Age \u0394 int \u0394 std Age \u0394 int \u0394 std First period 1999 38.9 0.0 41.5 0.0 39.7 0.0 0.8 41.9 0.0 0.4 37.3 0.0 -1.6 40.1 0.0 -1.4 37.2 0.0 -1.7 39.3 0.0 -2.2 2000 39.0 0.1 41.6 0.1 40.0 0.3 1.0 42.2 0.3 0.6 37.4 0.1 -1.6 40.1 0.0 -1.5 37.5 0.3 -1.5 39.4 0.1 -2.2 2001 39.2 0.3 41.7 0.2 40.2 0.5 1.0 42.5 0.6 0.8 37.5 0.2 -1.7 40.1 0.0 -1.6 37.6 0.4 -1.6 39.6 0.3 -2.1 2002 39.3 0.4 41.8 0.3 40.5 0.8 1.2 42.8 0.9 1.0 37.6 0.3 -1.7 40.2 0.1 -1.6 37.8 0.6 -1.5 39.7 0.4 -2.1 2003 39.4 0.5 41.9 0.4 40.7 1.0 1.3 43.0 1.1 1.1 37.7 0.4 -1.7 40.2 0.1 -1.7 38.0 0.8 -1.4 39.8 0.5 -2.1 Second period 2004 39.6 0.7 42.0 0.5 40.9 1.2 1.3 43.1 1.2 1.1 37.8 0.5 -1.8 40.3 0.2 -1.7 38.1 0.9 -1.5 40.0 0.7 -2.0 2005 39.7 0.8 42.0 0.5 41.1 1.4 1.4 43.4 1.5 1.4 37.9 0.6 -1.8 40.3 0.2 -1.7 38.3 1.1 -1.4 40.1 0.8 -1.9 2006 39.8 0.9 42.1 0.6 41.3 1.6 1.5 43.5 1.6 1.4 37.9 0.6 -1.9 40.2 0.1 -1.9 38.5 1.3 -1.3 40.4 1.1 -1.7 2007 39.8 0.9 42.1 0.6 41.5 1.8 1.7 43.8 1.9 1.7 37.8 0.5 -2.0 40.1 0.0 -2.0 38.6 1.4 -1.2 40.5 1.2 -1.6 2008 39.9 1.0 42.1 0.6 41.7 2.0 1.8 44.0 2.1 1.9 37.8 0.5 -2.1 40.1 0.0 -2.0 38.7 1.5 -1.2 40.5 1.2 -1.6 Third period 2009 39.9 1.0 42.1 0.6 41.9 2.2 2.0 44.2 2.3 2.1 37.8 0.5 -2.1 40.0 -0.1 -2.1 38.8 1.6 -1.1 40.6 1.3 -1.5 2010 40.0 1.1 42.1 0.6 42.1 2.4 2.1 44.4 2.5 2.3 37.8 0.5 -2.2 40.0 -0.1 -2.1 38.9 1.7 -1.1 40.6 1.3 -1.5 2011 40.1 1.2 42.2 0.7 42.3 2.6 2.2 44.5 2.6 2.3 37.9 0.6 -2.2 39.9 -0.2 -2.3 39.0 1.8 -1.1 40.7 1.4 -1.5 2012 40.2 1.3 42.2 0.7 42.4 2.7 2.2 44.6 2.7 2.4 37.9 0.6 -2.3 39.9 -0.2 -2.3 39.1 1.9 -1.1 40.8 1.5 -1.4 2013 40.2 1.3 42.2 0.7 42.4 2.7 2.2 44.7 2.8 2.5 38.0 0.7 -2.2 39.9 -0.2 -2.3 39.2 2.0 -1.0 40.9 1.6 -1.3 \u0394 int correspnds to the change since start \u0394 std corresponds to the change compared to national average","title":"Example 2"},{"location":"tables/#example-3","text":"1 2 3 4 5 6 7 8 9 10 11 library ( htmlTable ) # given the data in the first row htmlTable ( txtRound ( mx , 1 ), align = 'rrrr|r' , cgroup = cgroup , n.cgroup = n.cgroup , rgroup = c ( 'First period' , 'Second period' , 'Third period' ), n.rgroup = rep ( 5 , 3 ), tfoot = txtMergeLines ( '&Delta;<sub>int</sub> correspnds to the change since start' , '&Delta;<sub>std</sub> corresponds to the change compared to national average' )) Sweden Norrbotten county Stockholm county Uppsala county Men Women Men Women Men Women Men Women Age \u0394 int Age \u0394 int Age \u0394 int \u0394 std Age \u0394 int \u0394 std Age \u0394 int \u0394 std Age \u0394 int \u0394 std Age \u0394 int \u0394 std Age \u0394 int \u0394 std First period 1999 38.9 0.0 41.5 0.0 39.7 0.0 0.8 41.9 0.0 0.4 37.3 0.0 -1.6 40.1 0.0 -1.4 37.2 0.0 -1.7 39.3 0.0 -2.2 2000 39.0 0.1 41.6 0.1 40.0 0.3 1.0 42.2 0.3 0.6 37.4 0.1 -1.6 40.1 0.0 -1.5 37.5 0.3 -1.5 39.4 0.1 -2.2 2001 39.2 0.3 41.7 0.2 40.2 0.5 1.0 42.5 0.6 0.8 37.5 0.2 -1.7 40.1 0.0 -1.6 37.6 0.4 -1.6 39.6 0.3 -2.1 2002 39.3 0.4 41.8 0.3 40.5 0.8 1.2 42.8 0.9 1.0 37.6 0.3 -1.7 40.2 0.1 -1.6 37.8 0.6 -1.5 39.7 0.4 -2.1 2003 39.4 0.5 41.9 0.4 40.7 1.0 1.3 43.0 1.1 1.1 37.7 0.4 -1.7 40.2 0.1 -1.7 38.0 0.8 -1.4 39.8 0.5 -2.1 Second period 2004 39.6 0.7 42.0 0.5 40.9 1.2 1.3 43.1 1.2 1.1 37.8 0.5 -1.8 40.3 0.2 -1.7 38.1 0.9 -1.5 40.0 0.7 -2.0 2005 39.7 0.8 42.0 0.5 41.1 1.4 1.4 43.4 1.5 1.4 37.9 0.6 -1.8 40.3 0.2 -1.7 38.3 1.1 -1.4 40.1 0.8 -1.9 2006 39.8 0.9 42.1 0.6 41.3 1.6 1.5 43.5 1.6 1.4 37.9 0.6 -1.9 40.2 0.1 -1.9 38.5 1.3 -1.3 40.4 1.1 -1.7 2007 39.8 0.9 42.1 0.6 41.5 1.8 1.7 43.8 1.9 1.7 37.8 0.5 -2.0 40.1 0.0 -2.0 38.6 1.4 -1.2 40.5 1.2 -1.6 2008 39.9 1.0 42.1 0.6 41.7 2.0 1.8 44.0 2.1 1.9 37.8 0.5 -2.1 40.1 0.0 -2.0 38.7 1.5 -1.2 40.5 1.2 -1.6 Third period 2009 39.9 1.0 42.1 0.6 41.9 2.2 2.0 44.2 2.3 2.1 37.8 0.5 -2.1 40.0 -0.1 -2.1 38.8 1.6 -1.1 40.6 1.3 -1.5 2010 40.0 1.1 42.1 0.6 42.1 2.4 2.1 44.4 2.5 2.3 37.8 0.5 -2.2 40.0 -0.1 -2.1 38.9 1.7 -1.1 40.6 1.3 -1.5 2011 40.1 1.2 42.2 0.7 42.3 2.6 2.2 44.5 2.6 2.3 37.9 0.6 -2.2 39.9 -0.2 -2.3 39.0 1.8 -1.1 40.7 1.4 -1.5 2012 40.2 1.3 42.2 0.7 42.4 2.7 2.2 44.6 2.7 2.4 37.9 0.6 -2.3 39.9 -0.2 -2.3 39.1 1.9 -1.1 40.8 1.5 -1.4 2013 40.2 1.3 42.2 0.7 42.4 2.7 2.2 44.7 2.8 2.5 38.0 0.7 -2.2 39.9 -0.2 -2.3 39.2 2.0 -1.0 40.9 1.6 -1.3 \u0394 int correspnds to the change since start \u0394 std corresponds to the change compared to national average","title":"Example 3"},{"location":"tables/#example-4","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 library ( htmlTable ) # given the data in the first row htmlTable ( txtRound ( mx , 1 ), col.columns = c ( rep ( '#E6E6F0' , 4 ), rep ( 'none' , ncol ( mx ) - 4 )), align = 'rrrr|r' , cgroup = cgroup , n.cgroup = n.cgroup , rgroup = c ( 'First period' , 'Second period' , 'Third period' ), n.rgroup = rep ( 5 , 3 ), tfoot = txtMergeLines ( '&Delta;<sub>int</sub> correspnds to the change since start' , '&Delta;<sub>std</sub> corresponds to the change compared to national average' )) Sweden Norrbotten county Stockholm county Uppsala county Men Women Men Women Men Women Men Women Age \u0394 int Age \u0394 int Age \u0394 int \u0394 std Age \u0394 int \u0394 std Age \u0394 int \u0394 std Age \u0394 int \u0394 std Age \u0394 int \u0394 std Age \u0394 int \u0394 std First period 1999 38.9 0.0 41.5 0.0 39.7 0.0 0.8 41.9 0.0 0.4 37.3 0.0 -1.6 40.1 0.0 -1.4 37.2 0.0 -1.7 39.3 0.0 -2.2 2000 39.0 0.1 41.6 0.1 40.0 0.3 1.0 42.2 0.3 0.6 37.4 0.1 -1.6 40.1 0.0 -1.5 37.5 0.3 -1.5 39.4 0.1 -2.2 2001 39.2 0.3 41.7 0.2 40.2 0.5 1.0 42.5 0.6 0.8 37.5 0.2 -1.7 40.1 0.0 -1.6 37.6 0.4 -1.6 39.6 0.3 -2.1 2002 39.3 0.4 41.8 0.3 40.5 0.8 1.2 42.8 0.9 1.0 37.6 0.3 -1.7 40.2 0.1 -1.6 37.8 0.6 -1.5 39.7 0.4 -2.1 2003 39.4 0.5 41.9 0.4 40.7 1.0 1.3 43.0 1.1 1.1 37.7 0.4 -1.7 40.2 0.1 -1.7 38.0 0.8 -1.4 39.8 0.5 -2.1 Second period 2004 39.6 0.7 42.0 0.5 40.9 1.2 1.3 43.1 1.2 1.1 37.8 0.5 -1.8 40.3 0.2 -1.7 38.1 0.9 -1.5 40.0 0.7 -2.0 2005 39.7 0.8 42.0 0.5 41.1 1.4 1.4 43.4 1.5 1.4 37.9 0.6 -1.8 40.3 0.2 -1.7 38.3 1.1 -1.4 40.1 0.8 -1.9 2006 39.8 0.9 42.1 0.6 41.3 1.6 1.5 43.5 1.6 1.4 37.9 0.6 -1.9 40.2 0.1 -1.9 38.5 1.3 -1.3 40.4 1.1 -1.7 2007 39.8 0.9 42.1 0.6 41.5 1.8 1.7 43.8 1.9 1.7 37.8 0.5 -2.0 40.1 0.0 -2.0 38.6 1.4 -1.2 40.5 1.2 -1.6 2008 39.9 1.0 42.1 0.6 41.7 2.0 1.8 44.0 2.1 1.9 37.8 0.5 -2.1 40.1 0.0 -2.0 38.7 1.5 -1.2 40.5 1.2 -1.6 Third period 2009 39.9 1.0 42.1 0.6 41.9 2.2 2.0 44.2 2.3 2.1 37.8 0.5 -2.1 40.0 -0.1 -2.1 38.8 1.6 -1.1 40.6 1.3 -1.5 2010 40.0 1.1 42.1 0.6 42.1 2.4 2.1 44.4 2.5 2.3 37.8 0.5 -2.2 40.0 -0.1 -2.1 38.9 1.7 -1.1 40.6 1.3 -1.5 2011 40.1 1.2 42.2 0.7 42.3 2.6 2.2 44.5 2.6 2.3 37.9 0.6 -2.2 39.9 -0.2 -2.3 39.0 1.8 -1.1 40.7 1.4 -1.5 2012 40.2 1.3 42.2 0.7 42.4 2.7 2.2 44.6 2.7 2.4 37.9 0.6 -2.3 39.9 -0.2 -2.3 39.1 1.9 -1.1 40.8 1.5 -1.4 2013 40.2 1.3 42.2 0.7 42.4 2.7 2.2 44.7 2.8 2.5 38.0 0.7 -2.2 39.9 -0.2 -2.3 39.2 2.0 -1.0 40.9 1.6 -1.3 \u0394 int correspnds to the change since start \u0394 std corresponds to the change compared to national average","title":"Example 4"},{"location":"tables/#example-5","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 library ( htmlTable ) # given the data in the first row htmlTable ( txtRound ( mx , 1 ), col.rgroup = c ( 'none' , '#FFFFCC' ), col.columns = c ( rep ( '#EFEFF0' , 4 ), rep ( 'none' , ncol ( mx ) - 4 )), align = 'rrrr|r' , cgroup = cgroup , n.cgroup = n.cgroup , # I use the &nbsp; - the no breaking space as I don't want to have a # row break in the row group. This adds a little space in the table # when used together with the cspan.rgroup=1. rgroup = c ( '1st&nbsp;period' , '2nd&nbsp;period' , '3rd&nbsp;period' ), n.rgroup = rep ( 5 , 3 ), tfoot = txtMergeLines ( '&Delta;<sub>int</sub> correspnds to the change since start' , '&Delta;<sub>std</sub> corresponds to the change compared to national average' ), cspan.rgroup = 1 ) Sweden Norrbotten county Stockholm county Uppsala county Men Women Men Women Men Women Men Women Age \u0394 int Age \u0394 int Age \u0394 int \u0394 std Age \u0394 int \u0394 std Age \u0394 int \u0394 std Age \u0394 int \u0394 std Age \u0394 int \u0394 std Age \u0394 int \u0394 std 1st period 1999 38.9 0.0 41.5 0.0 39.7 0.0 0.8 41.9 0.0 0.4 37.3 0.0 -1.6 40.1 0.0 -1.4 37.2 0.0 -1.7 39.3 0.0 -2.2 2000 39.0 0.1 41.6 0.1 40.0 0.3 1.0 42.2 0.3 0.6 37.4 0.1 -1.6 40.1 0.0 -1.5 37.5 0.3 -1.5 39.4 0.1 -2.2 2001 39.2 0.3 41.7 0.2 40.2 0.5 1.0 42.5 0.6 0.8 37.5 0.2 -1.7 40.1 0.0 -1.6 37.6 0.4 -1.6 39.6 0.3 -2.1 2002 39.3 0.4 41.8 0.3 40.5 0.8 1.2 42.8 0.9 1.0 37.6 0.3 -1.7 40.2 0.1 -1.6 37.8 0.6 -1.5 39.7 0.4 -2.1 2003 39.4 0.5 41.9 0.4 40.7 1.0 1.3 43.0 1.1 1.1 37.7 0.4 -1.7 40.2 0.1 -1.7 38.0 0.8 -1.4 39.8 0.5 -2.1 2nd period 2004 39.6 0.7 42.0 0.5 40.9 1.2 1.3 43.1 1.2 1.1 37.8 0.5 -1.8 40.3 0.2 -1.7 38.1 0.9 -1.5 40.0 0.7 -2.0 2005 39.7 0.8 42.0 0.5 41.1 1.4 1.4 43.4 1.5 1.4 37.9 0.6 -1.8 40.3 0.2 -1.7 38.3 1.1 -1.4 40.1 0.8 -1.9 2006 39.8 0.9 42.1 0.6 41.3 1.6 1.5 43.5 1.6 1.4 37.9 0.6 -1.9 40.2 0.1 -1.9 38.5 1.3 -1.3 40.4 1.1 -1.7 2007 39.8 0.9 42.1 0.6 41.5 1.8 1.7 43.8 1.9 1.7 37.8 0.5 -2.0 40.1 0.0 -2.0 38.6 1.4 -1.2 40.5 1.2 -1.6 2008 39.9 1.0 42.1 0.6 41.7 2.0 1.8 44.0 2.1 1.9 37.8 0.5 -2.1 40.1 0.0 -2.0 38.7 1.5 -1.2 40.5 1.2 -1.6 3rd period 2009 39.9 1.0 42.1 0.6 41.9 2.2 2.0 44.2 2.3 2.1 37.8 0.5 -2.1 40.0 -0.1 -2.1 38.8 1.6 -1.1 40.6 1.3 -1.5 2010 40.0 1.1 42.1 0.6 42.1 2.4 2.1 44.4 2.5 2.3 37.8 0.5 -2.2 40.0 -0.1 -2.1 38.9 1.7 -1.1 40.6 1.3 -1.5 2011 40.1 1.2 42.2 0.7 42.3 2.6 2.2 44.5 2.6 2.3 37.9 0.6 -2.2 39.9 -0.2 -2.3 39.0 1.8 -1.1 40.7 1.4 -1.5 2012 40.2 1.3 42.2 0.7 42.4 2.7 2.2 44.6 2.7 2.4 37.9 0.6 -2.3 39.9 -0.2 -2.3 39.1 1.9 -1.1 40.8 1.5 -1.4 2013 40.2 1.3 42.2 0.7 42.4 2.7 2.2 44.7 2.8 2.5 38.0 0.7 -2.2 39.9 -0.2 -2.3 39.2 2.0 -1.0 40.9 1.6 -1.3 \u0394 int correspnds to the change since start \u0394 std corresponds to the change compared to national average","title":"Example 5"},{"location":"tables/#example-6","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 library ( htmlTable ) # given the data in the first row htmlTable ( out_mx , caption = 'Average age in Sweden counties over a period of 15 years. The Norbotten county is typically known for having a negative migration pattern compared to Stockholm, while Uppsala has a proportionally large population of students.' , pos.rowlabel = 'bottom' , rowlabel = 'Year' , col.rgroup = c ( 'none' , '#FFFFCC' ), col.columns = c ( rep ( '#EFEFF0' , 4 ), rep ( 'none' , ncol ( mx ) - 4 )), align = 'rrrr|r' , cgroup = cgroup , n.cgroup = n.cgroup , rgroup = c ( '1st&nbsp;period' , '2nd&nbsp;period' , '3rd&nbsp;period' ), n.rgroup = rep ( 5 , 3 ), tfoot = txtMergeLines ( '&Delta;<sub>int</sub> correspnds to the change since start' , '&Delta;<sub>std</sub> corresponds to the change compared to national average' ), cspan.rgroup = 1 ) Average age in Sweden counties over a period of 15 years. The Norbotten county is typically known for having a negative migration pattern compared to Stockholm, while Uppsala has a proportionally large population of students. Sweden Norrbotten county Stockholm county Uppsala county Men Women Men Women Men Women Men Women Year Age \u0394 int Age \u0394 int Age \u0394 int \u0394 std Age \u0394 int \u0394 std Age \u0394 int \u0394 std Age \u0394 int \u0394 std Age \u0394 int \u0394 std Age \u0394 int \u0394 std 1st period 1999 38.9 0.0 41.5 0.0 39.7 0.0 0.8 41.9 0.0 0.4 37.3 0.0 -1.6 40.1 0.0 -1.4 37.2 0.0 -1.7 39.3 0.0 -2.2 2000 39.0 0.1 41.6 0.1 40.0 0.3 1.0 42.2 0.3 0.6 37.4 0.1 -1.6 40.1 0.0 -1.5 37.5 0.3 -1.5 39.4 0.1 -2.2 2001 39.2 0.3 41.7 0.2 40.2 0.5 1.0 42.5 0.6 0.8 37.5 0.2 -1.7 40.1 0.0 -1.6 37.6 0.4 -1.6 39.6 0.3 -2.1 2002 39.3 0.4 41.8 0.3 40.5 0.8 1.2 42.8 0.9 1.0 37.6 0.3 -1.7 40.2 0.1 -1.6 37.8 0.6 -1.5 39.7 0.4 -2.1 2003 39.4 0.5 41.9 0.4 40.7 1.0 1.3 43.0 1.1 1.1 37.7 0.4 -1.7 40.2 0.1 -1.7 38.0 0.8 -1.4 39.8 0.5 -2.1 2nd period 2004 39.6 0.7 42.0 0.5 40.9 1.2 1.3 43.1 1.2 1.1 37.8 0.5 -1.8 40.3 0.2 -1.7 38.1 0.9 -1.5 40.0 0.7 -2.0 2005 39.7 0.8 42.0 0.5 41.1 1.4 1.4 43.4 1.5 1.4 37.9 0.6 -1.8 40.3 0.2 -1.7 38.3 1.1 -1.4 40.1 0.8 -1.9 2006 39.8 0.9 42.1 0.6 41.3 1.6 1.5 43.5 1.6 1.4 37.9 0.6 -1.9 40.2 0.1 -1.9 38.5 1.3 -1.3 40.4 1.1 -1.7 2007 39.8 0.9 42.1 0.6 41.5 1.8 1.7 43.8 1.9 1.7 37.8 0.5 -2.0 40.1 0.0 -2.0 38.6 1.4 -1.2 40.5 1.2 -1.6 2008 39.9 1.0 42.1 0.6 41.7 2.0 1.8 44.0 2.1 1.9 37.8 0.5 -2.1 40.1 0.0 -2.0 38.7 1.5 -1.2 40.5 1.2 -1.6 3rd period 2009 39.9 1.0 42.1 0.6 41.9 2.2 2.0 44.2 2.3 2.1 37.8 0.5 -2.1 40.0 -0.1 -2.1 38.8 1.6 -1.1 40.6 1.3 -1.5 2010 40.0 1.1 42.1 0.6 42.1 2.4 2.1 44.4 2.5 2.3 37.8 0.5 -2.2 40.0 -0.1 -2.1 38.9 1.7 -1.1 40.6 1.3 -1.5 2011 40.1 1.2 42.2 0.7 42.3 2.6 2.2 44.5 2.6 2.3 37.9 0.6 -2.2 39.9 -0.2 -2.3 39.0 1.8 -1.1 40.7 1.4 -1.5 2012 40.2 1.3 42.2 0.7 42.4 2.7 2.2 44.6 2.7 2.4 37.9 0.6 -2.3 39.9 -0.2 -2.3 39.1 1.9 -1.1 40.8 1.5 -1.4 2013 40.2 1.3 42.2 0.7 42.4 2.7 2.2 44.7 2.8 2.5 38.0 0.7 -2.2 39.9 -0.2 -2.3 39.2 2.0 -1.0 40.9 1.6 -1.3 \u0394 int correspnds to the change since start \u0394 std corresponds to the change compared to national average","title":"Example 6"},{"location":"tables/#example-7","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 library ( htmlTable ) # given the data in the first row htmlTable ( out_mx , caption = 'Average age in Sweden counties over a period of 15 years. The Norbotten county is typically known for having a negative migration pattern compared to Stockholm, while Uppsala has a proportionally large population of students.' , pos.rowlabel = 'bottom' , rowlabel = 'Year' , col.rgroup = c ( 'none' , '#FFFFCC' ), col.columns = c ( rep ( '#EFEFF0' , 4 ), rep ( 'none' , ncol ( mx ) - 4 )), align = 'rrrr|r' , cgroup = cgroup , n.cgroup = n.cgroup , rgroup = c ( '1st&nbsp;period' , '2nd&nbsp;period' , '3rd&nbsp;period' ), n.rgroup = rep ( 5 , 3 ), tfoot = txtMergeLines ( '&Delta;<sub>int</sub> correspnds to the change since start' , '&Delta;<sub>std</sub> corresponds to the change compared to national average' ), cspan.rgroup = 1 ) Average age in Sweden counties over a period of 15 years. The Norbotten county is typically known for having a negative migration pattern compared to Stockholm, while Uppsala has a proportionally large population of students. Sweden Norrbotten county Stockholm county Uppsala county Men Women Men Women Men Women Men Women Year Age \u0394 int Age \u0394 int Age \u0394 int \u0394 std Age \u0394 int \u0394 std Age \u0394 int \u0394 std Age \u0394 int \u0394 std Age \u0394 int \u0394 std Age \u0394 int \u0394 std 1st period 1999 38.9 0.0 41.5 0.0 39.7 0.0 0.8 41.9 0.0 0.4 37.3 0.0 -1.6 40.1 0.0 -1.4 37.2 0.0 -1.7 39.3 0.0 -2.2 2000 39.0 0.1 41.6 0.1 40.0 0.3 1.0 42.2 0.3 0.6 37.4 0.1 -1.6 40.1 0.0 -1.5 37.5 0.3 -1.5 39.4 0.1 -2.2 2001 39.2 0.3 41.7 0.2 40.2 0.5 1.0 42.5 0.6 0.8 37.5 0.2 -1.7 40.1 0.0 -1.6 37.6 0.4 -1.6 39.6 0.3 -2.1 2002 39.3 0.4 41.8 0.3 40.5 0.8 1.2 42.8 0.9 1.0 37.6 0.3 -1.7 40.2 0.1 -1.6 37.8 0.6 -1.5 39.7 0.4 -2.1 2003 39.4 0.5 41.9 0.4 40.7 1.0 1.3 43.0 1.1 1.1 37.7 0.4 -1.7 40.2 0.1 -1.7 38.0 0.8 -1.4 39.8 0.5 -2.1 2nd period 2004 39.6 0.7 42.0 0.5 40.9 1.2 1.3 43.1 1.2 1.1 37.8 0.5 -1.8 40.3 0.2 -1.7 38.1 0.9 -1.5 40.0 0.7 -2.0 2005 39.7 0.8 42.0 0.5 41.1 1.4 1.4 43.4 1.5 1.4 37.9 0.6 -1.8 40.3 0.2 -1.7 38.3 1.1 -1.4 40.1 0.8 -1.9 2006 39.8 0.9 42.1 0.6 41.3 1.6 1.5 43.5 1.6 1.4 37.9 0.6 -1.9 40.2 0.1 -1.9 38.5 1.3 -1.3 40.4 1.1 -1.7 2007 39.8 0.9 42.1 0.6 41.5 1.8 1.7 43.8 1.9 1.7 37.8 0.5 -2.0 40.1 0.0 -2.0 38.6 1.4 -1.2 40.5 1.2 -1.6 2008 39.9 1.0 42.1 0.6 41.7 2.0 1.8 44.0 2.1 1.9 37.8 0.5 -2.1 40.1 0.0 -2.0 38.7 1.5 -1.2 40.5 1.2 -1.6 3rd period 2009 39.9 1.0 42.1 0.6 41.9 2.2 2.0 44.2 2.3 2.1 37.8 0.5 -2.1 40.0 -0.1 -2.1 38.8 1.6 -1.1 40.6 1.3 -1.5 2010 40.0 1.1 42.1 0.6 42.1 2.4 2.1 44.4 2.5 2.3 37.8 0.5 -2.2 40.0 -0.1 -2.1 38.9 1.7 -1.1 40.6 1.3 -1.5 2011 40.1 1.2 42.2 0.7 42.3 2.6 2.2 44.5 2.6 2.3 37.9 0.6 -2.2 39.9 -0.2 -2.3 39.0 1.8 -1.1 40.7 1.4 -1.5 2012 40.2 1.3 42.2 0.7 42.4 2.7 2.2 44.6 2.7 2.4 37.9 0.6 -2.3 39.9 -0.2 -2.3 39.1 1.9 -1.1 40.8 1.5 -1.4 2013 40.2 1.3 42.2 0.7 42.4 2.7 2.2 44.7 2.8 2.5 38.0 0.7 -2.2 39.9 -0.2 -2.3 39.2 2.0 -1.0 40.9 1.6 -1.3 \u0394 int correspnds to the change since start \u0394 std corresponds to the change compared to national average","title":"Example 7"},{"location":"tables/#the-ztable-package","text":"The package can also export to \\LaTeX \\LaTeX .","title":"The ztable package"},{"location":"tables/#example-1_4","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 library ( ztable ) options ( ztable.type = 'html' ) # given the data in the first row zt <- ztable ( out_mx , caption = 'Average age in Sweden counties over a period of 15 years. The Norbotten county is typically known for having a negative migration pattern compared to Stockholm, while Uppsala has a proportionally large population of students.' , zebra.type = 1 , zebra = 'peach' , align = paste ( rep ( 'r' , ncol ( out_mx ) + 1 ), collapse = '' )) # zt <- addcgroup(zt, # cgroup = cgroup, # n.cgroup = n.cgroup) # Causes an error: # Error in if (result <= length(vlines)) { : zt <- addrgroup ( zt , rgroup = c ( '1st&nbsp;period' , '2nd&nbsp;period' , '3rd&nbsp;period' ), n.rgroup = rep ( 5 , 3 )) print ( zt )","title":"Example 1"},{"location":"time_series_in_r_the_power_of_xts_and_zoo/","text":"Foreword Snippets and results. Source: \u2018Time Series in R, The Power of xts and zoo \u2018 from DataCamp fitted into Jupyter/IPython using the IRkernel. Introduction to eXtensible Time \u00b6 What is an xts object xts , a constructor or a subclass that inherits behavior from parents. xts (as a subclass) extends the popular zoo class (as a parent). Most zoo methods work for xts . xts is a matrix objects; subsets always preserve the matrix form. xts are indexed by a formal time object. You can time-stamp the data. The main xts constructor two most important arguments are x for the data and order.by for the index. x must be a vector or matrix. order.by is a vector of the same length or number of rows of x ; it must be a proper time or date object and it must be in increasing order. xts also allows you to bind arbitrary key-value attributes to your data. This lets you keep metadata about your object inside your object. 1 2 3 4 # load zoo library ( zoo ) # Load xts library ( xts ) # could be done alone; it will call zoo A first xts object 1 2 3 4 5 6 7 8 core <- matrix ( c ( rep ( 1 , 3 ), rep ( 2 , 3 )), nrow = 3 , ncol = 2 ) core idx <- as.Date ( c ( \"2016-06-01\" , \"2016-06-02\" , \"2016-06-03\" )) idx ex_matrix <- xts ( core , order.by = idx ) ex_matrix 1 2 1 2 1 2 2016-06-01 2016-06-02 2016-06-03 1 2 3 4 [,1] [,2] 2016-06-01 1 2 2016-06-02 1 2 2016-06-03 1 2 More than a matrix xts objects are normal R matrices, but with special powers. 1 2 # View the structure of ex_matrix str ( ex_matrix ) 1 2 3 4 5 An \u2018xts\u2019 object on 2016-06-01/2016-06-03 containing: Data: num [1:3, 1:2] 1 1 1 2 2 2 Indexed by objects of class: [Date] TZ: UTC xts Attributes: NULL 1 2 3 4 5 # Extract the 3rd observation of the 2nd column of ex_matrix ex_matrix[3 , 2 ] # Extract the 3rd observation of the 2nd column of core core[3 , 2 ] 1 2 [,1] 2016-06-03 2 2 Your first xts object Vector must be of the same length or number of rows as x and, this is very important: it must be a proper time or date object and it must be in increasing order. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 # Create the object data using 5 random numbers data <- rnorm ( 5 ) data # Create dates as a Date class object starting from 2016-01-01 dates <- seq ( as.Date ( \"2016-01-01\" ), length = 5 , by = \"days\" ) dates # Use xts() to create smith smith <- xts ( x = data , order.by = dates ) smith # Create bday (1899-05-08) using a POSIXct date class object bday <- as.POSIXct ( \"1899-05-08\" ) bday # Create hayek and add a new attribute called born hayek <- xts ( x = data , order.by = dates , born = bday ) hayek 0.226117402549514 -0.214261010302668 -0.199347621952038 0.133952199089896 0.96057628145059 2016-01-01 2016-01-02 2016-01-03 2016-01-04 2016-01-05 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 [,1] 2016-01-01 0.2261174 2016-01-02 -0.2142610 2016-01-03 -0.1993476 2016-01-04 0.1339522 2016-01-05 0.9605763 [1] \"1899-05-08 EST\" [,1] 2016-01-01 0.2261174 2016-01-02 -0.2142610 2016-01-03 -0.1993476 2016-01-04 0.1339522 2016-01-05 0.9605763 Deconstructing xts Separate a time series into its core data and index attributes for additional analysis and manipulation. Functions are methods from the zoo class. 1 2 3 4 5 6 7 8 9 10 11 # Extract the core data of hayek hayek_core <- coredata ( hayek ) # View the class of hayek_core class ( hayek_core ) # Extract the index of hayek hayek_index <- index ( hayek ) # View the class of hayek_index class ( hayek_index ) \u2018matrix\u2019 \u2018Date\u2019 Time-based indices xts objects get their power from the index attribute that holds the time dimension. One major difference between xts and most other time series objects in R is the ability to use any one of various classes that are used to represent time. Whether POSIXct , Date , or some other class, xts will convert this into an internal form to make subsetting as natural to the user as possible. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 # Create dates dates <- as.Date ( \"2016-01-01\" ) + 0 : 4 # Create ts_a ts_a <- xts ( x = 1 : 5 , order.by = dates ) # Create ts_b ts_b <- xts ( x = 1 : 5 , order.by = as.POSIXct ( dates )) # Extract the rows of ts_a using the index of ts_b ts_a [index ( ts_a ) ] # Extract the rows of ts_b using the index of ts_a ts_b [index ( ts_a ) ] 1 2 3 4 5 6 7 8 9 10 [,1] 2016-01-01 1 2016-01-02 2 2016-01-03 3 2016-01-04 4 2016-01-05 5 [,1] First Order of Business \u2013 Basic Manipulations \u00b6 Converting xts objects xts provides methods to convert all of the major objects you are likely to come across - suitable native R types like matrix, data.frame, and ts are supported, as well as contributed ones such as timeSeries , fts and of course zoo . as.xts is the workhorse function to do the conversions to xts , and similar functions will provide the reverse behavior. 1 2 # using a R dataset head ( austres ) 13067.3 13130.5 13198.4 13254.2 13303.7 13353.9 1 2 3 4 5 6 7 8 9 10 11 # Convert austres to an xts object called au au <- as.xts ( austres ) head ( au , 5 ) # Convert your xts object (au) into a matrix am am <- as.matrix ( au ) head ( am , 5 ) # Convert the original austres into a matrix am2 am2 <- as.matrix ( austres ) head ( am2 , 5 ) 1 2 3 4 5 6 [,1] 1971 Q2 13067.3 1971 Q3 13130.5 1971 Q4 13198.4 1972 Q1 13254.2 1972 Q2 13303.7 1971 Q2 13067.3 1971 Q3 13130.5 1971 Q4 13198.4 1972 Q1 13254.2 1972 Q2 13303.7 13067.3 13130.5 13198.4 13254.2 13303.7 Importing data Read raw data from files on disk or the web and convert data to xts . Read the same data into a zoo object using read.zoo and convert the zoo into an xts object. 1 2 3 # Create dat by reading tmp_file dat <- as.matrix ( read.csv ( 'tmp_file.csv' , sep = ',' , header = TRUE )) dat a b 1/02/2015 1 3 2/03/2015 2 4 1 2 # Convert dat into xts xts ( dat , order.by = as.Date ( rownames ( dat ), \"%m%d%Y\" )) 1 2 3 a b &lt;NA&gt; 1 3 &lt;NA&gt; 2 4 1 2 3 # Read tmp_file using read.zoo and as.xts dat_xts <- as.xts ( read.zoo ( 'tmp_file.csv' , index.column = 0 , sep = \",\" , format = \"%m/%d/%Y\" )) dat_xts 1 2 3 a b 2015-01-02 1 3 2015-02-03 2 4 Exporting xts objects 1 2 3 4 head ( sunspots , 5 ) # Convert sunspots to xts sunspots_xts <- as.xts ( sunspots ) 58 62.6 70 55.7 85 1 2 3 4 5 6 7 # Get the temporary file name tmp <- tempfile () # Write the xts object using zoo to tmp write.zoo ( sunspots , sep = \",\" , file = tmp ) tmp \u2018/tmp/RtmptS40lc/file18ce2b6bba8\u2019 1 2 3 4 5 6 7 # Read the tmp file # FUN=as.yearmon will convert strings such as Jan 1749 into a proper time class sun <- read.zoo ( tmp , sep = \",\" , FUN = as.yearmon ) # Convert sun into xts sun_xts <- as.xts ( sun ) head ( sun_xts , 5 ) 1 2 3 4 5 6 [,1] jan 1749 58.0 f\u00e9v 1749 62.6 mar 1749 70.0 avr 1749 55.7 mai 1749 85.0 The ISO8601 Standard The ISO8601 standard is the internationally recognized and accepted way to represent dates and times. From the biggest to the smallest, left to right, YYYY-MM-DDTHH:MM:SS. The standard allows for a common format to not only describe dates, but also a way to represent ranges and repeating intervals: 2004 or 2001/2015 201402/03 2014-02-22 08:30:00 T08:00/T09:00 Querying for dates Date ranges can be extracted from xts objects by simply specifying the period(s) you want using special character strings in your subset. Code: 1 2 3 4 5 6 7 8 9 A[ \"201601\" ] ## Jan 2016 A[ \"20160125\" ] ## Jan 25, 2016 A[ \"201203/201212\" ] ## Feb to Dec 2012 # Select all of 2016 from x x_2016 <- x[ \"2016\" ] # Select January 2016 to March 22 jan_march <- x[ \"201601/20160322\" ] Extracting recurring intraday intervals Most common data \u201cin the wild\u201d is daily. On ocassion you may find yourself working with intraday data, that is date plus times. You can slice days easily by using special notation in the i = argument to the single bracket extraction (i.e. [i,j] ). Code: 1 2 3 4 5 6 7 8 # Intraday times for all days NYSE[ \"T09:30/T16:00\" ] # Extract all data between 8AM and 10AM morn_2010 <- irreg[ \"T8:00/T10:00\" ] # Extract the observations for January 13th morn_2010[ \"2010-01-13\" ] Alternating extraction techniques Often times you may need to subset an existing time series with a set of Dates, or time-based objects. These might be from as.Date() , or as.POSIXct() , or a variety of other classes. Code: 1 2 3 4 5 # Subset x using the vector dates x[dates] # Subset x using dates as POSIXct x [as.POSIXct ( dates ) ] Extracting recurring intraday intervals The most common time series data is daily, intraday data, which contains both dates and times. 1 2 3 4 5 # Extract all data between 8AM and 10AM morn_2010 <- irreg[ \"T8:00/T10:00\" ] # Extract the observations for January 13th morn_2010[ \"2010-01-13\" ] Row selection with time objects Subset an existing time series with a set of Dates, or time-based objects. 1 2 3 4 5 # Subset x using the vector dates x[dates] # Subset x using dates as POSIXct x [as.POSIXct ( dates ) ] Update and replace elements Replace known intervals or observations with an NA, say due to a malfunctioning sensor on a particular day or a set of outliers given a holiday. Code: 1 2 3 4 5 # Replace all values in x on dates with NA x[dates] <- NA # Replace dates from 2016-06-09 and on with 0 x[ \"20160609/\" ] <- 0 Find the first or last period of time Sometimes you need to locate data by relative time. Instead of using an absolute offset, you describe a position relative in time. A simple example would be something like the last 3 weeks of a series, or the first day of current month . Code: 1 2 3 4 5 6 7 8 # Create lastweek with using the last 1 week of temps lastweek <- last ( temps , \"1 week\" ) # Print the last 2 observations in lastweek last ( lastweek , 2 ) # Extract all but the last two days of lastweek last ( lastweek , \"-2 day\" ) Combining first and last Code: 1 2 3 4 5 # Last 3 days of first week last ( first ( Temps , '1 week' ), '3 days' ) # Extract the first three days of the second week of temps first ( last ( first ( temps , \"2 week\" ), \"1 week\" ), \"3 day\" ) Matrix arithmetic - add, subtract, multiply and divide in time! When you perform any binary operation using two xts objects, these objects are first aligned using the intersection of the indexes to preserve the point-in-time aspect of your data, assuring that you don\u2019t introduce accidental look ahead (or look behind!) bias into your calculations. Your options include: coredata() or as.numeric() (drop one to a matrix or vector). Manually shift index values - i.e. use lag(). Reindex your data (before or after the calculation). xts respects time and will only return the intersection of times when doing various mathematical operations. First option: 1 2 3 4 5 # Add a and b a + b # Add a with the numeric value of b a + as.numeric ( b ) Math with non-overlapping indexes This third way involves modifying the two series you want by aligning assuring you have some union of dates - the dates you require in your final output. This makes it possibly to preserve dimensionality of the data: 1 2 3 4 5 6 7 merge ( b , index ( a )) # Add a to b, and fill all missing rows of b with 0 a + merge ( b , index ( a ), fill = 0 ) # Add a to b and fill NAs with the last observation a + merge ( b , index ( a ), fill = na.locf ) Merging and modifying time series \u00b6 Combining xts by column with merge xts makes it easy to join data by column and row using a few different functions. xts objects must be of identical type (e.g. integer + integer), or be POSIXct dates vector, or be atomic vectors of the same type (e.g. numeric), or be a single NA. It does not work on data.frames with various column types. One of the most important functions to accomplish this is merge . It works like a cbind or and SQL join : inner join (and) outer join (or) left join right join 1 2 3 4 5 6 7 8 9 10 # Basic argument use merge ( a , b , join = \"right\" , fill = 9999 ) # Perform an inner join of a and b merge ( a , b , join = \"inner\" ) # Perform of a left-join of a and b, fill mising values with 0 merge ( a , b , join = \"left\" , fill = 0 ) # fill = na.locf, fill = NA Combining xts by row with rbind Easy to add new rows to your data. 1 2 3 4 5 # Row bind temps_june30 to temps, assign this to temps2 temps2 <- rbind ( temps , temps_june30 ) # Row bind temps_july17 and temps_july18 to temps2, call this temps3 temps3 <- rbind ( temps2 , temps_july17 , temps_july18 ) Fill missing values using last or previous observation The na.locf function takes the last-observation-carried-forward approach. The xts package leverages the power of zoo for help with this. zoo provides a variety of missing data handling functions which are usable by xts , and very handy. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 # Last obs. carried forward na.locf ( x ) # Next obs. carried backward na.locf ( x , fromLast = TRUE ) # Fill missing values using the last observation na.locf ( temps , na.rm = TRUE , fromLast = TRUE ) # Fill missing values using the next observation na.locf ( temps , na.rm = TRUE ) # maxgap = Inf # rule = 2, NA interpolation From zoo . On occassion, a simple carry forward approach to missingness isn\u2019t appropriate. It may be that a series is missing an observation due to a higher frequency sampling than the generating process. You might also encounter an observation that is in error, yet expected to be somewhere between the values of its neighboring observations. Both of these cases are examples of where interpolation is useful. 1 2 # Interpolate NAs using linear approximation na.approx ( AirPass ) Combine a leading and lagging time series Another common modification for time series is the ability to lag a series. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 # Your final object lag ( x , k = 1 , na.pad = TRUE ) # k = -1 leading # k = 1 lagging # zoo uses the other way around # k = c(0, 1, 5) multiple leads or lags # Create a leading object called lead_x lead_x <- lag ( x , k = -1 ) # Create a lagging object called lag_x lag_x <- lag ( x , k = 1 ) # Merge your three series together and assign to z z <- merge ( lead_x , x , lag_x ) Calculate a difference of a series with diff Another common operation on time series, typically on those that are non-stationary, is to take a difference of the series. differences is the order of the difference (how many times diff is called). A simple way to view a single (or first order ) difference is to see it as x(t) - x(t-k) where k is the number of lags to go back. Higher order differences are simply the reapplication of a difference to each prior result (like a second derivative or a difference of the difference). 1 2 3 4 5 6 7 diff ( x , lag = 1 , difference = 1 , log = FALSE , na.pad = TRUE ) # calculate the first difference of AirPass using lag and subtraction AirPass - lag ( AirPass , k = 1 ) # calculate the first order 12-month difference if AirPass diff ( AirPass , lag = 12 , differences = 1 ) What is the key difference in lag between xts and zoo? The k argument in zoo uses positive values for shifting past observations forward. Apply and aggregate by time \u00b6 Find intervals by time in xts The main function in xts to facilitate this is endpoints. It takes a time series (or a vector of times) and returns the locations of the last observations in each interval. For example, the code below locates the last observation of each year for the AirPass dataset: 1 2 3 4 5 6 7 8 9 10 11 endpoints ( AirPass , on = \"years\" ) # The argument on supports a variety of periods, including \"years\", \"quarters\", \"months\", as well as intraday intervals such as \"hours\", and \"minutes\" # on = \"weeks\", k = 2, the result would be the final day of every other week in your data # Locate the final day of every week endpoints ( temps , on = \"weeks\" ) # Locate the final day of every two weeks endpoints ( temps , on = \"weeks\" , k = 2 ) Apply a function by time period(s) xts provides period.apply , which takes a time series, an INDEX of endpoints, and a function to apply : 1 2 3 4 5 6 7 period.apply ( x , INDEX , FUN , ... ) # Calculate the weekly endpoints ep <- endpoints ( temps , on = \"weeks\" ) # Now calculate the weekly mean and display the results period.apply ( temps[ , \"Temp.Mean\" ] , INDEX = ep , FUN = mean ) Using lapply and split to apply functions on intervals Often it is useful to physically split your data into disjoint chunks by time and perform some calculation on these periods. 1 2 3 4 5 # Split temps by week temps_weekly <- split ( temps , f = \"weeks\" ) # Create a list of weekly means lapply ( X = temps_weekly , FUN = mean ) Selection by endpoints vs. split-lapply-rbind 1 2 3 4 5 6 7 # use the proper combination of split, lapply and rbind. #T1 <- do.call(rbind, ___(___(Temps, \"weeks\"), function(w) last(w, n=\"___\"))) T1 <- do.call ( rbind , lapply ( split ( Temps , \"weeks\" ), function ( w ) last ( w , n = \"1 day\" ))) # now subset Temps using the results of 'endpoints' last_day_of_weeks <- endpoints ( Temps , on = \"weeks\" ) T2 <- Temps[last_day_of_weeks] Convert univariate series to OHLC data In financial series it is common to find Open-High-Low-Close data calculated over some repeating and regular interval. Also known as range bars, aggregating a series based on some regular window can make analysis easier amongst series that have varying frequencies. For example, a weekly economic series and a daily stock series can be compared more easily if the daily is converted to weekly. OHLC means Opening, High, Low, Closing values. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 to.period ( x , period = \"months\" , k = 1 , indexAt , name = NULL , OHLC = TRUE , ... ) # Convert USDEUR to weekly # OHLC = FALSE by default USDEUR_weekly <- to.period ( USDEUR , period = \"weeks\" ) head ( USDEUR ) head ( USDEUR_weekly ) # Convert USDEUR_weekly to monthly USDEUR_monthly <- to.period ( USDEUR_weekly , period = \"months\" ) # Convert USDEUR_monthly to yearly univariate USDEUR_yearly <- to.period ( USDEUR_monthly , period = \"years\" , OHLC = FALSE ) Convert a series to a lower frequency Besides converting univariate time series to OHLC series, to.period() also lets you convert OHLC to lower regularized frequency - something like subsampling your data. For example, when using the shortcut function to.quarterly() xts will convert your index to the yearqtr class to make periods more obvious. 1 2 3 4 5 6 # Convert EqMktNeutral to quarterly OHLC mkt_quarterly <- to.period ( EqMktNeutral , period = \"quarters\" ) # Convert EqMktNeutral to quarterly using shortcut function # Change the base name of each OHLC column to EDHEC.Equity and change the index to \"firstof\" mkt_quarterly2 <- to.quarterly ( EqMktNeutral , name = \"EDHEC.Equity\" , indexAt = \"first\" ) Calculate basic rolling value of series by month Rolling windows can be discrete: use lapply with ( cumsum, cumprod, cummax, cummin ) and split : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 # split-lapply-rbind pattern x_split <- split ( x , f = \"months\" ) x_list <- lapply ( x_split , cummax ) x_list_rbind <- do.call ( rbind , x_list ) # Split edhec into years edhec_years <- split ( edhec , f = \"years\" ) # Use lapply to calculate the cumsum for each year edhec_ytd <- lapply ( edhec_years , FUN = cumsum ) # Use do.call to rbind the results edhec_xts <- do.call ( rbind , edhec_ytd ) Calculate the rolling standard deviation of a time series Rolling windows can be continuous: use rollapply : 1 2 3 4 rollapply ( x , 10 , FUN = max , na.rm = TRUE ) # Use rollapply to calculate the rolling 3 period sd of EqMktNeutral eq_monthly <- rollapply ( EqMktNeutral , 3 , FUN = sd ) Extra features of xts \u00b6 Index, attributes, and time zones All time is stored in seconds since 1970-01-01. Time via .index () () and index() (raw seconds since 1970-01-01). index() returns the time of an xts object, as a vector of class indexClass . Sys.setenv() . Class attributes - tclass , tzone and tformat xts objects are somewhat tricky when it comes to times. Internally, we have now seen that the index attribute is really a vector of numeric values corresponding to the seconds since the UNIX epoch (1970-01-01). How these values are displayed on printing and how they are returned to the user when using the index() function is dependent on a few key internal attributes. The information that controls this behavior can be viewed and even changed through a set of accessor functions detailed below: The index class using indexClass (e.g. from Date to chron) The time zone using indexTZ (e.g. from America/Chicago to Europe/London) The time format to be displayed via indexFormat (e.g. YYYY/MM/DD) 1 2 3 4 5 6 7 8 9 10 11 # Get the index class of temps indexClass ( temps ) # Get the timezone of temps indexTZ ( temps ) # Change the format of the time display indexFormat ( temps ) <- \"M/DD/YYYY\" # Extract the new index time ( temps ) Time zones - (and why you should care!) R provides time zone support in native classes POSIXct and POSIXlt . xts extends this power to the entire object, allowing you to have multiple timezones across various objects. One very important thing to remember is that some internal operation system functions require a timezone to do date math, and if not set one is chosen for you! Be careful to always set a timezone in your environment to prevent very hard to debug errors when working with dates and times. xts provides the function tzone . This function allows to you to extract, or set timezones. tzone ( x ) <- \"Time_Zone\" In this exercise you will work with an object called times to practice constructing your own xts objects with custom time zones. 1 2 3 4 5 6 7 8 # Construct times_xts with TZ set to America/Chicago times_xts <- xts ( 1 : 10 , order.by = times , tzone = \"America/Chicago\" ) # Change the time zone to Asia/Hong_Kong tzone ( times_xts ) <- \"Asia/Hong_Kong\" # Extract the current timezone indexTZ ( times_xts ) Periods, periodicity, and timestamps Periods (yearly or intradays time index?): periodicity() Broken downtime with .index * , .index () , .indexmday () , indexyday() , indexyear() . Timestamps. align.time() round time stamps to another period. make.index.unique() removes observations from duplicate time stamps. Determining periodicity The idea of periodicity is pretty simple; with what regularity does your data repeat? For stock market data, you might have hourly prices or maybe daily open-high-low-close bars. For macro economic series, it might be monthly or weekly survey numbers. 1 2 3 4 5 6 7 8 9 10 11 # Calculate the periodicity of temps periodicity ( temps ) # Calculate the periodicity of edhec periodicity ( edhec ) # Convert edhec to yearly edhec_yearly <- to.yearly ( edhec , 12 ) # Calculate the periodicity of edhec_yearly periodicity ( edhec_yearly ) Find the number of periods in your data Often times it is handy to know not just the range of your time series index, but also have an idea of how many discrete irregular periods this covers. xts provides a set of functions to do just that. If you have a time series, it is now easy to see how many days, weeks or years your data contains. Count: nseconds() , nminutes() , nhours() , etc. 1 2 3 4 5 6 7 8 ` # Count the months nmonths ( edhec ) # Count the quarters nquarters ( edhec ) # Count the years nyears ( edhec ) Secret index tools Normally you want to access the times you stored. index() does this magically for you by using your indexClass . To get to the raw vector another function is provided, .index () . Note the critical dot before the function name. More useful than extracting raw seconds is the ability to extract time components similar to the POSIXlt class, which mirrors closely the underlying POSIX internal compiled structure tm . This is provided by a handful of functions such as .indexday () , .indexmon () , .indexyear () and more. 1 2 3 4 5 6 7 8 9 # Explore underlying units of temps .index ( temps ) # in seconds .indexwday ( temps ) # (0 for Sun, 1, 2,..., 6 for Sat) day of the week # Create an index using which (Sunday has a value of 0, and Saturday has a value of 6) index <- which ( .indexwday ( temps ) == 0 | .indexwday ( temps ) == 6 ) # Select the index temps[index] Modifying timestamps Depending on your field you might encounter higher frequency data - think intraday trading intervals, or sensor data from medical equipment. If you find that you have observations with identical timestamps, it might be useful to perturb or remove these times to allow for uniqueness. On other ocassions you might find your timestamps a bit too precise. In these instances it might be better to round up to some fixed interval, for example an observation may occur at any point in an hour, but you want to record the latest as of beginning of the next hour. 1 2 3 4 5 6 7 8 9 10 11 12 make.index.unique ( x , eps = 1e-4 ) # Perturb make.index.unique ( x , drop = TRUE ) # Drop duplicates align.time ( x , n = 60 ) # Round to the minute # Make z have unique timestamps make.index.unique ( z , eps = 1e-4 ) # Remove duplicate times in z make.index.unique ( z , drop = TRUE ) # Round observations to the next time align.time ( z , n = 3600 ) # next hour","title":"Time Series in R, The Power of xts and zoo"},{"location":"time_series_in_r_the_power_of_xts_and_zoo/#introduction-to-extensible-time","text":"What is an xts object xts , a constructor or a subclass that inherits behavior from parents. xts (as a subclass) extends the popular zoo class (as a parent). Most zoo methods work for xts . xts is a matrix objects; subsets always preserve the matrix form. xts are indexed by a formal time object. You can time-stamp the data. The main xts constructor two most important arguments are x for the data and order.by for the index. x must be a vector or matrix. order.by is a vector of the same length or number of rows of x ; it must be a proper time or date object and it must be in increasing order. xts also allows you to bind arbitrary key-value attributes to your data. This lets you keep metadata about your object inside your object. 1 2 3 4 # load zoo library ( zoo ) # Load xts library ( xts ) # could be done alone; it will call zoo A first xts object 1 2 3 4 5 6 7 8 core <- matrix ( c ( rep ( 1 , 3 ), rep ( 2 , 3 )), nrow = 3 , ncol = 2 ) core idx <- as.Date ( c ( \"2016-06-01\" , \"2016-06-02\" , \"2016-06-03\" )) idx ex_matrix <- xts ( core , order.by = idx ) ex_matrix 1 2 1 2 1 2 2016-06-01 2016-06-02 2016-06-03 1 2 3 4 [,1] [,2] 2016-06-01 1 2 2016-06-02 1 2 2016-06-03 1 2 More than a matrix xts objects are normal R matrices, but with special powers. 1 2 # View the structure of ex_matrix str ( ex_matrix ) 1 2 3 4 5 An \u2018xts\u2019 object on 2016-06-01/2016-06-03 containing: Data: num [1:3, 1:2] 1 1 1 2 2 2 Indexed by objects of class: [Date] TZ: UTC xts Attributes: NULL 1 2 3 4 5 # Extract the 3rd observation of the 2nd column of ex_matrix ex_matrix[3 , 2 ] # Extract the 3rd observation of the 2nd column of core core[3 , 2 ] 1 2 [,1] 2016-06-03 2 2 Your first xts object Vector must be of the same length or number of rows as x and, this is very important: it must be a proper time or date object and it must be in increasing order. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 # Create the object data using 5 random numbers data <- rnorm ( 5 ) data # Create dates as a Date class object starting from 2016-01-01 dates <- seq ( as.Date ( \"2016-01-01\" ), length = 5 , by = \"days\" ) dates # Use xts() to create smith smith <- xts ( x = data , order.by = dates ) smith # Create bday (1899-05-08) using a POSIXct date class object bday <- as.POSIXct ( \"1899-05-08\" ) bday # Create hayek and add a new attribute called born hayek <- xts ( x = data , order.by = dates , born = bday ) hayek 0.226117402549514 -0.214261010302668 -0.199347621952038 0.133952199089896 0.96057628145059 2016-01-01 2016-01-02 2016-01-03 2016-01-04 2016-01-05 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 [,1] 2016-01-01 0.2261174 2016-01-02 -0.2142610 2016-01-03 -0.1993476 2016-01-04 0.1339522 2016-01-05 0.9605763 [1] \"1899-05-08 EST\" [,1] 2016-01-01 0.2261174 2016-01-02 -0.2142610 2016-01-03 -0.1993476 2016-01-04 0.1339522 2016-01-05 0.9605763 Deconstructing xts Separate a time series into its core data and index attributes for additional analysis and manipulation. Functions are methods from the zoo class. 1 2 3 4 5 6 7 8 9 10 11 # Extract the core data of hayek hayek_core <- coredata ( hayek ) # View the class of hayek_core class ( hayek_core ) # Extract the index of hayek hayek_index <- index ( hayek ) # View the class of hayek_index class ( hayek_index ) \u2018matrix\u2019 \u2018Date\u2019 Time-based indices xts objects get their power from the index attribute that holds the time dimension. One major difference between xts and most other time series objects in R is the ability to use any one of various classes that are used to represent time. Whether POSIXct , Date , or some other class, xts will convert this into an internal form to make subsetting as natural to the user as possible. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 # Create dates dates <- as.Date ( \"2016-01-01\" ) + 0 : 4 # Create ts_a ts_a <- xts ( x = 1 : 5 , order.by = dates ) # Create ts_b ts_b <- xts ( x = 1 : 5 , order.by = as.POSIXct ( dates )) # Extract the rows of ts_a using the index of ts_b ts_a [index ( ts_a ) ] # Extract the rows of ts_b using the index of ts_a ts_b [index ( ts_a ) ] 1 2 3 4 5 6 7 8 9 10 [,1] 2016-01-01 1 2016-01-02 2 2016-01-03 3 2016-01-04 4 2016-01-05 5 [,1]","title":"Introduction to eXtensible Time"},{"location":"time_series_in_r_the_power_of_xts_and_zoo/#first-order-of-business-basic-manipulations","text":"Converting xts objects xts provides methods to convert all of the major objects you are likely to come across - suitable native R types like matrix, data.frame, and ts are supported, as well as contributed ones such as timeSeries , fts and of course zoo . as.xts is the workhorse function to do the conversions to xts , and similar functions will provide the reverse behavior. 1 2 # using a R dataset head ( austres ) 13067.3 13130.5 13198.4 13254.2 13303.7 13353.9 1 2 3 4 5 6 7 8 9 10 11 # Convert austres to an xts object called au au <- as.xts ( austres ) head ( au , 5 ) # Convert your xts object (au) into a matrix am am <- as.matrix ( au ) head ( am , 5 ) # Convert the original austres into a matrix am2 am2 <- as.matrix ( austres ) head ( am2 , 5 ) 1 2 3 4 5 6 [,1] 1971 Q2 13067.3 1971 Q3 13130.5 1971 Q4 13198.4 1972 Q1 13254.2 1972 Q2 13303.7 1971 Q2 13067.3 1971 Q3 13130.5 1971 Q4 13198.4 1972 Q1 13254.2 1972 Q2 13303.7 13067.3 13130.5 13198.4 13254.2 13303.7 Importing data Read raw data from files on disk or the web and convert data to xts . Read the same data into a zoo object using read.zoo and convert the zoo into an xts object. 1 2 3 # Create dat by reading tmp_file dat <- as.matrix ( read.csv ( 'tmp_file.csv' , sep = ',' , header = TRUE )) dat a b 1/02/2015 1 3 2/03/2015 2 4 1 2 # Convert dat into xts xts ( dat , order.by = as.Date ( rownames ( dat ), \"%m%d%Y\" )) 1 2 3 a b &lt;NA&gt; 1 3 &lt;NA&gt; 2 4 1 2 3 # Read tmp_file using read.zoo and as.xts dat_xts <- as.xts ( read.zoo ( 'tmp_file.csv' , index.column = 0 , sep = \",\" , format = \"%m/%d/%Y\" )) dat_xts 1 2 3 a b 2015-01-02 1 3 2015-02-03 2 4 Exporting xts objects 1 2 3 4 head ( sunspots , 5 ) # Convert sunspots to xts sunspots_xts <- as.xts ( sunspots ) 58 62.6 70 55.7 85 1 2 3 4 5 6 7 # Get the temporary file name tmp <- tempfile () # Write the xts object using zoo to tmp write.zoo ( sunspots , sep = \",\" , file = tmp ) tmp \u2018/tmp/RtmptS40lc/file18ce2b6bba8\u2019 1 2 3 4 5 6 7 # Read the tmp file # FUN=as.yearmon will convert strings such as Jan 1749 into a proper time class sun <- read.zoo ( tmp , sep = \",\" , FUN = as.yearmon ) # Convert sun into xts sun_xts <- as.xts ( sun ) head ( sun_xts , 5 ) 1 2 3 4 5 6 [,1] jan 1749 58.0 f\u00e9v 1749 62.6 mar 1749 70.0 avr 1749 55.7 mai 1749 85.0 The ISO8601 Standard The ISO8601 standard is the internationally recognized and accepted way to represent dates and times. From the biggest to the smallest, left to right, YYYY-MM-DDTHH:MM:SS. The standard allows for a common format to not only describe dates, but also a way to represent ranges and repeating intervals: 2004 or 2001/2015 201402/03 2014-02-22 08:30:00 T08:00/T09:00 Querying for dates Date ranges can be extracted from xts objects by simply specifying the period(s) you want using special character strings in your subset. Code: 1 2 3 4 5 6 7 8 9 A[ \"201601\" ] ## Jan 2016 A[ \"20160125\" ] ## Jan 25, 2016 A[ \"201203/201212\" ] ## Feb to Dec 2012 # Select all of 2016 from x x_2016 <- x[ \"2016\" ] # Select January 2016 to March 22 jan_march <- x[ \"201601/20160322\" ] Extracting recurring intraday intervals Most common data \u201cin the wild\u201d is daily. On ocassion you may find yourself working with intraday data, that is date plus times. You can slice days easily by using special notation in the i = argument to the single bracket extraction (i.e. [i,j] ). Code: 1 2 3 4 5 6 7 8 # Intraday times for all days NYSE[ \"T09:30/T16:00\" ] # Extract all data between 8AM and 10AM morn_2010 <- irreg[ \"T8:00/T10:00\" ] # Extract the observations for January 13th morn_2010[ \"2010-01-13\" ] Alternating extraction techniques Often times you may need to subset an existing time series with a set of Dates, or time-based objects. These might be from as.Date() , or as.POSIXct() , or a variety of other classes. Code: 1 2 3 4 5 # Subset x using the vector dates x[dates] # Subset x using dates as POSIXct x [as.POSIXct ( dates ) ] Extracting recurring intraday intervals The most common time series data is daily, intraday data, which contains both dates and times. 1 2 3 4 5 # Extract all data between 8AM and 10AM morn_2010 <- irreg[ \"T8:00/T10:00\" ] # Extract the observations for January 13th morn_2010[ \"2010-01-13\" ] Row selection with time objects Subset an existing time series with a set of Dates, or time-based objects. 1 2 3 4 5 # Subset x using the vector dates x[dates] # Subset x using dates as POSIXct x [as.POSIXct ( dates ) ] Update and replace elements Replace known intervals or observations with an NA, say due to a malfunctioning sensor on a particular day or a set of outliers given a holiday. Code: 1 2 3 4 5 # Replace all values in x on dates with NA x[dates] <- NA # Replace dates from 2016-06-09 and on with 0 x[ \"20160609/\" ] <- 0 Find the first or last period of time Sometimes you need to locate data by relative time. Instead of using an absolute offset, you describe a position relative in time. A simple example would be something like the last 3 weeks of a series, or the first day of current month . Code: 1 2 3 4 5 6 7 8 # Create lastweek with using the last 1 week of temps lastweek <- last ( temps , \"1 week\" ) # Print the last 2 observations in lastweek last ( lastweek , 2 ) # Extract all but the last two days of lastweek last ( lastweek , \"-2 day\" ) Combining first and last Code: 1 2 3 4 5 # Last 3 days of first week last ( first ( Temps , '1 week' ), '3 days' ) # Extract the first three days of the second week of temps first ( last ( first ( temps , \"2 week\" ), \"1 week\" ), \"3 day\" ) Matrix arithmetic - add, subtract, multiply and divide in time! When you perform any binary operation using two xts objects, these objects are first aligned using the intersection of the indexes to preserve the point-in-time aspect of your data, assuring that you don\u2019t introduce accidental look ahead (or look behind!) bias into your calculations. Your options include: coredata() or as.numeric() (drop one to a matrix or vector). Manually shift index values - i.e. use lag(). Reindex your data (before or after the calculation). xts respects time and will only return the intersection of times when doing various mathematical operations. First option: 1 2 3 4 5 # Add a and b a + b # Add a with the numeric value of b a + as.numeric ( b ) Math with non-overlapping indexes This third way involves modifying the two series you want by aligning assuring you have some union of dates - the dates you require in your final output. This makes it possibly to preserve dimensionality of the data: 1 2 3 4 5 6 7 merge ( b , index ( a )) # Add a to b, and fill all missing rows of b with 0 a + merge ( b , index ( a ), fill = 0 ) # Add a to b and fill NAs with the last observation a + merge ( b , index ( a ), fill = na.locf )","title":"First Order of Business -- Basic Manipulations"},{"location":"time_series_in_r_the_power_of_xts_and_zoo/#merging-and-modifying-time-series","text":"Combining xts by column with merge xts makes it easy to join data by column and row using a few different functions. xts objects must be of identical type (e.g. integer + integer), or be POSIXct dates vector, or be atomic vectors of the same type (e.g. numeric), or be a single NA. It does not work on data.frames with various column types. One of the most important functions to accomplish this is merge . It works like a cbind or and SQL join : inner join (and) outer join (or) left join right join 1 2 3 4 5 6 7 8 9 10 # Basic argument use merge ( a , b , join = \"right\" , fill = 9999 ) # Perform an inner join of a and b merge ( a , b , join = \"inner\" ) # Perform of a left-join of a and b, fill mising values with 0 merge ( a , b , join = \"left\" , fill = 0 ) # fill = na.locf, fill = NA Combining xts by row with rbind Easy to add new rows to your data. 1 2 3 4 5 # Row bind temps_june30 to temps, assign this to temps2 temps2 <- rbind ( temps , temps_june30 ) # Row bind temps_july17 and temps_july18 to temps2, call this temps3 temps3 <- rbind ( temps2 , temps_july17 , temps_july18 ) Fill missing values using last or previous observation The na.locf function takes the last-observation-carried-forward approach. The xts package leverages the power of zoo for help with this. zoo provides a variety of missing data handling functions which are usable by xts , and very handy. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 # Last obs. carried forward na.locf ( x ) # Next obs. carried backward na.locf ( x , fromLast = TRUE ) # Fill missing values using the last observation na.locf ( temps , na.rm = TRUE , fromLast = TRUE ) # Fill missing values using the next observation na.locf ( temps , na.rm = TRUE ) # maxgap = Inf # rule = 2, NA interpolation From zoo . On occassion, a simple carry forward approach to missingness isn\u2019t appropriate. It may be that a series is missing an observation due to a higher frequency sampling than the generating process. You might also encounter an observation that is in error, yet expected to be somewhere between the values of its neighboring observations. Both of these cases are examples of where interpolation is useful. 1 2 # Interpolate NAs using linear approximation na.approx ( AirPass ) Combine a leading and lagging time series Another common modification for time series is the ability to lag a series. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 # Your final object lag ( x , k = 1 , na.pad = TRUE ) # k = -1 leading # k = 1 lagging # zoo uses the other way around # k = c(0, 1, 5) multiple leads or lags # Create a leading object called lead_x lead_x <- lag ( x , k = -1 ) # Create a lagging object called lag_x lag_x <- lag ( x , k = 1 ) # Merge your three series together and assign to z z <- merge ( lead_x , x , lag_x ) Calculate a difference of a series with diff Another common operation on time series, typically on those that are non-stationary, is to take a difference of the series. differences is the order of the difference (how many times diff is called). A simple way to view a single (or first order ) difference is to see it as x(t) - x(t-k) where k is the number of lags to go back. Higher order differences are simply the reapplication of a difference to each prior result (like a second derivative or a difference of the difference). 1 2 3 4 5 6 7 diff ( x , lag = 1 , difference = 1 , log = FALSE , na.pad = TRUE ) # calculate the first difference of AirPass using lag and subtraction AirPass - lag ( AirPass , k = 1 ) # calculate the first order 12-month difference if AirPass diff ( AirPass , lag = 12 , differences = 1 ) What is the key difference in lag between xts and zoo? The k argument in zoo uses positive values for shifting past observations forward.","title":"Merging and modifying time series"},{"location":"time_series_in_r_the_power_of_xts_and_zoo/#apply-and-aggregate-by-time","text":"Find intervals by time in xts The main function in xts to facilitate this is endpoints. It takes a time series (or a vector of times) and returns the locations of the last observations in each interval. For example, the code below locates the last observation of each year for the AirPass dataset: 1 2 3 4 5 6 7 8 9 10 11 endpoints ( AirPass , on = \"years\" ) # The argument on supports a variety of periods, including \"years\", \"quarters\", \"months\", as well as intraday intervals such as \"hours\", and \"minutes\" # on = \"weeks\", k = 2, the result would be the final day of every other week in your data # Locate the final day of every week endpoints ( temps , on = \"weeks\" ) # Locate the final day of every two weeks endpoints ( temps , on = \"weeks\" , k = 2 ) Apply a function by time period(s) xts provides period.apply , which takes a time series, an INDEX of endpoints, and a function to apply : 1 2 3 4 5 6 7 period.apply ( x , INDEX , FUN , ... ) # Calculate the weekly endpoints ep <- endpoints ( temps , on = \"weeks\" ) # Now calculate the weekly mean and display the results period.apply ( temps[ , \"Temp.Mean\" ] , INDEX = ep , FUN = mean ) Using lapply and split to apply functions on intervals Often it is useful to physically split your data into disjoint chunks by time and perform some calculation on these periods. 1 2 3 4 5 # Split temps by week temps_weekly <- split ( temps , f = \"weeks\" ) # Create a list of weekly means lapply ( X = temps_weekly , FUN = mean ) Selection by endpoints vs. split-lapply-rbind 1 2 3 4 5 6 7 # use the proper combination of split, lapply and rbind. #T1 <- do.call(rbind, ___(___(Temps, \"weeks\"), function(w) last(w, n=\"___\"))) T1 <- do.call ( rbind , lapply ( split ( Temps , \"weeks\" ), function ( w ) last ( w , n = \"1 day\" ))) # now subset Temps using the results of 'endpoints' last_day_of_weeks <- endpoints ( Temps , on = \"weeks\" ) T2 <- Temps[last_day_of_weeks] Convert univariate series to OHLC data In financial series it is common to find Open-High-Low-Close data calculated over some repeating and regular interval. Also known as range bars, aggregating a series based on some regular window can make analysis easier amongst series that have varying frequencies. For example, a weekly economic series and a daily stock series can be compared more easily if the daily is converted to weekly. OHLC means Opening, High, Low, Closing values. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 to.period ( x , period = \"months\" , k = 1 , indexAt , name = NULL , OHLC = TRUE , ... ) # Convert USDEUR to weekly # OHLC = FALSE by default USDEUR_weekly <- to.period ( USDEUR , period = \"weeks\" ) head ( USDEUR ) head ( USDEUR_weekly ) # Convert USDEUR_weekly to monthly USDEUR_monthly <- to.period ( USDEUR_weekly , period = \"months\" ) # Convert USDEUR_monthly to yearly univariate USDEUR_yearly <- to.period ( USDEUR_monthly , period = \"years\" , OHLC = FALSE ) Convert a series to a lower frequency Besides converting univariate time series to OHLC series, to.period() also lets you convert OHLC to lower regularized frequency - something like subsampling your data. For example, when using the shortcut function to.quarterly() xts will convert your index to the yearqtr class to make periods more obvious. 1 2 3 4 5 6 # Convert EqMktNeutral to quarterly OHLC mkt_quarterly <- to.period ( EqMktNeutral , period = \"quarters\" ) # Convert EqMktNeutral to quarterly using shortcut function # Change the base name of each OHLC column to EDHEC.Equity and change the index to \"firstof\" mkt_quarterly2 <- to.quarterly ( EqMktNeutral , name = \"EDHEC.Equity\" , indexAt = \"first\" ) Calculate basic rolling value of series by month Rolling windows can be discrete: use lapply with ( cumsum, cumprod, cummax, cummin ) and split : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 # split-lapply-rbind pattern x_split <- split ( x , f = \"months\" ) x_list <- lapply ( x_split , cummax ) x_list_rbind <- do.call ( rbind , x_list ) # Split edhec into years edhec_years <- split ( edhec , f = \"years\" ) # Use lapply to calculate the cumsum for each year edhec_ytd <- lapply ( edhec_years , FUN = cumsum ) # Use do.call to rbind the results edhec_xts <- do.call ( rbind , edhec_ytd ) Calculate the rolling standard deviation of a time series Rolling windows can be continuous: use rollapply : 1 2 3 4 rollapply ( x , 10 , FUN = max , na.rm = TRUE ) # Use rollapply to calculate the rolling 3 period sd of EqMktNeutral eq_monthly <- rollapply ( EqMktNeutral , 3 , FUN = sd )","title":"Apply and aggregate by time"},{"location":"time_series_in_r_the_power_of_xts_and_zoo/#extra-features-of-xts","text":"Index, attributes, and time zones All time is stored in seconds since 1970-01-01. Time via .index () () and index() (raw seconds since 1970-01-01). index() returns the time of an xts object, as a vector of class indexClass . Sys.setenv() . Class attributes - tclass , tzone and tformat xts objects are somewhat tricky when it comes to times. Internally, we have now seen that the index attribute is really a vector of numeric values corresponding to the seconds since the UNIX epoch (1970-01-01). How these values are displayed on printing and how they are returned to the user when using the index() function is dependent on a few key internal attributes. The information that controls this behavior can be viewed and even changed through a set of accessor functions detailed below: The index class using indexClass (e.g. from Date to chron) The time zone using indexTZ (e.g. from America/Chicago to Europe/London) The time format to be displayed via indexFormat (e.g. YYYY/MM/DD) 1 2 3 4 5 6 7 8 9 10 11 # Get the index class of temps indexClass ( temps ) # Get the timezone of temps indexTZ ( temps ) # Change the format of the time display indexFormat ( temps ) <- \"M/DD/YYYY\" # Extract the new index time ( temps ) Time zones - (and why you should care!) R provides time zone support in native classes POSIXct and POSIXlt . xts extends this power to the entire object, allowing you to have multiple timezones across various objects. One very important thing to remember is that some internal operation system functions require a timezone to do date math, and if not set one is chosen for you! Be careful to always set a timezone in your environment to prevent very hard to debug errors when working with dates and times. xts provides the function tzone . This function allows to you to extract, or set timezones. tzone ( x ) <- \"Time_Zone\" In this exercise you will work with an object called times to practice constructing your own xts objects with custom time zones. 1 2 3 4 5 6 7 8 # Construct times_xts with TZ set to America/Chicago times_xts <- xts ( 1 : 10 , order.by = times , tzone = \"America/Chicago\" ) # Change the time zone to Asia/Hong_Kong tzone ( times_xts ) <- \"Asia/Hong_Kong\" # Extract the current timezone indexTZ ( times_xts ) Periods, periodicity, and timestamps Periods (yearly or intradays time index?): periodicity() Broken downtime with .index * , .index () , .indexmday () , indexyday() , indexyear() . Timestamps. align.time() round time stamps to another period. make.index.unique() removes observations from duplicate time stamps. Determining periodicity The idea of periodicity is pretty simple; with what regularity does your data repeat? For stock market data, you might have hourly prices or maybe daily open-high-low-close bars. For macro economic series, it might be monthly or weekly survey numbers. 1 2 3 4 5 6 7 8 9 10 11 # Calculate the periodicity of temps periodicity ( temps ) # Calculate the periodicity of edhec periodicity ( edhec ) # Convert edhec to yearly edhec_yearly <- to.yearly ( edhec , 12 ) # Calculate the periodicity of edhec_yearly periodicity ( edhec_yearly ) Find the number of periods in your data Often times it is handy to know not just the range of your time series index, but also have an idea of how many discrete irregular periods this covers. xts provides a set of functions to do just that. If you have a time series, it is now easy to see how many days, weeks or years your data contains. Count: nseconds() , nminutes() , nhours() , etc. 1 2 3 4 5 6 7 8 ` # Count the months nmonths ( edhec ) # Count the quarters nquarters ( edhec ) # Count the years nyears ( edhec ) Secret index tools Normally you want to access the times you stored. index() does this magically for you by using your indexClass . To get to the raw vector another function is provided, .index () . Note the critical dot before the function name. More useful than extracting raw seconds is the ability to extract time components similar to the POSIXlt class, which mirrors closely the underlying POSIX internal compiled structure tm . This is provided by a handful of functions such as .indexday () , .indexmon () , .indexyear () and more. 1 2 3 4 5 6 7 8 9 # Explore underlying units of temps .index ( temps ) # in seconds .indexwday ( temps ) # (0 for Sun, 1, 2,..., 6 for Sat) day of the week # Create an index using which (Sunday has a value of 0, and Saturday has a value of 6) index <- which ( .indexwday ( temps ) == 0 | .indexwday ( temps ) == 6 ) # Select the index temps[index] Modifying timestamps Depending on your field you might encounter higher frequency data - think intraday trading intervals, or sensor data from medical equipment. If you find that you have observations with identical timestamps, it might be useful to perturb or remove these times to allow for uniqueness. On other ocassions you might find your timestamps a bit too precise. In these instances it might be better to round up to some fixed interval, for example an observation may occur at any point in an hour, but you want to record the latest as of beginning of the next hour. 1 2 3 4 5 6 7 8 9 10 11 12 make.index.unique ( x , eps = 1e-4 ) # Perturb make.index.unique ( x , drop = TRUE ) # Drop duplicates align.time ( x , n = 60 ) # Round to the minute # Make z have unique timestamps make.index.unique ( z , eps = 1e-4 ) # Remove duplicate times in z make.index.unique ( z , drop = TRUE ) # Round observations to the next time align.time ( z , n = 3600 ) # next hour","title":"Extra features of xts"},{"location":"visualization_cs/","text":"cartography \u00b6 cartography . PDF. ggmap \u00b6 ggmap . PDF. ggplot2 \u00b6 ggplot2 . PDF. ggvis \u00b6 ggvis . PDF. ggvis cont\u2019d . PDF. Leaflet \u00b6 Leaflet . PDF. plotly \u00b6 plotly . PDF. shiny \u00b6 shiny . PDF. Visualization Parameters \u00b6 Colors \u00b6 Graphs \u00b6 Graphs . PDF. Lines \u00b6 Points \u00b6","title":"Visualization Cheat Sheets"},{"location":"visualization_cs/#cartography","text":"cartography . PDF.","title":"cartography"},{"location":"visualization_cs/#ggmap","text":"ggmap . PDF.","title":"ggmap"},{"location":"visualization_cs/#ggplot2","text":"ggplot2 . PDF.","title":"ggplot2"},{"location":"visualization_cs/#ggvis","text":"ggvis . PDF. ggvis cont\u2019d . PDF.","title":"ggvis"},{"location":"visualization_cs/#leaflet","text":"Leaflet . PDF.","title":"Leaflet"},{"location":"visualization_cs/#plotly","text":"plotly . PDF.","title":"plotly"},{"location":"visualization_cs/#shiny","text":"shiny . PDF.","title":"shiny"},{"location":"visualization_cs/#visualization-parameters","text":"","title":"Visualization Parameters"},{"location":"visualization_cs/#colors","text":"","title":"Colors"},{"location":"visualization_cs/#graphs","text":"Graphs . PDF.","title":"Graphs"},{"location":"visualization_cs/#lines","text":"","title":"Lines"},{"location":"visualization_cs/#points","text":"","title":"Points"},{"location":"working_with_rstudio_ide/","text":"Foreword Output options: the \u2018tango\u2019 syntax and the \u2018readable\u2019 theme. Snippets and results. Help: Alt + Shift + K or Tools/Keyboard Shortcuts Help. Orientation \u00b6 Commands Ctrl + Up ; command history. Tab ; completion for all. Ctrl + L ; clear the console. Tools/Global Options\u2026 set up RStudio. view(dataframe) ; open a spreadsheet, show in new window, sort the rows, search, filter the rows. 1 2 df <- data.frame ( colA = c ( 1 , 2 ), colB = c ( 3 , 6 )) df 1 2 3 ## colA colB ## 1 1 3 ## 2 2 6 1 View ( df ) # in a new window IDE panes Environment pane; load, save, remove objects, read a dataset, import dataset. History pane; idem, clear all or one item at the time, copy the command in the console pane, reload a command with Shift + Enter and run it. File pane; working directories, files, add a new folder, rename getwd() ; get working directory. setwd() ; set working directory. Plot pane; save (extension, size, resolution). Package pane; update. Help pane; pages. Viewer pane; more than plots! Programming \u00b6 Scripting Ctrl + Shift + M ; %>% . Alt + - ; <- . Ctrl + Shift + C ; add/delete a # for commenting. 1 2 3 4 5 %>% <- # comment Code Ctrl + Alt + I ; new chunk. Convert into a function. Type code, Highlight it, Code/Extract Function to create a function: This: 1 rnorm ( 10 , 0 , 1 ) Becomes: 1 2 3 rnorm <- function () { rnorm ( 10 , 0 , 1 ) } Ctrl + Alt + click ; multiple cursors for typing! Switch between Default, Vim and Emacs modes with Tools/Global Options/Code/Keybindings. Shift + Alt + G ; go to line. Ctrl + F : find/replace in the current document. Alt + O , Alt + Shift + O ; fold/unfold the code. Ctrl + P ; jump between symbols like (), {}, []. Ctrl + Shift + Enter ; run and source the code. Ctrl + Enter ; run and source the code. Ctrl + Shift + F10 ; restart, refresh the R session. Error handling RStudio traces back the error origin. Toggle the show/hide traceback in the console when there is an error or rerun with the bug, watch the right pane for traceback. Investigate, highlight the next line of code, click in the traceback window, click the dropdown menu in the Global Environment (upper-right). Stop, continue in the debugger mode, press C , press Q . Add/remove breakpoints, where the line numbers are. debugonce ; automatically call the debugger when the function is called, but only once. debug , undebug ; automatically call the debugger when the function is called. Add options(error=browser) or options(error=NULL) at the beginning of the script; R automatically open the debugger mode. N ; next line. step-into icon. Shift + F4 ; step into. Shift + F6 ; execute the remainder of the bug. Project \u00b6 Create a project with a folder and all the files (Global Environment, History, etc.): New Directory, Existing Directory, Version Control\u2026 Empty Project, R Package (project), Shiny Web Application. Create a Git repository with the new project Commands Ctrl + Shift + F ; find in files. Ctrl + F9 , F10 ; go backwards/forwards. Packrat Packrat is a dependency management system for R. Use one version of a package for one project, another version of a package for another project. Associate a project with its own set of packages. Packrat . Activate Packrat when creating a project or Tools/Project Options/Packrat. The library is virtually separate from R library. Perfect for collaborating with GitHub Packages \u00b6 Introduction to R packages Anything that can be automated, should be automated. Do as little as possible by hand. Do as much as possible with functions. Process: Writing R functions. Documenting functions. Writing tests. Checking compatibility. Building the package for sharing and using. See the book: R Packages . Create a new R package Create a new directory with new files, folders, and meta-information. To update or not to update RStudio generates the NAMESPACE content for you automatically. See the book: R Packages . Import & load source files Move existing functions from an existing project into the new package. The function are moves to a new folder in the project. Test created functions all in once with the Load All command in the Build Tab. Simulations: Ctrl + Shift + L Ctrl + Shift + F10 ; restart a R session. Packages documentation Create a help page (.Rd file). Written in HTML. Use the roxigen package to document. Tools/Project Optons/Build Tools/Generate documentation. Generate the doc, add comments. First, create a doc skeleton above the function: Ctrl + Alt + Shift + R 1 2 3 4 5 6 7 8 9 10 11 #' Title #' #' @param x #' #' @return #' @export #' #' @examples cEnter <- function(x) { x - mean(x) } Fill in the blanks. Package documentation (2) Title of the help page. Text on how to use. The 4 tags ( @ ) help organizing the doc: @export (tells R that this function should be made available to people who load your package. ), @params , @return , and @examples . There are many more advanced tags. Highlight a section and test it, run it. Package documentation (3) Build Tab/build the package. Ctrl + Shift + D Compile all. Load all. Open the help page with ?function . Learn the roxigen workflow to ease the work. Test your package With the testthat (and devtools ) package. Install both packages. Run devtools::use_testthat() and a now directory with subdir appears in the package. This is where we save tests. Test your package (2) Ready. Write tests. Open a new R script and save it to the tests directory. Create a context function. Add test_that functions with arguments. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 context(\"cEnter\") test_that(\"cEnter handles integers\", { expect_equal(cEnter(1:3), -1:1) expect_equal(cEnter(-(1:3)), 1:-1) }) context(\"scale\") test_that(\"scale handles integers\", { expect_equal(scale(1:3), 1:3) expect_equal(scale(-(1:3)), -(1:3)) }) context(\"standardize\") test_that(\"standardize handles integers\", { expect_equal(standardize(1:3), -1:1) expect_equal(standardize(-(1:3)), 1:-1) }) Test your package (3) Run the tests. Ctrl + Shift + T Get a summary (pass or not pass). Test and retest the package. See the book: R Packages . Time to test your package Run the test with Build Tab/Test package. Check your package Upload the package to GitHub. Recreate the package structure in the repo. Download and install the package with install_github() from the devtools package. Test the package on GitHub to complete the tests. Type R CMD check in the terminal. Or Build Tab/Check icon. Ctrl + Shift + E Build your package A package is a tarball or package bundle. Build Tab/Build & Reload. Install and load the package. Ctrl + Shift + B Build and reload will overwrite the existing package. Run devtools::dev_mode() creates a separate library for development. Running the command again cancels it. Two formats: source package or binary package (more compresses and optimized). See the book: R Packages . Wrap-up Get the cheat sheet Version Control \u00b6 Introduction to Git Use Git to work in team. Even on R scripts, reports and packages. Install Git. In RStudio, Tools/Global Options/Git/SVN + Tools/Project Options/Select VCS, restart RStudio. RStudio has now a Git pane and a Git icon. Stage & commit The Git Tab is a directory. The real life (local) version of the project. The official version of the project as recorded with Git. The two versions are different. View the differences. When you commit, you add thing from the real to the official version. Green highlighting indicates something you\u2019ve added to the official version, while red highlighting indicates lines you have removed. .gitignore Inside the project, ther is a .gignore file. Add file that are excluded from the offcial version. The file can be accessed from the Git pane. Git icons Add, (cancel), commit, (cancel). The icons will changed. Commit history History viewer: each commitment. HEAD commit and parent commit. Master branch or another. Undo commited changes: checkout Checkout command. Go back to a previous commit. The commit stays in the history (not deleted). Run it in the shell: Tools/shell. git checkout sha# nameofthefile and Git will reverse the commit. The file is in the stage area as before the commit. Undo uncommited changes The file is in the stage area, ready to commit. Cancel the addition. In the Change window, click on the Revert button. Or, click on Discard chunk. Or, Ctrl + Z to undo. Then, Save the file. Introduction to GitHub Centralize, host, track issues, track metrics. Install a R package From GitHub with install_github from the the devtools packages. Pull & Push Local and GitHub Wrap-up Go to help.github.com Go to stackoverflow.com Reporting \u00b6 Tools for reporting R Markdown and Shiny (over the web). Introduction to R Markdown Text and code. Web link: <http://www> R Markdown in RStudio Report (HTML, PDF, Word) or presentation(slide). Create a template. Load in new templates. The rticles package has templates for academic journals. The outline (name the chunks). Help/Markdown Quick Reference Add a code chunk wih Ctrl + Alt + I Rendering R Markdown Knit or preview. For pdf, need \\LaTeX \\LaTeX (install). Adding runtime : shiny generates a Shiny report. Launch the interactive app. Publish reports online. Compile notebook File/Compile Notebook for R script. The script becomes a report (Word, PDF, HTML). RStudio\u2019s \\LaTeX \\LaTeX editor Install \\LaTeX \\LaTeX . Open a .tex file in RStudio. RStudio has limited options to edit \\LaTeX \\LaTeX ; enough to write and compile. The preview window is linked to the source window. If we click on a character in the source window, press Ctrl + click : the corresponding character is highlighted in the preview window. Vice-versa. Tools/Global options/Sweave to change the \\LaTeX \\LaTeX options. Shiny The server is online. shiny.rstudio.com When we create a Shiny app, we create two files: ui.R and server.R Run App (the app) or Ctrl + Shift + Enter Publish Shiny apps Need an account and the shinyapps package. devtools::install_github(\"rstudio/shinyapps\") Deploy the local app online. Unique URL. Monitor usage, view logs, archive or delete the app. Max 5 apps at a time for free. Paid account.","title":"Working with the RStudio IDE"},{"location":"working_with_rstudio_ide/#orientation","text":"Commands Ctrl + Up ; command history. Tab ; completion for all. Ctrl + L ; clear the console. Tools/Global Options\u2026 set up RStudio. view(dataframe) ; open a spreadsheet, show in new window, sort the rows, search, filter the rows. 1 2 df <- data.frame ( colA = c ( 1 , 2 ), colB = c ( 3 , 6 )) df 1 2 3 ## colA colB ## 1 1 3 ## 2 2 6 1 View ( df ) # in a new window IDE panes Environment pane; load, save, remove objects, read a dataset, import dataset. History pane; idem, clear all or one item at the time, copy the command in the console pane, reload a command with Shift + Enter and run it. File pane; working directories, files, add a new folder, rename getwd() ; get working directory. setwd() ; set working directory. Plot pane; save (extension, size, resolution). Package pane; update. Help pane; pages. Viewer pane; more than plots!","title":"Orientation"},{"location":"working_with_rstudio_ide/#programming","text":"Scripting Ctrl + Shift + M ; %>% . Alt + - ; <- . Ctrl + Shift + C ; add/delete a # for commenting. 1 2 3 4 5 %>% <- # comment Code Ctrl + Alt + I ; new chunk. Convert into a function. Type code, Highlight it, Code/Extract Function to create a function: This: 1 rnorm ( 10 , 0 , 1 ) Becomes: 1 2 3 rnorm <- function () { rnorm ( 10 , 0 , 1 ) } Ctrl + Alt + click ; multiple cursors for typing! Switch between Default, Vim and Emacs modes with Tools/Global Options/Code/Keybindings. Shift + Alt + G ; go to line. Ctrl + F : find/replace in the current document. Alt + O , Alt + Shift + O ; fold/unfold the code. Ctrl + P ; jump between symbols like (), {}, []. Ctrl + Shift + Enter ; run and source the code. Ctrl + Enter ; run and source the code. Ctrl + Shift + F10 ; restart, refresh the R session. Error handling RStudio traces back the error origin. Toggle the show/hide traceback in the console when there is an error or rerun with the bug, watch the right pane for traceback. Investigate, highlight the next line of code, click in the traceback window, click the dropdown menu in the Global Environment (upper-right). Stop, continue in the debugger mode, press C , press Q . Add/remove breakpoints, where the line numbers are. debugonce ; automatically call the debugger when the function is called, but only once. debug , undebug ; automatically call the debugger when the function is called. Add options(error=browser) or options(error=NULL) at the beginning of the script; R automatically open the debugger mode. N ; next line. step-into icon. Shift + F4 ; step into. Shift + F6 ; execute the remainder of the bug.","title":"Programming"},{"location":"working_with_rstudio_ide/#project","text":"Create a project with a folder and all the files (Global Environment, History, etc.): New Directory, Existing Directory, Version Control\u2026 Empty Project, R Package (project), Shiny Web Application. Create a Git repository with the new project Commands Ctrl + Shift + F ; find in files. Ctrl + F9 , F10 ; go backwards/forwards. Packrat Packrat is a dependency management system for R. Use one version of a package for one project, another version of a package for another project. Associate a project with its own set of packages. Packrat . Activate Packrat when creating a project or Tools/Project Options/Packrat. The library is virtually separate from R library. Perfect for collaborating with GitHub","title":"Project"},{"location":"working_with_rstudio_ide/#packages","text":"Introduction to R packages Anything that can be automated, should be automated. Do as little as possible by hand. Do as much as possible with functions. Process: Writing R functions. Documenting functions. Writing tests. Checking compatibility. Building the package for sharing and using. See the book: R Packages . Create a new R package Create a new directory with new files, folders, and meta-information. To update or not to update RStudio generates the NAMESPACE content for you automatically. See the book: R Packages . Import & load source files Move existing functions from an existing project into the new package. The function are moves to a new folder in the project. Test created functions all in once with the Load All command in the Build Tab. Simulations: Ctrl + Shift + L Ctrl + Shift + F10 ; restart a R session. Packages documentation Create a help page (.Rd file). Written in HTML. Use the roxigen package to document. Tools/Project Optons/Build Tools/Generate documentation. Generate the doc, add comments. First, create a doc skeleton above the function: Ctrl + Alt + Shift + R 1 2 3 4 5 6 7 8 9 10 11 #' Title #' #' @param x #' #' @return #' @export #' #' @examples cEnter <- function(x) { x - mean(x) } Fill in the blanks. Package documentation (2) Title of the help page. Text on how to use. The 4 tags ( @ ) help organizing the doc: @export (tells R that this function should be made available to people who load your package. ), @params , @return , and @examples . There are many more advanced tags. Highlight a section and test it, run it. Package documentation (3) Build Tab/build the package. Ctrl + Shift + D Compile all. Load all. Open the help page with ?function . Learn the roxigen workflow to ease the work. Test your package With the testthat (and devtools ) package. Install both packages. Run devtools::use_testthat() and a now directory with subdir appears in the package. This is where we save tests. Test your package (2) Ready. Write tests. Open a new R script and save it to the tests directory. Create a context function. Add test_that functions with arguments. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 context(\"cEnter\") test_that(\"cEnter handles integers\", { expect_equal(cEnter(1:3), -1:1) expect_equal(cEnter(-(1:3)), 1:-1) }) context(\"scale\") test_that(\"scale handles integers\", { expect_equal(scale(1:3), 1:3) expect_equal(scale(-(1:3)), -(1:3)) }) context(\"standardize\") test_that(\"standardize handles integers\", { expect_equal(standardize(1:3), -1:1) expect_equal(standardize(-(1:3)), 1:-1) }) Test your package (3) Run the tests. Ctrl + Shift + T Get a summary (pass or not pass). Test and retest the package. See the book: R Packages . Time to test your package Run the test with Build Tab/Test package. Check your package Upload the package to GitHub. Recreate the package structure in the repo. Download and install the package with install_github() from the devtools package. Test the package on GitHub to complete the tests. Type R CMD check in the terminal. Or Build Tab/Check icon. Ctrl + Shift + E Build your package A package is a tarball or package bundle. Build Tab/Build & Reload. Install and load the package. Ctrl + Shift + B Build and reload will overwrite the existing package. Run devtools::dev_mode() creates a separate library for development. Running the command again cancels it. Two formats: source package or binary package (more compresses and optimized). See the book: R Packages . Wrap-up Get the cheat sheet","title":"Packages"},{"location":"working_with_rstudio_ide/#version-control","text":"Introduction to Git Use Git to work in team. Even on R scripts, reports and packages. Install Git. In RStudio, Tools/Global Options/Git/SVN + Tools/Project Options/Select VCS, restart RStudio. RStudio has now a Git pane and a Git icon. Stage & commit The Git Tab is a directory. The real life (local) version of the project. The official version of the project as recorded with Git. The two versions are different. View the differences. When you commit, you add thing from the real to the official version. Green highlighting indicates something you\u2019ve added to the official version, while red highlighting indicates lines you have removed. .gitignore Inside the project, ther is a .gignore file. Add file that are excluded from the offcial version. The file can be accessed from the Git pane. Git icons Add, (cancel), commit, (cancel). The icons will changed. Commit history History viewer: each commitment. HEAD commit and parent commit. Master branch or another. Undo commited changes: checkout Checkout command. Go back to a previous commit. The commit stays in the history (not deleted). Run it in the shell: Tools/shell. git checkout sha# nameofthefile and Git will reverse the commit. The file is in the stage area as before the commit. Undo uncommited changes The file is in the stage area, ready to commit. Cancel the addition. In the Change window, click on the Revert button. Or, click on Discard chunk. Or, Ctrl + Z to undo. Then, Save the file. Introduction to GitHub Centralize, host, track issues, track metrics. Install a R package From GitHub with install_github from the the devtools packages. Pull & Push Local and GitHub Wrap-up Go to help.github.com Go to stackoverflow.com","title":"Version Control"},{"location":"working_with_rstudio_ide/#reporting","text":"Tools for reporting R Markdown and Shiny (over the web). Introduction to R Markdown Text and code. Web link: <http://www> R Markdown in RStudio Report (HTML, PDF, Word) or presentation(slide). Create a template. Load in new templates. The rticles package has templates for academic journals. The outline (name the chunks). Help/Markdown Quick Reference Add a code chunk wih Ctrl + Alt + I Rendering R Markdown Knit or preview. For pdf, need \\LaTeX \\LaTeX (install). Adding runtime : shiny generates a Shiny report. Launch the interactive app. Publish reports online. Compile notebook File/Compile Notebook for R script. The script becomes a report (Word, PDF, HTML). RStudio\u2019s \\LaTeX \\LaTeX editor Install \\LaTeX \\LaTeX . Open a .tex file in RStudio. RStudio has limited options to edit \\LaTeX \\LaTeX ; enough to write and compile. The preview window is linked to the source window. If we click on a character in the source window, press Ctrl + click : the corresponding character is highlighted in the preview window. Vice-versa. Tools/Global options/Sweave to change the \\LaTeX \\LaTeX options. Shiny The server is online. shiny.rstudio.com When we create a Shiny app, we create two files: ui.R and server.R Run App (the app) or Ctrl + Shift + Enter Publish Shiny apps Need an account and the shinyapps package. devtools::install_github(\"rstudio/shinyapps\") Deploy the local app online. Unique URL. Monitor usage, view logs, archive or delete the app. Max 5 apps at a time for free. Paid account.","title":"Reporting"}]}